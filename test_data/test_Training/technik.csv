"In der Nähe von Pittsburgh im US-Bundesstaat Pennsylvania ist ein Model X des Elektroautoherstellers Tesla von der Straße abgekommen und überschlug sich. Verletzt wurde niemand, doch der Fahrer des Wagens gab an, der Autopilot sei zum Zeitpunkt des Crashs angeschaltet gewesen. Wie genau es zu dem Unfall kam, untersucht jetzt die Verkehrssicherheitsbehörde (NHTSA). Dabei geht es vor allen Dingen um die Frage, ob der Autopilot eingeschaltet war oder nicht. 
Nach eigenen Angaben sei die Behörde dabei, Informationen der Polizei, des Unternehmens und des Fahrers des Unfallwagens zusammenzutragen. Eine Sprecherin von Tesla sagte, es sei dem Unternehmen derzeit noch unklar, ob der Autopilot eingeschaltet gewesen war. Dies könne daran liegen, dass die Antennen des Autopiloten bei dem Unfall beschädigt worden seien könnten. Das Unternehmen habe bereits dreimal vergeblich versucht, mit dem Fahrer des Wagens Kontakt aufzunehmen, sagte die Sprecherin. 
In der vergangenen Woche hatte Tesla den ersten tödlichen Unfall mit einem Wagen des Modell S eingeräumt. Der Unfall wurde nach Unternehmensangaben durch eine falsche Einschätzung des Autopiloten verursacht. Das Auto war in einen Lastzug gefahren, der vor dem Wagen die Straße kreuzte. Das System habe den querenden Lastwagenanhänger, unter den die Limousine Model S fuhr, für ein hochhängendes Straßenschild gehalten, zitierte das Blog Electrec den Autokonzern.  ""Bei diesem Unfall führte die hohe weiße Seitenwand des Anhängers zusammen mit einer Radar-Signatur, die der eines hochhängenden Straßenschilds sehr ähnlich war, dazu, dass keine automatische Bremsung ausgelöst wurde "", schrieb demnach Tesla. Behörden hatten vermutet, dass das System den weißen Anhänger vor dem weißen Himmel nicht habe erkennen können. 
Bei dem in den Unfall in Pennsylvania involvierten Modell X handelt es sich um einen Geländewagen des Herstellers, der erst seit Kurzem im Handel ist. Sein Autopilotsystem hatte Tesla im vergangenen Jahr vorgestellt. Das System ist in der Lage, die Spur zu wechseln, die Geschwindigkeit zu verändern und die Bremse auszulösen. Das autonome Fahrsystem kann aber vom Fahrer überstimmt werden. Fahrer müssen daher auch bei eingeschaltetem Autopiloten jederzeit die Hände am Lenker halten."	technik
"Fabelhaft seien die Fahrzeuge: schnell wie Porsche, zuverlässig wie Mercedes und mit der Verarbeitungsqualität von Rolls-Royce. BMW sparte in der amerikanischen Anzeigenkampagne nicht mit vollmundigen Versprechen, als das BMW Sports Coupé vor 50 Jahren auf den Markt kam. Doch auch die US-Fachpresse würdigte die  ""Bavarian Motor Works "" wenig später als eine der fünf besten Marken der Welt. Die deutschen Medien waren ebenso begeistert vom BMW 2000 CS, den sie als  ""formvollendeten Zweitürer "" feierten, für alle, die  ""sicher und schnell fahren "" wollten. 
Allein der exorbitant hohe Preis von 17.000 Mark verwunderte sehr und verhinderte einen Absatz in hoher Stückzahl. Gab es für dieses Geld doch lediglich einen zwei Liter großen Vierzylinder, während fast alle Wettbewerber dieser Preisklasse mit weit mächtigeren Motoren aufwarteten. Nur der Porsche 912 verfügte ebenfalls über einen kleinen Vierzylinder, punktete dafür aber mit dem optischen Prestige des Elfers. 
Andererseits war es schon immer etwas teurer als bei anderen Marken gewesen, ein BMW Coupé zu fahren. Dies hatte zuletzt der unmittelbare Vorgänger der Neuen Klasse, der 3200 CS V8, gezeigt. Der 2000 C wie der 2000 CS waren mondän gezeichnet – doch der Erfolg des neuen Designkonzepts zeigte sich erst beim 1968 präsentierten Nachfolger, dem 2800 CS. Das neue BMW-Prestigecoupé wurde mit verschiedenen Motoren bis 1975 mehr als 30.000 mal verkauft. Dagegen entschieden sich für den bis 1970 angebotenen 2000 C/CS nur knapp 14.000 Käufer. 
Für die Rolle eines Imageträgers genügte das aber. Schließlich zählten zu den Bewunderern und Käufern zahlreiche Prominente aus Showgeschäft und Sport. Etwa der damalige deutsche Fußballnationalspieler Berti Vogts, der sich für einen 2000 CS entschied, und der brasilianische Fußball-Superstar Pelé. Der besuchte 1968 die BMW-Werke besuchte und lobte die Coupés als  ""schnelle Stürmer unter den Automobilen "". Auch die Filmstars Peter Ustinov und Joachim Fuchsberger zeigten sich als Fans der BMW-Zweitürer. 
Die Formgebung des Hardtop-Coupés mit seiner flachen Gürtellinie und dem lichten Dachpavillon erinnerte an Gran-Turismo-Modelle italienischer Marken. Innen dagegen zeigten die feinen Rundinstrumente, das filigrane Lenkrad, die schweren Clubsessel mit Chromverzierung und die warmen Edelhölzer einen Stil, den sonst britischer Autoadel à la Aston Martin trug. 
Ein Coupé von Welt also, bei dem nur praktische Accessoires deutlich an die bodenständige bajuwarische Abstammung des BMW erinnerten. So erwähnte die erste Pressemappe zum 2000 CS, dass auch an einen als  ""Brotzeitkasterl bezeichneten Behälter "" auf dem Mitteltunnel gedacht wurde. Schließlich sollte der 2000 CS kein abgehobener Traumwagen wie zuletzt das Coupé 3200 CS sein, sondern  ""die Wiedergeburt des BMW 327 "". Und wie dieser legendäre Vorkriegstyp Harmonie schaffen zwischen  ""Fahrleistung eines rassigen Sportwagens und dem Fahrkomfort eines luxuriösen Reisewagens "". 
 
Tatsächlich zählte der 2000 CS zu den schnellsten Autos mit Vierzylindermotor. Das sicherte dem BMW in Amerika dann auch die Rolle eines Road-Stars, der beim Prestige zeitweise sogar englischen Ikonen den Rang ablief. Das wiederum gelang dem 2000 CS in Großbritannien nie. Dort fanden nur 148 rechtsgelenkte Coupés Käufer. Die störte es offenkundig nicht, dass sogar Supersportler wie der Jaguar E-Type preiswerter zu haben waren als das bayerische Modell. 
Stolz war BMW auch auf das automatische Getriebe. Der 2000 C war der erste BMW überhaupt, der mit Vollautomatik ausgestattet wurde. Und dann bot der von ZF zugelieferte Automat mit drei Gängen sogar eine Fahrstufe mehr als die verbreiteten US-Automatiksysteme. Zudem waren die Beschleunigungszeiten laut BMW für den  ""Normalfahrer mit denen eines handgeschalteten Getriebes mindestens gleich, wenn nicht besser "". 
Auch Journalisten äußerten sich positiv über den Automaten. Nur bei bergigen Strecken konnte er nicht mit der Agilität der manuellen Vierganggetriebe mithalten. Von europäischen Käufern wurde er dennoch selten geordert. Vielleicht, weil er schlicht zu teuer war. Verlangte BMW doch für den nur 100 PS leistenden 2000 C mit Automatik einen noch höheren Preis als für den 120 PS starken, wenn auch handgeschalteten 2000 CS. 
1965: Am 19. Juni erste Pressevorstellung des 2000 CS; Publikumsweltpremiere der neuen BMW-Coupés als 2000 C mit Automatik und 2000 CS auf der Frankfurter IAA. Start der Vorserienproduktion am 22. November bei Karmann. 
1966: BMW 2000 CS mit 88 kW (120 PS) starkem Doppelvergaser-Motor und 2000 C mit ZF-Getriebeautomatik sowie 74 kW (100 PS) leistendem Motor gehen in Großserie. Die Coupés erleben ihr erfolgreichstes Jahr mit über 7.000 verkauften Einheiten, verfehlen aber eine Prognose aus dem Vorjahr, die noch von 8.000 Stück ausging. 
1967: Der BMW 2000 C mit 100-PS-Motorisierung ist nun auch mit manuellem Getriebe lieferbar. BMW feiert die Auslieferung des 10.000sten Coupés der neuen Klasse. Preise für 2000 C ab 16.950 Mark, für 2000 CS ab 17.500 Mark, für 2000 C Automatik ab 17.750 Mark 
1968: Produktionsende für BMW 2000 C mit Schaltgetriebe. Im September Vorstellung und Produktionsstart der neuen Coupé-Baureihe E9 mit dem BMW 2800 CS als erstem Typ. Der E9 orientiert sich am Design des BMW 2000 CS, differenziert sich aber durch Doppelscheinwerfer und einen längeren Vorderwagen. 
1970: Am 11. Juli verlässt die letzte Rohkarosserie für BMW 2000 C mit Automatik und BMW 2000 CS das Karmann-Werk. Der 2800 CS ersetzt die BMW 2000 C/CS. Zwischen 1965 und 1970 entstehen insgesamt 443 BMW 2000 C, 3.249 BMW 2000 CA (Automatik) und 9.999 BMW 2000 CS. 
Länge: 4,53 Meter Breite: 1,68 Meter Höhe: 1,36 Meter Radstand: 2,55 Meter Leergewicht: 1.200 Kilogramm (CA: 1.220 Kilogramm) 
BMW 2000 C/CA Motorbauart: 2,0-Liter-Vierzylinder-Benzinmotor Leistung: 74 kW (100 PS) Höchstgeschwindigkeit: 172 km/h (CA: 168 km/h) Normverbrauch: 10,7 Liter je 100 km 
BMW 2000 CS Motorbauart: 2,0-Liter-Vierzylinder-Benzinmotor Leistung: 88 kW (120 PS) Höchstgeschwindigkeit: 185 km/h Normverbrauch: 10,9 Liter je 100 km 
Als Meilenstein der Designgeschichte wurde von den meisten Kritikern die Couture des Coupés gefeiert. BMW-Chefdesigner Wilhelm Hofmeister hatte unter anderem mit trapezförmigen Plastikabdeckungen über den Scheinwerfern einzigartige stilistische Akzente gesetzt. Gleichwohl hielt er sich an die Vorgabe, eine deutliche Verwandtschaft zu den Limousinen der Neuen Klasse erkennen zu lassen. Möglich machte dies die auf Höhe der Gürtellinie umlaufende Zierleiste. 
Ansonsten brach Hofmeister mit den üblichen Grundsätzen der Coupéentwicklung. Bis dahin mussten bei von Limousinen abgeleiteten Coupés meist kostengünstige Detailveränderungen genügen: ein verkürzter Radstand, der Wegfall der B-Säule und eine stärker geneigte C-Säule. BMW dagegen wählte den Weg einer komplett neuen Formfindung. An der hatten sich vor Hofmeister bereits namhafte Karossiers vergeblich versucht. Giovanni Michelotti etwa hatte einen Entwurf präsentiert, der von BMW ebenso abgelehnt wurde wie ein Coupé, das die Karosseriefirma Baur vorgestellt hatte. 
Aus der Rückblende eine weitsichtige Entscheidung. So waren die 2000 C/CS keiner kurzlebigen coupétypischen Mode unterworfen, sondern von zeitloser Eleganz. Das bewiesen die optisch nur sanft weiterentwickelten und bis Ende 1975 gebauten 2800 CS/3.0 CS. Nachfolger wurde der erste BMW 6er."	technik
"Nach dem ersten tödlichen Unfall mit einem vom Computer gesteuerten Auto hat der Hersteller Tesla Stellung genommen. Das System habe den querenden Lastwagen-Anhänger, unter den die Limousine Model S fuhr, für ein hochhängendes Straßenschild gehalten, zitierte das Blog Electrec den Autokonzern. 
 ""Bei diesem Unfall führte die hohe weiße Seitenwand des Anhängers zusammen mit einer Radar-Signatur, die der eines hochhängenden Straßenschilds sehr ähnlich war, dazu, dass keine automatische Bremsung ausgelöst wurde "", schrieb demnach Tesla. Behörden hatten vermutet, dass das System den weißen Anhänger vor dem weißen Himmel nicht habe erkennen können. 
Der Unfall hatte sich bereits im März ereignet, war aber erst diese Woche durch eine Untersuchung der amerikanischen Verkehrsaufsicht NHTSA öffentlich bekannt geworden. Der Fahrer kam ums Leben, als die Unterkante des Anhängers die Windschutzscheibe traf. Der Tesla fuhr danach laut Polizeiangaben zunächst weiter, kam von der Fahrbahn ab und traf einen Strommast. 
Tesla hatte stets darauf hingewiesen, dass der Fahrmodus  ""Autopilot "" seine Fahrzeuge nicht zu einem selbstfahrenden Wagen mache. Er sei vielmehr ein Fahrassistenz-System, bei dem die Fahrer stets den Verkehr im Blick behalten sollten. Dennoch gibt es im Netz seit der Einführung des Systems im vergangenen Herbst zahlreiche Videos, auf denen Fahrer sich mit anderen Dingen beschäftigen, während der Autopilot die Wagen steuert. 
Zudem gibt es eine Debatte, ob die Autopilot-Technologie von Tesla überhaupt schon querfahrenden Verkehr erkennen könne. Einer der Zulieferer von Tesla, der Spezialist für Selbstfahrtechnologie Mobileye, stellte das infrage. Die aktuellen Systeme von Mobileye seien für Verkehrssituationen wie bei dem Crash nicht ausgelegt. Die Technologie solle querende Fahrzeuge erst ab 2018 erkennen, teilte ein Sprecher mit. Mobileye hatte am Freitag eine groß angelegte Kooperation mit BMW bekannt gegeben. 
Tesla stellte daraufhin klar, dass man sich nicht nur auf die Mobileye-Technologie verlasse. Das Autopilot-System greife vielmehr auf eine Kombination von Technik verschiedener Anbieter zurück, um Hindernisse auf der Fahrbahn zu erkennen."	technik
"Die meisten Leute finden, selbstfahrende Autos sollen sich moralisch verhalten – und notfalls auch das Leben des Fahrers opfern, wenn sich dadurch weitere Tote verhindern lassen. Kaufen würden sie aber lieber ein Fahrzeug, das ihr eigenes Leben über das aller anderen stellt. Das geht aus einer neuen Studie hervor, die im Fachmagazin Science veröffentlicht wurde. 
Dieses Dilemma könnte dem Erfolg der neuen Technik nachhaltig schaden. Die Studienergebnisse befeuern die Debatte um das autonome Fahren, die durch den tödlichen Unfall eines Tesla Model S, der bei eingeschaltetem Autopiloten mit einem Lkw kollidierte, gerade ohnehin verstärkt geführt wird. 
Forscher der Universität Toulouse, des Massachusetts Institute of Technology (MIT) und der Universität von Kalifornien hatten knapp 2.000 Probanden in einer Onlineumfrage vor moralisch schwierige Fragen im Zusammenhang mit autonom fahrenden Fahrzeugen gestellt. Zum Beispiel musste das wohl klassischste Dilemma-Szenario für Roboterautos bewertet werden: Ein autonomes Fahrzeug rast auf eine Menschenmenge zu. Entweder fährt es mitten hinein und tötet die Passanten. Oder es weicht aus, prallt gegen eine Barriere und tötet seinen Insassen. 
Für ein Großteil der Befragten war klar, wie ein Auto in solch einer Situation programmiert sein sollte: Rund drei Viertel würden lieber den einen Mensch opfern als die ganze Menschenmenge. Das nahezu gleiche Ergebnis gab es, als die Befragten annehmen sollten, sie selbst oder Mitglieder ihrer Familie säßen im Fahrzeug. Der Großteil der Probanden wählte also eine rationale Lösung, orientierte sich an der Zahl der Opfer und versuchte, diese möglichst gering zu halten. 
Das zeigt sich auch an der Antwort auf die Frage, ob das Leben des Fahrers auch für lediglich einen Passanten geopfert werden sollte. Diese beantworteten nur 23 Prozent der Befragten mit Ja. 
Doch Einsicht und die Einigung auf ein gesellschaftlich erwünschtes Handeln schlagen sich nicht zwangsläufig im Handeln des Einzelnen nieder. Denn als die Forscher den Probanden die Frage nach den eigenen Fahrzeugwünschen stellten, war die Antwort ähnlich eindeutig – allerdings in die komplett andere Richtung. Lediglich 19 Prozent würden das rational abwägende Auto wählen; 50 Prozent wollen das Auto, das unter allen Umständen ihr eigenes Leben schützen würde. Auch wenn die Befragten also zustimmen, dass autonome Autos bei einem Unfall so viele Leben wie möglich retten sollten: Für sich selbst würden sie solch ein Fahrzeug nicht wollen. 
Derartige soziale Dilemmata sind nicht selten. Auch wenn die Gesellschaft als Ganzes von moralisch richtig entscheidenden Autos profitieren würde, wählt das Individuum den Weg, der ihm die größten Chancen ermöglicht. In diesem Fall die Chance, einen Unfall zu überleben. 
Die Mehrheit der Befragten spricht sich angesichts dessen generell gegen eine Regulierung des Entscheidungsalgorithmus im autonomen Auto aus, sei es durch Regierung oder Fahrzeughersteller. Bislang ist noch nicht entschieden, wer künftigen autonomen Fahrzeugen ihre Entscheidungslogik einpflanzt. Diese Frage zählt zu den zahlreichen juristischen, technischen und gesellschaftlichen offenen Punkten, die vor dem endgültigen Start des autonomen Fahrens noch zwischen Industrie, Politik und Kundschaft geklärt werden müssen. 
Ethische Fragen haben dabei durchaus große Relevanz, auch wenn sie auf den ersten Blick konstruiert erscheinen mögen. Die Wissenschaftler geben zu, dass autonome Autos wohl extrem selten vor derartige Entscheidungen auf Leben und Tod gestellt werden. Allerdings steige die Wahrscheinlichkeit eines derartigen Falles mit der Zahl der Fahrzeuge. Und die soll langfristig weltweit in die zig Millionen gehen. 
Die Forscher ziehen aus ihrer Studie einen wichtigen Schluss: Eine Regulierung der Entscheidungsfindung ist möglicherweise aus gesellschaftlicher Sicht notwendig, dürfte den Erfolg des autonomen Autos jedoch verzögern. Die potenzielle Kundschaft wird wohl misstrauisch auf Fahrzeuge reagieren, die ihr Leben im Zweifelsfall absichtsvoll opfern würden. Eine Verzögerung auf dem Weg zum autonomen Fahren wiederum würde ebenfalls Menschenleben kosten: Denn solange es nicht autonom fahrende Autos gibt, wird auch die Zahl der Verkehrstoten hoch bleiben."	technik
"Mein neues Betriebssystem Ubuntu bringt den Firefox-Browser mit, den ich sofort mit einigen der wichtigsten Add-ons ausstatte: Zum Blockieren von Tracking-Diensten eignen sich Ghostery, Do Not Track Plus oder NoScript. Und HTTPS Everywhere von der Electronic Frontier Foundation (EFF) versucht, immer die HTTPS-Version einer Website aufzurufen. Wer mehr dazu wissen will, findet ausführliche Beschreibungen im Privacy-Handbuch. 
Doch um wirklich anonym surfen zu können, so dass weder mein Internetprovider noch die Zielseite weiß, wer was aufruft, brauche ich mehr als ein paar Browser-Erweiterungen. Tor ist dafür das bekannteste Hilfsmittel. Die kostenlose Open-Source-Software dient dazu, die eigene IP-Adresse zu verschleiern, indem sie Anfragen nicht direkt an die Zieladresse im Netz schickt, sondern über eine Kette von Proxyservern leitet. Jeder Proxy kennt nur seinen Vorgänger und Nachfolger, aber keiner kennt den ursprünglichen Absender der Anfrage und gleichzeitig den Empfänger. 
Bei Wikipedia wird es etwas ausführlicher erklärt, hier soll es vor allem um die Installation und Bedienung des Tor Browser Bundles gehen. Das enthält einen modifizierten Firefox-Browser, den sogenannten TorButton sowie das Vidalia genannte Kontrollpanel. 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
Die Installation 
Auf torproject.org gibt es eine Download- und Installationsanleitung für Windows, OS X und Linux. Die für mein Linux-System ist simpel. Ich suche mir die für mein Betriebssystem passende Version der Installationsdatei auf der Projektseite und lade sie herunter. Das Download-Paket öffne ich über einen Rechtsklick und das Feld Mit Archivverwaltung öffnen. Im neuen Fenster klicke ich mit der rechten Maustaste auf den Tor-Browser-Ordner und wähle Entpacken. Nun kann ich festlegen, wo ich meinen Tor-Ordner ablege, nach dem erfolgreichen Entpacken klicke ich auf Schließen. Im Tor-Ordner muss ich von nun an nur noch die Datei start-tor-browser doppelklicken und Ausführen wählen. Kurz darauf öffnen sich das Kontrollpanel und der Browser. Wer die folgende Meldung nicht zu sehen bekommt, muss es noch einmal versuchen. 
Bei mir klappt es beim ersten Versuch. Die Macher des Tor-Projekts empfehlen übrigens noch eine Reihe von Add-ons, um den Browser zu verfeinern. Ich installiere nur eines von ihnen, nämlich das bekannte, wenn auch umstrittene AdBlock Plus, um unerwünschte Werbung zu stoppen. 
Es bietet sich außerdem an, den Ordner mit den Tor-Dateien auf den Desktop zu ziehen, um das Programm beim nächsten Mal schnell wiederfinden und mit einem Doppelklick auf Start Tor Browser nutzen zu können. 
Die Installation auf einem Mac oder in Windows ist auch nicht komplizierter, sie wird hier erklärt. Mehr als ein paar Klicks sind nicht nötig. 
Der Alltag mit dem Tor Browser Bundle 
Die Installation des Tor Browser Bundles ist wahrlich keine Hexerei, aber wer mit Tor surft, muss mit Einschränkungen leben. Ich steuere www.zeit.de an und stelle fest: Der Seitenaufbau ist langsam, aber gerade noch erträglich. Es ist zwar möglich, über Vidalia einen leistungsstarken Server als sogenannten Exit Node auszuwählen, also den Server, über den man letztlich auf die Zielseite zugreift. Dadurch kann man die Surfgeschwindigkeit erhöhen. Doch die Konfiguration ist für Anfänger zu kompliziert. Die beste Anleitung, die ich bislang gefunden habe, ist noch die von Martin Brinkmann von ghacks.net. 
Nicht nur die Geschwindigkeit ist gewöhnungsbedürftig. Auch die Schrift sieht anders aus als gewohnt. Das liegt an den Voreinstellungen des Bündels, die man besser auch nicht ändert, weil die Wahrung der Anonymität sonst gefährdet ist. So sind zum Beispiel die Schriftarten im Tor-Browser andere als die im normalen Firefox. Das Flash-Plugin für den Firefox ist deaktiviert – was bedeutet, dass Flash-Inhalte nicht mehr angezeigt werden können. YouTube, Vimeo und die Brightcove-Videos auf zeit.de kann ich mir also nicht ansehen. 
Eine Liste der besuchten Websites wird im Tor-Browser auch nicht angelegt, eingetragene Formulardaten werden nicht gespeichert, alle Cookies werden nach Beendigung der Tor-Sitzung gelöscht. Wer Tor dann neu startet, muss Webadressen und Formulareinträge komplett neu eingeben und wird von einer Website nicht wiedererkannt. 
Das alles mag unpraktisch sein, weil es das Surfen langsamer und umständlicher macht. Es ist aber der Preis, den man für konsequente Anonymität zahlen muss. Zudem gehört auch noch ein gewisses Verhalten dazu: Wer mit Tor surft und sich dann in seinen normalen E-Mail-Account oder bei Facebook oder bei seiner Bank einloggt, verrät zwar nicht seinem Internetprovider, was er gerade im Netz tut – aber natürlich dem E-Mail-Provider, Facebook oder der Bank. Die Anonymität ist dann für die ganze Sitzung dahin, weil der Nutzer zumindest an einer Stelle seine derzeitige IP-Adresse verrät, mit der er auch alle anderen Seiten aufruft. 
Tor ist nützlich, wenn ich sensible Informationen im Netz suchen will. Wem da nur Pornos als Beispiel einfallen, der hat wenig Fantasie. Es geht nämlich auch niemanden etwas an, wenn ich Artikel über Krankheiten oder Medikamente lese, oder über Geldfragen und Urlaubsziele. Und wenn ich für einen Artikel recherchiere, möchte ich auch die Möglichkeit haben, dies unbeobachtet tun zu können. 
Wenn ich aber doch YouTube nutzen will, brauche ich eine Alternative zu Tor. Ein VPN (Virtual Private Network) ist eine solche Alternative. Mehr dazu im nächsten Kapitel. 
 
Die Zugabe: ein Tor-Relais betreiben 
Um dem Tor-Projekt zu helfen, bitten die Entwickler darum, ein Relais einzurichten, eine Weiterleitung. Damit wird das System insgesamt schneller und kann von mehr Menschen genutzt werden. Viel mehr als ein paar Klicks im Vidalia-Kontrollpanel sind dazu theoretisch nicht nötig. Ich wähle Relais-Verkehr im Tor-Netzwerk (kein Ausgangsrelais) aus. Denn ein Ausgangsrelais (Exit Node) zu betreiben, kann zu Problemen führen:  ""Der Betreiber einer Exit-Node als solcher ist nicht anonym. Dadurch kann es vorkommen, dass, wenn jemand anderes Unsinn über Tor macht, die Polizei dann bei einem nachfragt beziehungsweise man beschuldigt wird, dass man selbst die Straftat begangen hat. Auch Beschlagnahmung von Tor-Servern gab es bereits in Deutschland "", heißt es im Ubuntu-Wiki. Das will ich natürlich vermeiden, gleichzeitig möchte ich das Projekt aber unterstützen. Deshalb mache ich meinen Rechner nur zu einem Mittelrelais. 
Ich trage bei den allgemeinen Einstellungen noch meine E-Mail-Adresse ein, lasse alles andere aber unverändert. Unter dem Reiter Bandbreitenbegrenzung wähle ich 768 Kilobit pro Sekunde, da ich zu Hause nicht übermäßig viel Bandbreite zur Verfügung habe. 
Leider bekomme ich zunächst Fehlermeldungen im Vidalia-Logbuch. Das dürfte am Router liegen, heißt es in den Erläuterungen auf torproject.org. Der blockiert solche Weiterleitungen unter Umständen. Ich rufe den technischen Support meines Providers an, der sich zwar nicht mit Tor auskennt, aber mir erklärt, wie ich eine sogenannte Portweiterleitung einrichten kann. Das bedeutet grob gesagt, ich definiere einen bestimmten Punkt, an dem Tor auf die Datenpakete anderer Tor-Nutzer wartet und diese dann weiterleitet. Ich erlaube es anderen also, meine Bandbreite für das Surfen über Tor mitzunutzen. Wie das gemacht wird, hängt vom jeweiligen Router ab, deshalb ist eine Anleitung an dieser Stelle wenig sinnvoll. Wer es nachmachen möchte und sich damit nicht auskennt, sollte auf jeden Fall seinen Provider fragen. 
Ich gebe also die beiden Ports frei, die Vidalia mir unter Weiterleitung einrichten anzeigt und starte den Router neu. Und tatsächlich klappt das, auch wenn ich nur ungefähr verstehe, was ich da eigentlich getan habe. Im Logbuch steht nun jedenfalls, der Selbsttest habe ergeben, dass meine Ports von außen erreichbar sind. Mit anderen Worten: Ich bin nun Teil des Tor-Netzwerks, wenn auch nur ein winzig kleiner."	technik
"Am vergangenen Mittwoch begannen die Olympischen Sommerspiele 2016 in Rio. Nichts davon mitbekommen? Das liegt am speziellen Kalender des Internationalen Olympischen Komitees, kurz IOC: Die Funktionäre haben den offiziellen Start der Spiele auf den 27. Juli datiert, neun Tage vor der eigentlichen Eröffnungsfeier. 
Für die Sportler bedeutet der vorgezogene Startschuss: Von nun an stehen sie unter Beobachtung – noch nicht im Wettkampf, dafür jedoch auf Facebook und Twitter. Jedes falsche Bild, jeder arglos gewählte Hashtag kann in den nächsten Wochen zum Problem werden, vielleicht sogar den Traum von der Goldmedaille zerstören. 
Klingt irre? Ist es auch. 
Grund sind die strikten Werbeverbote, die das IOC den Olympioniken und ihren Sponsoren diktiert. Verboten ist laut Olympischer Charta, Regel 40:  ""Kein Wettkampfteilnehmer, Trainer, Betreuer oder Funktionär darf seine Person, seinen Namen, sein Bild oder seine sportliche Leistung für Werbezwecke während der Olympischen Spiele einsetzen, außer dies wurde vom IOC genehmigt. "" 
Das Problem für Sportler und Sponsoren: Für den IOC stellt schon jeder Tweet, in dem ein Sprinter seinen Schuhhersteller erwähnt, eine unerlaubte Werbung dar. Das Komitee greift damit tief in die Social-Media-Aktivitäten der Athleten ein. Penibel ist in seitenlangen Leitfäden geregelt, wer was wie im Internet posten darf. Und vor allem was nicht. 
Unternehmen ist es demnach strikt verboten, sich selbst in irgendeiner Weise mit der Olympiateilnahme der von ihnen gesponserten Athleten in Verbindung zu bringen. Konkret heißt das: Hashtags wie #Olympia oder #Olympisch dürfen von Unternehmen während der Spiele grundsätzlich nicht genutzt werden. Steht laut IOC ein Tweet in  ""inhaltlichem Bezug zu Olympia "", umfasst die Verbotsliste sogar vermeintlich unverfängliche Hashtags wie #Sommer, #Sieg oder #Performance. 
Stellenweise lesen sich die Vorgaben des Komitees zu Social-Media-Posts der Sportler wie eine Anleitung zum Spieleklassiker Tabu, bei dem Spieler Begriffe erklären und dabei offensichtliche Assoziationen vermeiden müssen. Sonst gibt es Punktabzug. 
 
Sportler müssen zudem ihre Websites und öffentlichen Facebook-Profile während der Spiele quasi selbst zensieren. Sponsorenlogos dürfen dort in keinerlei Zusammenhang zu Olympia gebracht werden; dafür reicht beispielsweise schon ein Wettkampfbild aus, auf dem ein Trikotsponsor zu erkennen ist. 
Während der Olympischen Winterspiele in Sotschi 2014 nahm daher der norwegische Biathlet Ole Einar Bjørndalen seine komplette Homepage offline. Anders, so erklärte er, könne er nicht sicher sein, nicht gegen das Werbeverbot zu verstoßen. Zu groß war seine Angst vor Sanktionen. 
Die umstrittene Regel 40 des IOC verbietet Unternehmen auch das Posten alter Olympiabilder und die Weiterverbreitung fremder Tweets, selbst wenn sie keinerlei Verbindung zu den Sportlern haben. In einer Broschüre des Deutschen Olympischen Sportbunds (DOSB) für die Sportler heißt es dazu:  ""Nicht-olympische Sponsoren dürfen keinesfalls Social-Media-Inhalte mit olympischem Bezug von IOC/RIO2016/DOSB/Deutsche Olympiamannschaft retweeten oder teilen. "" 
Zu den  ""olympischen Ausdrücken und Symbolen, die von nicht-olympischen Sponsoren nicht verwendet werden dürfen "", zählt der DOSB unter anderem auch die Jahreszahl 2016 sowie die Begriffe Rio und Rio de Janeiro, Spiele und Podest. Thomas Stadler, Fachanwalt für IT-Recht und Gewerblichen Rechtsschutz, bezweifelt aber, dass alle diese Begriffe juristisch als olympische Bezeichnung betrachtet werden können. In seinem Blog weist Stadler darauf hin, dass im Olympiaschutzgesetz, der rechtlichen Grundlage für die Verbote in Deutschland, lediglich die Wörter Olympiade, Olympia und olympisch als offizielle Bezeichnungen und nur die Olympischen Ringe als offizielles Symbol genannt sind. 
 ""Es ist also nicht davon auszugehen "", schreibt Stadler,  ""dass das IOC oder der DOSB – jedenfalls nach dem Maßstab des deutschen und europäischen Rechts – die Benutzung des Hashtags 'Rio2016' erfolgreich verbieten kann, auch nicht solchen Unternehmen, die keine offiziellen Sponsoren sind. "" 
Das IOC versucht es trotzdem und schafft damit eine Zweiklassengesellschaft unter den Unternehmen. Denn für die offiziellen Olympiasponsoren – darunter etwa Coca-Cola oder Samsung – gelten die Verbote ausdrücklich nicht. Im Klartext: Gewinnt ein von McDonalds gesponserter Athlet eine Goldmedaille, darf das Unternehmen den Sieg im Netz bejubeln. Ist der Sponsor jedoch eine andere Fastfoodkette, muss der Erfolg quasi totgeschwiegen werden. Kein Tweet, kein Hashtag, keine Pressemitteilung. 
Für die Sportler, gerade aus der zweiten und dritten Reihe, wird das zum gigantischen Problem. Sie können sich oftmals nur mithilfe ihrer Sponsoren finanziell über Wasser halten. Doch was bringt dem Sponsor ein Athlet, über den er während dem größten Sportereignis der Welt nicht sprechen darf? Nicht einmal die Stadt Berlin dürfte per Twitter ihrem Diskuswerfer Robert Harting zu einem möglichen Titel gratulieren. Für das IOC fällt auch das unter unzulässige Werbung. Die Stadt ist zwar kein Sponsor von Harting, aber das IOC denkt, Berlin könne sich wie jeder andere Ort auch mit seinen Athleten schmücken und das als Tourismuswerbung nutzen. 
Dutzende Olympioniken, darunter die amerikanische Hürdenläuferin Emma Coburn, erwähnten daher am vergangenen Mittwochabend ein letztes Mal ihre Sponsoren – kurz vor der Verbotsfrist.  ""Morgen tritt Regel 40 in Kraft "", twitterte Coburn,  ""dann werde ich mich nicht mehr bei meinem Sponsor bedanken können "". Dahinter verlinkte sie den Schuhhersteller New Balance. 
Nicht nur der Jurist Stadler sieht die Auslegung von Regel 40 der Olympischen Charta kritisch.  ""Ich finde das alles albern und höchstgefährlich, wenn private Akteure dermaßen die öffentliche Kommunikation kontrollieren wollen "", schreibt etwa der Grünen-Politiker Malte Spitz in seinem Blog. Deutsche Athleten schweigen sich zu dem Thema bislang aus. 
Der DOSB sieht keinen Grund, wieso etwa die Hashtag-Verbote juristisch angreifbar sein sollten.  ""Wenn Sie an sportlichen Wettkämpfen teilnehmen, halten Sie sich an die entsprechenden Spielregeln, die dokumentiert sind "", teilt ein Sprecher mit. Das gelte auf und neben dem Platz.  ""Die Athletinnen und Athleten sollen sich auf sportliche Höchstleistungen konzentrieren und sich nicht durch Werbung und Kommerzialisierung ablenken lassen. "" Wenn es um offizielle Olympiasponsoren geht, scheint die Ablenkung allerdings kein Problem zu sein. 
Das strikte Werbeverbot des IOC stammt noch aus Zeiten, als bei Olympia vor allem Amateursportler gegeneinander antraten. Dass es damals noch keine sozialen Medien gab, stört die Funktionäre heute nicht. Das Verbot wird einfach beliebig ausgeweitet. 2012 verteidigte der damalige IOC-Präsident Jacque Rogge die neue Onlinerichtlinien mit den Worten:  ""Wir müssen unsere Sponsoren schützen. Ohne Sponsoren gibt es keine Olympischen Spiele. "" Als würde ein Hashtag das millionenschwere Engagement von Adidas infrage stellen. 
Für die Spiele von Rio konnten Athleten erstmals eine Ausnahmegenehmigung beantragen. Die erlaubt es ihren Sponsoren, auch während Olympia mit den Sportlern zu werben. Voraussetzung: Die Werbekampagne muss bereits mehrere Monate laufen, jeder kleinste Bezug auf Olympia ist streng verboten. Wie viele solcher Erlaubnisse für deutsche Sportler erteilt wurden, darüber schweigt sich der DOSB aus. Auf Anfrage von ZEIT ONLINE hieß es lediglich, dass zahlreiche Anträge gestellt wurden, ein Großteil davon erfolgreich. 
So ist auch die Frage nach den Konsequenzen der Regelung weiter offen. Was passiert, wenn ein Sportler oder sein Sponsor tatsächlich einen verbotenen Hashtag nutzen, ein unerlaubtes Bild veröffentlichen? Gibt es eine Geldstrafe? Gar einen Ausschluss von den Spielen? 
Es ist eine absurde Vorstellung: Ein Sportler gewinnt Gold im Stadion – und verliert es anschließend wieder im Internet."	technik
"Tesla-Chef Elon Musk lässt den Fahrassistenten, der in den ersten tödlichen Unfall mit einem vom Computer gesteuerten Auto verwickelt war, inmitten der Ermittlungen von US-Behörden weiter in Betrieb. Den Fahrern solle in einem Blogeintrag aber besser erklärt werden, wie der Fahrassistent funktioniere und wie man sich verhalten solle, sagte Musk dem Wall Street Journal.  ""Viele Leute verstehen nicht, was es ist und wie man es einschaltet. "" Tesla habe das System so schnell wie möglich auf den Markt bringen wollen,  ""weil wir wussten, dass es unterm Strich Leben retten wird "". 
Unterdessen wurde ein weiterer Unfall mit einem Tesla öffentlich. Im US-Bundesstaat Montana in der Nähe von Whitehall kam ein Model X von der Straße ab und fuhr gegen einen Begrenzungspfeiler. Fahrer und Beifahrer blieben laut der Detroit Free Press unverletzt. Der Polizei sagte der Fahrer, das Fahrassistenzsystem sei zum Zeitpunkt des Unfalls eingeschaltet gewesen. 
Die US-Verkehrsbehörde NHTSA forderte bei Tesla unterdessen eine Vielzahl von Informationen zum Autopilotassistenten an. Unter anderem will sie eine Beschreibung des Systems, Daten zu Modifikationen, Testergebnisse und alle Beschwerden und Berichte zu Unfällen. 
Tesla soll der Behörde auch Kopien aller Logdaten aus dem tödlichen Unfall in Florida Anfang Mai aushändigen und erklären, zu welchem Zeitpunkt das System die Kollisionsgefahr hätte erkennen müssen. Bei dem tödlichen Unfall Anfang Mai raste ein Tesla mit eingeschaltetem Autopilotsystem unter einen Lastwagenanhänger, der die Straße querte. Tesla zufolge hielt die Software die weiße Seite des Anhängers für ein hoch hängendes Autobahnschild. Ein Sprecher der NHTSA sagte der Nachrichtenagentur Bloomberg, die Anfrage sei ein üblicher Schritt bei Ermittlungen und bedeute keine Vorentscheidung. 
Das Autopilotsystem kann vor allem Spur, Tempo und Abstand zum vorderen Fahrzeug halten. Tesla betonte stets, es mache seine Elektrowagen nicht zu selbstfahrenden Autos. Viele Fahrer überlassen ihm jedoch weitgehend die Kontrolle, wie in diversen Videos im Internet zu sehen ist. 
Ein Tesla-Manager, der nicht namentlich genannt werden wollte, sagte der New York Times, das Autopilotsystem sei sicher, aber Nutzer müssten verstehen, dass es  ""den Unterschied zwischen Leben und Tod bedeuten kann "", wenn man es falsch nutze. 
Tesla kann die Software seiner Autos per Datenfunk aktualisieren. So wurde im vergangenen Oktober die Autopilotfunktion aufgespielt."	technik
"Selten hat der Dieselmotor so prominent in der Öffentlichkeit gestanden. Die vermeintlich so sauberen TDI-Clean-Selbstzünder von Volkswagen stoßen in Wirklichkeit deutlich mehr Stickoxide aus als vom Gesetz erlaubt, wie amerikanische Behörden herausfanden. Gerade VW, dessen im Zuge des Skandals zurückgetretener Chef Martin Winterkorn noch vor wenigen Jahren tönte, Volkswagen werde 2020 nicht nur der größte, sondern auch der umweltfreundlichste Autohersteller der Welt sein. 
Stickoxide (NOx) reizen die Atemwege, sind verantwortlich für den sauren Regen und stehen im Verdacht, Krebs zu erregen. Mit dem Abgas haben Motoringenieure ihre liebe Not. Bei der Verbrennung des Diesels entstehen prinzipbedingt viele NOx-Verbindungen. Je heißer der Prozess abläuft, umso mehr. Daran lässt sich wenig machen, weil Dieselmotoren mit höherer Verdichtung als Ottomotoren laufen und zudem mit sehr hohem Luftüberschuss arbeiten. Dieser begünstigt zusammen mit den hohen Temperaturen bei der Verbrennung die chemische Reaktion der in der Luft vorhandenen Gase Stickstoff (N) und Sauerstoff (O). 
Wirklich eliminiert werden können die Stickoxide daher nur durch eine aufwendige Nachbehandlung im Abgasstrang. Schon heute gleicht das, was zwischen Motor und Auspuffendrohr liegt, einer kleinen Chemiefabrik, die viel Geld kostet. Im modernen Diesel-Auspuffstrang sitzen eine Abgasrückführung, ein Oxidationskatalysator und ein Partikelfilter für Rußteilchen. Darauf folgt ein Stickoxid-Speicherkatalysator, auch Lean NOx Trap genannt. Hier lagern sich die Stickoxide an, ehe sie in regelmäßigen Abständen durch das Einspritzen von zusätzlichem Kraftstoff entfernt werden. 
Bei kleineren Motoren kann ein Speicherkat die NOx-Emissionen um bis zu 90 Prozent reduzieren. Damit auch größere Motoren und Fahrzeuge die strengen NOx-Grenzwerte erfüllen, setzen die meisten Hersteller hier auch noch auf einen SCR-Katalysator, in den eine Harnstofflösung eingespritzt wird. Mit dieser reagieren die Stickoxide und werden harmlos. Dazu bedarf es dann aber eines bis 20 Liter großen, beheizbaren Harnstofftanks – wohin damit? – sowie beheizter Leitungen und einer aufwendigen Elektronik, die alles steuert. Die Kosten beziffern Experten auf über 1.500 Euro. Zusätzlich zum ohnehin teureren Dieselmotor. 
Besserung ist nicht in Sicht. Im Gegenteil: Die Grenzwerte für die europaweite Abgasnorm werden zunehmend strenger.  ""Die Branche muss in den nächsten Jahren Milliardenbeträge in die Abgasreinigung stecken "", sagt Ferdinand Dudenhöffer voraus, Leiter des Center Automotive Research (CAR) an der Universität Duisburg-Essen. 
Die Autohersteller stehen vor einem Dilemma. Denn um die niedrigen europäischen CO2-Flottenwerte für 2020 zu erreichen (95 Gramm je Kilometer), gilt der Dieselmotor wegen seines geringeren Verbrauchs – etwa 20 Prozent weniger als ein Benziner – als unabdingbar.  ""Ohne den Diesel erreicht niemand die Grenzwerte "", sagte kürzlich der BMW-Vorstandschef Harald Krüger bei der Preisverleihung eines Automagazins. Seine Marke verkauft in Deutschland gut 70 Prozent aller Modelle mit Dieselmotor. Bei Volvo ist der Anteil noch höher. Sogar der Sportwagenhersteller Porsche ist mittlerweile mit 40 Prozent dabei. 
Am wenigsten Dieselmodelle verkauft laut Dudenhöffer der japanische Autobauer Toyota. Der größte Automobilhersteller der Welt kommt in Deutschland gerade einmal auf 20 Prozent, er setzt stattdessen vermehrt auf Hybridantrieb. 
Kein unkluger Schachzug. Toyotas Hybridtechnik, eine Kombination aus Benziner und Elektromotor, liegt kostentechnisch schon heute fast auf dem Niveau eines Diesels. Es ist nur eine Frage der Zeit, bis sie sogar inklusive der Batterie günstiger sein wird als ein Dieselaggregat mit seiner nachgeschalteten Abgasreinigung.  ""Die steigenden Kosten werden dem Diesel bis in die Kompaktklasse zu schaffen machen "", prognostiziert Autoexperte Stefan Bratzel. Der Direktor des Center of Automotive Management (CAM) in Bergisch Gladbach sieht als Ausweg nur die Elektrifizierung, speziell den Einsatz von an der Steckdose aufladbaren Plug-in-Hybriden. 
Auch für Herbert Diess, heute Markenchef von VW, erreichen die Kosten der Abgasreinigung eine kritische Größe, die sich nicht in allen Fahrzeugklassen durchsetzen lässt. Bereits voriges Jahr, noch als damaliger BMW-Entwicklungsvorstand, prophezeite Diess:  ""Spätestens 2020 kippt der Diesel im Kompaktsegment. "" 
Es wundert also nicht, dass Autobauer beginnen, auf den Selbstzünder zu verzichten. Bei manchen Kleinwagen flog er bereits aus der Motorenpalette. Smart fährt ohne Diesel, ebenso Nissan Micra, Mitsubishi Space Star, VW Up, Honda Jazz, Toyota Aygo, Peugeot 108 und Citroën C1. Lieber setzen die Hersteller auf kleine Dreizylinder-Turbobenziner, die deutlich günstiger, leiser, sauberer und fast genauso sparsam sind. 
Doch noch ist Deutschland Dieselland. 48 Prozent aller neu zugelassenen Autos haben einen Selbstzünder, damit liegt der Anteil auf Rekordniveau. Ob dies so bleibt, hängt maßgeblich auch davon ab, wie lange Dieselkraftstoff an der Zapfsäule noch geringer besteuert wird als Benzin. Hierzulande beträgt die Differenz rund 18 Cent pro Liter. Umfragen haben ergeben, dass der Preisvorteil beim Kunden das kaufentscheidende Kriterium für ein Dieselmodell ist. Was an Schadstoffen hinten raus kommt, spielt dagegen nur eine untergeordnete Rolle."	technik
"Früher hieß Urlaubsfotos machen: 36-mal die Landschaft knipsen, den Film entwickeln lassen und eine Woche später feststellen, dass 24 Bilder verwackelt sind. Heute zücken wir unser Smartphone, blicken bedeutend in die Kamera, gucken das Bild an, löschen es, machen es noch mal, noch mal, noch mal, verfremden es anschließend mit einem Filter, teilen das fertige Meisterwerk in den sozialen Netzwerken unseres Misstrauens und hoffen, dass es nicht nur der Schwester oder Freundin gefällt – Hashtag sei dank. 
Im Zeitalter der Smartphone-Fotografie ist nicht nur die Entstehung eines mutmaßlich guten Bilds harte Arbeit, es kommt auch auf die Tools an. Filter à la Instagram bieten inzwischen jedes halbwegs relevante soziale Netzwerk und Messenger an. Um sich abzugrenzen, suchen App-Entwickler deshalb nach neuen Möglichkeiten. In der vergangenen Woche haben mit Polaroid Swing und Prisma gleich zwei neue iOS-Apps für Schlagzeilen gesorgt. Wir haben uns beide Apps angesehen. 
Was kann die App? 
Bilder mit lustigen, künstlerisch angehauchten Filtern versehen. Die heißen beispielsweise  ""Curly Hair "",  ""The Scream "",  ""Mondrian "" oder  ""Heisenberg "" und verfremden die Fotos der Nutzer so, als seien sie wahlweise mit dem Pinsel gemalt, mit dem Bleistift gezeichnet oder als hätten Künstler aus dem Expressionismus oder der Pop Art mal eben betrunken Hand angelegt. Die fertigen Fotos lassen sich anschließend direkt aus der App heraus auf Instagram oder Facebook teilen oder auf dem Smartphone speichern. Eine Anmeldung erfordert Prisma nicht, eine Android-Version ist in Arbeit. 
Was ist die Besonderheit? 
Anders als etwa Instagram editiert Prisma die Fotos nicht direkt auf dem Gerät, sondern auf den Servern des Unternehmens. Deshalb dauert es zirka fünf Sekunden, bis ein Filter angewendet ist. Das Prozedere hat einen guten Grund: Die Bilder werden nicht nur mit einem Filter versehen, sondern von einer künstlichen Intelligenz neu zusammengesetzt. Neuronale Netzwerke (wir erinnern uns an die abgefahrenen Google-Bilder im vergangenen Jahr) analysieren das Foto der Nutzer, wenden anschließend Parameter auf Basis bekannter Kunstwerke wie etwa Edvard Munchs Der Schrei an, und schicken den Nutzern ein neues Bild zurück. Die Idee kam den Entwicklern, nachdem Forscher der Universität von Tübingen einen entsprechenden Algorithmus vorstellten. Die Forscher haben inzwischen mit deepart.io einen eigenen Fotodienst ins Leben gerufen, den es allerdings nicht als App gibt. 
Und das funktioniert? 
Nun gut, vielleicht sollte man nicht gleich von Kunstwerken sprechen. Wer aber einen einfachen Weg sucht, Fotos mit einem bestimmten Stil aus der Kunstgeschichte zu versehen, wird bei Prisma fündig. Die App ist simpel, die Ergebnisse erinnern teilweise an kitschige Photoshop-Filter, wie es sie bereits vor 15 Jahren gab. Aber manchmal kommen tatsächlich ansprechende Bilder dabei heraus. 
Hat Prisma Potenzial? 
Der Resonanz der Nutzer zufolge ja. Kurz nach der Veröffentlichung Mitte Juni stand die App in mehreren osteuropäischen Ländern an der Spitze in Apples App Store. Nach eigenen Angaben hat Prisma inzwischen eine Million aktiver Nutzer täglich. Dank der direkten Anbindung an Instagram und Facebook können die Nutzer die Bilder teilen, ohne sich zusätzlich anmelden zu müssen. Allerdings: Je erfolgreicher Prisma ist, desto wahrscheinlicher ist es, dass Instagram seine Filteroptionen eines Tages entsprechend erweitert und externe Apps somit überflüssig macht. 
Wo ist der Haken? 
Es gibt Datenschutzbedenken. In den Datenschutzbestimmungen steht, dass die Entwickler von Prisma die Nutzerdaten mit Drittanbietern teilen können. Die Nutzer behalten zwar das Urheberrecht, sie treten aber bestimmte Nutzungslizenzen an Prisma ab. Ähnlich steht es allerdings auch in den AGB von Instagram oder Google. Zudem können in Prisma Daten wie Log-Dateien, Ortsdaten und weitere Informationen an Dritte übermittelt werden. Es ist denkbar, dass bei aktivierter Standortfreigabe in iOS der Standort der Nutzer an Prisma übermittelt wird, wenn die Nutzer ein Foto an die Server schicken, um es zu bearbeiten. 
Ist das schlimm? 
Aus Basis dieser Metadaten könnte das Unternehmen eines Tages personalisierte Werbung ausspielen (zurzeit ist die App noch werbefrei). Oder Bewegungsprofile der Nutzer erstellen, falls gewollt. Allerdings gilt auch hier: Metadaten fallen praktisch bei jeder Verbindung zu einem Server im Internet an und wer auf Instagram oder Twitter seine Beiträge mit GPS-Daten verknüpft, macht die Informationen sogar öffentlich. Die Entwickler von Prisma sagten dem IT-Portal Techcrunch, auf ihren Servern würden weder die Originale dauerhaft gespeichert werden noch Informationen darüber, von wem sie stammen. Ob das stimmt, lässt sich nicht sagen. 
Wer steckt hinter der App? 
Mitgründer ist der russische Entwickler Alexey Moiseenkov, der früher für die Holding Mail.ru arbeitete. Diese betreibt auch das bekannte russische soziale Netzwerk VK und zählt derzeit zu den größten Investoren in Prisma. Wie Moiseenkov sagt, sollen die Filter demnächst erweitert werden, zudem sei eine Kurzvideofunktion angedacht. 
 
Was kann die App? 
Bewegte Fotos machen. Wer innerhalb der Swing-App den Auslöser betätigt, nimmt nicht nur ein einzelnes Bild auf, sondern eine komplette Sekunde, also quasi ein sehr kurzes Video oder animiertes Gif. Das kann anschließend mit Filtern versehen und auf Facebook und Twitter geteilt werden, aber nicht auf Instagram. Stattdessen gibt es eine interne Community, in der man anderen Nutzern folgen und deren Bilder favorisieren kann. Deshalb ist eine Anmeldung erforderlich, entweder per Twitter, Facebook oder E-Mail. 
Was ist die Besonderheit? 
Sie liegt in der Präsentation der animierten Fotos. Die App erkennt, wie die Nutzer das Smartphone halten: Halten sie es still, wirken die Bilder wie klassische Fotos, also bewegungslos. Neigen sie den Bildschirm aber nach links oder rechts, wird das Bild entweder vorwärts oder rückwärts abgespielt. Je schneller man das Gerät bewegt, desto schneller die Animation. Das funktioniert übrigens auch mit der Maus, wenn die Bilder auf eine Website eingebettet sind. Probieren Sie es aus: 
Gibt es das nicht schon? 
Versuche, Fotos, Videos und animierte Gifs zusammenzubringen, gibt es schon länger. HTC hatte mit Zoe bereits vor Jahren eine ähnliche Funktion in seine Smartphones integriert. Apple hat mit dem iPhone 6S seine Live Photos eingeführt: Ist die Funktion aktiviert, nimmt das iPhone zusätzlich zu jedem Foto noch anderthalb Sekunden vor und nach dem Drücken des Auslösers auf. Instagram hat eine zusätzliche App namens Boomerang, es gibt den Dienst Phhhoto und bereits vor zwei Jahren hatte das Start-up Moju die Idee, den Bewegungssensor des Smartphones mit animierten Fotos zu kombinieren. Revolutionär ist Swing also nicht. 
Hat die Technik Potenzial? 
Es gibt dem Bewegtbild eine zusätzliche interaktive Komponente. Es macht Spaß, Swing-Fotos entweder ganz langsam in Zeitlupe abzuspielen oder ganz schnell. Die Qualität ist hoch, die Animationen flüssiger als bei anderen Apps. 200 Künstler und Fotografen sind zum Start dabei, ihre Werke tauchen derzeit in den Highlights auf. Sie zeigen aber auch das Problem von Swing: Die Technik funktioniert nicht für jedes beliebige Motiv, die Sekunde muss gut ausgewählt und komponiert sein. Sonst wirkt das Resultat schnell albern. Das zweite Problem: Menschen, die bereits auf Instagram sind, benötigen vermutlich keine zusätzliche Foto-Community. Deshalb dürfte Swing ein Nischenangebot bleiben. 
Steckt dahinter wirklich Polaroid? 
Nicht wirklich, auch wenn das US-Unternehmen seinen Namen zur Verfügung stellt. Hinter Swing steckt ein zehnköpfiges Entwicklerteam aus dem Silicon Valley, der Twitter-Mitgründer Biz Stone zählt zu den Geldgebern und ist einer der Vorsitzenden. Der Name ist übrigens Anspielung an die Polaroid Swinger, eine der ersten erschwinglichen Sofortbildkameras aus den Sechziger Jahren."	technik
"Vielerorts wurde die Benutzungspflicht für Fahrradwege aufgehoben und das entsprechende Verkehrszeichen abmontiert. Es sind dann nur separate Streifen auf dem Bürgersteig, von diesem kaum zu unterscheiden. Darf man sie als Radfahrer überhaupt noch benutzen?, möchte ZEIT-ONLINE-Leser Bernhard Gill wissen. 
Ohne Verkehrsschild, das den Weg als Radweg ausweist, ist er auch keiner? So einfach ist es leider nicht. Die Straßenverkehrsordnung (StVO) liefert eine präzisere Erklärung. Paragraf 2, Absatz 4 legt fest, dass es benutzungspflichtige Radwege gibt, die mit blauen Schildern gekennzeichnet sind, und Radwege ohne diese Zeichen.  ""Auf rechts verlaufenden Radwegen ohne Radwegschilder darf man Rad fahren, man muss es aber nicht "", stellt Roland Huhn, Rechtsreferent beim Bundesverband des Allgemeinen Deutschen Fahrrad-Clubs (ADFC), klar. Wichtig: Erst das Verkehrsschild aus Metall macht den Radweg benutzungspflichtig, nur ein auf die Radspur gemaltes Fahrradsymbol nicht. 
Bleibt die Frage: Woran erkennt man sie sonst – beschildert sind sie ja nicht? 
Ob überfahrene rote Ampeln, Unfälle oder Streit beim Gebrauchtwagenkauf: Rund um den Straßenverkehr gibt es viele knifflige Rechtsfragen. Eine davon beantworten Fachanwälte für Verkehrsrecht jede Woche donnerstags hier in unserer Serie  ""Gesetz der Straße "". 
Schreiben Sie uns Ihre Frage (und geben Sie dabei bitte Ihren Namen und Ihren Wohnort an). Wir wählen jede Woche eine Frage aus und beantworten sie hier. 
Bitte beachten Sie: ZEIT ONLINE, die Autorin und die beteiligten Fachanwälte übernehmen keinerlei Gewähr für die Aktualität, Korrektheit, Vollständigkeit oder Qualität der bereitgestellten Antworten und Informationen sowie der Rechtsprechung. Haftungsansprüche gegen ZEIT ONLINE und den Autor, welche sich auf Schäden materieller oder ideeller Art beziehen, die durch die Nutzung oder Nichtnutzung der dargebotenen Informationen bzw. durch die Nutzung fehlerhafter und unvollständiger Informationen verursacht wurden, sind grundsätzlich ausgeschlossen. 
 ""In vielen Städten sind die Radwege farbig gekennzeichnet, beispielsweise rot gepflastert, im Unterschied zum grauen Gehweg. Oder Rad- und Gehweg verlaufen nebeneinander auf dem Bürgersteig und sind baulich durch eine Kante getrennt; das ist wegen der Sturzgefahr aber eher ungünstig "", erläutert Huhn. 
Ungefährlicher für Radfahrer ist es, wenn die Trennung nur aus einer durchgezogenen weißen Linie besteht.  ""In aller Regel ist dann der näher an der Fahrbahn gelegene Teil der Radweg "", sagt der Verkehrsrechtsexperte.  ""Ein sicheres Zeichen für einen Radweg entlang einer Vorfahrtstraße ist seine Weiterführung über eine Seitenstraße hinweg. Dazu wird mit unterbrochenen Linien rechts und links eine Furt markiert. "" 
Das gilt für Rad- und Gehwege, die baulich, farblich oder durch eine weiße Linie voneinander getrennt sind. Manchmal ändern Kommunen aber auch den Verlauf von Radwegen. Aus einem ehemals gemeinsamen Geh- und Radweg wird durch Entfernen des Verkehrsschilds 240 ein Gehweg. 
 ""Ein über seine gesamte Breite einheitlicher Bürgersteig ohne Beschilderung ist kein Radweg. Städte können den Bürgersteig aber durch das Verkehrszeichen 'Gehweg' in Verbindung mit dem Zusatz 'Radverkehr frei' für Radfahrer freigeben. Eine Benutzungspflicht entsteht auf diese Weise nicht "", erklärt der ADFC-Jurist. Die StVO verlangt auf solchen für den Radverkehr freigegebenen Gehwegen vom Radfahrer allerdings, dass er Schrittgeschwindigkeit fährt."	technik
"Mitten in Köln: Eine 62-Jährige überquert auf ihrem Fahrrad eine Kreuzung und wird dabei von einem Auto erfasst. Sie stürzt auf den Asphalt und bleibt leicht verletzt liegen. Der Autofahrer hält an und steigt aus. Doch statt sich um die Radfahrerin zu kümmern, biegt er nur das verbogene Nummernschild seines Wagens gerade und fährt weiter. Passanten, die den Zusammenstoß beobachten, sind sprachlos angesichts der Dreistigkeit des Unfallverursachers. 
Ein Einzelfall war das nicht. Überall in Deutschland registrieren Polizeibeamte eine erhebliche Zunahme von Delikten, die offiziell als  ""Unerlaubtes Entfernen vom Unfallort "" geführt werden. Im Klartext heißt das: Fahrerflucht. Es ist eine Straftat, für die neben Führerscheinentzug oder Fahrverbot auch hohe Geldstrafen drohen, in besonders schweren Fällen sogar Gefängnis. 
Doch das schreckt viele Unfallverursacher nicht ab.  ""Es vergeht kein Tag, an dem keine Geschädigten auf den Polizeidienststellen Anzeige erstatten, weil jemand nach einem Unfall das Weite gesucht hat "", beschreibt Sylvia Frech vom Polizeipräsidium Mittelhessen die Situation, die Ordnungshüter aus anderen Regionen bestätigen. In Berlin ist laut Polizei inzwischen bei rund 22 Prozent aller Verkehrsunfälle Fahrerflucht im Spiel, in Stuttgart beträgt die Quote knapp 24 Prozent, und in Düsseldorf machen sich nach Unfällen mit Blechschäden sogar zwei Drittel der Autofahrer aus dem Staub. 
Fürs Bundesgebiet gibt es keine genauen Zahlen, weil die Statistik nur Karambolagen mit Toten, Verletzten und schwerem Sachschaden erfasst. Aber schon vor Jahren berichteten Juristen der Universität Göttingen von insgesamt 300.000 bis 350.000 Fluchtdelikten pro Jahr und sprachen in diesem Zusammenhang bereits von  ""Massenkriminalität "". Inzwischen dürften die Zahlen noch höher sein. Nach einer Erhebung in elf Bundesländern spricht der Auto Club Europa (ACE) von  ""deutlich über 500.000 Fluchtdelikten "", die jährlich von der Polizei bearbeitet werden. Stimmt diese Hochrechnung, machen sich Autofahrer bei jedem fünften aller polizeilich erfassten Unfälle aus dem Staub. 
Tatsächlich vermutet der ACE sogar noch mehr Vergehen dieser Art, weil viele Geschädigte keine Anzeige erstatten. Rechtswissenschaftler halten bei diesem Delikt eine Dunkelziffer von 1:10 für wahrscheinlich – das würde bedeuten, dass nur jeder zehnte Unfall mit Fahrerflucht aktenkundig wird. Kein Wunder also, wenn ACE-Jurist Florian Wolf vor einer  ""dramatischen Entwicklung bei Unfallfluchten mit Sachschäden "" warnt. 
 ""Eine Mischung aus Irrglauben, Unwissenheit, aber auch mangelnder sozialer Verantwortung "" nennt Polizeisprecherin Sylvia Frech als Hauptgrund für die hohe Zahl von Fahrerfluchtdelikten auf Deutschlands Straßen. Der Irrglaube: Viele Unfallverursacher meinen, ein Zettel am Scheibenwischer des beschädigten Autos reiche aus und man könne danach einfach weiterfahren (siehe Kasten). Die Unwissenheit betrifft die Folgen des Delikts.  ""Schon bei einem Schaden bis 1.300 Euro muss man mit einer Geldstrafe bis zu einem Monatsgehalt plus Punkteintrag und maximal drei Monaten Fahrverbot rechnen "", erklärt Katrin Müllenbach-Schlimme vom ADAC. 
Wer einen Unfall verursacht hat, ist laut Straßenverkehrsordnung verpflichtet, eine  ""angemessene Zeit zu warten und am Unfallort den eigenen Namen und die eigene Anschrift zu hinterlassen, wenn niemand bereit war, die Feststellung zu treffen "" (Paragraf 34). Das ist missverständlich. Der ADAC betont, dass es durchaus nicht ausreicht, eine Visitenkarte unter den Scheibenwischer des beschädigten Autos zu klemmen. Statt dessen sollten Autofahrer besser sofort die Polizei verständigen und den Unfall melden. 
Hat man kein Handy dabei, sollte man mindestens eine halbe Stunde am Unfallort warten und danach zur Telefonzelle gehen, um die Polizei anzurufen. Wer den Unfall erst nachträglich meldet, kann trotzdem wegen Fahrerflucht angezeigt werden, erklärt der ADAC. Die Straftat werde bereits durch das Wegfahren von der Unfallstelle begangen. 
Noch härtere Konsequenzen drohen, wenn sich Autofahrer nach schweren Unfällen aus dem Staub machen. Liegt der Sachschaden bei mehr als 1.300 Euro, riskieren Autolenker durch Fahrerflucht den sechsmonatigen Entzug der Fahrerlaubnis. 1.300 Euro – ein solcher Betrag kommt heutzutage bereits bei der Reparatur eines Parkplatzremplers zusammen. 
 
Dass Autofahrer nach Unfällen die Flucht ergreifen, erklären manche Verkehrspsychologen mit den instinktiven Handlungen des Menschen, der schon vor Urzeiten in Stress- oder Notsituationen mit Kampf oder Flucht reagiert habe. Doch diese Begründung klingt kurzsichtig. Tatsächlich steckt nach Beobachtungen von Polizeibeamten oft berechnendes Kalkül dahinter, wenn sich Autofahrer nach einem verschuldeten Unfall aus der Affäre ziehen. Sekundenschnell würden die Konsequenzen der Tat abgewogen und auch das Risiko beurteilt, ertappt zu werden. So ist nicht selten Alkohol im Spiel, der durch die Flucht unentdeckt bleiben soll. Andere wollen dagegen nur den Zeitverlust durch die Unfallaufnahme und den Ärger mit der Polizei vermeiden oder haben Sorge, den Schadenfreiheitsrabatt bei der eigenen Versicherung zu verlieren. 
Oliver Malchow von der Gewerkschaft der Polizei (GdP) beschreibt das Phänomen mit dem Begriff  ""Wertewandel "":  ""Wir beobachten eine zunehmende Neigung, Regeln zu missachten. Ursachen dafür sind ein sich veränderndes gesellschaftliches Wertesystem, das unter anderem von Konkurrenz und Hetze, aber auch durch individuelle Überforderung geprägt ist. "" 
Die Folgen dieses Trends verändern laut GdP das Klima auf unseren Straßen:  ""Ellenbogenverhalten "" und  ""mangelndes allgemeines Verantwortungsbewusstsein "" machen sich breit. Sie führen dazu, dass Autofahrer auch bei Unfällen nur an sich selbst denken und die Regeln für Anstand, Fairness und Respekt außer Acht lassen. Andere Verkehrsexperten sprechen von der  ""täglichen Arroganz "", die viele Zeitgenossen veranlasst, einfach abzuhauen. 
Zwar werden rund 40 Prozent der Täter von der Polizei ermittelt und angezeigt, doch das bedeutet keineswegs, dass diese Autolenker auch tatsächlich vor dem Strafrichter landen.  ""Fahrerflucht ist ein sogenanntes Vorsatzdelikt "", sagt Martin Bock, Vertrauensrechtsanwalt beim Automobil von Deutschland (AvD).  ""Das Verhalten wird nur unter Strafe gestellt, wenn die Tathandlung auch vorsätzlich begangen wurde. "" Und das gilt es zu beweisen. 
Gibt der Beschuldigte beispielsweise nach einem Parkplatzrempler an, er habe keinen Aufprall gehört, muss die Staatsanwaltschaft ihm nachweisen, dass er den Schaden nicht nur verursacht und bemerkt hat, sondern auch, dass er danach vorsätzlich die Flucht ergriffen hat.  ""Bei der massiven Bauweise vieler Personenwagen ist es durchaus möglich, dass eine Kollision nicht wahrgenommen wird "", sagt Bock. Der Anwalt beschreibt, was in solchen Fällen neben detektivischer Spurensuche notwendig ist, um Tat und Vorsatz zu beweisen:  ""Dann werden Geräuschmessungen vorgenommen, die feststellen sollen, ob das Geräusch beim Anstoß gegen das andere Auto noch im Innenraum hätte gehört werden können. "" 
Ebenso kommen bei der Beweisaufnahme Kameras zum Einsatz, die den Sichtbereich des Autofahrers kontrollieren, oder Beschleunigungssensoren an den Karosserien, die die Wucht des Aufpralls messen und so die Frage klären sollen, ob der beschuldigte Autofahrer etwas bemerkt hat oder nicht. Viel Aufwand, den die Strafverfolger nur bei wirklich folgenschweren Unfällen mit Fahrerflucht betreiben. 
So kommt es, dass viele Beschuldigte mit einem blauen Auge davonkommen. Ihre Verfahren werden entweder vorzeitig eingestellt oder enden mit einem Freispruch. Übrig bleiben jährlich nur etwa 31.000 Autofahrer, die tatsächlich wegen Fahrerflucht verurteilt werden. 31.000 von über 500.000 Ermittlungsverfahren."	technik
"Auf winterlich glatter Straße ereignet sich ein Auffahrunfall, beide Fahrzeuge tragen nur einen Blechschaden davon. Müssen die Unfallbeteiligten die Polizei zu Hilfe rufen oder reicht es, wenn sie Adressen, Telefonnummern und die Daten zu ihren Versicherungen austauschen? 
Ist an beiden Fahrzeugen nur ein Blechschaden entstanden, dann reicht es, wenn die Beteiligten Namen und Adressen austauschen. Zusätzlich ist es aber sinnvoll, sich das Kennzeichen des Unfallgegners aufzuschreiben, denn über den Zentralruf der Autoversicherer (Telefon 0800-2502600) lässt sich die gegnerische Versicherung ermitteln. 
Einen Anruf bei der Polizei kann man sich bei Kleinunfällen sparen, denn die Beamten nehmen nur die Personalien der Beteiligten auf – wenn sie denn überhaupt kommen. Anders ist es, wenn es Verletzte gibt. Dann ist die Polizei verpflichtet, zum Unfallort zu kommen und ein Protokoll aufzunehmen. Allerdings können die Polizeibeamten zur Schuldfrage in der Regel nichts sagen, da sie selbst den Unfall nicht gesehen haben. 
Ob überfahrene rote Ampeln, Unfälle oder Streit beim Gebrauchtwagenkauf: Rund um den Straßenverkehr gibt es viele knifflige Rechtsfragen. Eine davon beantworten Fachanwälte für Verkehrsrecht jede Woche donnerstags hier in unserer Serie  ""Gesetz der Straße "". 
Schreiben Sie uns Ihre Frage (und geben Sie dabei bitte Ihren Namen und Ihren Wohnort an). Wir wählen jede Woche eine Frage aus und beantworten sie hier. 
Bitte beachten Sie: ZEIT ONLINE, die Autorin und die beteiligten Fachanwälte übernehmen keinerlei Gewähr für die Aktualität, Korrektheit, Vollständigkeit oder Qualität der bereitgestellten Antworten und Informationen sowie der Rechtsprechung. Haftungsansprüche gegen ZEIT ONLINE und den Autor, welche sich auf Schäden materieller oder ideeller Art beziehen, die durch die Nutzung oder Nichtnutzung der dargebotenen Informationen bzw. durch die Nutzung fehlerhafter und unvollständiger Informationen verursacht wurden, sind grundsätzlich ausgeschlossen. 
Als Geschädigter ist es in jedem Fall sinnvoll, den Schaden an den beteiligten Fahrzeugen und den Unfallort mit Fotos zu dokumentieren. Da fast jeder ein Mobiltelefon mit einer Kamera mit sich herumträgt, lässt sich das einfach umzusetzen. 
Eine Besonderheit sind nicht gestreute Straßen. Hier könnte man auch versuchen, den für die Straße Verantwortlichen zu belangen, wenn er gegen seine Räum- und Streupflicht verstoßen hat. Doch allzu viel Hoffnung sollte man sich nicht machen, denn solche Ansprüche sind in aller Regel nur sehr schwer durchsetzbar, wie Florian Schmidtke, Rechtsanwalt für Verkehrsrecht aus München, erklärt. 
Für denjenigen, der mit seinem Wagen aufgefahren ist, könnte es allerdings problematisch werden, wenn er mit Sommerreifen auf verschneiten oder eisglatten Straßen unterwegs war. Will er den Vollkaskoschutz seiner eigenen Versicherung in Anspruch nehmen und den an seinem Wagen entstandenen Schaden geltend machen, verweist die Versicherung in diesem Fall möglicherweise darauf, dass das Auto verkehrsunsicher bereift war. Je nach Versicherungsvertrag und Schwere des Verschuldens kann nämlich die Versicherungsleistung gekürzt werden."	technik
"Manipulierte Abgastests, zu hoher Stickoxidausstoß, Krebsrisiko: Hat der Dieselmotor ein Problem? Nein, antworten die deutschen Autohersteller unisono und loben diese Antriebstechnik als Garant für Fahrspaß, Wirtschaftlichkeit und Klimaschutz.  ""Moderne Dieselantriebe sind für die Erreichung der europäischen Klimaschutzziele unverzichtbar "", ist der Verband der Automobilindustrie (VDA) überzeugt – und beschreibt damit zugleich das große Dilemma der Branche: Der Diesel ist  ""unverzichtbar "", weil die Firmen keine Alternativen haben. 
 ""Die deutschen Hersteller haben sich in den vergangenen 20 Jahren zumeist erfolgreich vom Trend der Elektrohybridisierung in den beiden Leitmärkten Japan und USA abgekoppelt – zum Schaden der Umwelt "", urteilt Eckard Helmers vom Umwelt-Campus der Hochschule Trier in einem aktuellen Gutachten, das er für den Verkehrsclub Deutschland (VCD) und den BUND erarbeitete. 
Der Wissenschaftler kritisiert, dass die Autoindustrie am Dieselmotor festhält. Der Selbstzünder stoße  ""rund siebenmal mehr Stickoxide "" aus als ein Benziner.  ""Das ist keine Strategie für saubere, 'blaue' Luft "", meint Helmers in Anspielung auf die mehr oder minder sinnfreien Zusatzbezeichnungen BlueTec, BlueTDI oder BluePerformance, mit denen Autohersteller ihre modernen Dieselmodelle belegen. 
Als Problempunkt gelten aber nicht nur die Stickoxide. Auch die Bedeutung des Dieselantriebs für den Klimaschutz ist nach Ansicht von Fachleuten weitaus geringer, als es die Industrie behauptet. Denn obwohl in Deutschland immer mehr Autos mit Dieselmotor unterwegs sind, macht sich das in der Kohlendioxidbilanz des Pkw-Verkehrs kaum bemerkbar. Zwischen 2005 und 2013 sind die Emissionen nur um 1,6 Prozent gesunken, wie das Statistische Bundesamt feststellte. 
Zwar verbrauchen Dieselmotoren weniger Kraftstoff als Benzinantriebe, doch die Selbstzünder wurden in den letzten Jahren immer leistungsstärker und stoßen deshalb auch mehr Kohlendioxid (CO2) aus. Im Jahr 2005 leistete ein durchschnittlicher Personenwagen laut Statistischem Bundesamt 123 PS; bis heute stieg die Leistung auf 137 PS. Wäre die Motorleistung unverändert geblieben, hätte die Pkw-Flotte im Jahr 2013 etwa zwölf Prozent weniger CO2 ausgestoßen, rechnet das Bundesamt vor – also gut zehn Prozentpunkte mehr, als tatsächlich erreicht wurden. 
Dafür hat der Umweltforscher Helmers eine Erklärung:  ""Offenbar haben viele Kunden in Deutschland durch eine übermäßige Subventionierung von Dieselsprit immer größere und stärker motorisierte Modelle gekauft. "" Dieser Trend werde durch das immer größere Angebot an spritschluckenden SUVs noch weiter verstärkt. Derzeit ist bereits rund jeder fünfte Neuwagen ein SUV oder Offroader.  ""Da sie damit Geld verdient, wird die Autoindustrie alles daran setzen, den SUV-Boom nicht enden zu lassen "", sagt Helmers. Hinzu kommt nach Beobachtungen des Wissenschaftlers, dass die Besitzer PS-starker Dieselmodelle die angebotene Motorleistung auf der Straße auch abrufen und dadurch einen Praxisverbrauch erzielen, der weit über den Herstellerangaben liegt. 
Das Argument der Autoindustrie, nur mit dem Dieselmotor könne man die Klimaschutzziele von 2020 erreichen, steht also auf tönernem Fundament. Es beruht auf Daten über Verbrauch und CO2-Emissionen, die bei Prüfstandtests mehr oder minder trickreich ermittelt werden. Die Wirklichkeit sieht anders aus. 
 ""Um die Ziele beim Energieverbrauch und Klimaschutz zu erreichen, ist eine grundlegende Transformation des Verkehrssektors unverzichtbar "", urteilt denn auch Norbert Schwieters. Er leitet bei der Unternehmensberatung PwC den Bereich Energiewirtschaft. Laut einer PwC-Studie sind nicht Dieselmotoren, sondern Elektro- und Hybridautos notwendig, um im Verkehrssektor wirklich nennenswert CO2 einzusparen. 
 
Trotzdem halten Deutschlands Autohersteller unverdrossen am Dieselantrieb fest und wollen dessen Technik in den nächsten Jahren sogar noch mit großem Aufwand weiterentwickeln. Für manche Branchenkenner ist es ein riskanter Kurs, der die Industrie womöglich in eine Sackgasse führt. Denn nicht nur die Abgasgesetze mit ihren immer strengeren Stickoxidlimits werden für die Dieselautos zum Problem. Auch das wichtige Exportgeschäft dürfte die Firmen schon bald zum Nachdenken über ihr Dieselengagement veranlassen. 
Nach dem VW-Abgasskandal wird es noch schwerer sein, den Dieselmotor in den USA zu etablieren, sodass de facto nur noch die Länder Europas bleiben, in denen diese Antriebstechnik nennenswerte Absatzchancen hat. Der Rest der Welt fragt nach anderen Antriebstechnologien – nach ebenso sauberen wie sparsamen und erschwinglichen Alternativen, die Deutschlands Autohersteller aber nicht liefern können. 
Beispiel Hybridantrieb: Als Toyota im Jahre 1997 das Modell Prius in Japan erstmals auf den Markt brachte, starteten deutsche Autohersteller ihre Dieseloffensive und verkauften seitdem rund 23 Millionen neue Dieselmodelle. Vom ruß- und stickoxidarmen Benzin-Hybridantrieb à la Toyota, der im Stadt- und Kurzstreckenverkehr ein emissionsfreies Fahren ermöglicht, wollten die Firmen hingegen nichts wissen. Man scheute die notwendigen Investitionen, sah aber auch angesichts der Abgasgesetze keinen Grund für einen Kurswechsel. Im Gegenteil: Die EU-Schadstofflimits wurden bisher stets so definiert, dass sie den Dieselmotor begünstigen. Hinzu kommt die geringere Mineralölsteuer – eine politisch beabsichtigte Absatzförderung für Dieselautos, die das Umweltbundesamt jedoch zu den  ""wichtigsten umweltschädlichen Subventionen im Verkehrssektor "" zählt. 
Zwar behauptet der VDA, der Hybrid gehöre  ""inzwischen zum Standardprogramm der deutschen Hersteller "", doch tatsächlich ist das Angebot an solchen Modellen nicht nur überschaubar, sondern obendrein auch teuer. Autos, die technisch und preislich mit Toyota-Modellen wie dem Prius (26.850 Euro), Auris (22.990 Euro) oder Yaris (17.300 Euro) vergleichbar sind, gibt es bei den inländischen Herstellern nicht. 
Audi, Daimler und Volkswagen bieten erst seit Kurzem Modelle mit dem aufwendigeren Plug-in-Verfahren an, bei dem die Antriebsbatterie an der Steckdose aufgeladen werden muss. Der Audi A3 e-tron mit dieser Technik kostet beispielsweise 37.900 Euro, der Golf GTE 36.900 Euro, und bei Daimler kostet der auf 293 PS hochgezüchtete Mercedes-Benz C 350 mit Benzin-Hybridantrieb knapp 51.000 Euro. Die Folge dieser Hochpreispolitik: Nur ein Prozent der von Januar bis September dieses Jahres zugelassenen Neuwagen sind Hybridautos. 
 ""Lustlosigkeit "" bescheinigt Eckard Helmers den deutschen Autoherstellern auch beim Gasantrieb. Die Firmen vergäben damit eine große Chance zur Verringerung klimaschädlicher Emissionen, sagt der Umweltforscher. Denn Benzinmotoren mit Flüssig- oder Erdgasbetrieb stoßen nicht nur weitaus weniger Stickoxide aus, sondern sind auch bei den CO2-Emissionen dem Diesel überlegen.  ""Hierin hätte eine kostengünstige Alternative zum europäischen Diesel-Pkw-Boom gelegen. "" 
Schon mit dem in anderen Ländern weitverbreiteten Flüssiggas (LPG) könne der CO2-Ausstoß gesenkt werden, doch deutsche Firmen betrieben auf diesem Gebiet keine Entwicklungsarbeit und verpassten deshalb die Chance, mit speziell abgestimmten Motoren umwelt- und klimafreundlichere Diesel-Alternativen anzubieten, urteilt Helmers. 
Die ökologisch noch bessere Variante ist Erdgas (CNG), denn es verbrennt am saubersten und lässt sich mithilfe erneuerbarer Energien klimaneutral herstellen. Immerhin: Hier zeigen die inländischen Automarken Präsenz und bieten auch in der Klein- und Kompaktwagenklasse schadstoffarme Diesel-Alternativen an. Zum Beispiel den VW Up für 12.950 Euro oder den bivalenten Audi A3 g-tron (kombinierter Benzin-/Erdgasbetrieb), der mit 26.450 Euro nur rund 200 Euro teurer als die Diesel-Version ist. 
Doch Gasautos haben angesichts der Übermacht der Dieselmodelle keine Chance und fristen nur ein Schattendasein in den Autohäusern: Von den rund drei Millionen neu zugelassenen Personenwagen des letzten Jahres waren nur 8.194 mit Erdgas- und 6.234 mit Flüssiggasantrieb ausgestattet. 
Es ist noch nicht lange her, da waren Deutschlands Autoentwickler buchstäblich elektrisiert. Das Elektroauto beschrieben sie als wichtigsten Beitrag zum Klimaschutz, sprachen von  ""emissionsfreier Mobilität "" und sahen eine  ""spektakuläre Wende am Automarkt "" voraus. Es wurden  ""Taskforces "" innerhalb der Entwicklungsbereiche gebildet, gewaltige PR-Kampagnen gestartet, spektakuläre Messeauftritte mit der Berliner Politik-Prominenz inszeniert. Doch die Hersteller präsentierten dabei meist nur sogenannte Showcars – Attrappen visionärer Elektromodelle, die nie eine Serienchance hatten. 
Seit dem Elektro-Hype der deutschen Autoindustrie sind inzwischen sechs Jahre vergangen, und die Bilanz fällt mager aus. Man hatte die Rechnung ohne die Kunden gemacht, die von der Technik, der geringen Reichweite und der eingeschränkten Praxistauglichkeit der Elektroautos nicht überzeugt sind. Trotz aller Werbekampagnen sind bisher nur insgesamt rund 26.000 E-Modelle in Deutschland registriert. Vom Ziel, im Jahr 2020 eine Million Autos mit dieser Antriebstechnik auf deutschen Straßen zu haben, sind Bundesregierung und Autoindustrie meilenweit entfernt. Der VDA glaubt dennoch, Deutschland sei weiter auf dem Weg,  ""Leitanbieter und Leitmarkt "" für die Elektromobilität zu werden. 
Die Absatz- und Akzeptanzkrise beschreibt Stefan Bratzel, Autoexperte an der Fachhochschule der Wirtschaft in Bergisch Gladbach, doppelbödig mit dem Begriff R.I.P.-Problem:  ""Solange sich Reichweite, Infrastruktur fürs Batterieladen und Preis nicht verbessern, kommt das Elektroauto nicht in Fahrt "", sagt er. 
Eine der entscheidenden Hürden: Elektroautos sind zu teuer. Der Kleinwagen VW Up mit Elektroantrieb kostet beispielsweise 26.900 Euro und ist damit rund 15.500 Euro teurer als die Benzin-Version. Das hält Umweltforscher Helmers angesichts fallender Batteriepreise für übertrieben.  ""Bei geschätzten Kosten von 200 Euro pro Kilowattstunde würde die Batterie des Elektro-Up weniger als 4.000 Euro kosten "", erläutert er und betont, dass E-Autos in vielen Bereichen technisch einfacher konstruiert sind als Modelle mit Verbrennungsmotor. Der hohe Aufpreis sei auch deshalb nicht gerechtfertigt. 
Die ausländische Konkurrenz hat die Trendwende bereits vollzogen: Statt ursprünglich jeweils rund 35.000 Euro kostet der Mitsubishi i-Miev jetzt nur noch 23.790 Euro, und der Preis des Citroën C-Zero sank auf 17.850 Euro. 
Auch bei der mit Abstand zukunftsfähigsten Antriebsalternative haben Deutschlands Autohersteller den Anschluss verloren. Das Elektroauto mit der wasserstoffbetriebenen Brennstoffzelle könnte tatsächlich viele Energie- und Umweltprobleme des Straßenverkehrs lösen, denn es produziert weder giftige Schadstoffe, noch belastet dieser Antrieb das Klima mit CO2-Emissionen. Schon 1994 hatten Daimler-Forscher das erste Brennstoffzellenfahrzeug erprobt und später mit PR-Aktionen wie der Weltumrundung dreier F-Cell-Modelle (fuel cell, die englische Bezeichnung für die Brennstoffzelle) für Aufsehen gesorgt, ohne dass die Kunden davon profitieren konnten. Von den  ""mehreren Hunderttausend Brennstoffzellenautos "", die Konzernchef Dieter Zetsche für dieses Jahr versprochen hatte, sind gegenwärtig aus Daimler-Produktion nur 200 F-Cell-Modelle der Mercedes B-Klasse und 23 Busse mit Brennstoffzellenantrieb unterwegs. 
Die Rolle als Pionier und Schrittmacher der Brennstoffzellentechnik haben deutsche Autohersteller längst aufgegeben. Im Lauf der Forschungsarbeit hatten sie das Interesse an diesem Antrieb verloren, zumal die Geschäfte mit den Dieselmodellen stets gut liefen und das Elektroauto als ökologisches Aushängeschild genügte. 
Es war ein Trugschluss. 
Jetzt erkennen die Firmen, dass die Emissionen des Dieselmotors in Zukunft ehrlich – also auch im realen Fahrbetrieb – gemessen werden und dass sie deshalb die ab 2021 gültige CO2-Flottenvorschrift von durchschnittlich 95 Gramm pro Kilometer möglicherweise nicht erfüllen werden. Und weil dieses Ziel wohl auch mit den beim Käuferpublikum unbeliebten Elektromodellen nicht zu erreichen ist, besteht plötzlich Nachholbedarf: Die Hersteller brauchen einen Alternativantrieb, der ebenso sauber wie langstreckentauglich ist. 
Beides kann die Brennstoffzelle, zumal für die Wasserstoffgewinnung inzwischen genügend Strom aus erneuerbaren Energien zur Verfügung steht. Die Zeit drängt für die Autofirmen. Darum greifen sie auch zu ungewöhnlichen Maßnahmen, um technologisch wieder aufzuholen. Konkurrenten rücken zusammen: BMW arbeitet mit Toyota an der Brennstoffzelle, Daimler kooperiert mit Ford, und VW/Audi stieg beim kanadischen Brennstoffzellenspezialisten Ballard ein. 
Allerdings: Vor 2017 wird man kein Brennstoffzellenauto aus deutscher Produktion kaufen können. Damit wiederholt sich die gleiche Schlappe wie schon beim Hybridantrieb. Während deutsche Firmen nur Showcars präsentieren, hat die ausländische Konkurrenz wieder die Nase vorn: Hyundai und Toyota bringen die ersten Brennstoffzellenautos auf den europäischen Markt und nennen dafür auch konkrete Preise. 65.450 Euro kostet der Hyundai ix35 Fuel Cell, und die Toyota-Limousine Mirai wird im Leasing (inklusive Versicherung, Service, Reifen usw.) vier Jahre lang für monatlich 1.450 Euro angeboten. Das ist viel Geld, aber immerhin ein konkreter Anfang für eine völlig neuartige Fahrzeugtechnik. 
Für das Center of Automotive Management der Fachhochschule der Wirtschaft ist der Toyota Mirai die wichtigste Auto-Innovation der letzten zehn Jahre.  ""Wir haben dem Brennstoffzellenfahrzeug von Anfang an die besseren Zukunftschancen eingeräumt als einem Auto mit rein batterieelektrischem Antrieb "", erklärt Toyota-Sprecher Dirk Breuer den Vorsprung des japanischen Autoherstellers. 
Der Mirai startet mit einer Jahresproduktion von rund 2.100 Fahrzeugen. Das Wichtigste bei der Einführung neuer Technologien sei ein  ""langer Atem "", heißt es bei Toyota. Auch der erste Hybrid-Toyota sei 1997 nur in kleiner Stückzahl gestartet, doch heute produziere der Konzern jährlich bereits über eine Million Hybridautos."	technik
"Selten sind sich Richter, Kläger und Beklagter so einig, dass ein Gesetz nichts taugt, wie an diesem Donnerstag im Juli. Im Sitzungssaal E.06 des Oberlandesgerichts München (OLG) geht es um das Leistungsschutzrecht für Presseverleger, in Kraft getreten vor ziemlich genau drei Jahren und seither Musterbeispiel für gesetzgeberischen Murks. 
Eigentlich wollte die damalige schwarz-gelbe Bundesregierung mit den neuen Paragrafen 87f bis 87h, die sie ins Urheberrechtsgesetz geschrieben hat, den deutschen Presseverlagen helfen. Die klagten, Google schade ihnen und ihrem Geschäftsmodell mit seinen kostenlos angebotenen Ausschnitten von Presseartikeln. Also verpflichtete die Regierung alle Betreiber von Suchmaschinen und ähnlichen Diensten, Lizenzverträge mit Verlagen abzuschließen, wenn sie deren Inhalte in kurzen Vorschautexten zusammenfassen wollen. Für alles, was über  ""einzelne Wörter oder kleinste Textausschnitte "" hinausgehe, sollten Suchmaschinen den Verlagen Geld bezahlen. So weit die Theorie. 
Die vorläufige Bilanz: Das Leistungsschutzrecht für Presseverleger kennt nur Verlierer. Die damalige Bundesregierung hat sich blamiert, weil das Leistungsschutzrecht wie ein fortschrittsfeindliches Geschenk an die mächtigen deutschen Verleger wirkte. Die Verleger verlieren, weil sie von Google kaum Geld bekommen. Bisher haben sie ganze 714.540 Euro aus dem neuen Recht eingenommen – davon exakt nichts von Google. Im Gegenteil: Die juristische Auseinandersetzung mit dem Suchmaschinenanbieter hat die Verlage schon jetzt 3,3 Millionen Euro gekostet. Vergleichsweise kleine, innovative Onlinedienste verlieren, weil sie durch das Leistungsschutzrecht in ihrer Existenz gefährdet sind. Dabei hätte sich die Bundesregierung so gerne auf die Fahnen geschrieben, gerade sie zu fördern. 
Einer dieser Dienste ist uberMetrics, gegründet 2011 von Patrick Bunk. Der sitzt nun mit seinen Anwälten im Sitzungssaal E.06 und versucht darzulegen, wie praxisfern das Leistungsschutzrecht ist. 
UberMetrics bietet einen sogenannten Medienbeobachtungsdienst an, er heißt Delta. Bunks Firma analysiert vom Alexanderplatz in Berlin aus die Medienlandschaft und stellt gegen Gebühr zusammen, was Online- und Offlinemedien über die vom Kunden gewünschten Themen berichten. Neben Überschrift, Erscheinungsdatum, Quelle und Autor zeigt Delta auch, wie groß die Reichweite der gefundenen Artikel oder Sendungen war, wie schnell sie sich in sozialen Netzwerken verbreitet haben und ob sie eher positiv oder eher negativ über das Thema berichteten. Wer den Dienst nutzt, kann beispielsweise sehen, wie erfolgreich die eigene PR und das Marketing sind, wer zu einem bestimmten Themengebiet etwas Wichtiges zu sagen hat, oder auch, welcher Zulieferer im Ausland gerade in Schwierigkeiten steckt. Patrick Bunk bezeichnet Delta als  ""High-End-Variante von Google Alerts "". 
Der Süddeutsche Verlag findet, Bunk hätte eine Lizenz kaufen müssen, um in seinem Dienst auch Textausschnitte der Süddeutschen Zeitung (SZ) anzeigen zu dürfen. Daher sitzen Bunk im Gerichtssaal in München nun Vertreter der Dokumentations- und InformationsZentrum München GmbH (DIZ) gegenüber. Die DIZ vermarktet im Auftrag der SZ die Nutzungsrechte an den Inhalten der Zeitung und sie verfolgt Verstöße gegen diese Rechte. Die SZ und ihr Verlag gehören zwar nicht zu jenen Leistungsschutzrecht-Befürwortern, die sich in der VG Media zusammengeschlossen haben und seither gemeinsam, aber weitgehend erfolglos mit Google streiten. Der Anwalt der DIZ sagt sogar:  ""Wir sind auch nicht glücklich mit diesem Gesetz. "" Trotzdem geht der Süddeutsche Verlag gegen uberMetrics vor. 
Im Kern geht es in der Auseinandersetzung um einige Artikel, die in der Onlineausgabe der SZ erschienen sind, und darum, was Delta mit ihnen gemacht hat. Wie andere Medienbeobachtungsdienste und Suchmaschinen auch erstellt Delta nach Eingabe bestimmter Suchbegriffe kurze Vorschautexte der Artikel – sogenannte Snippets. Die werden den Nutzern zusammen mit einem Link auf die Quelle angezeigt. Einige dieser Snippets waren ziemlich lang, nämlich zwischen 235 und 250 Zeichen. Bunk erklärt das mit einer  ""temporären Überlastung unserer Schnittstelle in Verbindung mit einem Implementierungsfehler "". Doch auch wenn die Snippets länger waren als von uberMetrics vorgesehen, seien sie  ""im Markt bei anderen durchaus üblich und unserer Ansicht nach rechtskonform "" gewesen. 
Der Süddeutsche Verlag sieht das anders. 235 bis 250 Zeichen sind seiner Ansicht nach keine  ""kleinsten Textausschnitte "" mehr, wie sie das Gesetz erlaubt. UberMetrics hätte für eine solche Verwertung nach dem Leistungsschutzrecht Geld bezahlen müssen, wie es konkurrierende Diensteanbieter angeblich auch getan haben. Die DIZ hatte daher eine einstweilige Verfügung gegen uberMetrics erwirkt und der Firma verboten, Ausschnitte von Artikeln der SZ in einer solchen Länge  ""öffentlich zugänglich zu machen "". Bei Nichtbeachtung muss Bunk 250.000 Euro Ordnungsgeld zahlen oder bis zu sechs Monate in Haft. Der legte Berufung ein, über die nun im Sitzungssaal E.06 drei Richter des OLG entscheiden müssen. 
 
Dem Start-up-Gründer geht es ums Prinzip. Wie klein ist jene  ""kleinste Textmenge "", die das Leistungsschutzrecht erlaubt? Wo genau ist die Grenze? Und braucht uberMetrics wirklich eine kostenpflichtige Lizenz des Verlags, wenn dieser doch mit simplen technischen Mitteln sicherstellen könnte, dass der Crawler von uberMetrics gar nicht erst auf Verlagsinhalte zugreift? 
Der Gesetzgeber wollte diese Fragen zum Leistungsschutzrecht nie beantworten und hat das Problem den Gerichten überlassen. 
Aber auch nach drei Jahren gibt es noch keine Antworten darauf. Die Gerichte hatten bisher keinen Grund, eine Obergrenze für die Länge von Snippets zu definieren. Im Rahmen von Zivilverfahren müssen sie nur entscheiden, ob die konkreten Klageanträge auf Unterlassung zulässig und begründet sind. Ein Urteil in einem Hauptsacheverfahren gibt es bisher nicht. Auch deshalb, weil die Koalition in das Gesetz hineinschrieb,  ""das Recht (ein Presseerzeugnis oder Teile hiervon zu gewerblichen Zwecken öffentlich zugänglich zu machen) erlischt ein Jahr nach der Veröffentlichung des Presseerzeugnisses "". Sprich: Ein Jahr nach der Veröffentlichung eines Artikels kann der Verlag niemanden mehr auf Unterlassung verklagen. Bevor es vor einem deutschen Gericht zu einem Hauptsacheverfahren kommt, vergeht allerdings oft mehr als ein Jahr. Union und FDP haben die Regelung der komplizierten Details des Leistungsschutzrechts also an Gerichte delegiert und es ihnen gleichzeitig schwer gemacht, allgemein gültige Regeln festzulegen. 
 ""Es wäre schön gewesen, wenn der Gesetzgeber klargemacht hätte, was kleinste Textausschnitte sind "", sagt der Vorsitzende Richter Tobias Pichlmaier. Und dass es nicht die Aufgabe des OLG sei, diese Klarstellung zu treffen.  ""Klären Sie das doch direkt mit der Süddeutschen Zeitung "", rät Richterin Ulrike Holzinger dem uberMetrics-Gründer,  ""dann haben Sie Ihre Grenze "". 
Bunk hilft das ebenso wenig wie die Erklärung des DIZ-Anwalts, aus den Allgemeinen Geschäftsbedingungen der Zeitung gehe doch klar hervor, dass uberMetrics für das, was Delta tut, eine Lizenz vom Verlag brauche. Denn jeder Verlag könnte eine andere Obergrenze definieren, jede neue Seite im Netz könnte einem Verlag gehören, ohne dass es für einen automatisierten Crawler wie den von uberMetrics ersichtlich wäre. Solche Crawler, die jede Suchmaschine einsetzt, um Websites zu indizieren, verstehen natürlich auch keine Allgemeinen Geschäftsbedingungen, weil sie nicht maschinenlesbar sind. Bunk will Rechtssicherheit, er will allgemein gültige, präzise Längenangaben, die seine Software verstehen und umsetzen kann. 
Ihm wäre es sogar recht, wenn die sueddeutsche.de seinem Crawler in den Meta-Tags die Erstellung von Snippets oder in der Datei robots.txt die Indizierung der Seite untersagen würde. Das würde seinen Dienst weniger attraktiv machen, weil er Suchergebnisse von sueddeutsche.de nur noch in Form einer verlinkten Überschrift oder eben gar nicht anzeigen würde. Aber wenigstens wüsste er, woran er ist. 
Der Anwalt der DIZ lehnt das ab, weil es einem Verlag nicht zuzumuten sei, angesichts der Vielzahl von Crawlern im Netz Vorgaben für jeden einzelnen zu machen. Welche Snippetlänge der Süddeutsche Verlag für unkritisch hält, wie viel uberMetrics -Konkurrenten zahlen, um eine Lizenz zur Verwendung bestimmter Snippetlängen zu bekommen und ob der Verlag auf der Grundlage des Leistungsschutzrechts auch gegen andere Unternehmen als uberMetrics vorgeht, beantwortet der Anwalt auf Anfrage von ZEIT ONLINE nicht. Es handele sich um  ""Geschäftsentscheidungen und -geheimnisse unserer Mandantin oder ihr konkretes Verhalten bei Vertragsabschlüssen "". Dazu könne oder wolle sich der Verlag nicht öffentlich äußern. 
Die Gerichte sind nicht bereit, sich in diesem Streit zwischen die Fronten zu stellen. Auch nicht das Oberlandesgericht München. Am Nachmittag gibt Richter Pichlmaier die Entscheidung bekannt: Es bleibt bei der einstweiligen Verfügung, Bunk darf längere Snippets der SZ nicht nutzen. Eine Snippethöchstlänge definiert das OLG nicht. 
Patrick Bunk ist damit kein bisschen schlauer. Aber ärmer. Der Prozess kostet ihn eine sechsstellige Summe. Für ein Start-up können solche Verfahren schnell existenzgefährdend werden. Nicht nur wegen der unmittelbaren Kosten. Sondern vor allem, weil Geldgeber nicht investieren, wenn laufende Verfahren Teile des Geschäftsmodells infrage stellen. 
Bunks Dienst Delta zeigt Suchergebnisse von sueddeutsche.de jetzt nur noch mit Snippets an, die höchstens sieben Wörter lang sind. Das entspricht dem unverbindlichen Schlichtungsvorschlag des Deutschen Patent- und Markenamtes zu diesem Streit. Sinnvoll findet Bunk die Grenze nicht. Er findet, damit sei niemandem geholfen, den Verlagen nicht, den Lesern nicht und ihm auch nicht. Er sagt:  ""Den durch diese Rechtsunsicherheit bedingten Standortnachteil für Suchmaschinen-Technologien aus Deutschland müssen wir akzeptieren. "" 
Was andere möglicherweise nicht davon abhält, diesen Unsinn nachzumachen. Die EU-Kommission plant unter Leitung von Digitalkommissar Günther Oettinger eine Gesetzesinitiative, die unter anderem zu einem Leistungsschutzrecht für Presseverleger auf euroäischer Ebene führen könnte."	technik
"Linus Neumann, einer der Sprecher des Chaos Computer Clubs (CCC), nennt es einen  ""großen Durchbruch "": Let's Encrypt ist jetzt in der Public-Beta-Phase. Das bedeutete: Ab sofort kann sich jeder Websitebetreiber ohne vorherige Einladung kostenlose SSL-Zertifikate besorgen und damit die Übertragung von Nutzerdaten kryptografisch vor Schnüfflern absichern. 
Let's Encrypt verfolgt ein hehres Ziel: Das Internet soll weniger leicht zu überwachen sein. Bei ungesicherten HTTP-Verbindungen zwischen Client und Server werden übertragene Daten wie zum Beispiel Passwörter oder Suchbegriffe für jeden sichtbar, der sich in die Verbindung einklinken kann. Das kann jemand im selben WLAN sein, ein Internetprovider, aber auch ein Geheimdienst wie die NSA oder ihr britisches Gegenstück GCHQ, die zu diesem Zweck transatlantische Glasfaserkabel anzapfen. Zudem wird für die Beobachter erkennbar, welche Unterseiten einer Domain jemand aufruft, was potenziell die Privatsphäre der Nutzer verletzt. 
Die Transportverschlüsselung mit SSL beziehungsweise dessen Nachfolger TLS verhindert all das. Sie ist damit ein bewährtes Mittel gegen billige Massenüberwachung. Nicht umsonst haben große Unternehmen wie Facebook, Google und Yahoo nach den Snowden-Enthüllungen auf SSL/TLS per Default umgestellt. Google belohnt seinerseits auch Websites, die standardmäßig HTTPS bieten, mit einem besseren Ranking in seinen Suchergebnissen. 
Für die Betreiber kleiner Websites ist der Umstieg schwieriger. Erstens ist es nicht ganz trivial, die Technik richtig zu implementieren. Zweitens kostet es Geld, weil man besagte Zertifikate braucht, die den Nutzern zeigen, dass sie wirklich auf die gewünschte Website zugreifen. Sie werden von spezialisierten Unternehmen gegen Gebühr ausgestellt. Drittens laufen die Zertifikate irgendwann ab und müssen dann erneuert werden – was Administratoren gerne mal vergessen. Nicht zuletzt aus diesen Gründen sind ungefähr drei Viertel aller Internetverbindungen noch ungesichert. 
Let's Encrypt will das ändern. Yan Zhu, eine der Entwicklerinnen, versprach ZEIT ONLINE im September, mit Let's Encrypt könne jeder Admin SSL  ""in weniger als einer Minute "" implementieren, und zwar kostenlos und automatisch  ""in der sichersten bekannten Konfiguration "". Zudem lässt sich auch die Erneuerung der Zertifikate automatisieren. The Register hat es bereits ausprobiert und findet es  ""fast zu einfach "". 
Hinter dem Projekt steckt die gemeinnützige Internet Security Research Group, die mit Let's Encrypt eine freie, offene Certificate Authority (CA) erreichten will. Sie wird unter anderem unterstützt von Mozilla, der Electronic Frontier Foundation, Cisco, Akamai und seit gestern auch von Facebook. 
Zwar können Website-Betreiber, die keine CA bezahlen wollen, schon seit Längerem kostenlose und selbst signierte Zertifikate verwenden. Aber das führt dazu, dass Besucher der Website beim ersten Aufruf eine Sicherheitswarnung angezeigt bekommen und eine Ausnahmegenehmigung in ihrem Browser hinzufügen müssen – vorausgesetzt, sie vertrauen darauf, dass der Anbieter der Website wirklich der ist, für den er sich ausgibt. Mithilfe von gefälschten Zertifikaten, die ein Nutzer unvorsichtigerweise akzeptiert, können Angreifer Phishing-Attacken ausführen. Let's-Encrypt-Zertifikate dagegen werden bereits von allen großen Browsern unterstützt, weil sie durch die etablierte CA IdenTrust signiert wurden. 
Administratoren finden bei golem.de, ComputerBase und natürlich bei Let's Encrypt selbst genauere Anleitungen. Dennoch werden viele von ihnen vor einem Problem stehen, unter anderem auch ZEIT ONLINE: Werbung lässt sich nur schwer oder auch gar nicht verschlüsselt ausliefern. An den heutigen Echtzeit-Vermarktungssystemen sind zahlreiche Parteien beteiligt, die alle zusammenspielen müssten, damit eine Website komplett HTTPS-gesichert ausgeliefert werden kann, was verschiedene technische und organisatorische Probleme bereitet. Dieser heise-Artikel erklärt es recht ausführlich. Die Alternativen wären das sogenannte Native Advertising, das aber schwerer als Werbung zu erkennen ist und deshalb ebenfalls nicht unproblematisch – oder der Umstieg auf Paywalls, so dass die Nutzer für Inhalte zahlen müssen. 
Let's Encrypt ist also nicht die endgültige Lösung für alle Betreiber. Es bietet auch noch nicht alle Arten von Zertifikaten an, die es bei anderen, großen CAs gibt. Linus Neumann vom CCC nennt Let's Encrypt dennoch mit zumindest mittlerem Ernst  ""das einzige euphoriestiftende Projekt des Jahres ""."	technik
"Er ist ein Schatz, der im Verborgenen blühte. Der wahrscheinlich auf ewig im Schatten seines Nachfolgers stehen wird. Sogar unter Volvo-Fans kennen nur wahre Enthusiasten den Sport P 1900, der vor 60 Jahren Geschichte schrieb als erster schwedischer Sportwagen. Der Exot war der erste europäische Roadster mit damals futuristisch wirkender Fiberglas-Karosserie. 
Die neuartige Leichtbautechnik sollte den Volvo zum Konkurrenten der amerikanischen Corvette machen. Sie erwies sich aber als noch zu kostspielig und wurde zudem in zu kurzer Zeit realisiert. Die Folge der übereilten Entwicklung: massive Qualitätsprobleme. Nach nur 68 Exemplaren beendete Volvo die Produktion des Modells vorzeitig. Immerhin hatte der Sport P 1900 die Initialzündung zur Entwicklung des legendären Coupés P 1800 und zu leistungsgesteigerten Versionen des Buckel-Volvo PV 444 gesetzt – zwei Modelle, denen die Skandinavier den Durchbruch in Amerika verdanken. 
Statt nach New York, Genf, Frankfurt, Turin oder London zu gehen – im Jahr 1954 die internationalen Messestarts für glamouröse Sportwagen – wählte Volvo einen ungewöhnlichen Ort zur Präsentation: Warum nicht auf einen Flugplatz gehen, um einen neuen Sportler zu präsentieren? So traf sich die automobile Fachwelt an einem sonnigen Junitag auf der Startbahn des Flughafens Göteborg. 
Dort machte sich der Volvo Sport P 1900 mit mächtigem Kühlergrill nach Vorbild einer Turbine bereit. Der 1,4 Liter große Vierzylinder stammte aus dem Buckel-Volvo, war mit 51 kW (70 PS) aber leistungsstärker. Damit konnte der Motor das maximal 969 Kilogramm schwere Auto in seiner späteren Serienversion auf 150 bis 170 km/h beschleunigen. Ursprünglich sollte der P 1900 nur 1.900 Pounds, also 862 Kilogramm, wiegen – daher die Typenbezeichnung. 
Die Sensation auf dem Flugfeld Torslanda war aber die Karosserie aus glasfaserverstärktem Kunststoff: Ein Volvo-Sportwagen aus GFK? Damit hatte niemand gerechnet, stand Volvo doch traditionell für robuste und eher betuliche schwedische Volksautos. Und jetzt überholte der Göteborger Konzern sogar knapp den einheimischen Konkurrenten Saab, der ebenfalls einen Roadster vorbereitete. Der Saab Sonett konnte allerdings erst ein Jahr später vorgestellt werden und folgte dann dem Vorbild des Volvo mit einer leichten Fiberglas-Karosserie. 
Während allerdings Volvo für sein Modell eine noch bescheiden wirkende erste Serie von 300 Einheiten ankündigte, sollte der Sonett in mindestens 2.000 Einheiten gefertigt werden, wahlweise auch mit Aluminiumkarosserie. Am Ende wurden es 68 Volvo – aber nur sechs Saab Sonett Super Sport, denn Saab stoppte seine hochfliegenden Pläne für einen kostspieligen Kunststoff-Racer sofort, nachdem Volvo einen Schlussstrich unter das GFK-Kapitel gezogen hatte. Viel bewegt haben die Pioniere dennoch, vor allem der Sport P 1900, der Volvo für das folgende Jahrzehnt zu einer Marke mit stark sportlichem Image machte. 
Zunächst aber sorgte der Volvo Sport P 1900 für Aufregung. Firmenchef Assar Gabrielsson begeisterte sich 1953 für die Chevrolet Corvette mit ihrer leichten Kunststoffkarosserie, die amerikanische Antwort auf die erfolgreichen europäischen Sportwagen. Was dem Volvo-Gründer aber verborgen blieb, waren die Anlaufprobleme des amerikanischen Fiberglas-Pioniers. Die Produktion des glasfaserverstärkten Kunstharzes war anfangs zu aufwendig, zu teuer und langwierig – die Corvette war für Chevrolet zunächst kein Traumwagen, sondern ein Alptraum. 
 
Das Drama sollte sich bei Volvo wiederholen. Gabrielsson beauftragte den kalifornischen Fiberglas-Spezialisten Glasspar mit der Entwicklung eines entsprechenden Sportwagens. Während allerdings Chevrolet für die Produktionsprobleme eine Lösung fand, musste Volvo den Stecker ziehen. 
Anfangs lief noch alles nach Plan. Glasspar-Designer Bill Tritt lieferte nach wenigen Wochen einen Entwurf in schlichter Pontonform mit langer Motorhaube, gestreckter Flanke und kurzem Heck. Das Chassis hatte Volvo geliefert und dazu den PV 444 als Basis genutzt. Doch ein Problem bekamen die Spezialisten nicht in den Griff: Die Karosserie des Roadsters zeigte nach nur wenigen Kilometern Risse. Glasspar sah die Ursache für dieses Problem in dem zu schwachen Volvo-Chassis. 
Volvo hoffte die Qualitätsdefizite lösen zu können, wenn erst einmal die Karosserieproduktion nach Schweden verlegt wurde. Der Vertrag mit Glasspar umfasste nämlich auch die Ausbildung von Volvo-Mitarbeitern in der Produktion von Fiberglas-Karosserien. Tatsächlich gelang es aber nie, die Karosserie ausreichend solide zu bauen. Der im Frühjahr 1957 neu ernannte Volvo-Chef Gunnar Engellau beschloss darum nach ersten Probefahrten im P 1900 die sofortige Produktionseinstellung. 
Hinzu kamen extrem hohe Preise für den GFK-Roadster. In Großbritannien etwa kostete der Volvo Sport ab 2.100 Pfund, womit er 20 Prozent teurer war als ein ohnehin ungleich stärkerer Jaguar XK 140. Entsprechend nur tröpfchenweise liefen die Bestellungen für den Roadster bei Volvo ein – trotz mehrerer Premierenpartys, inklusive einem großen Messedebüt beim Brüsseler Salon 1955. 
Die ersten Kundenfahrzeuge wurden ab 1956 ausgeliefert, vor allem nach Nord- und Südamerika und Afrika. Dann fehlte es an Nachfrage, weshalb Volvo den Sportwagen entgegen der ursprünglichen Planung doch auf dem Heimatmarkt anbot. Der Bestelleingang blieb aber desaströs schlecht, bis zum Ende des ersten Modelljahres wurden nur 45 Wagen gebaut, im Jahr 1957 waren es weitere 23 Roadster. 
Als das Aus für Volvos Sportwagenhoffnung kam, lief allerdings schon die Entwicklung des P 1800 auf Hochtouren – dieser wurde Anfang 1960 als Weltpremiere präsentiert und ging 1961 in Serie. So wurde der unglückliche Roadster zum Start für ein begehrenswertes Coupé. Der Volvo Sport P 1900 dagegen zählt heute zu den gesuchtesten, weil seltensten Schweden-Autos."	technik
"Mimikatz hat den Abgeordneten des Bundestages einen gehörigen Schrecken eingejagt. Die frei verfügbare Software mit dem niedlichen Namen hatten Kriminelle im vergangenen Jahr genutzt, um das interne Netz des Bundestags auszuspähen. 
Die Angreifer stahlen nicht nur die Passwörter der Administratoren. Ihre Attacke legte vor allem offen, wie hilflos das Verfassungsorgan den Kriminellen ausgeliefert war: Verfassungsschutz und das Bundesamt für Sicherheit in der Informationstechnik (BSI) suchten hektisch nach den Lecks. Weil beide Behörden nicht genug Fachleute schicken konnten, musste eine private Firma aushelfen. Es dauerte drei Wochen, bis die Parlamentscomputer wieder abgedichtet waren. 
Das soll sich nicht wiederholen. Bundesinnenminister Thomas de Mazière will die Behörden komplett umbauen, die digitale Angriffe erkennen und abwehren sollen. Das geht aus einem vertraulichen Plan namens  ""Cybersicherheitsstrategie für Deutschland 2016 "" hervor, der ZEIT ONLINE und dem Deutschlandfunk vorliegt. Das Papier wird gegenwärtig zwischen den zuständigen Ministerien abgestimmt. Im Herbst soll es vom Kabinett verabschiedet werden. Dann bekäme Deutschland eine neue Sicherheitsarchitektur für den digitalen Raum. 
Entstehen soll eine größere und fast militärische Struktur aus verschiedenen Behörden, die nicht nur beraten, sondern zum ersten Mal auch schnell handeln können. Das Bundesamt für Sicherheit in der Informationstechnik, das bislang nahezu alleine kämpft, wenn es um digitale Sicherheit geht, wäre nach diesen Plänen nur noch eine von mehreren Säulen der staatlichen Cybersicherheit. Gleich drei Eingreiftruppen sollen entstehen, verschiedene Gremien und Behörden ausgebaut, Polizei, Bundeswehr, Regierung und Wirtschaft stärker miteinander vernetzt werden. 
Auf 33 Seiten beschreibt der Plan die neue Cyberstrategie. Ihr Kern: Das BSI und das Cyberabwehrzentrum des Bundes in Bonn werden stark ausgebaut. Außerdem soll eine weitere Institution gegründet werden, um sofort auf eventuelle Angriffe reagieren zu können: ein Computer Emergency Response Team (CERT) – Fachleute, die möglichst schnell Probleme analysieren und bei der Lösung helfen können. Solche CERTs gibt es schon an deutschen Universitäten und beim BSI. Nun soll ein nationales CERT entstehen. 
Wird der Plan umgesetzt, wird es in Zukunft unter dem Dach des Innenministeriums drei Arme der zivilen Cyberabwehr nebeneinander geben. Das CERT wäre so etwas wie das Lagezentrum. Dorthin könnten sich Behörden und Unternehmen wenden, die angegriffen wurden. Dort würde auch die Abwehr des Angriffs geleitet. Das BSI wäre dafür zuständig, die Methoden und Instrumente der Angreifer zu analysieren und es würde die technische Beratung übernehmen. 
Als dritter Arm würde das Cyberabwehrzentrum alle staatlichen Behörden miteinander verbinden, angefangen von BSI und Bundeswehr über Polizeien und Geheimdienste bis hin zum Zoll. Das tut es theoretisch jetzt schon. Die Bundesregierung hatte 2011 ihre erste Cybersicherheitsstrategie vorgestellt. Damals wurde das Cyberabwehrzentrum in Bonn gegründet. Doch drei Jahre später urteilte der Bundesrechnungshof, das Abwehrzentrum sei nahezu nutzlos, weil es von keiner der beteiligten Behörden ernst genommen werde. 
Damit soll es nun offensichtlich vorbei sein. Die Bundesregierung will das Abwehrzentrum mit mehr Geld und Einfluss ausstatten. Es soll Informationen über Angriffe verteilen und auch die Bundeswehr mit ihrer Cybertruppe einbeziehen. Wie genau, das bleibt allerdings unklar. Im Ministerpapier steht lediglich, dass die zivil-militärische Zusammenarbeit zwischen Abwehrzentrum und Bundeswehr neu konzipiert werden müsse. 
Der Plan des Innenministeriums sieht außerdem vor, dass drei Behörden jeweils eine digitale Eingreiftruppe aufbauen, die jederzeit ausrücken kann. Wie der Angriff auf den Bundestag gezeigt hat, ist das die bisher größte Schwäche: Keine Sicherheitsbehörde hat Mitarbeiter, die sie schnell irgendwohin schicken kann, um Rechner zu analysieren und Hacker zu jagen. 
 
Das Bundesamt für Verfassungsschutz, das Bundeskriminalamt und das BSI sollen nun jeweils eine Quick Reaction Force bekommen. Der Name stammt vom Militär und bezeichnet kleine, mobile Teams, die möglichst innerhalb von Stunden bei jedem Opfer sein können. Im Konzept des Innenministeriums steht dazu der Ausdruck  ""24/7 "" – rund um die Uhr, an jedem Tag der Woche. 
Im BSI soll dieses Team Mobile Incident Response Team (MIRT) heißen. Seine Aufgabe soll es sein, kritische Infrastrukturen zu reparieren. Die Eingreiftruppe des Verfassungsschutzes firmiert bislang unter  ""Cyber-Team "". Sie soll anrücken, wenn Geheimdienste oder Terroristen angreifen. Die Einheit des BKA, Quick Reaction Force genannt, soll Strafverfolger unterstützen und als digitale Polizei bei kriminellen Angriffen Daten sicherstellen. 
Das Problem: Es ist bei Hacks nur schwer bis gar nicht zu erkennen, wer sie mit welchem Motiv ausführt. Bis heute ist nicht klar, wer vor einem Jahr den Bundestag angegriffen hat und was er wollte. Waren es Kriminelle? Terroristen? Ein fremder Geheimdienst? Oder waren es Kriminelle im Auftrag eines Geheimdienstes, wie der Verfassungsschutz glaubt? 
Die Eingreifteams könnten sich also schon bald gegenseitig auf die Füße treten und miteinander um die Zuständigkeit rangeln. Dieser Punkt sei leider noch vollkommen ungeklärt, sagt ein Sicherheitsfachmann, der an dem Konzept beteiligt ist. 
Kritisch ist die Idee, dass das Innenministerium zusammen mit den Providern die  ""Sensorik im Netz ausbauen "" will, um Cyberangriffe und Infektionen besser erkennen zu können und laufende Angriffe abzuschwächen. Mit Sensoren im Netz könne man viele Angriffe erkennen, sagt Klaus Landefeld. Er ist Mitglied im Vorstand des Internetverbandes eco und im Beirat des DE-Cix, des weltweit größten Internetverbindungsknotens. Doch sollte das Innenministerium mit der Sensorik die sogenannte Deep Packet Inspection meinen, also das Durchsuchen aller in den Leitungen transportierten Daten, dann ist Landefeld klar dagegen.  ""Das ist nun einmal etwas, was man verfassungsrechtlich nicht will. Die Verkehre sind geschützt, man darf in die Daten nicht hineinsehen. "" Das Verbot aufzuweichen, sei gefährlich. 
Doch soll überhaupt der komplette Netzwerkverkehr automatisiert überwacht werden? Ein kleiner Zusatz im Ministerpapier legt das nahe:  ""Die Pseudonymisierung solcher Erkenntnisse ist dabei ein wirksames Mittel, um die Rechte der Betroffenen zu schützen "", heißt es dort. Übersetzt könnte das bedeuten: Alle Daten werden auf verdächtige Aktivitäten hin überprüft und die Privatsphäre indirekt geschützt, indem Klarnamen durch Nummern ersetzt werden. Eine solche Pseudonymisierung kann aber immer rückgängig gemacht werden. 
Auch bei anderen Punkten bleibt das Konzept im Ungefähren. So wird gefordert, das Strafrecht auszubauen. Neue Taten bräuchten neue Gesetze und neue Befugnisse für die Sicherheitsbehörden, schreibt das Innenministerium. Der Katalog der Straftaten, bei denen der Paragraf 100 a der Strafprozessordnung greife, müsse erweitert werden. Paragraf 100 a regelt, wann die Telekommunikation überwacht, wann Telefone abgehört, wann E-Mails mitgelesen werden dürfen. Die Cyberstrategie sagt dazu lediglich, es müssten jene Straftaten berücksichtigt werden,  ""die online und konspirativ verübt werden "". 
Was genau damit gemeint ist, wissen nicht einmal Strafrechtsexperten.  ""Angesichts der vagen Formulierungen ist schwer zu bestimmen, welche konkreten Ziele die Bundesregierung verfolgt "", sagt Tobias Singelnstein, Professor für Strafrecht und Strafverfahrensrecht an der Freien Universität Berlin. Er warnt, dass der Paragraf 100 a nur bei schweren Straftaten zum Einsatz kommen dürfe. Im Übrigen sei Cyberkriminalität gut durch bestehende Gesetze erfasst. 
Die neue Cyberstrategie sieht auch eine  ""Anpassung "" der Mitwirkungspflichten von Unternehmen vor, etwa bei der Identifizierung von Nutzern. Als Beispiel wird die Verifizierungspflicht für Prepaidhandys genannt, die mit dem jüngst vom Bundestag beschlossenen Anti-Terror-Paket eingeführt wird. Ob es ähnliche Pflichten auch für deutsche Anbieter von anonymen Internetdiensten geben könnte, lässt sich aus dem Dokument nicht direkt ableiten, vorstellbar wäre es aber. 
Ebenfalls heikel ist der Vorschlag, der Staat müsse sich stärker für private Sicherheitsdienstleister öffnen, weil es an Fachkräften mangele. Aufgaben von Polizei, Geheimdiensten und Militär zu privatisieren, ist stark umstritten. Trotzdem will das Innenministerium mehr private Sicherheitsfirmen einsetzen. Auch die Bundeswehr solle darüber nachdenken, Cybersöldner einzukaufen. Zumindest sei Unterstützung durch zivile Akteure  ""zulässig und denkbar "", heißt es in dem Konzept. 
Im Innenministerium soll außerdem eine zentrale Stelle entstehen, die Cyberwaffen beschafft und entwickelt. So zumindest kann die unscharfe Formulierung verstanden werden, nach der diese Stelle  ""technische Unterstützung für nationale Sicherheitsbehörden im Hinblick auf deren operative Cyberfähigkeiten "" leisten soll. 
Dafür würde sprechen, dass die staatliche Abwehr die gleichen Werkzeuge nutzen muss, die auch Angreifer verwenden. Das ist im Internet nicht anders als auf dem Schlachtfeld. Trotzdem gibt es viele Kritiker, die fordern, Deutschland solle sich nicht am internationalen Wettrüsten digitaler Waffen beteiligen. Diese Stabsstelle, die  ""bedarfsbezogen und zukunftsorientiert Methoden, Produkte und Strategien für die operative Umsetzung der Cyberfähigkeiten in den Sicherheitsbehörden erarbeitet "", könnte aber genau das tun. 
Solche Janusköpfigkeit zeigt sich auch beim Thema Verschlüsselung. Einerseits will das Innenministerium, dass Deutschland die besten Verschlüsselungswerkzeuge der Welt entwickelt. Andererseits sollen Geheimdienste und Polizei jedes Programm und jede Kommunikation knacken können. 
Cybersicherheit entstehe vor allem, wenn Anwender sichere Systeme einsetzten, heißt es in der Einleitung des Papiers. Viele Angriffe könnten so bereits abgewehrt werden. Wenige Absätze später wird sogar gefordert, alle Anwender müssten die Chance haben, vertrauenswürdige und sichere Systeme zu nutzen. Die Verschlüsselung privater Kommunikation müsse zum Standard werden und nicht mehr nur die Ausnahme sein. 
Gleichzeitig plant das Ministerium jedoch abseits der hier formulierten Strategie, eine weitere Behörde aufzubauen. Deren einziges Ziel: Verschlüsselte Daten zu knacken, damit Dienste und Behörden sie trotzdem lesen können. Bis 2020 sollen 400 Mitarbeiter bei dieser Zentralen Stelle für Informationstechnik im Sicherheitsbereich (Zitis) arbeiten. 
Die Cybersicherheitsstrategie erwähnt den Zitis-Plan nirgendwo, aber sie fasst diese beiden, sich widersprechenden Forderungen in einem Satz zusammen:  ""Die deutsche Kryptostrategie umfasst Sicherheit durch Verschlüsselung und Sicherheit trotz Verschlüsselung. "" 
 ""Kryptierung von Kommunikation ist eine der Grundvoraussetzungen für die Digitalisierung "", sagt Martin Schallbruch. Er war viele Jahre lang IT-Direktor im Innenministerium und dort zuständig für digitale Sicherheit. Verschlüsselung dürfe weder durch Hintertüren noch durch unsichere Algorithmen angebohrt werden. Natürlich müssten Sicherheitsbehörden auch im digitalen Raum ermitteln und dazu technisch aufrüsten, sagt Schallbruch.  ""Es ist jedoch nicht ratsam, Verschlüsselung abzuschwächen. Das würde unserer gesamten digitalen Gesellschaft auf die Füße fallen. "" 
Immerhin gibt es in dem Cyberplan auch Forderungen, die aus Sicht der Bürger und Internetnutzer uneingeschränkt positiv sind. 
So prüft die Bundesregierung demnach, ob Hersteller haftbar gemacht werden können, wenn sie Sicherheitsmängel in ihrer Software und ihrer Hardware nicht beheben. Die Industrie wehrt sich bislang mit allen Mitteln gegen solche Ansprüche. Es sei unmöglich, Software ohne Fehler zu produzieren, ist ihr Argument, Haftungsansprüche würden die Entwicklung neuer Produkte verhindern und Firmen ruinieren. 
Ähnlich schwer erreichbar scheint ein anderes Ziel. Digitale Bildung  ""muss zu einem festen Bestandteil des Bildungskanons werden "", steht in dem Konzept.  ""Jede Schulabgängerin und jeder Schulabgänger sollte Grundkenntnisse von Informatik haben. "" 
Das zu erreichen, ist ein weiter Weg. Wie überhaupt der ganze Plan an einem Problem krankt: Es gibt in Deutschland gar nicht genug Fachleute, um all die gewünschten Gremien zu besetzen. Sie auszubilden, wird Jahre dauern. Vielleicht steht in dem Cyberkonzept deshalb auch keine Jahreszahl, bis wann eines der genannten Ziele erreicht sein soll. 
Mehr dazu finden Sie auch beim Deutschlandfunk. 
Haben Sie Informationen zu diesem Thema? Oder zu anderen Vorgängen in Politik und Wirtschaft, von denen die Öffentlichkeit erfahren sollte? Wir sind dankbar für jeden Hinweis. Dokumente, Daten oder Fotos können Sie hier in unserem anonymen Briefkasten deponieren."	technik
"Die Debatte über die Sicherheit von Assistenzsystemen, die Autofahrern das Steuern des Wagens abnehmen sollen, ist neu befeuert: Wie erst jetzt bekannt wurde, starb bereits am 7. Mai der 40-jährige Fahrer eines Tesla Model S in Florida, als sein Wagen auf trockener Straße mit einem Sattelschlepper zusammenstieß. Bei dem Unfall steuerte der Computer den Pkw. 
Das Elektroauto war mit dem aktivierten Autopilot auf einer mehrspurigen Schnellstraße unterwegs, als ein Lkw an einer Kreuzung die Fahrspur des Model S querte, wie Tesla mitteilte. Im hellen Tageslicht hätten weder die Sensoren des Autopiloten noch der Fahrer die weiße Flanke des Aufliegers wahrgenommen – darum sei im Model S die Bremse nicht betätigt worden. Der Wagen sei unter den Lkw geraten, dabei habe dessen Unterkante die Windschutzscheibe des Teslas getroffen. Der Wagen kam von der Straße ab und kollidierte mit einem Strommast. 
Tesla-Gründer Elon Musk erklärte den Angehörigen des Toten auf Twitter sein Beileid. Noch im April hatte der nun tödlich verunglückte Fahrer den Autopiloten gelobt, nachdem dieser einen Unfall mit einem Lkw auf einer Schnellstraße verhindert habe. 
 
Tesla informierte nach eigenen Angaben unmittelbar nach dem Unfall die US-amerikanische Verkehrssicherheitsbehörde NHTSA. Diese hat Ermittlungen aufgenommen: Der Vorfall verlange eine  ""Untersuchung des Designs und der Wirkungsweise "" aller Fahrassistenzsysteme zum Zeitpunkt des Unfalls, teilte die NHTSA mit. Sie will zudem rund 25.000 Model S überprüfen, die mit dem Autopiloten ausgestattet sind. Dies ist der erste Schritt, bevor die Behörde einen Rückruf anordnen kann, wenn sie Autos für unsicher hält. 
Es ist laut Tesla der erste tödliche Unfall mit dem System. Allerdings berichteten Kunden schon vorher immer wieder über mehrere Beinahe-Unfälle, davon zeugen auch selbst gedrehte Videos auf YouTube. Erst kürzlich wurde ein Fall aus der Schweiz bekannt, wo ein Model S auf einen Lieferwagen auffuhr, weil der Autopilot nicht bremste. 
Wird das System eingeschaltet, hält der Wagen auf mehrspurigen Straßen außerhalb von Ortschaften sich automatisch zwischen den Fahrbahnmarkierungen auf der Spur und zugleich konstant einen sicheren Abstand zum Fahrzeug vor ihm. Das System kann beschleunigen, die Spur wechseln und bremst im Notfall. 
Für die Funktionen nutzt das System Radar- und Kameratechnik an der Front des Wagens sowie zwölf Ultraschallsensoren für eine Rundumüberwachung. Bei dem tödlichen Unfall in Florida könnte für Kameras der weiße Anhänger vor einem hell ausgeleuchteten Himmel eine Herausforderung gewesen sein, für den Radar sollte das eigentlich kein Problem darstellen. Die Fahrzeuge zeichnen während der Fahrt alle möglichen Daten auf und übertragen sie an Tesla. Das Unternehmen nutzt diese Informationen dazu, die Technik zu verbessern. 
Tesla verbaut die Hardware schon seit Herbst 2014 serienmäßig in den Fahrzeugen; die für den Autopiloten notwendige Software bietet der kalifornische Autobauer seit Oktober 2015 an. Um sie nutzen zu können, muss der Kunde einen Aufpreis zahlen. Dann wird die Software freigeschaltet. Die Bezeichnung  ""Autopilot "" ist allerdings irreführendes Tesla-Marketing: Ein vollautomatisiertes Fahren, das ohne menschlichen Fahrer auskommt, ist aus rechtlichen Gründen noch nicht erlaubt. Womöglich führt die Bezeichnung aber dazu, dass sich Fahrer in falscher Sicherheit wiegen. 
 
Juristisch ist Tesla wohl auf der sicheren Seite. Das Unternehmen weist die Kunden, die den Autopiloten bestellt haben, vor der Freischaltung ausdrücklich darauf hin, dass sie beim Fahren mit  ""Autopilot "" trotzdem jederzeit eingreifen und die Technik überstimmen können müssen. Das heißt: Sie dürfen dem Auto nicht vertrauen, sondern müssen weiter selbst auf den Verkehr achten. Bei der Vorstellung der Software im Herbst 2015 betonte Tesla-Chef Musk, dass bei einem Unfall mit Autopilot immer der Fahrer hafte. Damit folgt Tesla der Wiener Verkehrskonvention, einem internationalen Abkommen: Es schreibt vor, dass der Fahrer sein Fahrzeug stets unter Kontrolle haben müsse – also auch die Assistenzsysteme. 
Laut Tesla wird der Nutzer beim Einschalten des Autopiloten während der Fahrt außerdem darauf hingewiesen, dass er die Kontrolle und Verantwortung über das Fahrzeug behalten müsse. Darum nennt das Unternehmen den Autopiloten auch nur ein  ""Komfortmerkmal "": Er soll den Fahrer unterstützen, nicht ersetzen. Das System erlaubt allerdings, bei eingeschaltetem Autopiloten die Hände vom Lenkrad zu nehmen – auch über einen längeren Zeitraum. 
Tesla selbst gibt an, seine Autos seien bis zu diesem ersten tödlichen Unfall bereits mehr als 130 Millionen Meilen (rund 210 Millionen Kilometer) mit eingeschalteter Autopilotfunktion gefahren. Im Schnitt sei es damit immer noch sicherer als andere Autos. Denn im gesamten Straßenverkehr der USA kommt ein Todesfall auf nur 94 Millionen gefahrene Meilen. 
Das Unternehmen lebt vom Image, den etablierten Autoherstellern voraus zu sein. In den USA kam bereits die Frage auf, ob Tesla den Autopiloten zu früh auf den Markt gebracht habe. Das Unternehmen selbst spricht von der  ""Betaversion "" des Systems. Damit bezeichnen Softwareentwickler eine frühe Fassung eines Programms, die zu Testzwecken veröffentlicht wird, um Fehler in der Praxisanwendung aufzuspüren. 
Allerdings gibt es Fahrassistenten, die dem Tesla-Autopiloten ähnlich sind, auch in Fahrzeugen anderer Hersteller, etwa bei Mercedes-Benz und BMW. Auch diese weisen die Kunden ausdrücklich darauf hin, dass sie die Systeme permanent überwachen und im Notfall eingreifen müssen. 
Nach Bekanntwerden des Unfalls fielen Tesla-Aktien am US-Aktienmarkt zeitweise um mehr als drei Prozent. Analysten gehen aber nicht davon aus, dass der Unfall dem Unternehmen dauerhaft schaden wird. Schließlich musste man immer damit rechnen, dass es irgendwann einen tödlichen Unfall geben würde – auch modernste Technik garantiert keine völlige Sicherheit. 
Experten befürchten, dass der tödliche Tesla-Unfall Vorbehalte gegen selbstfahrende Autos verstärkt. Kritiker sehen den Crash als klares Zeichen dafür, dass die Technik des automatisierten Fahrens eben noch nicht so weit sei, wie manche Hersteller glauben machen wollten. So trifft der Unfall auch Google: Der US-Konzern arbeitet ebenfalls an selbstfahrenden Autos. Bei Google gab es in diesem Jahr auch bereits einen Unfall mit einem vom Computer gesteuerten Wagen: Er kam bei niedriger Geschwindigkeit einem Bus in die Quere, der Fahrer blieb aber unverletzt. 
Das Aus für selbstfahrende Fahrzeuge bedeutet der Unfall aber natürlich nicht. Die Hersteller weltweit arbeiten weiter an vollautomatisch fahrenden Autos, auch in Deutschland. Experten betonen, dass nach wie vor die Technik die Chance bietet, den Straßenverkehr sicherer zu machen und die Zahl der Verkehrstoten zu senken."	technik
"Meine Ein-Mann-Crypto-Party beginnt bei einem Computerhändler in Berlin. Ich kaufe ein gebrauchtes ThinkPad T400 von Lenovo, ohne Betriebssystem, für 250 Euro. Was ich für dieses Projekt an Diensten und Software benötige, bekomme ich – mit einer Ausnahme – kostenlos. 
ThinkPads sollen sich generell gut mit Ubuntu vertragen. Ubuntu ist die derzeit beliebteste Linux-Distribution, kann kostenlos oder für einen selbst festzulegenden Betrag heruntergeladen werden und gilt als vergleichsweise einsteigerfreundlich. Das muss sie auch sein, denn ich habe keine Ahnung von Linux. Aber da ich vor allem Open-Source-Software nutzen will, komme ich an Linux nicht vorbei. Zudem kann das System getrost als sicher vor Viren und Trojanern bezeichnet werden. Selbst das sonst sehr auf Vorsicht bedachte Bundesamt für Sicherheit in der Informationstechnologie schreibt:  ""Die Installation eines Virenschutzprogramms ist, basierend auf dem aktuellen Stand der Bedrohungslage in Bezug auf Schadsoftware für Linux, unter Ubuntu nicht notwendig. "" 
Das ist natürlich kein vollständiges Ubuntu-Handbuch, das würde den Rahmen eines Artikels sprengen. Hier geht es um die Installation der aktuellen Version 12.10 alias Quantal Quetzal und darum, den Grundstein für die weiteren Kapitel dieser Serie zu legen. Eine ausführliche Bedienungsanleitung gibt es beim Ubuntu-Sponsor Canonical. (Als ebenfalls einsteigerfreundliche Alternative empfehlen Linux-Kenner übrigens das auf Ubuntu basierende Linux Mint.) 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
Die Installation 
Es folgt eine Schritt-für-Schritt-Anleitung für die Ubuntu-Installation von einem USB-Stick aus. Wer etwas nicht versteht, findet vielleicht auf der offiziellen, englischen Ubuntu-Website ubuntu.com oder im deutschen Wiki der Ubuntu-Community Hilfe. Ich selbst habe beide gebraucht und musste Ubuntu insgesamt dreimal installieren, weil ich immer mal wieder an einen Punkt kam, an dem ich zunächst nicht weiterwusste. 
Zu Beginn muss der USB-Stick soweit vorbereitet werden, dass der Computer nachher die Installationsdatei für das Betriebssystem darauf erkennt und ausführt – er muss bootfähig gemacht werden. Das klingt komplizierter, als es ist. Ich habe zwar selbst eine Stunde gebraucht, um zu verstehen, was ich tun muss, aber als ich es begriffen hatte, ging es ganz schnell. 
Benötigt werden die Ubuntu-Installationsdatei, die auch kostenlos heruntergeladen werden kann, sowie das kleine Programm Universal USB Installer. Beide lade ich mir von einem Windows-PC auf den USB-Stick. Den Universal USB Installer führe ich dann direkt aus, während der Stick noch am Windows-PC steckt. In dem kleinen Fenster des Installers wähle ich aus dem Menü den Punkt Ubuntu 12.10, dann die Ubuntu-ISO-Datei auf dem Stick und schließlich das USB-Laufwerk des Windows-Rechners als Ziel aus. Nach einem Klick auf Create entsteht auf meinem Stick die Art von Datei, die das ThinkPad später nutzen kann, um Ubuntu auf seiner leeren Festplatte zu installieren. 
Der nächste Schritt erfordert von Laien ein wenig Mut. Wer ein Betriebssystem von einem USB-Stick (oder einer DVD) aus installieren will, muss beim Hochfahren des Rechners ins BIOS wechseln, in die sogenannte Firmware. In meinem Fall funktioniert das mit der Taste F1, bei anderen Rechnern ist es häufig die Entfernen-Taste (Del) oder F2. Das BIOS (Basic Input / Output System) sieht ein wenig gruselig aus, und das ist auch gut so. Wer hier etwas falsch macht, kann seinen Rechner anschließend vielleicht nicht mehr benutzen. 
Im BIOS ändere ich die Boot-Reihenfolge. Das bedeutet, ich lege neu fest, auf welchem Laufwerk zuerst nach einem Betriebssystem gesucht wird. Die Festplatte des ThinkPads wäre in diesem Fall falsch, denn dort gibt es ja noch keines. Stattdessen muss das USB-Laufwerk an die erste Stelle gerückt werden, bevor die Installation fortgesetzt werden kann. Wer vor so einem Schritt zurückschreckt, fragt am besten jemanden, der Erfahrung damit hat. 
Der Installationsassistent von Ubuntu selbst ist dann weitgehend selbsterklärend und anfängerfreundlich. Ich wähle Deutsch als Sprache aus und klicke auf Ubuntu installieren. Die Alternative wäre, Ubuntu erst einmal nur zu testen, das System wird dann vom USB-Stick aus hochgefahren, damit sich der Nutzer einen ersten Eindruck verschaffen kann. 
Im nächsten Fenster empfiehlt es sich, das Häkchen bei Aktualisierungen während der Installation herunterladen anzuklicken. Das geht jedoch nur, wenn der Rechner mit dem Internet verbunden ist – am besten über ein LAN-Kabel. 
Im folgenden Schritt wählen Laien einfach Festplatte löschen und Ubuntu installieren aus. Dieser Wortlaut erscheint nur, wenn noch kein Betriebssystem installiert wurde, der Rechner also komplett  ""leer "" ist. 
Wer jetzt die Option Zur Sicherheit verschlüsseln sieht, sollte sich den nächsten Schritt genau überlegen. Denn die Verschlüsselung der Festplatte hat zur Folge, dass bei jedem späteren Start des Rechners das Passwort zur Entschlüsselung eingegeben werden muss. Eines von vielen neuen Passwörtern, die im Laufe dieses Projekts erstellt werden. Wer es vergisst, hat eine unbrauchbare Festplatte und keine Chance mehr, auf seine Daten zuzugreifen. 
Wer aus Prinzip ein Maximum an Sicherheit bevorzugt, kann diesen Schritt aber natürlich gehen. Es ist eine Abwägungssache, Bequemlichkeit gegen maximale Sicherheit, wie so häufig. Ich entscheide mich für etwas mehr Bequemlichkeit, weil ich lieber nur ausgewählte Dateien verschlüssele. Wie das mit der Software TrueCrypt möglich ist, erkläre ich im letzten Teil dieser Serie. Der Nachteil: Ich gehe damit ein gewisses, für Laien wie mich schwer überschaubares Risiko ein, dass private Informationen, Dateien oder ihre Spuren für jemanden zugänglich bleiben, der den Rechner in die Hände bekommt. Mehrere Experten jedenfalls haben meine Entscheidung, nur auf TrueCrypt zu vertrauen, kritisiert und empfehlen dringend, die gesamte Festplatte zu verschlüsseln. 
Fortgeschrittene können zuvor noch manuell die Festplatte partitionieren. Wer zum Beispiel ein zweites Laufwerk haben möchte, um dort Bilder oder Dokumente zu speichern, muss eine neue Partitionstabelle anlegen. Ich gebe zu, davon verstehe ich zu wenig. Ich versuche es trotzdem, mithilfe zweier Anleitungen im Ubuntuuser-Wiki und einer E-Mail an Jörg Thoma, Linux-Spezialist bei golem.de. Das Ergebnis, vom Experten Thoma abgenickt, ist diese Tabelle: Eine großzügig ausgelegte Partition für das Betriebssystem, eine ebenfalls großzügige Swap-Partition (für ein Back-up des Arbeitsspeichers, wenn das ThinkPad mal im Ruhezustand ist) und der Rest für persönliche Daten. 
Der Rest der Installation ist ein Kinderspiel. Berlin wird allen deutschen Nutzern als Standort angezeigt, ich sehe keinen Grund, das zu ändern. Die Tastaturbelegung sollte man anschließend auf Deutsch setzen. Abschließend wähle ich einen Namen für den Rechner, sowie Benutzernamen und Passwort. Ein paar Minuten später ist die Installation beendet. Nun ist noch ein Neustart nötig – bei dem ich die Boot-Reihenfolge im BIOS wieder ändere – und dann sehe ich endlich die Ubuntu-Oberfläche vor mir. 
Und dort erkenne ich als erstes ein Amazon-Icon. Es handelt sich um eine Neuerung in Ubuntu, die für viel Unmut in der Linux-Community sorgt. Benutzt jemand die Suchfunktion von Ubuntu, werden die Suchbegriffe über einen Umweg dazu verwendet, passende Ergebnisse von Amazon einzublenden. Das ist aus mehreren Gründen, die hier erläutert werden, kritisch zu sehen. Die Kurzform: Diese Funktion ist mit dem Ziel, einen privatsphärefreundlichen Rechner zusammenzustellen, nicht vereinbar. Bei YouTube gibt es aber ein Video, in dem ein junger Mann erklärt, wie sich diese Verbindung zu Amazon einfach und dauerhaft kappen lässt. 
Die Installation ist damit abgeschlossen, nun ist es Zeit, sich an die Ubuntu-Oberfläche zu gewöhnen, sich mit der Bedienung vertraut zu machen und persönliche Einstellungen festzulegen. Das sei aber dem Spieltrieb jedes Einzelnen überlassen. Noch einmal der Hinweis: Eine ausführliche Bedienungsanleitung gibt es hier. 
Wichtig ist noch, das sogenannte Terminal in die Startleiste auf der linken Bildschirmseite zu holen, denn das wird in den weiteren Kapiteln häufiger gebraucht. Dazu klicke ich oben links das Symbol für die Dash-Startseite an, gebe in die Suchmaske Terminal ein und ziehe das entsprechende Symbol in die Startleiste. 
Der Alltag mit Ubuntu 
Das Erfreuliche zuerst: Mein ThinkPad fährt in wenigen Sekunden hoch. Da kann mein altes, zugegebenermaßen vollgestopftes Windows-Notebook nicht mithalten. Und um die Aktualisierung von Virenscannern und Firewalls muss ich mir keine Gedanken mehr machen. 
Was es zum alltäglichen Umgang mit Ubuntu zu wissen und zu sagen gibt, würde den Rahmen dieses Artikels sprengen. Wichtig für dieses Projekt ist, dass ich Programme finden, herunterladen, installieren und benutzen kann. Das alles bekomme ich auch ohne größere Probleme hin, da die Ubuntu-Oberfläche zwar neu für mich ist, aber keineswegs undurchschaubar. 
Die Nachteile: Nicht alles, was ich bislang auf meinem Windows-Rechner gemacht habe, kann ich auch ohne Weiteres mit Ubuntu umsetzen. Ich brauche immer mal wieder Hilfe aus den Ubuntu-Wikis. Es dauert außerdem eine Weile, bis ich mich an die Befehlseingabe im Terminal gewöhnt habe. Die wirkt immer so, als könne man im Zweifel etwas kaputtmachen. Was durchaus vorkommt, wie ich bei einem zunächst missglückten Versuch, etwas zu deinstallieren, lernen musste. 
Deshalb der Hinweis: Wer Linux zwar ausprobieren, aber nicht zum alleinigen Betriebssystem machen möchte, findet beispielsweise bei Spiegel Online eine ausführliche Anleitung, wie man Ubuntu als zweites System neben Windows installiert."	technik
"Sion nimmt es gleich mit mehreren Fragen zur Mobilität der Zukunft auf. Zwei elektrisch angetriebene Kleinbusse bieten in dem 33.000-Einwohner-Städtchen im Schweizer Kanton Wallis nicht nur lokal emissionsfreie Fahrten, sondern fahren dank Stereokameras, GPS-System und eingebauten Lidar-Sensoren auch autonom. Das Ziel: maximale Flexibilität des Shuttleverkehrs. Der Betreiber PostAuto setzt die beiden fahrerlosen Busse mit je elf Sitzplätzen nämlich nicht nach regulärem Fahrplan ein, sondern sie werden je nach Bedarf mit dem Smartphone angefordert. 
Die von einem französischen Hersteller gebauten Fahrzeuge ersetzen keine bereits fahrenden Busse. Sie sind nur 4,80 Meter lang und 2,05 Meter breit und ergänzen aufgrund ihrer Wendigkeit und der kompakten Bauweise das existierende Streckennetz um bisher nicht befahrene Routen auf engen Straßen. Ihre Tour durch die Fußgängerzone und das Stadtzentrum von Sion ist genau vermessen und in 3D-Karten erfasst. Hindernisse und gefährliche Situationen sollen die mit Elektronik vollgestopften Busse in Sekundenbruchteilen erkennen. 
In der zwei Jahre laufenden Testphase, die im März begonnen hat, rollen die Busse mit rund 20 km/h statt der erreichbaren Höchstgeschwindigkeit von 45 km/h durch die Stadt und später auch zur weiter entfernt liegenden Touristenattraktion Schloss Tourbillon. Aber funktioniert das Selbstfahrsystem auch unter erschwerten Bedingungen? Bei Regen, Schnee und Dunkelheit? Wie reagiert das System in unfallträchtigen Situationen, wenn Fußgänger plötzlich zur Seite springen, Radfahrer einen unerwarteten Schlenker machen, Hunde vor den Bus laufen? 
Genau wegen dieser Fragen ist Sion für Verkehrsexperten und Programmierer ein faszinierendes Labor, in dem man Schwachstellen der für autonomes Fahren entwickelten Algorithmen sondieren und weiterentwickeln kann. Tatsächlich kam es bei ersten Testfahrten zu einigen kuriosen Zwischenfällen, die allerdings harmlos verliefen: Eine vor den Bus gewehte Plastiktüte, die von den Kameras offenbar als gefährliches Objekt wahrgenommen wurde, löste eine abrupte Notbremsung aus; außerdem sorgte ein Mann mit einer Abfallkarre, der zu dicht am Bus die Fahrbahn überquerte, für einen weiteren Notstopp. 
Die Betreiber haben daraufhin den Bussen weitere Sensoren implantiert. Mit ihnen können Objekte leichter identifiziert werden, um unnötige Bremsmanöver zu verhindern. 
Die Eliminierung solcher Systemdefizite ist Kern der Testphase. Schließlich sollen Unfälle wie im Silicon Valley, wo ein Google-Auto mit allzu forscher Autonomie in einen Bus krachte, weil es auf haltende Busse und das eigene Weiterfahren programmiert war, vom System in Sion verhindert werden. Für die Passagiere besteht keine Gefahr. Ein PostAuto-Fahrbegleiter ist immer an Bord, um den Fahrgästen das Prozedere im fahrerlosen Bus ohne Lenkrad, Brems- und Gaspedal zu erklären und in eventuellen kritischen Situationen den roten Alarmknopf zu drücken. 
Ängstliche Naturen registrieren beruhigt, dass man im Postbus meistens langsamer unterwegs ist als mit 20 km/h – doch das Schneckentempo wird gelegentlich auch kritisch kommentiert:  ""Da gehe ich doch lieber gleich zu Fuß oder nehme das Fahrrad, weil der Bus zu oft vor irgendwelchen Hindernissen bremst und eh zu langsam fährt "", meinte etwa ein Student. 
 
Von außen schaut man gerade wegen der eingesetzten Selbstfahrtechnik auf das Sion-Projekt. Schließlich spielen mögliche Schwachpunkte solcher Systeme in aktuellen Debatten über autonomes Fahren eine große Rolle. So monierte etwa Missy Cummings, Professorin für Robotertechnik an der Duke University in North Carolina, im März in einer Senatsanhörung in Washington, dass es noch zu viele technische Probleme mit Sensoren und Kameras gebe: Sie funktionierten bei Dunkelheit, Regen und Schnee oder bei fehlenden Fahrbahnmarkierungen nicht richtig und seien  ""absolut ungeeignet "" für einen Einsatz in schnellen Automobilen. Auch das Erkennen von Verkehrszeichen und Ampeln sei für Sensoren und Kameras schwierig, darum müsse man mit der gesetzlichen Zulassung autonom fahrender Autos wohl erst für 2035 oder noch später rechnen, so Cummings. 
Jörn Meier-Berberich, Experte für autonomes Fahren beim Berliner Mobilitätsmanagement-Unternehmen team red, mahnt dagegen zu Eile.  ""Ein Blick auf die Statistik mit den verheerenden Unfallzahlen verdeutlicht doch, wie dringend eine technische Aufrüstung von Sicherheitssystemen und die baldige Zulassung autonomer Systeme ist "", sagt Meier-Berberich und verweist darauf, dass in über 90 Prozent der Unfälle mit Verkehrstoten menschliches Versagen die Hauptursachen gewesen sei.  ""Mit autonomen Fahrsystemen würde diese Rate drastisch reduziert. "" 
Solche kritisch geführten Debatten über die Zuverlässigkeit der Systeme betreffen das Sion-Projekt allerdings nur am Rande. Die autonom fahrenden Kleinbusse rollen mit einer auf zwei Jahre befristeten Sondergenehmigung der Stadt auf eng begrenzten und genau vermessenen Arealen und nicht auf Landstraßen. Darum sind sie keine Messlatte etwa für die im Silicon Valley fahrenden Google-Autos. Trotzdem kann man einige im Praxistest gewonnene Erkenntnisse später wohl auf autonom fahrende Pkw übertragen. 
Raphael Gindrat sieht in dem Experiment indes vor allem einen Test für ein neues On-Demand-System, das zwischen individuell zugeschnittenem und öffentlichem Nahverkehrssystem angesiedelt ist. Gindrat ist Chef von BestMile, einem Start-up, das er zusammen mit anderen Absolventen der ETH Lausanne gegründet hat; der Spezialist für Flottenmanagement betreut das Selbstfahrprojekt in Sion. 
 ""Wir wollen herausfinden, wie das autonome Postbus-Shuttlesystem auf Kurzstrecken als ergänzendes, flexibles Angebot in den öffentlichen Nahverkehr integriert werden kann und wie das Publikum darauf reagiert "", sagt BestMile-Sprecherin Maud Simon. Dafür muss der vor Kurzem erst gestartete Test noch eine Weile laufen. Simons Chef Raphael Gindrat ist aber zuversichtlich: Er glaubt, dass der autonom fahrende, flexible Service sich bald auch in anderen Städten durchsetzen werde. Das System bietet nämlich einen weiteren großen Vorteil: die ferngesteuerte Überwachung der autonom fahrenden Busse. Bei Unfällen oder Staus könnte man leicht auf andere Routen umdirigieren. Ein Problem, das sich in der Innenstadt von Sion vermutlich erst einmal nicht stellt."	technik
"David Petraeus kann ein Lied davon singen, wie schwierig es ist, anonym per E-Mail mit jemandem zu kommunizieren. Der US-General trat zurück, weil er genau das nicht schaffte. Das FBI deckte seine außereheliche Affäre anhand seiner E-Mails auf, obwohl er und seine Geliebte einige Vorsichtsmaßnahmen getroffen hatten, um nicht ertappt zu werden. 
Die Bürgerrechtsorganisation Electronic Frontier Foundation (EFF) hat eine Anleitung veröffentlicht, die beschreibt, was wirklich nötig ist, um unerkannt E-Mails auszutauschen. Sie heißt Don’t be a Petraeus und richtet sich an Menschen, die miteinander in Kontakt bleiben wollen, ohne dass für Dritte nachvollziehbar wäre, wer hier kommuniziert. 
Das müssen nicht unbedingt ein General und seine Geliebte sein, auch ein Journalist und ein Informant können diesen Weg nutzen, wenn sie über einen längeren Zeitraum einen anonymen Kanal zum Informationsaustausch brauchen. 
Die Installation 
Der erste Schritt ist die Installation und Inbetriebnahme des Tor Browser Bundles, wie im vorherigen Teil dieser Serie beschrieben. Wichtig ist, von nun an ausnahmslos immer Tor zu benutzen. Andernfalls ist die Anonymität der E-Mail-Kommunikation nicht gewährleistet. Das Tor Browser Bundle verschleiert die IP-Adresse eines Rechners im Netz und verhindert, dass der Internetprovider speichern kann, welche Seiten der Nutzer ansteuert. In diesem Fall heißt das: Niemand weiß überhaupt, dass er oder sie einen Webmail-Dienst aufgerufen hat. 
Ich benutze in diesem Fall den von der EFF empfohlenen kanadischen Webmail-Anbieter Hushmail. Der erlaubt es, sich über Tor zu registrieren und bietet außerdem standardmäßig eine HTTPS-Verschlüsselung – eine seltene Kombination, wie die EFF schreibt. Ich richte mir dort also ein kostenloses E-Mail-Konto ein. Das heißt, ich versuche es. Beim ersten Mal bekomme ich angezeigt, dass mein Computer von Hushmail blockiert wird,  ""wahrscheinlich wegen Missbrauchs des Dienstes oder wegen Spam "", wie es heißt. Ich starte Tor neu und greife dadurch mit einer neuen IP-Adresse auf Hushmail zu. Dieses Mal funktioniert es. 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
Ich wähle einen Benutzernamen, der keine Rückschlüsse auf meine Person zulässt. Das heißt, er sollte am besten kein deutsches Wort enthalten, nichts mit meinen Hobby oder gar Namen zu tun haben und keine Absicht erkennen lassen, wozu das Konto genutzt wird. Außerdem wähle ich ein starkes Passwort. 
Die anonyme Kommunikation funktioniert nun so: Wenn ich mich über Tor bei Hushmail einlogge und dort eine Nachricht verfasse und im Entwurfsordner ablege, kann jemand anderes, der von mir die Zugangsdaten zum Konto bekommen hat, sie dort lesen. Versendet wird dabei nichts. Die Zugangsdaten werden am besten bei einem persönlichen Treffen ausgetauscht. 
Die EFF empfiehlt zusätzlich, nicht das heimische Netzwerk, sondern ein öffentliches WLAN oder ein Internetcafé zu benutzen. Zu diesem Zweck empfiehlt es sich, Tor auf einem USB-Stick einzurichten. Wie das geht, wird zum Beispiel hier und hier beschrieben. 
Kleiner Bonus: Hushmail bietet die Möglichkeit, Mails mit einem Klick in einer Checkbox zu verschlüsseln – für den Fall, dass man doch mal etwas versendet. Die E-Mails sind damit für jemanden, der sie unterwegs abfängt, nicht zu entziffern. Der E-Mail-Header, also unter anderem Absender, Empfänger und Betreff werden allerdings nicht verschlüsselt. 
 
Wer ein kostenloses Hushmail-Konto anlegt, muss sich mindestens alle drei Wochen einmal einloggen, andernfalls wird das Konto gelöscht. Es muss aber auch nicht Hushmail sein. Letztlich eignet sich jeder Dienst, der Daten speichert, also zum Beispiel auch Dropbox. Die Daten von EU-Bürgern sind aber bei US-Dienstleistern nicht vor staatlichen Überwachungsmaßnahmen sicher, weshalb ein kanadischer Anbieter möglicherweise die bessere Wahl ist. 
Die Experten von JonDos raten allerdings auch von Hushmail ab, weil Hushmail unter anderem eine Reihe von Daten über die Aktivitäten auf der Webmail-Seite 18 Monate lang speichert. 
Hushmail bietet zudem keine vollständige End-to-end-Verschlüsselung. Das bedeutet: Den verschlüsselten Inhalt einer versendeten Mail kann und muss Hushmail bei einem entsprechenden richterlichen Beschluss entschlüsseln und an die Behörden übergeben, ebenso wie gespeicherte Metadaten wie etwa den genauen Zeitpunkt des Ein- und Ausloggens. Die EFF weist darauf hin, dass dies in der Vergangenheit schon vorgekommen ist. Zwar brauchen Strafverfolger überhaupt erst einen Verdacht, aber ich möchte diesen Extremfall zumindest nicht unerwähnt lassen. 
Disziplin ist notwendig 
Grundsätzlich ist die Lösung der EFF nicht weiter kompliziert. Das Problematische daran ist, dass keiner der Beteiligten unachtsam sein darf. Verzichtet ein Teilnehmer auch nur einmal auf den Einsatz von Tor, taucht seine echte IP-Adresse in den Log-Dateien von Hushmail auf. Das kann schon zur Identifizierung genügen, wenn staatliche Stellen denn einen Grund für derart aufwendige Ermittlungen haben. Auch der Inhalt der einzelnen Botschaften sollte keine Klarnamen und sonstigen Hinweise auf die wahre Identität beinhalten – was von den Kommunikationspartnern viel Disziplin erfordert. 
Für jemanden, dessen E-Mail-Korrespondenz überaus brisant sein könnte, heißt das alles: Eine End-to-End-Verschlüsselung von E-Mails, die höchstens noch auszuhebeln ist, wenn jemand Zugang zum Computer des Senders oder Empfängers hat, erfordert eine andere Lösung. Und die ist technisch anspruchsvoller. Sie wird im nächsten Kapitel der Serie beschrieben, in dem es um die Verschlüsselung von E-Mails mit OpenPGP geht."	technik
"Das Tor-Netzwerk macht es einem nicht leicht, Spaß an der Anonymität zu haben. Nur weil ich im Internet unerkannt bleiben will, kann ich plötzlich keine Videos mehr gucken? Zum Glück gibt es noch einen anderen Weg, zumindest meinem Internetprovider und den Betreibern von Websites nicht zu verraten, wo ich mich im Netz bewege: das Surfen über ein Virtual Private Network, kurz VPN, zusammen mit Browser-Erweiterungen wie Ghostery, NoScript und HTTPS Everywhere, die in dieser praktischen Auflistung für alle bekannteren Browser beschrieben werden. 
Das Prinzip des VPN: Über eine verschlüsselte Verbindung, Tunnel genannt, schickt der Internetnutzer seine Anfrage an den Server des VPN-Provider. Der leitet diese Anfrage an die Zielseite im Internet weiter, verpasst dem Nutzer dabei aber eine neue, anonyme IP-Adresse. Der große Vorteil: Alles, was von meinem Rechner abgeht, steckt in diesem sicheren Tunnel und ist von außen nicht einsehbar – Passwörter oder die Adresse eines Firmen-Intranets zum Beispiel. Somit ist VPN bestens geeignet, um auch im offenen WLAN eines Cafés oder einer Konferenz zu surfen. Dafür muss ich als Nutzer meine Daten über eine Zwischenstation schicken, den VPN-Provider. Und dem muss ich vertrauen können, dass er meine Daten für sich behält. Der VPN-Provider nämlich weiß, wer ich bin und was mich im Internet interessiert. 
Aber für welchen der Dutzenden Anbieter soll ich mich entscheiden? Ich bitte Michael Horn um Rat. Er ist unter dem Spitznamen nibbler bekannt und beschäftigt sich schon seit Jahren mit sicherer Kommunikation. Sein persönlicher Favorit unter den VPN-Providern: IPredator aus Schweden. (Die alte Version der Website findet sich hier.) Dahinter stecken die Macher des BitTorrent-Trackers The Pirate Bay, der wiederum von Mitgliedern des schwedischen Think Tanks Piratbyrån gegründet wurde, der auch Vorbild für die schwedische Piratenpartei war. Auch wenn Politik und Pirate Bay getrennte Wege gehen, darf man durchaus annehmen, dass die Betreiber von IPredator auch eine politische Motivation haben, nämlich den bestmöglichen Schutz der Anonymität ihrer Nutzer. 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
 ""Unterm Strich hat jeder VPN-Anbieter auch Nachteile "", sagt Michael.  ""Mal ist es die Jurisdiktion, der das Unternehmen durch den Standort seiner Server unterworfen ist, mal ist es die Anwendung selbst. "" Die Vorteile von IPredator sind für ihn erstens das Versprechen, nur das an Nutzerdaten zu speichern, was man selbst bei der Registrierung angibt. Zweitens ist es die komplexe Firmenstruktur, die im Extremfall den staatlichen Zugriff auf alle Daten erschwert. Drittens ist es der akzeptable Preis von 18 Euro für drei Monate. Viertens ist es die Möglichkeit, auf mehreren Wegen zahlen zu können. 
Fünftens unterstützt IPredator den Open-Source-Standard OpenVPN. Außerdem sei IPredator recht groß, sagt Michael, da falle der einzelne Nutzer weniger auf als bei einem Anbieter mit nur wenigen Kunden. Das könne aber auch ein Nachteil sein: Wird ein solcher Dienst zu beliebt, wächst vielleicht auch das Interesse des Staates, sich das Ganze genauer anzusehen. 
Ein anderer möglicher Nachteil ist, dass der IPredator-Server in Schweden steht, wo es mittlerweile die Vorratsdatenspeicherung gibt. Das Unternehmen ist strikt gegen die Datensammlung, aber seine Erklärung dazu beinhaltet gewisse Unschärfen. Die Nutzer müssen sich einfach darauf verlassen, dass IPredator wie schon vor der Einführung der Vorratsdatenspeicherung keine Verbindungsdaten aufbewahrt. 
Ich gebe IPredator eine Chance. 
Die Installation 
Zur Registrierung reicht es, einen beliebigen Benutzernamen und ein Passwort anzugeben. Da ich keinen Wert auf eine Bestätigungsmail lege, trage ich eine Wegwerf-E-Mail-Adresse ein, die ich mir bei guerillamail.org hole. Sie ist nur 60 Minuten lang gültig und wird dann gelöscht. Aber zur Anmeldung bei IPredator genügt das. 
Nach dem Einloggen werde ich darauf hingewiesen, dass ich noch kein aktives Konto habe. Ich klicke auf Renew Account. 
Als Bezahloption wähle ich Paypal. Wer wie ich dort kein Konto hat, kann im nächsten Schritt als sogenannter Paypal-Gast bezahlen, auch ohne Paypal-Account. Das Geld wird über die Kreditkarte abgebucht, oder man gibt seine Kontoverbindung an. Ich bekomme nach dem Bezahlvorgang dann aber noch einmal das Angebot, ein Paypal-Konto anzulegen. Ich klicke auf Nein, danke. Damit ist die Transaktion erledigt. (Update: Die Gast-Option bei Paypal gibt es offenbar nicht mehr. Einige andere Bezahldienste wie Payson akzeptieren außerdem keine Kreditkartenzahlungen mehr.) 
Zurück auf der Startseite gelange ich über Help zur Installationsanleitung für Ubuntu. Drei Dateien brauche ich demnach, um meine VPN-Verbindung einzurichten: Eine OpenVPN-Konfigurationsdatei namens IPredator-Ubuntu-Password.ovpn, ein Autorisierungszertifikat des VPN-Servers sowie den IPredator-Schlüssel, mit dem ich mich später beim Server identifiziere. Ich lade sie von der IPredator-Seite herunter und speichere sie auf meiner Festplatte. 
Nun werde ich aufgefordert, das Ubuntu-Terminal zu öffnen und darüber den Gnome Network Manager zu installieren. Das ist ein Programm, mit dem man OpenVPN-Verbindungen einrichten kann. Alternativ könnte ich es mir auch über das Ubuntu Software-Center holen. 
Windows- und Mac-Nutzer verwenden statt des Network Managers den OpenVPN-Client Viscosity. Die entsprechenden Anleitungen zur Einrichtung des VPN-Zugang gibt es hier. 
 
Bei Ubuntu gebe ich die beiden Befehle im Terminal ein, die in meiner Installationsanleitung vorgegeben sind. Sie lauten  ""sudo apt-get install - -reinstall network-manager network-manager-gnome    network-manager-openvpn network-manager-openvpn-gnome "" und  ""sudo service network-manager restart "". Damit wird der Gnome Network Manager installiert und neu gestartet. 
Als nächstes muss ich die VPN-Verbindung konfigurieren. Über das Symbol für meine Internetverbindung gelange ich zum Menüpunkt Verbindungen bearbeiten. 
Dort klicke ich auf VPN und dann auf Importieren. Nun muss ich die vorhin heruntergeladene Datei IPredator-Ubuntu-Password.ovpn auswählen. Im neuen Fenster benenne ich die Verbindung in IPredator um und deaktiviere die Checkbox bei Automatisch verbinden. Dann trage ich meine IPredator-Zugangsdaten ein. 
Im Feld Zertifikat der Zertifizierungsstelle lade ich die zweite der drei Dateien, sie heißt IPredator.se.ca.crt. Dann klicke ich auf Erweitert. Hier sind die meisten Einstellungen schon korrekt vorgenommen, ich muss nur unter dem Reiter TLS-Legitimierung die dritte meiner drei Dateien auswählen, sie heißt IPredator.se.ta.key. Bei Schlüsselrichtung wähle ich keine aus. Das Feld oben – Betreff-Übereinstimmung – bleibt leer. 
Nun muss ich im größeren Fenster nur noch auf Speichern klicken. Dann kann ich es schließen. 
In den Netzwerkverbindungen taucht nun unter VPN-Verbindungen die Option IPredator auf. Ein Klick – und ich bekomme eine Erfolgsmeldung. Ich bin nun über einen VPN-Tunnel mit dem IPredator-Server in Schweden verbunden. Erkennbar ist das für mich am kleinen Schloss am WLAN-Symbol oben rechts in Ubuntu. 
Der Alltag mit VPN 
Für 18 Euro kann ich drei Monate lang über IPredator surfen. Die Verbindungsgeschwindigkeit ist gut, jedenfalls nicht merklich langsamer als ohne den Umweg über den Server in Schweden. 
Angenehmer Nebeneffekt: Da ich mit einer IP-Adresse aus Schweden surfe, kann ich auch YouTube-Videos sehen, die in Deutschland nicht verfügbar sind. Und die oben bereits angesprochene Sicherheit in offenen Funknetzwerken ist ein weiterer Pluspunkt. Einen Grund, die normale Verbindung ohne VPN-Tunnel zu wählen, habe ich also nicht. Auch sonst bemerke ich keinen der Nachteile, die das – allerdings noch anonymere – Surfen über Tor mit sich bringt."	technik
"Verschlüsselte E-Mails sind nicht zwingend anonym. Denn die Adresse des Absenders wird unverschlüsselt übertragen. Dennoch ist die Technik bestens geeignet, um sich ein Stück Privatsphäre zu bewahren. Denn mit Ausnahme des jeweiligen Empfängers kann sie niemand lesen. Der Vorteil: Ein normales E-Mail-Konto reicht, zum Beispiel ein Gmail-Konto mit dem tatsächlichen Namen. Normalerweise durchsucht Google die Mails automatisiert nach werberelevanten Schlagworten. Mit verschlüsselten Mails aber kann Google nichts anfangen. 
Die Einrichtung ist allerdings aufwändig. Anstatt ein Handbuch zu nehmen, frage ich Samuel Carlisle, ob er mir zeigt, wie ich mein ThinkPad ausrüsten muss, um verschlüsselte Mails senden und empfangen zu können. Ich treffe ihn beim 29C3, dem Kongress des Chaos Computer Clubs. Carlisle, 25, ist einer der Organisatoren der CryptoPartys in London. Er hat viel Erfahrung damit, Anfängern die Grundlagen der Verschlüsselung beizubringen. 
Drei Dinge brauche ich dafür, sagt Sam: Erstens ein Konto bei einem der üblichen Webmail-Anbieter, also zum Beispiel Gmail oder GMX, zweitens das Mailprogramm Thunderbird – das wie der Firefox-Browser von Mozilla kommt – und drittens die Erweiterung Enigmail für die eigentliche Verschlüsselung. An diesr Stelle ein Hinweis für alle, die kein Linux-System verwenden: Mitunter wird noch ein vierter Baustein benötigt, nämlich das Programm GnuPG. Enigmail ist lediglich die Benutzeroberfläche für GnuPG in Thunderbird. Die meisten Linux-Systeme haben GnuPG an Bord, Windows-System dagegen nicht. Details dazu gibt es hier. 
Welchen Webmail-Anbieter ich nehme, spielt keine allzu große Rolle. Wichtig ist nur, dass er den Zugriff auf meine E-Mails in Thunderbird über IMAP oder POP3 ermöglicht. Ich nehme deshalb einfach mein Gmail-Konto. 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
Die Installation 
Thunderbird selbst ist schon in Ubuntu enthalten. (Eine Installationsanleitung für Enigmail in Thunderbird unter Windows gibt es übrigens auch im Wiki der Piratenpartei.) Und Enigmail bekomme ich in Thunderbird über den Reiter Extras, wo ich dann auf Add-ons klicke. Dort suche ich die Erweiterung und installiere sie mit einem Klick und einem Neustart von Thunderbird. Alternativ kann man Enigmail auf von der Website der Entwickler herunterladen und dann installieren. Enigmail taucht dann als Reiter OpenPGP oben im Thunderbird-Menü auf. OpenPGP ist der zugrundeliegende technische Standard, der das Verschlüsselungsverfahren beschreibt. 
Nun habe ich die drei Bausteine. Als nächstes muss ich sie verknüpfen. In Thunderbird richte ich mir zuerst das Gmail-Konto ein. Wer Thunderbird zum ersten Mal benutzt, bekommt dabei Hilfe von einem Einrichtungsassistenten. Ich belasse einfach alles bei den Voreinstellungen und wähle an der entsprechenden Stelle IMAP, um mit Thunderbird auf den Gmail-Server zuzugreifen. Die Alternative zu IMAP heißt POP3. Die hat den Nachteil, dass E-Mails nach dem Herunterladen in Thunderbird vom Gmail-Server verschwinden. 
Praktischerweise kennt Thunderbird die Servereinstellungen von Gmail für IMAP und POP3 schon, was die Verbindung kinderleicht macht. Wer einen anderen Dienst als Gmail nimmt, muss hier eventuell die Server- und Portnamen unter manuell einrichten selbst eintragen. Die entsprechenden Informationen sollten auf der Seite des jeweiligen Anbieters zu finden sein. 
Nun gehe ich über den Reiter OpenPGP zum Assistenten des Verschlüsselungsprogramms. Auch hier ist es ratsam, alles bei den Voreinstellungen zu belassen, auch beim Punkt Nein, ich möchte in den Empfängerregeln festlegen, wann verschlüsselt werden soll. Denn andernfalls, erklärt Sam, werden alle ausgehenden Nachrichten verschlüsselt. Das kann im Alltag unpraktisch sein, denn wenn der Empfänger gerade nur mit einem Gerät auf seine Mails zugreifen kann, auf dem er seinen privaten Schlüssel nicht hinterlegt hat – zum Beispiel auf dem Smartphone – kann er nicht lesen, was ich ihm schreibe. 
Beim letzten Punkt sollte man einfach auf Nein, danke klicken. 
Als nächstes brauche ich einen Schlüssel. Hier ist noch ein kleiner Exkurs nötig, um zu erklären, wie die Ver- und Entschlüsselung abläuft: Jeder Teilnehmer hat ein Schlüsselpaar: ein Schlüssel ist öffentlich, der andere privat. Der öffentliche wird verteilt, und zwar an jeden, mit dem man kommunizieren will. Der private bleibt bei einem selbst und darf unter keinen Umständen herausgegeben werden. Wer eine Mail verschlüsseln will, tut das mit dem öffentlichen Schlüssel des Empfängers. Der braucht seinen privaten, um die Mail dann wieder entschlüsseln zu können. Wer diesen privaten Schlüssel nicht hat, kann die Mail nicht lesen. Hier wird das Ganze ausführlich und anschaulich erklärt. 
Ich lege nun ein neues Schlüsselpaar an. Dazu klicke ich unter dem Reiter OpenPGP auf den Assistenten, der mich durch die selbsterklärende Prozedur leitet. Sie beginnt damit dass ich eine Passphrase für meinen privaten Schlüssel festlege. Die Passphrase sollte stark sein, aber nicht unmerkbar. Denn sie wird jedes Mal benötigt, wenn man eine empfangene verschlüsselte Mail lesen will. Außerdem ist es ratsam, ein Widerrufszertifikat anzulegen, um den eigenen Schlüssel unbrauchbar machen zu können, wenn er doch mal in falsche Hände gerät. 
 
Um nun verschlüsselte Mails austauschen zu können, braucht es den öffentlichen Schlüssel des jeweiligen Empfängers, in diesem Fall den von Sam. Es gibt verschiedene Wege für den Austausch der Schlüssel. 
Man kann seinen Schlüssel zum Beispiel im Netz veröffentlichen, auf einem sogenannten Schlüsselserver. Dazu öffne ich über den Reiter Open PGP den Bereich Schlüssel verwalten und klicke dann auf den neuen Reiter Schlüssel-Server. Dort gibt es die Option Schlüssel hochladen. Nach dem Upload kann jeder PGP-Nutzer meinen öffentlichen Schlüssel finden und mir sofort eine verschlüsselte Nachricht zukommen lassen. Die Suche funktioniert über denselben Weg wie das Hochladen: Man öffnet die Schlüsselverwaltung und wechselt dann über den Reiter Schlüssel-Server zu Schlüssel suchen. Hier muss man nur den Namen der Person eingeben, deren Schlüssel man sucht. Da es durchaus denkbar ist, dass sich ein Betrüger als jemand anderes ausgibt und einen Schlüssel in dessen Namen erstellt und hochgeladen hat, ist dieser Weg nicht ganz ohne Risiko. Um dieses Risiko zu mindern, gibt es aber die Möglichkeit, Schlüssel zu signieren: Person A signiert den Schlüssel von Person B, wenn sie Person B vertraut. Die Signatur ist öffentlich, so dass Person C sieht, dass Person B vertrauenswürdig ist. Web oft Trust heißt das Prinzip. 
Der dritte Weg ist, jemanden einfach in einer unverschlüsselten Mail nach seinem Schlüssel zu fragen und am besten den eigenen gleich mitzuschicken. Das funktioniert über den Reiter OpenPGP (oben in Thunderbird, nicht im Fenster der neuen E-Mail) und einen Klick auf Meinen öffentlichen Schlüssel anhängen. So tauschen Sam und ich unsere Schlüssel, denn schließlich sitzen wir nebeneinander und sehen, dass es wirklich wir beide sind, die sich gerade schreiben. Um seinen öffentlichen Schlüssel nutzen zu können, muss mich ihn importieren. Das geht zum Beispiel durch einen Rechtsklick auf den als Anhang mitgeschickten Schlüssel, woraufhin sich die Option OpenPGP-Schlüssel importieren anbietet. 
Den eigenen geheimen Schlüssel findet man außerhalb von Thunderbird nicht so einfach. Man kann ihn aber in einen beliebigen Ordner exportieren und dann zum Beispiel auf einen USB-Stick ziehen, um ihn auch außerhalb des Rechners aufzubewahren. Das funktioniert in der Schlüsselverwaltung über einen Rechtsklick auf den eigenen Schlüssel, dann das Feld In Datei exportieren und schließlich die Option Geheime Schlüssel exportieren. 
Nachdem Sam und ich die Schlüssel getauscht haben, machen wir eine Reihe von Tests: Kann der andere eine verschlüsselte Mail lesen? Kann er verschlüsselte Anhänge entschlüsseln? Ist die Mail außerhalb von Thunderbird, also zum Beispiel auf dem Smartphone oder bei Gmail, unleserlich? Das muss sie nämlich sein, weil auf dem Smartphone kein privater Schlüssel liegt – gleichzeitig ist das im Alltag natürlich unpraktisch. Die Versuche verlaufen allesamt erfolgreich. Nun kann ich mich auf die Suche nach weiteren Menschen machen, mit denen ich Schlüssel tauschen und dann vertraulich kommunizieren kann. 
Der Alltag mit Enigmail 
Die Einrichtung mag kompliziert sein, aber das Versenden und Empfangen von verschlüsselten Mails ist es nicht. Das gilt auch für E-Mail-Anhänge. Ich habe jedenfalls großen Gefallen daran gefunden und will es künftig so häufig wie möglich tun. Es fühlt sich einfach gut an zu wissen, dass mit Ausnahme des Empfängers wirklich niemand lesen kann, was ich schreibe. Außerdem ist es sinnvoll, möglichst viele Mails zu verschlüsseln. Denn hundert verschlüsselte Mails sind Alltag, nur zwei verschlüsselte zwischen hundert unverschlüsselten fallen einem eventuellen Beobachter auf. 
Wer verschlüsselt und gleichzeitig anonym kommunizieren will, muss – wie im ersten Teil beschrieben – einen E-Mail-Account wählen, der nichts über seine Person verrät. Denn die Headerdaten, also Absender, Empfänger, Betreffzeile und einige andere Informationen werden auch mit OpenPGP nicht verschlüsselt und sind deshalb von Dritten unter Umständen lesbar."	technik
"Sehr viele Websites (auch ZEIT ONLINE) finanzieren sich durch Werbung; ohne diese könnten sie nicht existieren. Gleichzeitig gibt es diverse Werbeformen, die so mancher als nervend empfindet. Daher haben eine Menge Nutzer sogenannte Werbeblocker installiert. Der bekannteste ist Adblock Plus. Allein die Statistik der Mozilla Foundation weist ihn mit fast 14 Millionen täglichen Nutzern aus , nach Angaben der Entwickler sind es insgesamt fast 30 Millionen Nutzer weltweit. 
Doch die Entwickler des Programms, das alle aufgerufenen Websites werbefrei macht, sehen im Blockieren keine Lösung. Sie wissen, dass es Werbung braucht, um Dienste im Netz zu finanzieren. 
Gebaut hat Adblock Plus der Kölner Wladimir Palant. Mehrere Jahre lang war die Zusatzfunktion für Browser (Add-on) ein Hobbyprojekt, getragen von ihm und vielen freiwilligen Helfern. Vor kurzem hat Palant dann gemeinsam mit Till Faida eine Firma gegründet , um seine Arbeit an Adblock Plus zu finanzieren und damit zu sichern. 
 ""Onlinewerbung ist außer Kontrolle geraten "" 
Faida nun möchte den Werbevermarktern wieder einen Weg zu ihrer Zielgruppe weisen. Seine Botschaft lautet: Macht bessere Werbung und lasst dem Nutzer die Wahl. Adblock Plus könnte in seinen Augen das Instrument sein, um Onlineanzeigen besser zu machen. 
 ""Internetwerbung ist außer Kontrolle geraten "", sagt Faida. Seit Jahren setzten Unternehmen vor allem auf Banner und nervende Pop-ups. Mit mäßigem Erfolg, die Klick-Quote liege im Promillebereich.  ""Die Werbewirtschaft reagiert auf diese Bannerblindheit, indem sie einfach noch mehr und noch aufdringlichere Banner einsetzt "", sagt Faida. 
Für Adblock Plus bedeutet das steigende Nutzerzahlen. Ungefähr 15 Prozent der deutschen Surfer setzen nach Faidas Berechnung den Werbeblocker ein. Seit Palant 2006 begann, das Projekt zu betreuen, hat sich das Programm zur populärsten Erweiterung für den Browser Firefox entwickelt. Auch für Googles Browser Chrome gibt es inzwischen eine Version, die von schätzungsweise einer Million Nutzern eingesetzt wird. 
Für die Werbeindustrie bedeutet das herbe Einbußen.  ""Es gibt Seiten, die uns Verluste von 50 bis 60 Prozent melden "", sagt Faida. Dabei haben die meisten Anwender von Adblock Plus nichts gegen Werbung an sich, sie wollen nur nicht genervt werden. Doch nervende von nicht-nervender Werbung unterscheiden kann das Programm nicht. Jeder Inhalt, der von bestimmten Servern kommt oder verräterische Dateinamen trägt, wird blockiert. 
 
Leidtragende sind vor allem technikorientierte Webseiten, ihre Nutzer verwenden besonders häufig Blocker. Das Fachmagazin t3n , das sich an Entwickler und IT-Unternehmer richtet, hat im Jahr 2010 mit einer besonderen Aktion begonnen: Wer mit Adblocker auf die Seite kommt, sieht statt Anzeigen eine Botschaft:  ""Werbung auf diesen Seiten wird überwiegend pro Einblendung bezahlt und diese Einnahmen ermöglichen uns, Dir die Inhalte von t3n kostenlos anzubieten. "" 
Ob der Appell etwas bewirkt, kann Geschäftsführer Andreas Lenz nicht sagen. Gerne würde er auf aufdringliche Werbeformen, die mit Pop-ups oder Flash arbeiten, verzichten. Aber noch ist das nicht realistisch. Nicht alle Werbeflächen werden von t3n selbst kontrolliert.  ""Die meisten Vermarkter und Agenturen verbreiten Werbung nach dem Gießkannenprinzip "", sagt Lenz. Will beispielsweise ein Autohersteller ein neues Modell auf den deutschen Markt einführen, wird die betreffende Anzeige auf möglichst vielen Webseiten eingeblendet. Mitspracherecht oder Individuallösungen werden bei dem Massengeschäft nur großen Kunden eingeräumt. 
Auch Faida ist noch nicht klar, wie er es bewerkstelligen will, dass sein Blocker bestimmte Werbung durchlässt. Auf der Onlinemarketing-Fachmesse dmexco versuchte er gerade, mit Werbern über die Idee zu reden. 
Der Erfolg war mäßig. Viele Werbetreibende und Technik-Dienstleister nehmen Adblocker noch gar nicht wahr. Erst wenn Faida seine Nutzerzahlen nennt, sind viele plötzlich interessiert und fragen, wie sich der Werbefilter denn umgehen ließe. 
Eine Lösung hat Faida nicht. Er will vor allem einen Dialog starten zwischen Werbern, der Gruppe freiwilliger Entwickler von Adblock Plus und den Nutzern – darüber, was eigentlich gute Werbung ist und welche also durch den Filter gelassen werden sollte. Einige Firmen wollen darüber sogar mit ihm ins Gespräch kommen. 
Branchenverband sieht Blocker nicht als Problem 
Bei den Branchenverbänden hingegen sieht man keinen Grund für einen Kurswechsel. Laut dem Bundesverband Digitale Wirtschaft (BVDW) werden im Jahr 2011 in Deutschland 6,23 Milliarden Euro in Onlinewerbung investiert, eine Steigerung von 16 Prozent gegenüber dem Vorjahr.  ""Insgesamt setzen nur sehr wenige Nutzer eine Adblocker-Software ein. Die große Mehrheit der deutschen Verbraucher hat verstanden, dass die attraktiven und zugleich kostenfreien Inhalte im Internet durch Werbung refinanziert werden "", sagt Christian Zimmer, stellvertretender Vorsitzender der Fachgruppe Online-Mediaagenturen im BVDW. 
Faidas Zahlen sprechen dagegen. Und auch die Bemühungen der Branche deuten nicht darauf hin, dass die Verbraucher begeistert sind von Onlinewerbung. Längst gibt es zum Beispiel Flash-Werbung mit Musikuntermalung, oder Fenster, die sich über den Inhalt einer Webseite legen, bis der Surfer den Störenfried wegklickt. Oder es gibt Pop-Unders, die sich im Hintergrund öffnen und erst sichtbar werden, wenn der Browser geschlossen wird. 
Und es gibt Formate, die Werbeblocker nicht erkennen können. Sponsored Posts beispielsweise, auch Advertorials genannt. Diese Texte sehen wie Texte aus, sind aber im Auftrag eines Kunden geschrieben und wurden bezahlt. Der Leser kann es im Zweifel erkennen, weil sie entsprechend überschrieben sind, das Programm kann das aber nicht.  ""Wenn Werbung wie ein redaktioneller Inhalt aussieht, können wir das nicht blockieren. Wir machen ja keine Zensur "", sagt Faida. 
Doch anstatt Werbung vor den Filtern zu verstecken, wünscht sich Faida ein Umdenken:  ""Das ist eine Negativ-Spirale, die wir aufhalten wollen "", sagt er, und dass Werbung nicht per se schlecht sei.  ""Werbung kann informativ, sie kann kreativ sein. Wenn sie nicht gegen den Nutzer gerichtet ist, haben wir gar nichts gegen sie. ""
""Es gibt mehrere Gründe, die eigene Festplatte oder zumindest bestimmte Dateien darauf zu verschlüsseln. Das Flughafen-Szenario, in dem der Zoll den Rechner ohne Aufsicht in Augenschein nimmt, fällt mir als erstes ein. Es ist nicht auszuschließen, allerdings auch nicht sehr wahrscheinlich. Oder wenn mehrere Menschen mit dem Computer arbeiten, zum Beispiel im Büro: dann kann man mit der Verschlüsselung bestimmter Dateien seine Privatsphäre und anderweitig vertrauliche Informationen schützen. Muss der Rechner mal zur Reparatur, empfiehlt sich die Verschlüsselung ebenfalls. Und sollte er gestohlen werden, kann der Dieb zumindest nicht auf die wichtigsten persönlichen Dateien zugreifen. 
Anstatt gleich die gesamte Festplatte beim Einrichten von Ubuntu zu verschlüsseln, will ich das nur mit bestimmten Dateien tun. Denn die vollständige Verschlüsselung hätte zur Folge, dass ich bei jedem Systemstart ein langes Passwort eingeben müsste. Und sollte jemand dann Zugriff auf mein ThinkPad haben, wäre die Verschlüsselung bereits ausgehebelt. Allein deshalb halte ich es für schlauer,  ""unverdächtige "" Dateien zu lassen, wie sie sind, und nur ausgewählte Dateien mit einem praktisch nicht zu knackenden Passwort zu schützen. 
Die bekannteste kostenlose Software dafür heißt TrueCrypt. Wie sie installiert und verwendet wird, erklärt das CryptoParty Handbook (derzeit in der Version 1.1) sehr ausführlich. 
Die Installation 
TrueCrypt wird offiziell nicht als Open-Source-Software gewertet. In Ubuntu ist sie auch deshalb nicht über das Software-Center zu finden, stattdessen muss ich die Installationsdatei auf truecrypt.org herunterladen. Ich wähle für mein ThinkPad die Standard-32-Bit-Version für Linux.  ""Standard "" heißt, ich bekomme eine grafische Benutzeroberfläche,  ""32 Bit "" bezieht sich auf die sogenannte Prozessorarchitektur meines Rechners. Ob man 32 oder 64 Bit wählen muss, erfährt man durch die Eingabe der Zeile uname –a ins Terminal. Nachdem man die Enter-Taste betätigt, erscheint entweder eine Zeile, die  ""x86_64 "" oder  ""i686 "" enthält. Im ersten Fall wählt man die 64-Bit-Version, im zweiten Fall die 32-Bit-Version. 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
Die Installation beginnt mit einem Rechtsklick auf die verpackte Datei (die mit der Endung tar.gz) und dann einem Klick auf Hier entpacken. Dann folgt ein Rechtsklick auf die neue, entpackte Datei und ein Klick auf Open. Dann wähle ich Run und im nächsten Fenster abschließend noch Install TrueCrypt und bekomme anschließend ein etwas kryptisch aussehendes, schwarzes Fenster mit der Überschrift TrueCrypt Setup angezeigt. Das kann ich getrost schließen. 
Nun suche ich das fertig installierte Programm über die Suchfunktion im Dashboard und ziehe es auf meinen Desktop. (Wie die Installation in Windows und OS X funktioniert, steht im CryptoParty Handbook ab Seite 284.) 
Mit TrueCrypt kann ich einen sogenannten Container anlegen, in den ich beliebige Dateien und Ordner stecken kann. Der Container wird dann verschlüsselt und mit einem Passwort geschützt. Ist das Passwort gut genug, ist der Container praktisch nicht aufzubrechen. Eine hübsche Geschichte – zugegebenermaßen schon etwas älter – veranschaulicht das: Die brasilianische Polizei hat einmal fünf Monate lang versucht, die Festplatten eines Verdächtigen zu entschlüsseln und dann Experten vom FBI zur Hilfe geholt, die sich weitere zwölf Monate lang die Zähne an der Verschlüsselung ausbissen. 
Das Programm hat aber noch eine Besonderheit. In manchen Ländern, zum Beispiel in Großbritannien, kann ich unter Umständen gezwungen werden, das Passwort für meinen Container preiszugeben. Deshalb gibt es in TrueCrypt eine zweite Ebene, die sogenannten Hidden Volumes. Das sind gesondert verschlüsselte Unterordner eines Containers mit einem eigenen Passwort. Gebe ich also nach und rücke das Passwort für den Container heraus, öffnet sich dieser. Hier sollten dann Dateien zu finden sein, die nahelegen, dass ich sie lieber nicht preisgegeben hätte. Beispiele wären vermeintlich sensible Informationen zu Krankheiten, Pornovideos, die auf ungewöhnliche sexuelle Vorlieben schließen lassen, oder Ratgeber für finanzielle Notlagen. Entscheidend ist, dass diese Dateien hinreichend peinlich oder zumindest privat sind, dass man mir abnimmt, dass ich nur diese schützen wollte. 
Die wirklich wichtigen Dateien aber lege ich im Hidden Volume ab, das sich nur öffnet, wenn ich das zweite Passwort eingebe. Nun ist die Existenz solcher Hidden Volumes an sich kein Geheimnis. Aber es ist schwer (wenn auch theoretisch nicht unmöglich), mir nachzuweisen, dass ich eines angelegt habe. Schließlich habe ich ja bereits – vermeintlich – sensible Dateien im ersten Container abgelegt. 
Aber zuerst richte ich den Standard-Container ein. Nach dem Start von TrueCrypt klicke ich dazu auf Create Volume und wähle im nächsten sich öffnenden Fenster Create an encrypted file container. Dann entscheide ich mich für Standard TrueCrypt volume. 
Nun suche ich mir aus, wo der Container liegen soll. Ich wähle einen Bereich meiner Datenpartition, nämlich den Ordner Musik, in dem sich bereits einige Ordner befinden, und nenne den Container der Einfachheit halber  ""Unsichtbar "". Dieser ist aber nur zu Demonstrationszwecken gedacht. Einem Container, den ich wirklich benutzen will, würde ich einen unauffälligen Namen geben. Dazu später mehr. Nur so viel muss man jetzt schon wissen: Ein Container wird wie ein Ordner beziehungsweise ein Verzeichnis genutzt, es ist aber eine Datei. Eine, der ich einen beliebigen Namen und eine beliebige Datei-Endung verpassen kann, so dass sie zwischen anderen Dateien nicht auffällt. 
Und noch ein Hinweis: Es reicht nicht, in diesem ersten Schritt eine bereits auf der Festplatte abgelegte Datei auszuwählen, um sie zu verschlüsseln. TrueCrypt verschlüsselt keine bereits vorhandenen Dateien, sondern kreiert zunächst nur einen Container in Form einer Datei, in den dann später die sensiblen Dateien verschoben werden. Wen man eine bereits bestehende Datei auswählt, löscht TrueCrypt diese und ersetzt sie durch einen Container mit gleichem Namen. 
Anfänger wie ich bleiben im nächsten Schritt bei den Voreinstellungen zum Verschlüsselungs- und dem sogenannten Hash-Algorithmus. 
Als nächstes gebe ich an, wie groß der Container sein soll. Das ist natürlich abhängig davon, was hinein soll. Möchte ich ganze Filmdateien darin ablegen, muss der Container groß sein. In meinem Fall soll er groß genug sein, um einige vermeintlich sensible Dateien aufnehmen zu können – sowie das Hidden Volume mit den Inhalten, die ich wirklich schützen will. 
Dann überlege ich mir ein langes, starkes Passwort. Um zu verdeutlichen, was ein starkes Passwort ist, hier ein kleiner Exkurs: Der jugendliche Held in Cory Doctorows neuem Roman Pirate Cinema erstellt eines, indem er die Anfangsbuchstaben aller Wörter eines Zitats seines Lieblingsschauspielers als Basis nimmt. Jeden Buchstaben ersetzt er durch den jeweils im Alphabet folgenden. Anschließend tauscht er einige Buchstaben gegen Zahlen aus, die dem Buchstaben ähnlich sehen. Aus einem Z wird zum Beispiel eine 2. Das fertige Passwort lautet 2uuz7,kd2xcu7qn?Bunju24xcuccb.2mt.2dbn,bf2dpun,Jdu7cJk2k. Sich das zu merken, ist allerdings etwas ganz anderes. Dennoch geht dieser Ansatz in die richtige Richtung: Das TrueCrypt-Passwort sollte mindestens 20 Zeichen haben und nicht aus normalen Worten bestehen. Man muss es jedes Mal eingeben, wenn man die Daten im verschlüsselten Container benötigt oder neue hinzufügen möchte. Sicherheit ist also unbequem, weshalb sie häufig vernachlässigt wird. 
Dann muss ich festlegen, ob ich einzelne Dateien in den Container legen will, die größer sind als vier Gigabyte – was zum Beispiel bei Spielfilmen der Fall wäre. Danach lege ich das Format des Dateisystems im Container fest. Ich entscheide mich für ext4, das Format meiner Datenpartition. 
Danach muss ich angeben, ob ich auch von einem anderen Betriebssystem aus den Container zugreifen möchte. Das kann für diejenigen gelten, die neben Linux auch Windows auf dem Rechner installiert haben, für mich aber nicht. 
Anschließend werde ich aufgefordert, den Cursor innerhalb des TrueCrypt-Fensters so zufällig wie möglich zu bewegen, während die Software den Container verschlüsselt. Das erhöht die kryptographische Stärke der Schlüssel. Ich tue das für etwa 30 Sekunden und klicke dann auf Format. 
Nun dauert es ein paar Minuten, bis TrueCrypt fertig ist. Ich werde abschließend noch nach dem Passwort meines Ubuntu-Nutzerkontos (nicht dem Container-Passwort) gefragt und bekomme dann eine Bestätigung, dass der Container eingerichtet wurde. 
In den fertigen Container lege ich nun zunächst einen Ordner mit durchaus privaten Dateien – nämlich mit meinen Steuerunterlagen. Dazu muss ich den Container zugänglich machen, bei TrueCrypt heißt das mounten. Ich öffne TrueCrypt, markiere einen beliebigen Slot und klicke auf Select File. 
Dann wähle ich im neuen Fenster den soeben erstellten Container aus und klicke anschließend im TrueCrypt-Fenster unten links auf Mount. Nun werde ich nach meinem TrueCrypt-Passwort gefragt – und aus irgendeinem Grund anschließend noch nach meinem Ubuntu-Nutzerpasswort. 
Jetzt ist der Container  ""gemounted "". Er erscheint in meinem Dateisystem, sieht aber nicht wie ein klassischer Ordner aus. Alles, was nun dort hinein verschiebe, wird dabei verschlüsselt. In diesem Fall ist das der Ordner mit meinen Steuerunterlagen. 
Anschließend ist es wichtig, den Container im TrueCrypt-Fenster wieder zu dismounten. Wenn man den Computer ausschaltet, passiert das aber auch automatisch. 
 
Zugabe Hidden Volume 
Es gibt Menschen, die versuchen, die TrueCrypt-Spuren auf ihrem Rechner zu verwischen. Das beginnt damit, das Start-Icon nicht auf dem Desktop zu belassen. Sie verstecken aber auch ihre Container, indem sie ihnen unauffällige Dateinamen samt Endungen wie .sys geben und sie tief im Dateisystem vergraben. Doch solche Tricks sind auch bei Strafverfolgern und anderen Behörden bekannt. Die finden in jedem Fall das Programm selbst und schließen dann daraus, dass sich auf dem Computer auch Container befinden. Es gibt auch spezielle Forensiksoftware wie TCHunt, mit der diese sich dann aufspüren lassen. Wer so  ""auffliegt "", ist erst recht verdächtig. 
Die meisten TrueCrypt-Nutzer verlassen sich auf das Prinzip der  ""glaubhaften Abstreitbarkeit "". Sie stecken ein Hidden Volume in den Container. Wird der Container entdeckt und werden sie zur Herausgabe des Passworts gezwungen, öffnet sich der Container, das Hidden Volume darin aber bleibt unsichtbar. Unsichtbar, weil es aussieht wie ein Haufen Zufallsdaten – und TrueCrypt füllt den nicht genutzten Raum eines Containers immer mit solchen Zufallsdaten. Nun kann man also versuchen abzustreiten, dass es ein solches verstecktes Volume gibt. Auch das dürften erfahrene Strafverfolger kennen, aber sie müssen dem Verdächtigen erst einmal nachweisen, dass er lügt. 
Um es aber noch einmal deutlich zu machen: Der Durchschnittsbürger wird nie in solche eine Situation geraten. Dennoch finde ich die Möglichkeiten von TrueCrypt so spannend, dass ich sie ausprobieren will. 
Ich erstelle nun als einen versteckten Container, ein Hidden Volume. Dazu starte ich TrueCrypt und klicke auf Create Volume und danach auf wie gehabt auf Create an encrypted file container. Danach wähle ich Hidden TrueCrypt volume. Nun kreiere ich erst einen neuen äußeren, später dann darin den inneren Container. Ich klicke mich dazu in den Ordner, in dem der neue äußere Container liegen soll, und gebe ihm einen Namen. Den Namen einer Datei, deren Existenz auf meinem Rechner so normal ist, dass sie keinen Verdacht auslöst. 
Die Voreinstellungen zur Verschlüsselung lasse ich wieder unverändert. Dann bestimme ich die Größe des äußeren Containers, in diesem Fall zwei Gigabyte. 
Jetzt muss ich mir wieder ein starkes Passwort überlegen – und später noch ein zweites für den inneren Container. Nebenbei bemerkt: Mit meinem Passwort für den Ubuntu-Nutzeraccount, für den VPN-Zugang, Hushmail, die Passphrase für meinen privaten PGP-Schlüssel sowie den im ersten Schritt angelegten TrueCrypt-Container sind das nun die Passworte Nummer sechs und sieben, die ich im Laufe dieses Projekts anlege und mir merken muss. Hätte ich bei der Installation von Ubuntu auch die Option zur Verschlüsselung der gesamten Festplatte genutzt, wäre es noch ein Passwort mehr. 
Jetzt werde ich wieder aufgefordert, den Cursor wild zu bewegen, während TrueCrypt die Verschlüsselung anlegt. 
Danach geht es mit einem Klick auf Next weiter. Ich klicke nun so lange Next, bis ich aufgefordert werde, die Größe des inneren Containers anzugeben. Ich wähle einfach 999 Megabyte. Nun muss ich mir, wie schon angekündigt, ein weiteres Passwort überlegen, das möglichst nichts mit dem vom äußeren Container zu tun hat. Abschließend wähle ich noch das Dateisystem für den inneren Container aus, bestätige, dass ich ihn nur vom Linux-System aus mounten werde und bewege im letzten Abschnitt wieder den Cursor kreuz und quer, während die Verschlüsselung erstellt wird. 
Nun ist das Hidden Volume fertig. TrueCrypt weist noch darauf hin, dass man bestimmte Vorgaben erfüllen muss, wenn man ausschließen will, dass jemand die Existenz des Hidden Volumes nachweisen kann. Auf der Website gibt es unter dem Punkt Protection of Hidden Volumes Against Damage außerdem noch Hinweise, wie man es vermeidet, beim Befüllen des äußeren Containers den Inhalt des inneren versehentlich zu überschreiben. 
Um die wirklich schützenswerten Dateien in den inneren Container zu bekommen, muss ich diesen nur noch mit dem entsprechenden zweiten Passwort mounten. Dann kann ich beliebige Dateien dort hinein verschieben. 
Der Alltag mit TrueCrypt 
Ich habe nicht sehr viele Dateien, die ich unbedingt verschlüsseln will, und es kommen vor allem nicht ständig neue hinzu. Deshalb ist TrueCrypt trotz der komplizierten Passworte und der Gefahr, versehentlich die Dateien in einem Hidden Volume zu überschreiben, recht überschaubar. 
Wer das Programm häufig nutzt, muss den Überblick über seine Container behalten: Wo liegen sie, was ist drin und beinhalten sie auch versteckte Container? Außerdem muss er sich für jeden Container das hoffentlich starke Passwort merken können oder sie alle mithilfe eines Passwortmanagers verwalten. 
Ich persönlich mag Passwortmanager nicht, weil dann gleich alles, was ich so mühsam schützen will, am Masterpasswort hängt. Deshalb benutze ich jedes Programm lieber so häufig, bis ich mir das jeweilige Passwort wirklich gemerkt habe. 
Das aufgerüstete ThinkPad hat meinen bisherigen Computer zu Hause ersetzt. Ich surfe nur noch über VPN oder Tor und einen um diverse Add-ons erweiterten Browser. Ich arbeite derzeit daran, einige Freunde zum Verschlüsseln von E-Mails zu bewegen. Viele meiner privaten Dateien habe ich mit TrueCrypt abgesichert. Schneller und sicherer als mein altes Windows-System ist das ThinkPad dank Ubuntu sowieso. 
Aber mir ist natürlich klar, dass mein Computer kein unknackbarer Tresor ist, und dass ich damit im Internet keineswegs völlig anonym bin. Ich habe meine Datenspur verringert, mehr nicht. Aber allein das war mir die Mühe wert. Des Weiteren habe ich die Möglichkeiten solcher Tools zumindest im Ansatz kennengelernt. Das wird mir helfen, wenn ich mich mit den Werkzeugen beschäftige, die in den kommenden Jahren entwickelt werden. 
Das enorme Feedback auf meine Anleitungen in Leserkommentaren, E-Mails und in den sozialen Netzwerken zeigt, dass es auf der einen Seite die echten Experten gibt, die noch sehr viel mehr über die Möglichkeiten und Grenzen von Tor, VPN und Verschlüsselung wissen. Sie haben mir wertvolle Tipps gegeben und mich gedrängt, Fehler in den Artikeln zu korrigieren und Details zu ergänzen. Weil es aber immer noch eine Ausnahme gibt, und weil die Ansprüche und Ziele mancher Experten andere sind als meine, können meine Anleitungen nicht alle Eventualitäten abdecken. Das gilt insbesondere dann, wenn es darum geht, sich aus Prinzip auch gegen aufwendige staatliche Überwachungsmaßnahmen zu wappnen. 
Auf der anderen Seite gibt es offenbar ein großes Bedürfnis nach solchen Anleitungen. Viele Leserkommentare zeigen, dass auch Anfänger gern mehr darüber wüssten, wie sie ihre Privatsphäre im digitalen Leben schützen können. Sie verdeutlichen aber auch, dass mein Ansatz, etwas aus der Perspektive eines Anfängers für andere Anfänger zu beschreiben, seine Grenzen hat. Auch die Schritt-für-Schritt-Beschreibungen sind manchen zu kompliziert. 
Computersicherheit und Nutzerfreundlichkeit sind also noch nicht so weit vereinbar, wie es nötig wäre, damit die Mehrzahl der Nutzer sich daran versucht. Ich freue mich also über jeden, der diese Serie als Schnupperkurs versteht und anfängt, sich mit dem Thema zu beschäftigen."	technik
"Wenn ich morgens aus dem Haus gehe, lasse ich meine Wohnungstür nicht offen stehen. Ich trage in der S-Bahn kein Namensschild und ich führe dort auch keine langen Telefongespräche, während andere neben mir sitzen und mithören. Meine Privatsphäre ist mir eben wichtig. Bis ich zu Hause meinen Computer anschalte. Dann lasse ich die Türen zu meinem digitalen Leben weit offen stehen, verrate permanent, wer ich bin und nehme in Kauf, dass jemand mitliest, was ich schreibe. 
Dabei ist mir eigentlich klar: Meine Internet- und E-Mail-Provider wissen praktisch alles über mich. Websitebetreiber und deren Werbepartner kennen mich ebenfalls besser, als mir lieb ist. Staatliche Stellen nicht nur in Deutschland könnten mein digitales Leben problemlos durchleuchten, selbst wenn sie keinen Grund haben, gegen mich zu ermitteln. Die Vorratsdatenspeicherung würde mich unter einen Generalverdacht stellen, sollte sie irgendwann eingeführt werden. Auch Kriminelle sind längst in meinen digitalen Lebensraum eingedrungen, trotz Firewall und Virenschutz. 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
Seit Jahren schreibe ich über die Bedeutung von Anonymität im Internet, über Verschlüsselung und das Aushebeln von Tracking und Trojanern. Über Werkzeuge wie Tor, TrueCrypt und PGP. Die meisten habe ich zumindest mal ausprobiert – nur um sie anschließend sofort wieder zu deinstallieren oder zu ignorieren. Surfen mit Tor zum Beispiel, das war bei meinen ersten Versuchen vor ein paar Jahren die reinste Pest. Wenn es überhaupt funktionierte, war die Verbindung quälend langsam. Und den alten Spruch, E-Mails seien so sicher vor fremden Blicken wie Postkarten, habe ich immer gerne zitiert – und selbst nicht ernst genommen. 
Nun aber möchte ich endlich praktizieren, was ich predige. Ich habe mir einen gebrauchten Computer gekauft und nach und nach mit allem ausgerüstet, was ich brauche, um mich online so sicher und diskret zu bewegen wie offline auch. 
Die Serie  ""Mein digitaler Schutzschild "" ist nicht nur das Protokoll meiner Versuche, sondern auch eine Schritt-für-Schritt-Anleitung zum Nachmachen. Denn wenn ein interessierter Laie wie ich diese Programme installieren und benutzen kann, können andere es auch. Die Anleitungen sind deshalb gedacht für diejenigen, die sich bisher nicht an Verschlüsselung und Verschleierung herangetraut haben, weil sie so etwas mit dubiosen  ""Hackerkreisen "" verbinden und nicht mit ihrem Recht auf Privatsphäre. 
Seit einigen Monaten finden in aller Welt die sogenannten Crypto-Partys statt, bei denen es genau darum geht: Experten zeigen Anfängern, wie sie ihre Rechner entsprechend aufrüsten und sich sicher in einem unsicheren Netz bewegen können. Ich feiere nun meine eigene CryptoParty – wer mitmachen will, ist herzlich eingeladen. 
Vier Ziele habe ich mir gesetzt: 
1. ein normales Notebook so mit Software auszustatten, dass meine Privatsphäre bei der alltäglichen Nutzung gewahrt bleibt, 2. zu dokumentieren, was auch Anfänger installieren und anwenden können – und mit welchen Einschränkungen sie dann leben müssen 3. möglichst solche Programme zu verwenden, deren Quellcode offen einsehbar ist und damit von Experten geprüft werden kann – Open-Source-Software eben, 4. immer dann, wenn ich selbst nicht weiter weiß, jemanden zu fragen, der sich auskennt: Hacker, Entwickler, Aktivisten. 
Im Einzelnen enthält diese Serie folgende Anleitungen: 
Wer nichts zu verbergen hat, hat auch nichts zu befürchten, heißt es gern. Nicht zuletzt von Politikern. Aber solche Aussagen sind nicht nur unsinnig, sondern sogar gefährlich. Wirklich frei ist nur, wer unbeobachtet ist. Das hat nicht zuletzt das Bundesverfassungsgericht in seinem Urteil zur Vorratsdatenspeicherung klargemacht. Jeder hat etwas zu  ""verbergen "", weil es irgendwann ein Nachteil für ihn sein könnte, wenn es öffentlich wird. Und niemand ist gezwungen, alles von sich preiszugeben, selbst wenn es ihm nicht schadet. 
Alle von mir getesteten Werkzeuge sind legal. Und sie sind, auch wenn sie etwas kompliziert erscheinen und ihnen teilweise der Ruf anhaftet, vor allem von Kriminellen genutzt zu werden, für den alltäglichen Gebrauch konzipiert. Ich betrachte ihren Einsatz als digitalen Schutzschild, mitunter als digitale Notwehr. 
Es gibt viele weitere Möglichkeiten, sich gegen Überwachung, Tracking oder Kriminelle zu wehren, von Hard- über Software hin zu bestimmten Verhaltensweisen im Netz. Das ist nur ein Anfang."	technik
"Der Moment ist fast schon rührend. Da hat man sich stundenlang durch 2-D-Labyrinthe gerätselt, die überall auf der Insel auf Touchscreens zu finden sind. Hat Linien vom Ein- zum Ausgang der Labyrinthe gezogen, dabei Hindernisse umgangen, Formen gedeutet und die eigenen grauen Zellen gemartert. Und plötzlich steht man in der sonnigen, blühenden Welt von The Witness vor einem echten, dreidimensionalen Irrgarten mit Hecken, Kies und Rosenbeeten. Der Ausgang ist leicht zu finden, die Hecken sind bloß hüfthoch geschnitten. Und doch ist die Gestaltung des Gartens keineswegs belanglos: Sie birgt den Schlüssel für das nächste Touchscreen-Rätsel. 
Diese Wechselwirkung ist eine der Besonderheiten von The Witness. Die farbenfrohe, aber menschenleere Insel ist viel mehr als nur eine Kulisse für die Rätselkaskaden, die Game-Designer Jonathan Blow dort bereithält. Blow, der 2008 mit Braid auf einen Schlag bekannt wurde, hat sieben Jahre Entwicklung in The Witness gesteckt. Erschienen ist der mit 36,99 Euro nach Ansicht einiger Kritiker ziemlich teure Download-Titel nun zunächst für Playstation 4 und PC, eine iOS-Version soll im Laufe des Jahres folgen. Die mindestens 80 Stunden Spielzeit relativieren den Preis aber. 
Nach der brillant durchdeklinierten Zeitmanipulation von Braid waren die Erwartungen an das Spiel außerordentlich hoch. Und sie werden erfüllt: The Witness ist ein Triumphzug in Sachen Rätsel-Design, weil es sich weigert, Spieler zu unterfordern. 
Die Ausgangslage des Spiels ist obskur. Man erwacht als namenlose, eigenschaftslose Figur auf einem Fleckchen Erde inmitten tiefblauer See. Wie man auf die Insel kam, was man dort will und wohin sämtliche Bewohner verschwunden sind, all das lässt The Witness offen. Stattdessen wird man im Garten der Villa, die als Startpunkt dient, sofort mit den ersten Rätseln konfrontiert: Um das Hoftor zu öffnen, gilt es nacheinander eine Reihe von Touchscreens freizuschalten, die durch Stromkabel verbunden sind. Einmal aktiviert, erscheint auf jedem Touchscreen ein zweidimensionales Raster, dessen Anfangs- und Endpunkt mit einer Linie verbunden werden müssen. 
Was simpel klingt, wird schnell zur echten Herausforderung. Zwar bestehen die allermeisten der rund 650 Rätsel aus Rastern unterschiedlicher Größen und Formen. Für enormes Kopfzerbrechen sorgen allerdings die Bedingungen, unter denen die Linien zu ziehen sind: Mal ist das Raster mit Symbolen gespickt, die es zu umranden gilt, mal ist die Zahl der umrandeten Felder entscheidend. 
Jonathan Blow gelingt es, aus einfachsten Grundregeln immer komplexere Rätsel zu erschaffen, die niemals langweilig, sondern höchstens frustrierend schwierig werden. In den Einstiegsrätseln erlernt man sozusagen das Grundvokabular einer Rätselreihe, das anschließend permanent variiert und erweitert wird. Wer also einen Walkthrough zu Hilfe nimmt und Rätsel überspringt, bestraft sich möglicherweise selbst, weil er die Regeln nicht spielend verinnerlicht. 
Die Insel lässt sich weitgehend frei erkunden. Landschaftlich ist The Witness von einer irritierend artifiziellen Schönheit: Die blühenden Bäume, Traumstrände und Wanderpfade erstrahlen in satten Farben und könnten ohne Weiteres aus einem Urlaubskatalog stammen. Gestört wird die Idylle allerdings von zahlreichen Ruinen und Baustellen, die sich über die gesamte Insel verteilen. Wer oder was, so fragt man sich unwillkürlich, hat die Betriebsamkeit gestört? Und was hat es mit den Statuen auf sich, die fast ein wenig wie versteinerte Menschen aussehen? Die Antworten auf diese Fragen müssen zunächst warten. Stattdessen ist man vollauf damit beschäftigt, die immer kniffligeren Rätsel zu lösen. 
 
Die Welt von The Witness lässt sich grob in zehn Bereiche unterteilen. Da gibt es zum Beispiel ein weitläufiges Areal direkt am Meer, in dem bis vor Kurzem Marmor abgebaut wurde. Die frisch geschnittenen Blöcke und Baumaschinen stehen in der prallen Sonne, die Rätsel-Touchscreens finden sich in Lagerräumen und Werkhallen. Weitere Inselareale sind beispielsweise eine Burg, ein asiatischer Tempel oder ein verrostetes Schiffswrack. 
Jedes dieser Gebiete – oft sind sie erst durch das Öffnen codierter Türen erreichbar – richtet seine Rätsel nach bestimmten Grundregeln aus. Hat man alle Touchscreens eines Areals freigeschaltet, richtet ein Laserwerfer seinen Strahl automatisch in Richtung eines mysteriösen, noch unzugänglichen Bergs. Zumindest das Freischalten der Laser gibt dem Spieler das Gefühl, sichtbare Fortschritte zu machen. Nicht alle 650 Rätsel – so viel sei verraten – sind fürs Erreichen des Spielziels vonnöten. 
The Witness mag manche Spieler irritieren, weil es keine Geschichte erzählt. Ein Gegenbeispiel ist The Talos Principle von 2014, das seine Logik- und Kombinationsrätsel mit einer Science-Fiction-Handlung um einen Roboter-Androiden unterfütterte. 
Zwar gibt es auch in The Witness hier und da Audio-Logs, die vor allem Philosophisches verbreiten. Doch kommuniziert das Spiel in allererster Linie über seine Rätsel. Je länger man über den Rastern grübelt, desto besser versteht man die Regeln, nach denen diese Welt aufgebaut ist. Wer nicht weiterweiß, der sollte vielleicht einfach mal den Blick vom Touchscreen heben: Die Natur – auch die domestizierte – ist in The Witness oftmals der beste Tippgeber."	technik
"Das Wort  ""eigentlich "" ist in Beschreibungen von Sicherheitstechnik immer ein schlechtes Zeichen.  ""Eigentlich sicher "" heißt streng genommen  ""nicht sicher "". Ein aktuelles Beispiel sind Kreditkarten mit Chip+PIN-Funktion. Die gelten – eigentlich – als sicher, vor allem im Vergleich zum alten System mit Magnetstreifen. Recherchen von c't und der ZEIT belegen nun aber, dass Kriminelle auch Kreditkarten mit Chip+PIN-Funktion klonen, die Sicherheitsvorkehrungen austricksen und mit den Karten einkaufen gehen können – und das auch tun. Die neue Sicherheitstechnik ist noch nicht einmal weltweit eingeführt und doch gibt es bereits einen Weg, sie auszuhebeln. 
Ein Aussteiger aus der Carder-Szene (so werden Kreditkartenbetrüger genannt), der einer osteuropäischen Bande angehörte, hat die dazu notwendigen Werkzeuge zur Verfügung gestellt. Alexander – der Name ist ein Pseudonym – erklärte anschließend die Funktion und gab Einblick in die kriminellen Geschäfte. Experten der c't analysierten außerdem die dazu eingesetzte Software. 
Eine Reihe von Daten, Programmen und Werkzeugen braucht es für den Betrug mit geklonten Chip+PIN-Karten und für die Umgehung des nach Europay, MasterCard und Visa benannten EMV-Bezahlsystems: 
Rohlinge vom Typ JCOP und Kartendrucker können bei spezialisierten Anbietern von jedem problemlos gekauft werden. Kreditkartendaten werden in den weniger zugänglichen Ecken des Internets gehandelt, immer wieder tauchen dort Tausende Daten zum Beispiel aus Hacks von Internetshops auf. Alles Übrige stellen Hintermänner der Organisierten Kriminalität zur Verfügung. Die Software verkaufen sie für bis zu fünfstellige Summen, vermieten sie aber auch tageweise. 
Mit der genannten Ausrüstung lässt sich keine exakte Kopie einer Kreditkarte mit Sicherheitschip erstellen, denn die Chips und die darin enthaltenen kryptografischen Schlüssel sind gut gesichert. Stattdessen sorgt die MacGyver-App der Kriminellen dafür, dass ein Bezahlvorgang auch ohne die bei Chip+PIN eigentlich vorgesehenen Sicherheitsüberprüfungen stattfindet. Das eigentliche Schloss wird damit nicht geknackt, sondern die geklonte Karte gaukelt der Kasse vor, dass der Schlüssel korrekt funktioniert, obwohl er es nicht tut. 
Etwas detaillierter: Will jemand mit seiner Kreditkarte im Laden bezahlen, muss er sich zu Beginn der EMV-Transaktion mit einer PIN gegenüber der Karte ausweisen. Die gibt er dazu in das Kartenterminal an der Kasse ein. Aber das von MacGyver auf die geklonte Karte installierte Protokoll sorgt dafür, dass das Kartenterminal einfach jede beliebige PIN akzeptiert, egal welche PIN die Bank ursprünglich der Kreditkarte zugewiesen hatte. 
Danach zwingt MacGyver das Kartenterminal zu einer bestimmten Form der Kommunikation. Es gibt mehrere Wege, auf denen eine Kreditkarte über das Terminal mit der Bank Daten tauschen kann. MacGyver erzwingt einen vergleichsweise Simplen, der als Fallback-Lösung gedacht ist, falls die primäre Methode mal nicht funktioniert – wofür es durchaus legitime Gründe geben kann. 
Bei diesem vereinfachten Verfahren werden nicht mehr alle für den Zahlungsvorgang relevanten Daten überprüft, sondern nur noch einige. Jene Daten, die durch Verschlüsselung gesichert sind, werden bei dieser Variante ignoriert. 
Vor allem in Asien, Süd- und Nord-Amerika funktioniert das offenbar recht häufig, weil die dortigen Banken bei der Einführung von EMV und Chip+PIN unvorsichtig waren. Steven J. Murdoch, Sicherheitsforscher aus London und Experte für das EMV-Bezahlsystem, hält die Nachlässigkeit der Banken ironischerweise für eine Vorsichtsmaßnahme:  ""Der Hauptgrund dafür, dass Sicherheitschecks nicht ordentlich implementiert werden, ist die Sorge, dass Umstellungen in der Banken-IT ungewollt auch bestimmte legitime Transaktionen stoppen könnte. Das ist insbesondere ein Problem mit veralteten Systemen, in denen Updates stets ein hohes Risiko für die Funktionsfähigkeit mit sich bringen. Jede Bank wägt für sich ab, ob sie das Risiko eingeht, dass Zahlungen nicht durchgehen oder das Risiko, das Opfer von Betrügern zu werden "", schreibt er. 
Das heißt, aus Sorge, dass dann zu viele Kreditkarten ihrer Kunden in den Geschäften nicht mehr akzeptiert werden, zögern die Banken damit, neue Sicherheitsstandards einzuführen. Mit dem Ergebnis, dass sie eine Lücke einführen, die Kriminelle ausnutzen. 
 
Wie MacGyver im Einzelnen funktioniert, beschreibt c't in diesem Artikel. Das Magazin beruft sich dabei auf eine Analyse der Software durch zwei Experten für Reverse Engineering. Die fanden nebenbei heraus, dass zumindest die ihnen vorliegende Version der Software – es gibt mindestens 40 verschiedene – heimlich alle zum Klonen benutzten Kreditkartendaten an die unbekannten Entwickler sendet. So profitieren die kriminellen Entwickler dieser speziellen MacGyver-Version doppelt davon, dass sie ihre Betrugssoftware an andere Kriminelle verkaufen oder vermieten. 
Die Masche mit den geklonten Karten funktioniert nicht unter allen Umständen. Geldabheben an einem Automaten etwa ist nicht möglich, weil dabei die PIN immer online mit den beim Herausgeber der Karte hinterlegten Informationen abgeglichen wird. Eine falsche PIN würde sofort auffallen und beim dritten Fehlversuch zum Einzug der Karte führen. 
Handelsübliche Bezahlterminals in Geschäften dagegen lassen sich von MacGyver missbrauchen. Der Insider erzählt, er habe so in einer europäischen Großstadt gemeinsam mit einem Komplizen mehr als ein Dutzend iPhones kaufen können – an einem Tag. 
Solche Waren müssen Kriminelle dann allerdings noch zu Geld machen. Alternativ können sie verschiedene Guthabenkarten aufladen, zum Beispiel die von Mobilfunkanbietern. Mit denen könnten sie selbst kontrollierte Premiumnummern anrufen. Das sind aber nur zwei von mehreren denkbaren Betrugsmaschen. 
Mit einem mobilen Kreditkartenlesegerät von payleven, iZettle oder SumUp etwa können die Betrüger auch gänzlich unbeobachtet abräumen. Und das geht so: Zunächst wird mittels eines Strohmanns ein Konto bei einer Bank oder Sparkasse eröffnet. Den Recherchen von c't und ZEIT zufolge nutzen die Betrüger dazu gefälschte Ausweise. Ein täuschend echt aussehender spanischer Ausweis zum Beispiel kostet auf dem Schwarzmarkt ein paar Hundert Euro. Das Konto hinterlegen sie als Bankverbindung eines simplen Onlineshops, den sie selbst eröffnet haben. 
Dann kaufen die Täter eines der oben erwähnten mobilen Lesegeräte für Kreditkarten. Mit den gefälschten Kreditkarten überweisen sie sich anschließend über das Lesegerät Geld für fiktive Waren aus dem Onlineshop, also auf das unter falschem Namen eröffnete Konto. 
Mechanismen zur Erkennung von Geldwäsche greifen nur bedingt, erst bei Abhebungen von mehr als 15.000 Euro ist eine Identitätsfeststellung zwingend vorgeschrieben. Ebenso sind die Überweisungen von payleven und iZettle unverdächtig, da sie immer gebündelt auf das Konto überweisen werden. Die Bank sieht also nicht viele einzelne Transaktionen der Betrüger, sondern nur eine große Überweisung, wie sie bei einem echten Händler völlig plausibel wäre. 
Hierzulande hält sich das Bedrohungsszenario in Grenzen, denn deutsche Banken und ihre Dienstleister prüfen die Echtheit der Kreditkartenchips offenbar so, wie es das EMV-System vorsieht. Wer eine deutsche Kreditkarte besitzt, dem kann MacGyver also höchstwahrscheinlich nichts anhaben. Das BKA rät dennoch  ""allen beteiligten deutschen Stellen "" sicherzustellen, dass sie Karten und Kartendaten genau überprüfen. 
Dass bisher insgesamt nur eine Handvoll Straftaten im Zusammenhang mit dieser Kartenbetrugsmasche registriert wurde, liegt am grundlegenden Konzept des Systems: Bei einer Kartenzahlung teilt die Bank dem Verkäufer mit, dass der Bezahler genug Guthaben hat oder kreditwürdig ist und gibt das Geld für die Ware oder Dienstleistung frei. Der Fehler liegt in diesem Fall also bei der Bank, weshalb alle anderen Beteiligten ein Recht auf ihr Geld haben und es auch bekommen. Wer zum Beispiel mit einer manipulierten Karte zehn teure Smartphones beim Elektronikhändler kauft, fügt dem Unternehmen keinen Schaden zu – die Filiale bekommt das Geld und der Umsatz stimmt. Der Besitzer der echten Karte wird hingegen belastet und sich bei seiner Bank beschweren. Weil die aber weiß, dass sie selbst für den Fehler verantwortlich ist, wird sie ihrem Kunden das Geld in aller Regel einigermaßen geräuschlos erstatten. 
Geschädigt werden nur die Aktionäre der Kreditkartenunternehmen und der Banken, doch im Vergleich zu deren weltweiten Umsätzen sind die Betrügereien offenbar so unbedeutend, dass sie sie verschmerzen. Aus den Antworten von drei betroffenen Kreditkartenunternehmen auf einen Fragenkatalog der ZEIT lässt sich jedenfalls schließen, dass sie entweder kaum Kenntnis von diesem Betrug haben oder er in ihren Augen nicht relevant genug ist. 
Am kommenden Donnerstag können Sie im Wirtschaftsteil der ZEIT den Bericht des Aussteigers aus der Szene lesen. Er hat mit seiner Bande Hunderte Kreditkarten geklont und sagt:  ""Ich habe mit der Software Geld geholt, und zwar nicht wenig. ""
""Die Onlinebank Comdirect hatte in der Nacht und am frühen Morgen mit technischen Schwierigkeiten zu kämpfen. So landeten Kunden nach dem Einloggen ins Onlinebanking auf fremden Konten und konnten dort alle Informationen abrufen. Dies berichtet das Handelsblatt und beruft sich dabei auf die Erfahrungen eines Redakteurs der Zeitung, der zeitweise vollen Zugriff auf ein fremdes Konto mit mehr als 50.000 Euro Guthaben auf Giro- und Tagesgeldkonto erhalten hatte. 
Nach Angaben der Bank werden regelmäßig in der Nacht zu Montag Softwareupdates eingespielt, so auch zu diesem Wochenbeginn. Eine Sprecherin bestätigte, dass es nach einem Softwareupdate in der Nacht technische Probleme gegeben habe. Was genau passiert sei und wie viele Comdirect-Kunden davon betroffen waren, könne sie noch nicht sagen.  ""Wir prüfen das. "" Inzwischen wurden die Systeme sicherheitshalber am Vormittag neu gestartet. 
Die Onlinebank, eine Tochterbank der Commerzbank, hat rund zwei Millionen Kunden. An diesem Vormittag konnten sie ihre Bank zeitweise nur per Telefon oder Fax erreichen. 
So wie die Kunden der Comdirect kennen auch die Kunden anderer Banken solche Probleme. Anfang Juni etwa hatte es eine technische Panne im Onlinebanking der Deutschen Bank gegeben. Dort waren Buchungen wie Überweisungen und Abbuchungen doppelt ausgeführt worden. Weil ihre Konten damit überzogen wurden, konnten viele Kunden am Automaten auch kein Geld abheben. 
Einen Grund für diese Pannen sehen Experten in der IT von Deutscher Bank, Commerzbank und Co. Die Informationstechnik vieler deutscher Geldhäuser gilt als veraltet."	technik
"Die Luft in deutschen Städten ist zu schmutzig – zumindest, wenn man die EU-Vorschriften für Stickoxide als Maßstab nimmt. Die zulässigen Grenzwerte werden seit Jahren überschritten, zum Beispiel in Berlin, Hamburg, Frankfurt, Köln, München oder Stuttgart. Zusätzlich gefährdet Feinstaub die Gesundheit der Einwohner. 
Jetzt berichtet die Frankfurter Allgemeine Zeitung (FAZ), dass einige Städte wegen der hohen Stickoxidbelastung über Fahrverbote und Sperrungen für Dieselautos nachdächten. Auch Personenkraftwagen wären betroffen. München könne die erste Stadt mit einem solchen Fahrverbot sein. 
Offenbar wissen die Städte sich nicht mehr anders zu helfen. Schon im Februar forderte der Deutsche Städtetag von der Bundesregierung, das Problem durch  ""anspruchsvolle Vorgaben zur Reduzierung der Emissionen an der Quelle "" anzugehen – sprich: durch strengere Abgasnormen für Autos. 
Die seit dem vergangenen September für Diesel-Pkw verbindlich vorgeschriebene Euro-6-Norm reiche nicht aus. Zwar hätten die Kommunen selbst Luftreinhaltepläne erarbeitet, doch ihre Möglichkeiten  ""stoßen zunehmend an ihre Grenzen "". Voraussichtlich würden die Grenzwerte für Stickstoffdioxid deshalb auch in diesem und in den folgenden Jahren weiter überschritten. 
Der Frankfurter Allgemeinen zufolge hat sich an dem Problem seither nichts geändert. Der Deutsche Städtetag mache dafür auch die Autohersteller verantwortlich. Aus seiner Sicht sei wegen des Dieselskandals die Kalkulation der zu erwartenden Luftbelastung für die Städte obsolet geworden. Hinzu kämen die stark zunehmenden Zulassungszahlen der vergangenen Jahre. 
Auch 2016 bleibt der Diesel trotz der Abgasmanipulationen beliebt. Gerade meldete der Branchenverband VDA einen neuen Rekord: Bis Ende Mai 2016 seien in Deutschland mehr als 656.000 Dieselfahrzeuge neu zugelassen worden. 
Anders als bei den Grenzwerten für Feinstaub hätten es die Städte nicht in der Hand, die Stickoxidbelastung so zu verringern, dass die Grenzwerte überall eingehalten würden, sagte der Hauptgeschäftsführer des Deutschen Städtetags, Helmut Dedy, der FAZ:  ""Das wird kurzfristig nicht gehen und ohne die Industrie wird uns das nicht gelingen. "" Die Ursache der erhöhten Werte liege nicht bei den Städten, doch diese müssten mit den Symptomen umgehen  ""und bekommen dabei wenig Unterstützung von den eigentlichen Akteuren, vom Bund und von der Industrie "". 
Für die Kommunen geht es um viel Geld. Halten sie die europäischen Grenzwerte nicht ein, könnte der Europäische Gerichtshof sie zu mehreren Hunderttausend Euro Strafe verurteilen – pro Tag. 
Dedy sagte, die Städte wollten die Fahrverbote und Straßensperrungen nicht. Aber ohne sie könnten sie sehr wahrscheinlich die Grenzwerte nicht einhalten. Er forderte  ""konkrete Lösungen "" von der Autoindustrie. Gebe es die nicht, müsse die Bundesregierung sich in der Europäischen Union für längere Übergangsfristen einsetzen."	technik
"Eine Ethik-Kommission im Bundesverkehrsministerium soll moralische Fragen im Zusammenhang mit der Zulassung selbstfahrender Autos im Straßenverkehr klären.  ""Da geht es darum, was die Algorithmen in den Fahrcomputern berücksichtigen müssen, beziehungsweise was die Programmierer dürfen und was nicht "", sagte Verkehrsminister Alexander Dobrindt (CSU) der Bild am Sonntag. Dafür werde eine Ethik-Kommission im Verkehrsministerium unter Vorsitz des früheren Bundesverfassungsrichter Udo di Fabio eingesetzt, kündigte Dobrindt an. 
Dobrindt betonte zwei Grundsätze, die gelten müssten:  ""Sachschaden geht immer vor Personenschaden. Und es darf keine Klassifizierung von Personen geben, etwa nach Größe oder Alter. "" Es sei sicher, dass die automatisierten Systeme die Zahl an Unfällen, Verletzten und Toten drastisch reduzieren würden.  ""Aber hundertprozentige Sicherheit kann kein technisches System der Welt garantieren. "" 
Der Minister bereitet den Angaben zufolge derzeit eine Änderung des Straßenverkehrsgesetzes vor, um vollautomatisiertes Fahren zuzulassen.  ""Darin wird geregelt, dass automatische Systeme mit voller Kontrolle über ein Fahrzeug dem menschlichen Fahrer rechtlich gleichgestellt werden, "" sagte Dobrindt. Dies bedeute, dass der Fahrer bei ordnungsgemäßer Nutzung automatisierter und vernetzter Fahrzeuge keine Sorgfaltspflichtverletzung begehe. 
Dadurch würden für Autofahrer keine zusätzlichen Haftungsrisiken entstehen.  ""Die Haftung für automatisierte Systeme liegt dann beim Hersteller "", sagte der Minister und kündigte an:  ""Wir werden diese Gesetzesänderung noch in diesem Jahr beschließen. "" 
Die jüngsten Unfälle beim US-amerikanischen Elektroautohersteller Tesla bedeuten für Dobrindt keinen Rückschlag für das autonome Fahren:  ""Ganz und gar nicht. Wir befinden uns in der Forschungs- und Testphase "", sagte er. Heute verfüge auch ein Tesla noch nicht über einen vollständigen Autopiloten, sondern über Assistenzsysteme.  ""Diese erfordern aber die hundertprozentige Aufmerksamkeit des Fahrzeugführers, der jederzeit in der Lage sein muss, die Systeme zu übersteuern "", betonte der Verkehrsminister. Teslas Autopilotsystem war nach zwei Unfällen in den USA, von denen einer für den Fahrer tödlich endete, in die Kritik geraten."	technik
"So also fühlt sich der kalte Entzug an. Ein Gedanke, ein Reflex, der Griff zum Telefon, der Wille, schnell einem Kollegen eine E-Mail zu schreiben – und dann das Aha-Erlebnis: Smartphone im Flugmodus. Es ist Mittwochabend, der dritte Tag, an dem ich die Bahnfahrten analog verbringen möchte. Keine Mails, keine Telefonate. Nur Zeitungen und Jonathan Franzens Buch Unschuld. 
Und es ist furchtbar absurd. Tatsächlich ertappe ich mich dabei, keine 40 Minuten am Stück zu lesen. Nach zehn Minuten greife ich zum Telefon, wenig später noch mal. Obwohl ja nichts passieren wird, außer die Uhrzeit zu sehen. Was ist da los mit mir? 
Vor dem Experiment hätte ich mich nicht als internetsüchtig bezeichnet. Schon allein deshalb nicht, weil ich gerade zwei Wochen Urlaub hinter mir habe, in dem ich mein Telefon – und damit das Internet – zu Hause gelassen hatte. Ich jage keine Pokémons und aktualisiere mein Facebook-Profil nur sporadisch. 
Dann kam der Arbeitsalltag. Am Montag konnte ich unmöglich das Telefon auslassen. Zwei Interviews standen an, und den aktuellen Stand zum Putsch in der Türkei wollte ich auch nicht ignorieren. 
Am Dienstag wird die ZEIT produziert, und war es nicht so, dass sich ein Pressesprecher am Morgen melden wollte? Während der Rückfahrt abends bleibt das Telefon natürlich auch an, könnte ja sein, dass der Spätdienst doch noch eine Rückfrage zum Text hat. 
Selbst am Mittwochmorgen schalte ich nicht in den Flugmodus. Der Zug fährt eine enorme Verspätung rein, das Ressort will informiert werden – und wenn ich danach das Telefon abstelle? Sähe das nicht sehr nach Arbeitsverweigerung aus? Es bleibt an. 
Dann kommt der Mittwochabend. Da sitze ich nun und bin drauf und dran, das Telefon anzulassen. 
Aber aus heißt aus. Vier, fünf Mails hätte ich sonst wohl unterwegs geschrieben. Wenn jede Mail nur drei Minuten Aufmerksamkeit bedeutet, dann wären das schon 15 Minuten oder acht Romanseiten oder zwei große Zeitungsreportagen oder ein Power-Nap. Am Ende schreibe ich gar keine Mail mehr. Sie waren eine Stunde später gar nicht mehr so wichtig. 
Viele Menschen in Deutschland pendeln jeden Tag zur Arbeit, stehen stundenlang im Stau, hängen in überfüllten Zügen fest. Einer von ihnen ist Claas Tatje: Der ZEIT-Redakteur fährt jeden Tag von Hannover nach Hamburg. Auf ZEIT ONLINE geht er in der Serie Rushhour der Frage nach: Warum tun er und Millionen andere sich dieses Leben an? 
Donnerstag stellt sich bereits Gewöhnung ein. Der Blick aufs Telefon wird deutlich seltener. Stattdessen lese ich ältere Zeitungstexte, zum Beispiel aus der Welt über  ""digitale Entgiftung "". Die Organisatorin einer Veranstaltung namens Unplugged Weekend berichtet darin, dass wir im Schnitt 150 Mal am Tag auf unser Telefon gucken:  ""Viele von uns verbringen mehr Zeit damit, auf ihr Smartphone zu schauen als ihrem Partner in die Augen. "" Professoren warnen:  ""Wer nie abschaltet und auch in der Freizeit stets direkt auf E-Mails und Anrufe reagiert, wird auf Dauer unproduktiv. "" Und eine Unternehmensberatung will schon berechnet haben, dass die Rund-um-die-Uhr-Mailerei Kosten von rund 130 Milliarden Euro im Jahr verursacht – weil always on die Menschen krank macht. 
Vor wenigen Wochen beschäftigte sich auch der Guardian mit dem Phänomen der Dauererreichbarkeit.  ""Der exzessive Gebrauch von E-Mails kann zu schnellerem Burn-out, Familienkonflikten, sinkender Arbeitszufriedenheit und einer schlechteren Gemütslage führen "", warnt dort ein renommierter US-Professor. Ein anderer stellt die Frage: Wenn jemand eine unverschämte Mail bekommt und zugleich mit seinen Kindern spielt, wie soll er damit umgehen? Kann er so einfach die Wut auf die Mail abstellen und sich wieder den Kindern zuwenden? 
Natürlich ist die Fahrt zur Arbeit für die meisten Menschen kein Freizeitvergnügen, aber bis auf wenige Ausnahmen ist sie eben auch keine Arbeitszeit. Mir hat die Woche sehr geholfen. Ich schreibe nicht nur weniger Mails, sondern habe auch gelernt, dass ich im Zug produktiver und entspannter bin, wenn ich mich nicht dauernd selbst ablenke. Nächste Woche habe ich außerdem anderes zu tun. Von Jonathan Franzens Buch habe ich im Zug nicht eine Seite gelesen."	technik
"In Regensburg gibt es eine Kreuzung, an der eine Fußgängerampel und eine normale Ampel stehen. Welche von beiden gilt hier für Radfahrer? Der Fahrradweg ist erhöht (Bordstein), aber getrennt vom Gehweg. Die zu überquerende Straße ist mehrspurig, und als Radfahrer brauche ich nur einen Bruchteil der Zeit, die ein Fußgänger benötigt, schreibt ZEIT-ONLINE-Leser Christoph Saller. 
Grundsätzlich gelten Ampeln für alle Verkehrsteilnehmer, sofern nicht für eine bestimme Gruppe besondere Wechsellichtzeichen – so heißen Ampeln im schönsten Behördendeutsch – angebracht sind. Viele Radfahrer fragen sich aber trotzdem, ob sie die allgemeine Ampel der Kraftfahrer oder die der Fußgänger beachten müssen, wenn es kein extra Lichtzeichen mit einem Radsymbol gibt. 
Antworten auf diese komplizierte Frage liefert Paragraf 37 der Straßenverkehrsordnung (StVO). Demnach haben Radfahrer die Ampeln für den Fahrverkehr zu beachten.  ""Voraussetzung hierfür ist aber, dass diese Lichtzeichen den Radfahrverkehr mitzuregeln beabsichtigen "", sagt die Verkehrsrechtsexpertin Bettina Löblein.  ""Das gilt insbesondere, wenn der Radfahrer nicht auf der Fahrbahn fährt. "" 
Doch woher weiß der Radler das? Es komme auf die Gesamtsituation an, fügt die Anwältin hinzu.  ""Unklarheiten dürfen dabei nicht zulasten des Radfahrers gehen. "" Zunächst einmal gilt eine Lichtzeichenanlage nur für den ihr zugeordneten Straßenteil. Entscheidend dafür ist etwa der Standort der Ampel. Aus der Drehung der Gläser kann sich ergeben, dass nur der Verkehr auf der Fahrbahn, nicht aber der auf einem davon abgesetzten Radweg betroffen ist. 
Davon abweichend haben Radfahrer auf benutzungspflichtigen Radwegen die besonderen Lichtzeichen für Radfahrer zu beachten, schreibt Paragraf 37 StVO weiter vor. Das können eigene Ampeln für Radler sein – gibt es sie nicht und die Radspur befindet sich unmittelbar neben einer Fußgängerfurt, so gelten für dort fahrende Radfahrer die Lichtzeichen für Fußgänger. Voraussetzung ist die deutliche Markierung der Radwegfurt auf der kreuzenden Fahrbahn. 
Handelt es sich um eine gemeinsame Furt für Fußgänger- und Radverkehr, so soll, wie Rechtsanwältin Löblein erläutert, in den roten und grünen Lichtzeichen zusätzlich zu dem Sinnbild für Fußgänger auch das Sinnbild eines Fahrrades gezeigt werden. Diese Kombination gilt dann als besonderes Lichtzeichen für Radfahrer im Sinne des Paragraf 37, Absatz 2 Nummer 6 der StVO. 
Ob überfahrene rote Ampeln, Unfälle oder Streit beim Gebrauchtwagenkauf: Rund um den Straßenverkehr gibt es viele knifflige Rechtsfragen. Eine davon beantworten Fachanwälte für Verkehrsrecht jede Woche donnerstags hier in unserer Serie  ""Gesetz der Straße "". 
Schreiben Sie uns Ihre Frage (und geben Sie dabei bitte Ihren Namen und Ihren Wohnort an). Wir wählen jede Woche eine Frage aus und beantworten sie hier. 
Bitte beachten Sie: ZEIT ONLINE, die Autorin und die beteiligten Fachanwälte übernehmen keinerlei Gewähr für die Aktualität, Korrektheit, Vollständigkeit oder Qualität der bereitgestellten Antworten und Informationen sowie der Rechtsprechung. Haftungsansprüche gegen ZEIT ONLINE und den Autor, welche sich auf Schäden materieller oder ideeller Art beziehen, die durch die Nutzung oder Nichtnutzung der dargebotenen Informationen bzw. durch die Nutzung fehlerhafter und unvollständiger Informationen verursacht wurden, sind grundsätzlich ausgeschlossen. 
Fehlen an Radwegen, die an Gehwege grenzen, eigene Lichtzeichen für Radfahrer, dann müssen diese noch bis Ende dieses Jahres die Ampeln für Fußgänger beachten. Danach gibt es entweder besondere Lichtzeichen für den Radverkehr oder die Lichtzeichen für den Fahrverkehr sind zu beachten."	technik
"So günstig war Tanken schon lange nicht mehr. Am Wochenende kostete ein Liter Diesel vielerorts weniger als ein Euro. Zuletzt war Diesel in Deutschland im Frühjahr 2009 so günstig. Am Dienstag lag der Literpreis bei knapp über einem Euro. Auch der Preis für Superbenzin sinkt seit Monaten. 
Der Grund für den Preisverfall liegt bei der Opec: Das tief zerstrittene Ölkartell hatte auf seiner Sitzung Ende vergangener Woche vergeblich versucht, sich auf eine Förderobergrenze zu einigen. Dadurch bleibt es beim Überangebot und sinkenden Preisen. Vor allem weil Saudi-Arabien hofft, mit dem Preisdumping ungeliebte Konkurrenten aus dem Markt zu drängen. Die wichtige US-Sorte WTI kostete etwa im Mai noch rund 65 Dollar je Fass (159 Liter). Inzwischen liegt der Preis bei unter 40 Dollar. Und viele Experten erwarten, dass er auch künftig so niedrig bleibt. 
Das allerdings ist keine gute Nachricht für Umwelt und Klima. Hält sich der Spritpreis längerfristig auf niedrigem Niveau, geht das ohnehin schon geringe Interesse an Elektroautos noch weiter zurück. Die Regierung kann ihr Ziel, im Jahr 2020 eine Million Elektroautos auf der Straße zu haben, dann endgültig vergessen. 
Auf Dauer günstiger Kraftstoff wird viele Autokäufer dazu treiben, weniger auf den Verbrauch und damit den Ausstoß von klimaschädlichem Kohlendioxid zu achten: Ob der Motor einen Liter mehr oder weniger schluckt, spielt doch keine große Rolle, wenn der Sprit billig ist! Und Modelle mit hoher Leistung werden noch beliebter. Seit Jahren steigt die durchschnittliche PS-Zahl in Deutschland. Lag sie 2005 noch bei 123 PS, sind es inzwischen 140 PS. 
Bleibt insbesondere der Dieselpreis unter der psychologisch wichtigen Schwelle von einem Euro, werden Diesel-Modelle noch attraktiver, als sie es ohnehin schon sind. Das gilt insbesondere für die beliebten SUVs, die überdurchschnittlich oft einen Dieselmotor haben – und im Schnitt 25 Prozent mehr Treibstoff verbrauchen als vergleichbare Limousinen. 
 ""Billiger Diesel treibt den SUV-Boom "", sagt Autoexperte Ferdinand Dudenhöffer. Er rechnet mit rund 665.000 verkauften SUV im nächsten Jahr. Allein in den vergangenen fünf Jahren hätten sich damit die Verkaufszahlen fast verdoppelt. 
Der Diesel-Boom hat Folgen für die Umwelt. Messungen der baden-württembergischen Landesanstalt für Umwelt haben gezeigt, dass moderne Dieselmotoren zwar im Labor die Abgasnorm Euro 6 erfüllen, aber auf der Straße erheblich mehr Stickoxid ausstoßen. Der VW-Skandal lässt grüßen. Dieselmotoren sind neben Bussen der wichtigste Grund, warum in vielen Großstädten – etwa in Stuttgart, Berlin, München und Teilen des Ruhrgebiets – die EU-Grenzwerte für Stickoxid und Feinstaub deutlich überschritten werden. Die EU-Kommission hat deshalb sogar ein Vertragsverletzungsverfahren gegen Deutschland eingeleitet. 
Die Bundesregierung hat es selbst in der Hand, dieses Problem zu lösen. Es ist völlig unverständlich, dass der Staat Diesel immer noch bevorzugt: Auf einen Liter Diesel fallen nur 47,04 Cent Energiesteuer an, auf einen Liter Benzin dagegen 65,45 Cent. Für einen Steuervorteil von rund 18 Cent gibt es gerade angesichts des VW-Skandals keinen guten Grund mehr. Die Subventionierung, eingeführt noch in der Helmut-Kohl-Ära, gehört abgeschafft. 
Frankreich, wo Diesel bisher ebenfalls steuerlich bessergestellt ist, macht es richtig. Dort hat die Regierung kurz vor dem Pariser Klimagipfel angekündigt, die Steuersätze für Benzin und Diesel in den nächsten Jahren anzugleichen. Bundesfinanzminister Schäuble sollte sich seinen französischen Amtskollegen Michel Sapin zum Vorbild nehmen. 
Preis je Liter Diesel, Oktober 2015 
Einkaufspreis (Notierung Rotterdam): 33,29 Cent Kosten*: 14,74 Cent Energiesteuer (fix): 47,04 Cent Mehrwertsteuer: 18,06 Cent 
Verbraucherpreis: 113,13 Cent 
*u.a. für Lagerhaltung, Transport, Vertrieb und die Beimischung von Biosprit 
Quelle: Mineralölwirtschaftsverband 
Preis je Liter Superbenzin E5, Oktober 2015 
Einkaufspreis (Notierung Rotterdam): 31,76 Cent Kosten*: 14,81 Cent Energiesteuer (fix): 65,45 Cent Mehrwertsteuer: 21,28 Cent 
Verbraucherpreis: 133,30 Cent 
*u.a. für Lagerhaltung, Transport, Vertrieb und die Beimischung von Biosprit 
Quelle: Mineralölwirtschaftsverband"	technik
"Facebook will beim Vorgehen gegen Hassparolen deutsches Recht über eigene Regeln stellen.  ""Wichtig ist, dass einheitlich deutsches Recht angewandt wird. Das heißt: nicht nur Nutzungsbestimmungen, so wie dies bisher in einigen Fällen der Fall war "", sagte Bundesjustizminister Heiko Maas (SPD). Zudem sei in einer gemeinsamen Arbeitsgemeinschaft mit Google, Twitter und Facebook vereinbart worden, rechtswidrige Inhalte innerhalb von 24 Stunden zu prüfen und gegebenenfalls zu entfernen – allerdings nur die  ""Mehrzahl "" der gemeldeten Kommentare. 
Laut dem Justizministerium haben sich die in der sogenannten Task Force gegen die Verbreitung von fremdenfeindlichen und rassistischen Botschaften im Netz vertretenen Unternehmen darauf verständigt, verstärkt geschulte deutschsprachige Experten mit der Prüfung von Beschwerden zu beauftragen. Zudem soll es vereinfachte Möglichkeiten geben, Hasskommentare zu melden. Außerdem wollen die Unternehmen Gegenrede zu hasserfüllter Rhetorik fördern. 
Die von Maas immer wieder geforderte Transparenz über gemeldete und gelöschte Kommentare wird es aber wohl weiter nicht geben. In der Vereinbarung heißt es dazu nur:  ""Die in der Task Force vertretenen Unternehmen stellen Transparenz sicher, indem sie der Öffentlichkeit darüber berichten, wie sie ihre Nutzungsbedingungen hinsichtlich der Entfernung gemeldeter Inhalte umsetzen. "" 
 ""Als Einstieg bin ich damit zufrieden, als Ergebnis nicht "", sagte der Minister. Er kündigte an, einen externen Anbieter mit der Überwachung der vereinbarten Maßnahmen zu beauftragen.  ""Es geht darum, den Druck auf die Unternehmen zu erhöhen. "" Er sagte auch, es handele sich um ein gesellschaftliches Problem und dass sich die schweigende Mehrheit erheben müsse. 
Die Diskussion um Hassbotschaften im Netz hatte sich nach menschenverachtenden und rechtsradikalen Parolen gegen Flüchtlinge intensiviert. Die betroffenen Unternehmen bewerten das Problem eher klein: Google-Justiziar Arndt Haller etwa sagte, es gebe einen  ""Anstieg, aber keinen signifikanten "". Google überprüfe von jeher jede Beschwerde sowohl auf einen Verstoß gegen die eigenen Inhalte-Richtlinien als auch aufgrund deutschen Rechts. 
Richard Allan von Facebook machte gar keine Veränderungen bei der Anzahl der Hassbotschaften aus. Details dazu, wie die Unternehmen zu dieser Erkenntnis gelangt waren, nannten Allan und Haller nicht. Auch zu konkreten Maßnahmen wie der Einstellung neuer Mitarbeiter wollte sich Allan nicht äußern. 
Laut einer Erhebung des Branchenverbandes Bitkom hat fast jeder zweite Internetnutzer schon mal Beschimpfungen, rassistische Beleidigungen und Gewaltandrohungen im Internet gelesen. Darüber hinaus hat sich demzufolge jeder neunte Internetnutzer schon einmal selbst als Opfer solcher Äußerungen empfunden. 
Grünen-Fraktionschefin Katrin Göring-Eckardt und Fraktionsvize Konstantin von Notz kritisierten die Vereinbarung: Justizminister Maas lasse sich von Facebook vorführen. Die Diskussion um effektiveres Prüfen und Löschen gefährlicher Inhalte werde seit Jahren geführt, doch außer bei Aufnahmen nackter Haut reagiere das Netzwerk weiterhin schwerfällig. Urheber von Hassbotschaften müssten deutschen Behörden rigoros gemeldet werden, so dass diese mit angemessener Personalausstattung strafrechtlich reagieren könnten. Die milliardenschweren Unternehmen dürften sich nicht so aus der Verantwortung stehlen. 
Die Hamburger Staatsanwaltschaft ermittelt inzwischen wegen Volksverhetzung gegen mehrere Facebook-Manager. Sie wurden angezeigt, weil das Unternehmen Hasskommentare nicht gelöscht hatte."	technik
"In Saudi-Arabien gibt es eine Fatwa gegen Pokémon Go. In Bosnien-Herzegowina laufen Spieler in vermintes Gebiet, um Pokémon zu fangen. In der Lüneburger Heide gerieten drei Pokémon-Fans in eine Schießübung der Bundeswehr, als sie auf einem Truppenübungsplatz nach den virtuellen Figuren suchten. 
Jeden Tag erscheinen solche Meldungen. Das ist Entertainment. Und damit Teil einer über Jahre eingeübten Reaktion auf Apps, die zum Hit werden. Menschen machen verrückte Sachen damit, findige Unternehmer springen aufs Trittbrett, Kriminelle verbreiten verseuchte Kopien oder vermeintlich hilfreiche Zusatzprogramme, Datenschützer warnen, Medien veröffentlichen Listicles mit 30 Dingen, die Sie unbedingt über den Hype wissen müssen und so weiter. Neu ist das alles nicht, das gab es so oder ähnlich auch bei Angry Birds oder Quizduell und im Prinzip schon bei Pac-Man. Neu ist nur die zeitliche Verdichtung. 
Was sich bei vorherigen Hypes über mehrere Monate zog, passiert nun in wenigen Tagen. Das ist schlicht professionell. Aufmerksamkeit und Geld, das haben App-Entwickler, Firmen, Politiker und Medien gelernt, bekommen nur die Schnellsten. Also diejenigen, die ihre werbefinanzierten Pokémon-Tools als erste in die App-Stores bekommen, die umgehend passende Dienstleistungen bis hin zum Pokémon-Go-basierten Dating-Service anbieten, die sich frühzeitig als Pokémon-Go-Experten und –Analysten positionieren, die Pokémon für Marketing und Werbung einsetzen, bevor die Kundschaft davon genervt ist, oder die als erste über eine besonders skurrile Anekdote berichten, die Spieler erlebt haben. 
Wer zu spät kommt, den bestraft der Google-Algorithmus. Der wird im App Store und in der Suchmaschine nicht gefunden und zurecht als Abklatsch wahrgenommen. Und verpasst möglicherweise auch die Phase, in der Pokémon Go sozusagen ein perfekter Sturm ist. Für den gibt es drei Gründe: Erstens ist Pokémon Go als Kombination aus Augmented-Reality-Spiel und einer weltweit bekannten Marke bisher einmalig. Es gibt keine vergleichbare Alternative. 
Zweitens ist das Timing, so zynisch das klingen mag, perfekt:  ""Wir brauchen jetzt sinnlosen Spaß, mehr als je zuvor "", schreibt zum Beispiel die Journalistin Grace Dent vom britischen Independent in Anspielung auf die Terroranschläge, Attentate und sonstigen schrecklichen Nachrichten der vergangenen Wochen. Pokémon Go kann und darf den Menschen eine Auszeit von jener Realität verschaffen, die sie derzeit mehr oder weniger in Echtzeit auf ihre Smartphones gepusht bekommen. 
Drittens könnte sogar das Wetter eine Rolle spielen. In fast allen Ländern, in denen das Spiel offiziell in den App Stores steht, ist gerade Sommer. Ob die Pokémon-Fans in vergleichbarer Anzahl auch im November vor die Tür gehen, um zu spielen, ist offen. 
Und viertens ist das Spiel möglicherweise nur so lange popluär, wie es ein casual game ist. Es kann passieren, dass viele Spieler das Interesse verlieren, wenn sie merken, dass es in den höheren Leveln sehr viel aufwändiger und frustrierender ist, signifikante Fortschritte zu machen, ohne sich diese zu erkaufen. Wie viele von ihnen lieber Geld ausgeben und In-App-Käufe tätigen, um schneller voranzukommen, kann man jetzt noch nicht abschätzen. 
Die Unsicherheit ist der Grund, warum jetzt alles so schnell gehen muss. Es ist aber durchaus möglich, dass Pokémon Go noch lange äußerst erfolgreich bleibt. Betreiber Niantic hat es selbst in der Hand, mit Updates neue Anreize zum Spielen zu schaffen. Und es kann auch im November noch warm genug sein, um abends im Park ein seltenes Mewtwo zu jagen."	technik
"Apple-Gründer Steve Jobs hat sich immer wieder gern als Kind der Hippiebewegung bezeichnet. Die in seiner Onlinegeschäftsplattform für Musik, Filme, Spiele und Bücher waltenden Zensoren scheinen nicht an diese freiheitliche Tradition anknüpfen zu wollen. Apple hat mehr als 40 Jahre nach der Revolte der 68er ganz offensichtlich Probleme mit Nacktheit. Und Spaß versteht der Konzern in diesem Punkt auch keinen. 
Das zumindest musste der Däne Peter Övig Knudsen erfahren. Er ist Autor zweier Bücher über die dänische Hippiebewegung der sechziger und siebziger Jahre. Im Apple-Buchladen iBookstore durften seine beiden Titel Hippie 1 und Hippie 2 erst verkauft werden, nachdem auf sämtlichen 47 Fotos aus der wilden Zeit die Geschlechtsteile der nackten Hippies verdeckt worden waren. 
Der Autor allerdings hielt es für den Ausdruck von Doppelmoral, die Nacktfotos in seinen Büchern über die aufmüpfige Hippiegeneration einfach mit schwarzen Balken auf Hintern und Brüsten zu entschärfen. Also verzierte er seine anstößigen Hippies an den entsprechenden Stellen mit prallen, roten Äpfelchen. Das half nur kurz. Die beiden Bücher wurden schnell wieder aus Apples Sortiment gelöscht. Vermutlich, nachdem Apple-Zensoren die Anspielung auf den eigenen Konzern aufgefallen war. 
Apple schweigt 
 ""Ich bin gespannt von Apple zu hören, warum die Bücher nun wieder aus dem Sortiment gelöscht wurden. Brüste, Pimmel und Popos sind ja nun alle ordentlich verdeckt "", sagt Övig.  ""Natürlich wollten wir Apple damit veräppeln, aber lustig ist die ganze Sache überhaupt nicht. "" Nicht einmal eine Erklärung gab es dafür. Er habe  ""monatelang versucht "", mit Apple ins Gespräch zu kommen, sagt er. Doch die privatwirtschaftlichen Geschmackshüter aus Kalifornien hätten sich nicht auf einen Dialog eingelassen. 
Es ist beileibe nicht das erste Mal, dass sich das Unternehmen als Kulturzensor aufspielt. Beispielsweise war die App der Pinakothek der Moderne gesperrt , weil sie die  ""Schlafende "" zeigte, ein Bild des Expressionisten Max Beckmann . Viele solcher Beispiele gibt es inzwischen. Der dänische Autor wollte die Verstümmelung seines Bildbandes nicht hinnehmen und schrieb einen offenen Brief ab den dänischen Kulturminister. 
Kulturminister Uffe Elbaek reagierte darauf mit einer Initiative.  ""Apple muss zurück auf den Teppich kommen, und sein Verhalten erklären. Alles andere ist unhaltbar "", sagte er am Freitag. Er kündigte an, beim kommenden EU-Kulturministertreffen mit seinen Kollegen eine EU-weite Initiative gegen solche Zensierungstendenzen zu diskutieren. Er habe bereits seine EU-Kollegen kontaktiert, um zu prüfen in wieweit sie ähnliche Probleme mit Apple haben, sagte er.  ""Der Fall wirft grundlegende Fragen auf: Was geschieht, wenn einige Akteure so groß werden, dass sie ein ganzes Feld so dominieren können? "", so der Minister. 
 
Auch der dänische Journalistenverband kritisiert den Apple-Konzern.  ""Das ist ein Eingriff in die künstlerische Freiheit "", so dessen Vorsitzender Mogens Blicher Bjerregaard. Da kehre die Kulturzensur in die freiheitlichen westlichen Gesellschaften zurück, warnte er.  ""Apple ist ein globales Unternehmen. Aber wir in Europa leben in einer Region, die vielleicht andere Werte bezüglich Ausdrucksfreiheit, ethischen Fragen und Fragen der geschlechtlichen Gleichstellung hat. Deshalb muss das Thema diskutiert werden. "" 
Natürlich sind die dänischen Hippie-Bücher völlig ohne Äpfel über andere Kanäle erhältlich. Apple ist selbstverständlich auch ein Unternehmen und darf nach eigenen Maßstäben handeln. Wer sich dem nicht unterwerfen mag, muss es nicht nutzen.  ""Aber "", so Autor Övig,  ""stellen Sie sich vor, was wäre, wenn Apple einmal eine Monopolstellung beim Vertrieb von Büchern einnimmt? "" Bei Musik beispielsweise ist das nahezu der Fall. Wer Musik vertreiben will, kommt an iTunes nicht vorbei. Und auch dort zensiert Apple nach Gutdünken Songtexte, Albumcover und Bandnamen . 
Das Verhalten Apples ist ein gutes Beispiel für das eigentliche Problem, das Vorgehen ist willkürlich. Heute können es Brüste sein, die nicht gezeigt werden dürfen, morgen politische Aussagen oder rote Haare. 
Die amerikanische Schriftstellerin Naomi Wolf schrieb ein Buch, in dem sie kritisch beleuchtet, warum das weibliche Geschlechtsorgan in vielen Kulturen als etwas angesehen wird, dessen man sich gefälligst zu schämen habe. Der Titel von Wolfs Buch im Original: VAGINA . Apples bewies eindrucksvoll, dass man Teil des Problems ist, das Wolf beschreibt. Im Online-Buchladen darf es nur unter dem Titel V****A verkauft werden ."	technik
"In einem Gerichtsverfahren in Köln sind Bewegungsdaten eines Carsharing-Fahrzeugs ausgewertet worden. Dabei ist unklar, wie das überraschend präzise Bewegungsprofil des Fahrers überhaupt zustande kam. 
Wie das manager magazin berichtet, forderte die Kammer die Informationen im Verfahren gegen einen BWL-Studenten an. Er war Ende Mai wegen fahrlässiger Tötung zu 33 Monaten Haft verurteilt worden, weil er im Juli 2015 mit einem BMW des Carsharing-Anbieters Drive Now einen Radfahrer überfahren hatte. 
Laut dem Bericht konnte die Kammer anhand der Daten von BMW die Wegstrecke sowie die gefahrenen Geschwindigkeiten genau rekonstruieren. Außerdem waren unter anderem die Außentemperatur oder die Position des zur Buchung verwendeten Mobiltelefons in den Daten enthalten. 
Ein Sprecher von BMW bestätigte dem Magazin, dass der Datensatz von BMW stammt. Nach dem Unfall hatte die Polizei das Fahrzeug eingezogen, nachträgliche Manipulationen durch Dritte sind damit ausgeschlossen. Allerdings ist ungeklärt, wieso BMW die Kundendaten so detailliert vorliegen hatte. 
Das Sharing-Unternehmen Drive Now bestritt auf Anfrage, Wegstrecken oder Geschwindigkeiten zu speichern. Lediglich Ort und Zeitpunkt des Mietbeginns und -endes würden erhoben. Das ist für das Stellen der Rechnung notwendig. 
Die Speicherung von Daten mit dem Ziel, ein Bewegungsprofil zu erstellen, ist nach Ansicht des früheren Datenschutzbeauftragten der Bundesregierung verboten.  ""Bewegungsprofile sind ohne ausdrückliche Einwilligung des Kunden unzulässig "", zitiert das Magazin Peter Schaar. 
Vor wenigen Jahren war ein ähnliches Problem bei Apple-Geräten bekannt geworden: iPhone und iPad speicherten Bewegungsprofile, ohne dass die Nutzer das konkret wussten oder verhindern konnten, ohne auf die Ortungsfunktionen der Apps zu verzichten. Nur in den Geschäftsbedingungen gab es einen verklausulierten Vermerk dazu. Angesichts in den USA drohender Sammelklagen reagierte das Unternehmen später und brachte eine Aktualisierung des Handy-Betriebssystems heraus, das weniger Daten von WLAN-Knoten und Funkzellen speichert."	technik
"Immer mehr Firmen bieten ihren Mitarbeitern an, anstelle eines Dienstwagens geleaste Fahrräder zu nutzen. Die beiden Anbieter JobRad in Freiburg und Lease-a-Bike in Cloppenburg haben inzwischen Verträge mit mehr als 4.000 Beschäftigten in etwa 2.500 Betrieben. Besonders beliebt sind dabei E-Bikes und Pedelecs, die etwa bei Lease-a-Bike einen Anteil von 80 Prozent ausmachen mit einem Durchschnittswert von 2.800 Euro. S-Pedelecs, die aufgrund der höheren Motorleistung ein Nummernschild und eine Versicherung benötigen, werden bei fast allen interessierten Unternehmen ausgeschlossen. 
Lease-a-Bike, eine Tochtergesellschaft des Fahrradherstellers Derby Cycle, ist erst seit dem August des vergangenen Jahres auf dem Markt. Seitdem entschieden sich mehr als 400 Unternehmen, Diensträder anzubieten. 1.800 Mitarbeiter haben mittlerweile entsprechende Verträge unterschrieben. Frank Stargardt, Leiter der 2014 gegründeten Abteilung Dienstrad bei Derby Cycle, ist davon überzeugt, dass hier ein zukunftsträchtiges Modell heranwächst.  ""Das Interesse ist riesig. Wir sind derzeit mit einigen großen Konzernen in Gesprächen, die kurz vor dem Abschluss stehen "", sagt Stargardt. 
Er nennt in diesem Zusammenhang das Projekt E-Bike-City in Hannover. 27 Unternehmen mit etwa 500.000 Beschäftigten aus dem Großraum der niedersächsischen Landeshauptstadt haben sich zum Ziel gesetzt, möglichst viele Mitarbeiter für das Fahrrad zu gewinnen und so 10.000 E-Bikes in die Stadt zu bekommen. Schon wenn 6.000 Elektroräder statt Autos ein halbes Jahr lang in der City unterwegs wären, würde das den CO2-Ausstoß um fünf Prozent reduzieren, haben die Experten ausgerechnet. Und nicht nur das. Andere Untersuchungen ergaben, dass radelnde Mitarbeiter bis zu einem Drittel weniger krank sind als andere. 
Um die Pläne in die Tat umzusetzen, muss in den Betrieben die Infrastruktur verändert werden. Beispielsweise seien mehr Abstellflächen für Räder notwendig, sagt Stargardt. Wichtig ist seiner Ansicht nach aber auch, dass der Aufwand für alle Beteiligten so gering wie möglich sein muss. So dürfe der Arbeitgeber nicht unnötig mit Verwaltungsaufgaben und möglichen Versicherungsproblemen belastet werden. 
Der Vorteil für den Mitarbeiter: Er muss zwar das Dienstfahrrad – wie auch einen Dienstwagen – als sogenannten geldwerten Vorteil versteuern, doch das kann sich gegenüber dem Privatkauf oder Privatleasing des Rades lohnen. Ein Beispiel: Bei einem Einkommen von 2.400 Euro, Steuerklasse III und einem Preis für ein E-Bike von 2.500 Euro liegt die monatliche Rate bei knapp 70 Euro; die tatsächliche Nettobelastung für den Mitarbeiter, inklusive der sogenannten Ein-Prozent-Regelung zur Versteuerung des geldwerten Vorteils, liegt dagegen bei nur etwas mehr als 50 Euro. 
Wichtig ist laut Stargardt, dass Rechtssicherheit besteht und die wirtschaftliche Zuordnung des Rades eindeutig geregelt ist. Das erfolgt über eine Zusatzvereinbarung zum Arbeitsvertrag. Auch müsse der Leasingvertrag so gestaltet sein, dass bei Diebstahl oder Schäden das Risiko nicht bei dem Mitarbeiter liege, so Stargardt, sondern über eine Versicherung abgedeckt werde, die Bestandteil des Leasingvertrags ist. Die einmal pro Jahr erforderliche Wartung des Rades beim Händler könne über ein elektronisches Ampelsystem gesteuert und kontrolliert werden. 
Generell übernimmt bei Lease-a-Bike der jeweilige Händler die komplette Abwicklung. Dass auch der Fachhandel von dem Konzept profitiert, ist für Stargardt sicher.  ""Gute Beratung und Abwicklung ist die beste Werbung. "" Es gebe Händler, die schon jetzt zwei Drittel ihres Jahresumsatzes mit Leasingrädern machen würden. 
Auch Ulrich Prediger, Geschäftsführer bei JobRad in Freiburg ist überzeugt von einer erfolgreichen Zukunft des Modells Leasingräder. 2.000 Arbeitgeber sind mittlerweile seine Kunden, die Leasingräder für ihr Personal anbieten – Tendenz steigend. Natürlich seien mit Fahrrädern keine Dienstwagen zu ersetzen, räumt Prediger ein. Doch es könnten andere Bereiche in Unternehmen damit angesprochen werden. Zum Beispiel Beschäftigte mit geringerem Einkommen und in Positionen, die keinen Anspruch auf Dienstwagen haben. Prediger sieht in dem Angebot einen hohen Motivationsschub und eine Wertschätzung der Mitarbeiter. 
Gleichzeitig setze das Unternehmen damit ein Zeichen in Sachen Umweltfreundlichkeit, das Parkplatzangebot könne möglicherweise reduziert und das Radfahren in das Gesundheitsmanagement eingebunden werden. Die Leasingdauer beträgt 36 Monate, dann kauft JobRad das Bike zurück und bietet es für kleines Geld dem Nutzer an. Bundesweit arbeitet das Unternehmen inzwischen mit 2.500 Händlern zusammen. Die verdienen am Mehrabsatz, das Leasingunternehmen an dem ausgehandelten Rabatt."	technik
"Wie leicht sich der Fingerabdrucksensor eines Smartphones austricksen lässt, ist seit 2013 bekannt. Der Berliner Sicherheitsforscher Jan Krissler hatte das damals an einem iPhone 5s demonstriert, das er mit einer Fingerattrappe entsperren konnte. Die US-Polizei versucht nun etwas Ähnliches, um das Smartphone eines Mordopfers durchsuchen zu können. Sie braucht also Kopien der Finger eines Toten. 
Helfen soll dabei der Informatikprofessor Anil Jain von der Michigan State University. Der hat, ähnlich wie Krissler, gewisse Erfolge im Hacken von Smartphone-Zugangssperren vorzuweisen. Im Frühjahr war es ihm gelungen, Fingerabdrucksensoren eines Samsung Galaxy S6 und eines Honor 7 zu täuschen: Dazu druckte Jain Kopien von Fingerabdrücken aus und hielt die Zettelchen zum Entsperren an den jeweiligen Home-Button. Das Besondere: Weil er leitfähiges Papier und leitfähige Silbertinte benutzte, reagierten die kapazitiven Sensoren darauf, als wären die Kopien lebende Finger. 
Die Ermittler aus den USA wollen nun mit Jains Hilfe an die Daten auf dem Smartphone eines Mordopfers herankommen und es nach Hinweisen auf den möglichen Täter durchsuchen. Sie könnten sich zwar auch an den Hersteller des Geräts wenden und hoffen, dass er ihnen beim Entsperren oder bei der Umgehung der Sperre behilflich ist. Aber der Fall Apple versus FBI hat gezeigt, dass sich Unternehmen mitunter nur ungern dafür einspannen lassen, eigene Sicherheitsmaßnahmen auszuhebeln. 
Die Fingerabdrücke des Opfers nachzumachen, ist zudem vergleichsweise einfach. Die waren nämlich schon vor der Tat bei der Polizei hinterlegt. Offenbar war das Opfer zu Lebzeiten festgenommen worden und musste Fingerabdrücke abgeben. Jain und seine Mitarbeiter mussten die zehn Abdrücke also nur noch auf Fingerattrappen aus dem 3-D-Drucker aufbringen. 
Damit die ähnlich wie die Finger eines lebenden Menschen Strom leiten und so auch kapazitive Sensoren auslösen, überzogen die Forscher sie mit einer dünnen Schicht metallischer Partikel. Vermutlich hätte das auch mit den echten Fingern des Toten funktioniert, aber offenbar stand der Polizei dieser Weg nicht zur Verfügung. Möglicherweise ist der Mann längst beerdigt. 
Krissler musste seine Attrappe damals nur anhauchen. Die Feuchtigkeit hatte genügt, um den iPhone-Scanner zu überlisten. Alternativ ließ sich die Leitfähigkeit seiner Attrappe mit Graphitspray verbessern. Die einzelnen Arbeitsschritte hat der Chaos Computer Club (CCC) hier beschrieben. 
Ob Jain und sein Team etwas substanziell Neues versuchen, ist nicht ganz klar, weil Jain aufgrund der laufenden Ermittlungen nicht alle Details preisgeben will. Zum Beispiel, um welches Smartphone-Modell es sich handelt. Möglicherweise machen sie praktisch genau das Gleiche wie im Frühjahr und zuvor schon Krissler und der CCC, nur eben mit den Fingerabdruck eines Toten. 
 
Noch arbeiten er und seine Kollegen jedenfalls an den Feinheiten, die Behörden haben also noch nicht ausprobiert, ob der Trick funktioniert. Zudem müssen die Polizeibeamten dann noch herausfinden, welchen Finger der Mann zum Entsperren benutzt hat. Wenn sie Pech haben und er anders als die meisten Nutzer nicht den Daumen- oder den Zeigefingerabdruck im Gerät hinterlegt hat, müssen sie nach fünf Fehlversuchen die PIN oder das Passwort eingeben – das sie aber nicht kennen. 
Viele Smartphone-Modelle verlangen nach einer gewissen Zeit, in der sie nicht per Fingerabdruck entsperrt wurden, die Eingabe einer PIN oder eines Passworts. In diesem Fall wäre die Polizei selbst dann keinen Schritt weiter, wenn einer der künstlichen Finger funktionieren würde. 
Grundsätzlich könnte die Technik auch angewandt werden, wenn sich Verdächtige weigern, ihr Passwort zu nennen. Denn mit einem entsprechenden Gerichtsbeschluss darf die US-Polizei ein Smartphone durchsuchen, und Fingerabdrücke nehmen darf sie sowieso. Auch in Deutschland ist so ein Vorgehen denkbar. 
Der Trick mit den Metallpartikeln funktioniert möglicherweise so lange, bis die Gerätehersteller einen Weg finden, den Unterschied zwischen lebenden und nicht-lebenden Fingerkuppen zweifelsfrei festzustellen. Zumindest beim iPhone 6 hat Apple das jedenfalls noch nicht hinbekommen, auch dessen Touch-ID-Sensor lässt sich von Attrappen täuschen."	technik
"Die gute Nachricht: Die CDU will die  ""Vorratsdatenspeicherung light "" nicht einführen. Bundesinnenminister Thomas de Maizière verzichtet offenbar auf einen Abschnitt im geplanten IT-Sicherheitsgesetz, der es Anbietern von Telekommunikationsdiensten erlaubt hätte,  ""zum Erkennen, Eingrenzen oder Beseitigen von Störungen oder Fehlern "" die Bestands- und Verkehrsdaten ihrer Nutzer zu erheben und zu verwenden. 
Die schlechte Nachricht: Die CDU will die Vorratsdatenspeicherung wieder einführen. So steht es im Antrag D 2 des Parteivorstands, der heute beim Parteitag in Köln beschlossen werden soll.  ""Die Wiedereinführung von Speicherfristen für Verbindungsdaten und die effektive Möglichkeit der Überwachung auch verschlüsselter Kommunikation (Quellen-Telekommunikationsüberwachung) sind (...) essentiell "", heißt es da. 
Die noch schlechtere Nachricht: Die CDU wird mit ihrem Wunsch in Brüssel auf offene Ohren stoßen. Denn die neue EU-Kommission will die Vorratsdatenspeicherung auch wiederhaben. Laut netzpolitik.org gehe es nur noch um das  ""wie "", nicht mehr um das  ""ob "". Angeblich schon Mitte 2015 wolle EU-Innenkommissar Dimitris Avramopoulos seinen Vorschlag für eine neue Richtlinie vorstellen. 
Man darf gespannt sein, wie der Vorschlag aussehen wird. Denn der Europäische Gerichtshof hat mit seinem Urteil vom April die bisherige EU-Richtlinie 2006/24/EG für ungültig erklärt und die Latte für eine grundrechtskonforme Ausgestaltung extrem hoch gelegt. Zwar hatte der EuGH die Datenspeicherung grundsätzlich als  ""nützliches Mittel für strafrechtliche Ermittlungen "" anerkannt, obwohl es dafür keine belastbaren Belege gibt. 
Der Eingriff in die Grundrechte auf Achtung des Privatlebens und auf Schutz personenbezogener Daten müsse sich aber  ""auf das absolut Notwendige beschränken "". Genau das aber gewährleistet die anlasslose und ausnahmslose Datenspeicherung, die alle EU-Bürger zu Verdächtigen macht, eben nicht, stellten die Richter fest. Damit erklärten sie, ohne es ganz ausdrücklich zu sagen, das Grundprinzip der Vorratsdatenspeicherung für unvereinbar mit EU-Recht. 
Im Urteil heißt es nur, der Unionsgesetzgeber habe  ""die Grenzen überschritten "", die er  ""zur Wahrung der Verhältnismäßigkeit (...) einhalten musste "". Wie die Kommission diese künftig wahren will, um nicht erneut 500 Millionen Verdächtige zu erfinden, ist völlig unklar. Klar ist nur: Der Zombie namens Vorratsdatenspeicherung ist noch immer nicht tot, und nur mit Argumenten ist er offenbar auch nicht totzukriegen. 
Wahrscheinlich setzen EU-Kommission und hierzulande die CDU darauf, dass den Gegnern der Vorratsdatenspeicherung nach ihrem jahrelangen Widerstand irgendwann einfach die Luft ausgeht. Und damit vor allem die Bereitschaft, erneut auf die Straße und vor die Gerichte zu ziehen."	technik
"Mal sind es Eimer, mal eine Eisenstange, Leitern oder Holzkeile. Selbst Kühlschränke, Heizkörper oder Stahlplatten liegen bisweilen auf der Fahrbahn. Täglich muss der Verkehrsfunk vor Gegenständen warnen, die von Lkw-Ladeflächen oder Anhängern heruntergefallen sind und ein erhebliches Risiko darstellen. Allein in Baden-Württemberg warnt die Polizei im Durchschnitt täglich rund 18 mal vor Ladung, die auf Autobahnen und Bundesstraßen verloren wurde. 
Kommt es doch zum Crash, heißt es nüchtern im Polizeibericht  ""Unfall durch verlorene Ladung "". Laut Statistik passieren deshalb bundesweit jährlich fast 1.800 schwere Unfälle. Dabei werden rund 600 Menschen verletzt und getötet. 
Experten befürchten, dass diese Zahlen mit der stetigen Zunahme des Lkw-Verkehrs auf unseren Straßen steigen werden. Laut einer Erhebung des Gesamtverbands der Versicherungswirtschaft (GDV) sind in Deutschland rund 70 Prozent aller Lkw-Ladungen nicht oder nur unzureichend gesichert. Jeder fünfte Unfall im Schwerlastverkehr sei auf mangelhafte Ladungssicherung zurückzuführen, melden die Unfallforscher der Versicherungen. Diese Karambolagen sollen Kosten von jährlich rund 300 Millionen Euro verursachen. 
Die richtige Ladungssicherung geht aber nicht nur die Fahrer von Lkw und Kleintransportern an. Auch Pkw-Besitzer sollten sich darüber Gedanken machen, wenn sie ihren Wagen vor der Urlaubsfahrt oder nach dem Einkauf beladen.  ""Die Ladung einschließlich Geräte zur Ladungssicherung sowie Ladeeinrichtungen sind so zu verstauen und zu sichern, dass sie selbst bei Vollbremsung oder plötzlicher Ausweichbewegung nicht verrutschen, umfallen, hin- und her rollen, herabfallen oder vermeidbaren Lärm erzeugen können "", lautet die Vorschrift in der Straßenverkehrsordnung, die nur die wenigsten Autofahrer kennen und beherzigen. 
Unfallforscher sprechen von  ""gefährlicher Bequemlichkeit "". Fast bei jedem zweiten von insgesamt rund 15.000 Autos, die der Auto Club Europa (ACE) vor einiger Zeit an Baumärkten, Möbelhäusern und Einkaufszentren kontrollierte, hatten Autofahrer die Ladung nicht richtig gesichert. Die Fachleute des ACE warnen vor der  ""brachialen Gewalt "", mit der ungesicherte Koffer, Taschen, Einkaufskörbe oder Getränkekisten beim Unfall nach vorne schleudern und die Autoinsassen verletzen können. 
 ""Wie ein Hammer schlagen die Gegenstände dann zu "", berichten auch die Sicherheitsexperten des TÜV Süd. Sie wissen, wovon sie reden: Bei einem Auffahrunfall mit 50 km/h haben sie an der Ladung Verzögerungswerte von bis zu 40 g gemessen, also das 40-Fache der Erdbeschleunigung. Anders gerechnet: Ein 15 Kilogramm schwerer Koffer katapultiert mit dem 40-fachen seines Eigengewichts nach vorne, hat dann also eine Aufprallwucht von mehr als einer halben Tonne. 
 
Doch auch weitaus kleinere Gegenstände können bei einer Vollbremsung oder einem Unfall große Wirkung haben: Der zwei Kilogramm schwere Autoatlas auf der Hutablage fliegt zum Beispiel mit der Kraft von 80 Kilogramm durch den Innenraum und wird zum gefährlichen Geschoss. Ein Regenschirm erreicht ein Aufprallgewicht von 25 Kilogramm, und selbst das Smartphone kann die Insassen beim Unfall mit einer Aufprallwucht von sechs bis sieben Kilogramm treffen. 
Besonders groß ist das Verletzungsrisiko bei Kompaktwagen, Kombis und Offroadern, die keine stabile Trennwand zwischen Gepäckraum und Passagierabteil haben. Hier empfehlen Experten dringend den Einbau von Trenngittern oder -netzen. Viele Autohersteller liefern solche aber nur gegen Aufpreis. Volkswagen beispielsweise verlangt dafür beim Golf Variant 180 Euro, Opel beim Insignia Sports Tourer 150 Euro und Ford beim Mondeo Turnier sogar 255 Euro. Audi, BMW und Mercedes liefern das Trennnetz bei ihren Kombis serienmäßig, bei anderen Modellen wie Audi A3, Mercedes B-Klasse oder BMW 2er kostet das Sicherheitsextra immerhin bis zu 230 Euro Aufpreis. 
ADAC-Fachleute warnen:  ""Wenn die Ladung über die Rücksitzlehne hinausragt, muss unbedingt ein Trennnetz oder ein Trenngitter installiert werden. "" Noch besser ist es, das Urlaubsgepäck im Kofferraum zusätzlich festzuzurren, so dass es beim Unfall nicht verrutschen kann. Dafür taugen aber keine einfachen Schnüre, denn sie halten beim Unfall den enormen Kräften nicht stand. Notwendig sind stattdessen Spanngurte mit mindestens 700 Kilogramm Reißfestigkeit. Auch geprüfte Gepäcknetze, die über Koffer und Taschen gespannt werden, bieten guten Schutz. 
Voraussetzung ist freilich, dass es im Kofferraum des Autos stabile Verzurrösen zum Einhängen der Gurte und Netze gibt. Eine Einbaupflicht für solche Sicherheitsmaßnahmen zum Schutz der Autoinsassen vor Ladung und Gepäck gibt es in Europa nicht. Rüstet der Autohersteller seine Modelle aber mit Verzurrösen und Trennnetzen aus, dann müssen sie den strengen Anforderungen entsprechen. Dabei wird die Stabilität der Zurrpunkte mit einer Zugkraft von jeweils 350 Kilogramm gemessen. 
Gegenstände, die während der Fahrt nicht benötigt werden, gehören in den Kofferraum. Wenn trotzdem Taschen oder Kartons auf dem Rücksitz abgestellt werden, sollten sie mit dem Gurt gesichert werden. Die Ablage hinter den Rücksitzen ist für schwere Gegenstände wie Autoatlas oder Walkman nicht geeignet. 
Im Kofferraum gehören schwere Gegenstände nach unten. Grundsätzlich gilt: Je tiefer die Ladung liegt, desto sicherer ist es. Damit die Gegenstände nicht verrutschen, sollte man sie mit reißfesten Spanngurten festzurren oder sie mit einem speziellen Gepäcknetz abdecken. 
Ebenso wichtig ist es, große, schwere Gepäckstücke im Kofferraum nach vorne zu schieben, so nah wie möglich an die Rücksitzlehne. Die Laderaumabdeckung sollte man stets zuziehen, denn sie verhindert, dass das Gepäck nach oben katapultiert wird. Ein Trennnetz oder -gitter ist stets erforderlich, wenn die Ladung im Kombi, Offroader oder Kompaktwagen über die Rücksitzlehne ragt. Beim Kauf von Spanngurten und Trenngittern darauf achten, dass die Zubehörteile das Prüfzeichen ECE R 17 oder DIN 75410-2 tragen. 
Auch Tiere, die ungesichert auf dem Rücksitz liegen, können bei einer Vollbremsung oder beim Unfall zur Gefahr für die Autoinsassen werden. Wer einen speziellen Hundegurt kaufen möchte, sollte nach Meinung des ADAC auf eine stabile Ausführung achten. Das Gurtband sollte möglichst kurz sein und mit dem Gurtschloss im Auto verbunden werden. 
Am sichersten ist dem Autoclub zufolge eine Transportbox, die in den Fußraum hinter den Vordersitzen passt. Größere Tiere reisen am besten im Gepäckraum eines Kombis oder Offroaders, der durch ein stabiles Netz oder Gitter vom Fahrgastraum abgetrennt wird. 
Auch für die umklappbaren Rücksitzlehnen von Kompakt- und Kombimodellen gibt es Prüfvorschriften. Die Norm schreibt unter anderem vor, dass die Lehnen und ihre Verriegelungspunkte bei einem Frontalcrash mit Tempo 50 zwei jeweils 18 Kilogramm schweren Holzwürfeln standhalten sollen – recht wenig angesichts der weitaus höheren Zuladungskapazität moderner Kombis. 
Darum sieht die Wirklichkeit oft anders aus. Was beim Unfall eines vollbeladenen Kombis tatsächlich passieren kann, zeigt ein Crashtest des TÜV: Bei 48,7 km/h Aufprallgeschwindigkeit knickte die Rücksitzlehne unter der Last der Ladung einfach um. Deshalb kamen die Kisten und Koffer aus dem Laderaum bereits 0,4 Sekunden nach dem Crash im vorderen Teil des Autos an und hätten dort Fahrer und Beifahrer unter sich begraben. Zum Glück war es nur ein Test."	technik
"Tausende liegen erschlagen herum, wie Gras werden die Soldaten vom strahlenden Helden niedergemäht. Die Gewaltdarstellung ist grausam, die Szenerie könnte abscheulicher kaum sein. Alexander heißt der Held, beschrieben werden seine Taten – teils faktisch, zu einem großen Teil jedoch fiktiv – in einem Roman, dem Alexanderroman. Entstanden im 12. Jahrhundert, bezeichnet man diese Gattung heute als mittelalterlichen Antikenroman. 
Die Literaturtheorie und -kritik hat ihre Kategorien, Gattungen und Begriffe, um ihr Medium ausreichend und exakt zu beschreiben. Jahrhundertelang hat sie daran gearbeitet, ihr eigenes Instrumentarium ausgebildet und angewendet. So kann auch Gewaltdarstellung in der Literatur analysiert und kritisiert werden. Videospiele hingegen kennen diese Begriffe und Kategorien kaum. Kulturkritik ist dem Medium oftmals fremd und das fällt jetzt wieder einmal auf. 
Denn wieder einmal geht das Schreckgespenst  ""Killerspiel "" um. Der Grund ist der Amoklauf eines 18-Jährigen in München und die reflexhafte Verschlagwortung in diesem Fall durch Bundesinnenminister Thomas de Maizière, der umgehend vom  ""unerträglichen Ausmaß von gewaltverherrlichenden Spielen im Internet "" sprach, die  ""auch eine schädliche Wirkung auf die Entwicklung von Jugendlichen "" hätten.  ""Das kann kein vernünftiger Mensch bestreiten "", fügte er noch in eigentlich überwunden geglaubter Ignoranz aller wissenschaftlicher Erkenntnisse hinzu, die alles andere als eindeutig sind. 
Hier soll keinesfalls abgestritten werden, dass eine Dominanz der Darstellung und Spielbarmachung von Gewalt in Videospielen dazu beitragen kann, Denkstrukturen zu formen. Dennoch kann, um im Duktus des Ministers zu bleiben, kein vernünftiger Mensch behaupten, dass ein Spiel mit expliziter Gewaltdarstellung monokausal zu ausgeübter Gewalt außerhalb eines Spiels führt. Vielmehr muss man immer auch Lebensumstände wie geringe gesellschaftliche Teilhabe, Ausgrenzung, Erkrankungen oder Diskriminierung mit einbeziehen. 
Unbestreitbar ist allerdings, dass körperliche Gewalt in vielen Videospielen die einzig mögliche Spielmechanik und damit auch der alleinige Lösungsweg ist – wenn auch zumeist eingebettet in einen erklärenden Kontext. Problematisch kann das sein, weil das Medium selbst durch immer größere Rechenleistung und immer gigantischere Budgets einerseits Geschichten kreiert, in denen die Spielfiguren als fühlende, denkende und auch empathische Charaktere dargestellt werden. Andererseits werden diese Geschichten durch eine Spielmechanik eingeschränkt, die vor allem Waffe und Ziel kennt. 
So ist etwa Nathan Drake in Uncharted 4 ein gefühlvoller Mann, hin- und hergerissen zwischen der Liebe zu seiner Frau und der zu seinem Bruder. Mit ausgefeilter Mimik und hervorragend synchronisierter Stimme konstruierten die Entwickler einen dreidimensionalen Charakter, der dennoch im Spiel mehrere Hundert Menschen, ohne mit der Wimper zu zucken, erschießt. 
Eine Spielreihe wie Uncharted ist bedacht darauf, einen Realismus zu verfolgen, den man aufgrund des fehlenden Vokabulars nur filmisch nennen kann. Die Charaktere sollen glaubhaft und nahbar wirken, werden durch nicht reflektierte Gewalt jedoch wieder nur zu steuerbaren Topoi. 
Leider macht die Videospielbranche beim ewigen Anführen von Statistiken, Studien und Gegenstudien zur Wirkung von Games einfach nur mit. Aber nach all den Jahren der Diskussion um den äußerst eindimensionalen Begriff  ""Killerspiel "" hat sie noch immer nicht gelernt, den Diskurs selbst anzuführen. 
 
Die Debatte um Gewalt in Videospielen hat das Wort  ""Killerspiel "" als Ausgangspunkt. Es stellt sich allerdings die Frage, ob es diese Kategorie Spiel tatsächlich gab, bevor sie mit dem Begriff versehen wurde. Oder aber, ob die Idee, dass es Killerspiele gibt, erst dadurch entstanden ist, dass für sie ein Begriff geschaffen wurde, der sie eindeutig kategorisiert und damit pathologisiert. 
Zwischentöne gehen in einer Debatte, die anklagend und monologisch ist, leider verloren. Dennoch existieren sie. Etwa in Form von Spielen, die ihr Spielprinzip nicht auf die Ausübung von Gewalt zur Problemlösung legen. The Beginner's Guide zum Beispiel ließ die Spielenden auf einer Metaebene das Medium Videospiel und die damit verbundene künstlerische Schaffungskraft selbst reflektieren. Auch können Videospiele wie Doom Gewalt so sehr zu einer Groteske machen, dass sie mehr zur darstellenden Kunst als zur Gewaltdarstellung werden. Zu nennen ist auch ein Spiel wie das 2012 erschienene Spec Ops: The Line, das zwar mit expliziten Gewaltdarstellungen nicht geizt, die Spielenden aber auch immer wieder zum Reflektieren zwingt und die eigenen Taten hinterfragen lässt. Ein gleichzeitiges Agieren und Reflektieren zu ermöglichen – genau darin kann die Stärke dieses Mediums liegen. 
Ebenso ist es nun an der Zeit für die Branche, zu agieren. Sehr lange hat sie den Diskurs an sich herantragen, sich die Begriffe und Kategorien vorgeben lassen. Besser wäre es, wenn sie ihn selbst anführte, wenn sie eigene Begriffe fände – abseits vom Marketingsprech der großen Publisher und von den einfachen Schlagworten der Politik. Nur so kann eine Videospieltheorie und -kritik organisch wachsen. 
Auch die Spielenden selbst könnten zu einer Erörterung dessen, was Videospiele sein können, beitragen – über das ebenfalls reflexhafte  ""das sind doch nur Spiele "" hinaus. Gute Beispiele gibt es bereits, wie den Versuch von Jugendlichen, im Voice-Chat in Counter-Strike mit Gedichten und Liedern einen neuen Ton zu setzen. 
Wer den Diskurs anführt, hat die Möglichkeit, das Thema Gewalt und Gewaltdarstellung von Videospielen zu lösen und auf die gesamte Gesellschaft samt ihren so verschiedenen Medien, kulturellen Strukturen und Bräuche auszuweiten."	technik
"Zuerst hat es die Frontschürze erwischt. Eine Woche war Eddie Brinkmans Fünfer-BMW da gerade alt.  ""Einen Monat später hat der Wagen in einem weiteren Schlagloch einen Stoßdämpfer eingebüßt. Nach den Salz-, Dreck- und Splittorgien im Winter war der Lack auf Front und Haube mit Pickeln übersäht wie ein Teenager "", erzählt der 43-Jährige. Das Blut steigt ihm auch vier Jahre später noch in den Kopf. 
Brinkman wohnt nicht etwa in den staubigen Weiten Nebraskas oder Oklahomas. Seine Heimat ist Manhattan.  ""Aber die Straßen sind schlimmer als im Irak "", da ist er sich sicher. Darum fährt Brinkman jetzt einen SUV – oder U-Bahn. Denn wo gibt es in Downtown schon einen Parkplatz? 
Dass der Banker nur wenig übertreibt, hat er amtlich. Selbst das New Yorker Straßenbauamt bekennt freimütig, dass der Kampf gegen Megaschlaglöcher, bröselnden Asphalt und bröckelnde Brücken kaum noch zu gewinnen ist. Umgerechnet mehr als 40 Milliarden Euro würde es die Stadt New York kosten, die bestehenden Schäden nur zu reparieren. In West Harlems Viertel Nummer 9 im nördlichen Manhattan sind bereits zwei Drittel aller Straßen im mäßigen bis desaströsen Zustand. 
Ziemlich marode ist aber auch die berühmte Park Avenue, die wie viele andere Lebensadern der Stadt fast durchgehend durch Baustellen, Unfälle und neue Straßenschäden malträtiert ist.  ""Die Park umfahre ich immer "", sagt Brinkman. Anderswo ist der Verkehr aber auch nur wenig fließender. Und die Straßenbeläge sind ähnlich katastrophal. Die Straßenarbeiter kommen einfach nicht nach im Verkehrsmoloch der Stadt mit ihren fast zehn Millionen Einwohnern im Großraum. 
Es rächt sich jetzt bitter, dass seit den achtziger Jahren nicht mehr nachhaltig in die Infrastruktur investiert wurde – weder in Neubau noch Erhaltung. So wie überall im Land. Laut dem US-Ingenieursverband American Society of Civil Engineers (ASCE) sind in den Vereinigten Staaten mehr als 150.000 Brücken einsturzgefährdet. Kein Wunder: Allein in New York zum Beispiel feierten 16 Brücken vor mehr als 100 Jahren Richtfest. Sieben Millionen Autos fahren dennoch täglich über Konstruktionen, die offiziell als  ""fracture critical "" eingestuft sind, bruchgefährdet also. 
Auf der Website der Straßenbehörde kann Eddie Brinkman den Zustand seiner Straße nach einem Rating eingestuft sehen und selbst akute Schäden melden – im Prinzip. Nur wann sich dann etwas tut, das steht in den Sternen. Immerhin hat Bürgermeister Bill de Blasio Verkehrsinvestitionen zu einem Schwerpunkt seiner aktuellen Amtszeit erklärt. Fehlt nur noch das Geld dazu. 
Die New Yorker, so sie denn überhaupt ein eigenes Auto fahren, agieren aber schon jetzt so pragmatisch wie der Rest des Landes: Sie bewundern zwar die hochgezüchteten Mustangs oder tiefergelegten AMG-Mercedes – aber kaufen vor allem Hochbeiniges. Die Hälfte aller Privatverkäufe sind Pickup-Trucks. In den Vorstädten und Citys sollen es SUV richten. 
Doch auch Brinkman ist klar, dass sein X6 die New Yorker Straßenprobleme nicht löst. Und unter dem Asphalt liegen erst die wirklichen Gefahren: mehr als 1.500 Kilometer Hauptwasserleitungen, Baujahr 1900 oder früher, ähnlich lieblos gewartet wie die Straßen darüber. Allein im Jahr 2015 gab es deswegen mehr als 400 große Wasserrohrbrüche in New York. Mit Belagskosmetik ist nichts mehr zu retten, wenn die halbe Fahrbahn ins Wasser fällt. 
Vielleicht feiert ja auf der nächsten New York Auto Show das Amphibienfahrzeug ein Comeback? Eddie Brinkman könnte sich womöglich dafür begeistern."	technik
"Niemand hat die Absicht, traditionelle Fotos abzuschaffen. Niemand, außer vielleicht Apple, Facebook, Google und einige Start-ups, die anstelle der unbewegten Aufnahmen allzu gerne kurze Videos einführen möchten. Diesen Eindruck könnte jedenfalls bekommen, wer sich die jüngsten Entwicklungen in der Smartphone- und App-Branche ansieht. 
Live Photos nennt Apple die neue Funktion im iPhone 6S. Standardmäßig nimmt die Software zusätzlich zu jedem Foto noch anderthalb Sekunden vor und nach dem Drücken des Auslösers auf. Dieser dreisekündige Clip lässt sich anschließend über die Force-Touch-Technik durch einen etwas festeren Druck auf das Display abspielen. Das Ergebnis erinnert an ein animiertes Gif, ist aber technisch gesehen ein JPG kombiniert mit einem Video – die Dateien sollen sowohl in der neuen iOS-Version als auch in der neuen Version von OS X (El Capitan) abspielbar sein. Nicht aber, wenn man sie per Mail verschickt oder auf einer Fotoplattform im Netz teilt. 
Schon länger testet Google eine Funktion namens Smart Burst, die künftig in der Kamerasoftware von Android 6.0 verankert sein soll. Das Prinzip: Die Kamera nimmt eine Vielzahl an Bildern pro Sekunde auf und erstellt aus einer Auswahl wahlweise ein animiertes Gif oder eine Collage. Offenbar unterstützt aber nicht jedes Android-Gerät die Funktion. Berichten zufolge ist Smart Burst auf dem neuen Nexus 6P verfügbar, nicht aber auf dem ebenfalls mit Android 6.0 laufenden und etwas günstigeren Nexus 5X. 
Wenn Apple und Google etwas entdeckt haben, lässt Facebook nicht lange auf sich warten. Zu Beginn des Monats präsentierte das soziale Netzwerk bereits animierte Profilfotos, nun legt die Tochter Instagram nach. Boomerang ist eine eigenständige App, mit der die Nutzer einsekündige Videos aufnehmen können. Die geloopten Aufnahmen im MP4-Format lassen sich sowohl über die normale Instagram-App als auch über Apps wie Twitter hochladen und anschließend einbinden. Boomerangs sind somit leichter im Netz zu teilen als die Apple-eigenen Live Photos. 
Einigen mag all das bekannt vorkommen. Bereits im vergangenen Jahr hat das Start-up Phhhoto nahezu das gleiche Prinzip in eine App eingeführt. Die auf Gifs spezialisierte Website Giphy hat inzwischen mit der Gipyh Cam ebenfalls eine App, über die Nutzer eigenes Gifs aus Videos aufnehmen können. Und wer noch weiter zurückdenkt, landet im Jahr 2013 bei HTCs Zoe Camera: Die konnte eine Serienaufnahme in dreisekündige Videoclips umwandeln. Besonders erfolgreich war Zoe nicht, was zur jüngsten Firmengeschichte passt: HTC hat gute Ideen, mit denen andere später Erfolg haben. 
Was nach einem kurzweiligen Trend aussieht, auf den die großen Unternehmen anspringen, ist Teil einer größeren und schon länger anhaltenden Entwicklung. Es geht nicht darum, die klassische Fotografie abzuschaffen, sondern darum, auf die veränderte Mediennutzung und das Nutzerverhalten zu reagieren. In den sozialen Netzwerken werden Bewegtbilder immer beliebter und wichtiger, wie vor allem drei Entwicklungen zeigen. 
Ein Beispiel ist Twitters Kurzvideodienst Vine. Wurden die Sechs-Sekunden-Clips bei der Einführung noch belächelt, hat schon wenig später auch Instagram nachgezogen und Videos von bis zu 15 Sekunden erlaubt. Heute sind Vines in der Netzkultur fest verankert; die Plattform hat ihre eigenen Persönlichkeiten und neue Erzählformate entwickelt und ist auch in anderen Bereichen, etwa der Technik- und Sportberichterstattung, angekommen. 
 
Ein zweites Beispiel ist Livestreaming. Von Google Hangouts über Plattformen wie twitch.tv ist es mittlerweile in der mobilen Welt angekommen. Apps wie Periscope, Meerkat oder Dienste wie YouNow machen es einfacher denn je, den Alltag an die Freunde oder gleich vom Smartphone aus in die gesamte Welt zu streamen. Über die Banalität der Inhalte lässt sich streiten, über den Einfluss nicht: Die Möglichkeit, jederzeit von überall – sofern es ausreichend schnelles mobiles Internet gibt – zu streamen, verändert sowohl die Medien als auch den sozialen Austausch im Netz. 
Ein drittes Beispiel ist die ungebrochene Beliebtheit von animierten Gifs. Das 28 Jahre alte Dateiformat war zwar nie wirklich weg, wurde aber in den vergangenen zehn Jahren wieder richtig populär. Dank schnellerer Internetverbindungen und neuer Aggregatoren wie Giphy sind Gifs beliebter denn je. Sie passen perfekt in unser Medienumfeld, sagte Giphy-Gründer Adam Leibsohn unlängst im Gespräch mit der Süddeutschen Zeitung:  ""Wir kommunizieren unterwegs, zwischendurch, pausenlos, und in Fragmenten. "" Gifs seien ähnlich wie Emojis die perfekte Kommunikationsform, um möglichst viel mit möglichst wenigen Zeichen zu sagen. 
Gifs, Livestreams, Kurzvideos – zusammen bilden sie die Grundlage für Live Photos, Smart Burst und Boomerang. Unternehmen wie Apple und Facebook reagieren mit der Technik auf das Bedürfnis vieler Menschen, persönliche Momente nicht bloß per Foto, sondern in bewegten Bildern festzuhalten, die sich aber trotzdem von klassischen Videos unterscheiden. Anders gesagt: Wenn Clips mit einer Länge von maximal 15 oder sechs Sekunden funktionieren, wieso sollten das nicht auch für solche mit drei oder auch nur einer Sekunde gelten? Und ist es wirklich so abwegig, dass wir in einigen Jahren durch unser Urlaubsalbum gehen und statt klassischen Standbildern jede Menge kleiner animierter Szenen sehen? 
Am Ende dürfte es eine Frage der Kompatibilität sein. Gifs können zwar im Browser und vielen Programmen dargestellt werden, sind aber qualitativ eingeschränkt. Der Image-Hoster Imgur hat das Format Gifv entwickelt, das auf Basis von MP4-Videodateien funktioniert. Diese nutzen wiederum auch Twitter und Instagram, versprechen sie doch bessere Bildqualität bei kleinerer Dateigröße. Apple geht dagegen mit Live Photos offenbar einen eigenen Weg; die bewegten Bilder sind vorerst nur in Apples Betriebssystemen abspielbar. 
Das scheint stark einschränkend, aber tatsächlich könnte vor allem Apple die Entwicklung vorantreiben: Weil Live Photos in den kommenden iPhones standardmäßig aktiviert sind, dürften sehr viele Menschen bald auf den Geschmack kommen. Das bedeutet zwar nicht das Ende des Fotos, könnte aber dennoch zu einer Veränderung führen: Unsere mobile Kommunikation wird bewegter werden."	technik
"Tesla-Gründer Elon Musk hat das erste Elektrofahrzeug seiner Firma für den Massenmarkt, das Model 3, enthüllt. Der Mittelklassewagen soll 35.000 Dollar kosten und ab Ende 2017 ausgeliefert werden.  ""Ich bin recht zuversichtlich, dass es nächstes Jahr wird "", sagte Musk, dessen Unternehmen in der Vergangenheit die Premieren mehrerer Modelle verschieben musste. 
Der Wagen soll in sechs Sekunden auf 100 Kilometer pro Stunde beschleunigen, die Batterie für etwa 350 Kilometer reichen. Das ist etwa doppelt so weit, wie die Batterien aktueller Modelle von Nissan oder BMW reichen.  ""Die Verkaufszahlen dürften deutlich besser sein als beim BMW i3 "", sagte Automobilexperte Ferdinand Dudenhöfer. Das Preisleistungsverhältnis sei besser. 
Tatsächlich hätten mehr als 100.000 Menschen binnen eines Tages ein Model 3 vorbestellt, sagte Musk. Vor Tesla-Filialen in Hongkong, Austin oder Washington warteten Menschen, um sich registrieren zu können. Dafür müssen sie 1.000 US-Dollar hinterlegen. Viele von ihnen hatten das Model 3 noch nicht gesehen. 
Tesla hatte schon länger geplant, einen Wagen für ein großes Publikum einzuführen. Im Jahr 2006 schrieb Musk in seinem Blog, dass Tesla eine große Auswahl an Modellen herstellen will,  ""inklusive bezahlbarer Familienautos "", um die Welt schnell in das Zeitalter der Solarenergie zu führen. Derzeit bietet das Unternehmen zwei hochpreisige Fahrzeuge an, das Model S für 71.000 Dollar und das Model X für 80.000 Dollar."	technik
"Matthias Müller steckt in einer schwierigen Doppelrolle. Der Volkswagen-Chef muss Zerknirschtheit zeigen und den Abgasskandal aufklären – und zugleich Aufbruchstimmung vermitteln. Der Betrug mit Diesel-Abgaswerten, der milliardenteure Rückruf und Schadenersatzklagen belasten VW, und nebenbei läuft das Geschäft auch in Russland, in Brasilien und auf dem wichtigsten Markt China nicht mehr rund. 
Leisere Töne sind also angesagt, aber eben auch klare Ankündigungen: Auf sechs Feldern und mit neuen Strategien muss Müller Gas geben, um Vertrauen wiederzugewinnen. 
Der Diesel ist nicht tot – er ist bloß gerade nicht im Angebot. So etwa lässt sich im Moment die Situation von VW auf dem amerikanischen Markt beschreiben. Vom Dieselmotor für US-Autos will sich in Wolfsburg dennoch niemand generell distanzieren. Aber nach dem rein elektrischen Bulli auf der CES in Las Vegas zeigte VW auch in Detroit lieber seinen ersten Plug-in-Hybrid mit Allrad, den Tiguan GTE Active Concept mit einem Turbobenziner und zwei Elektromotoren: einer vorn, einer hinten. Allrad schafft der Tiguan so rein elektrisch. 
 ""Bis 2020 werden wir 20 zusätzliche Elektrofahrzeuge und Plug-in-Hybride bringen "", verspricht Müller. Wenn der Ölpreis wieder steigt, soll VW schließlich den Teslas und Toyotas auch jenseits des Dieselantriebs Konkurrenz machen können. 
 ""Vernetzung ist eine große Chance für den VW-Konzern "", sagt Müller. Die Konzerntochter Audi hat mit dem Kauf des Navigationsdatendienstes Here zusammen mit Daimler und BMW schon ein strategisches Investment getätigt. Die eigene Navi-Software soll zum universellen Dienstleister ausgebaut werden, der – vernetzt mit Daten aus dem Auto, von anderen Verkehrsteilnehmern und der Infrastruktur für den Fahrer – sozusagen vorausdenken soll: den freien Parkplatz um die Ecke schon anzeigen, wenn dort gerade jemand wegfährt. Vor Glatteis hinter der übernächsten Kurve warnen. Die Route anhand des Kalenders im Handy vorplanen, den Wagen vor der Abfahrt temperieren und die Strecke optimieren. Mit solchen Diensten wollen sich die Wolfsburger von der Konkurrenz absetzen. 
Dieser eingebaute Butler wird modular auch in die Massenmodelle der Wolfsburger einziehen. Aber auch die Bedienung im Innenraum wird komfortabler werden. Die in Detroit gezeigte Studie kombiniert bereits ein volldigitales Display vor dem Fahrer mit einem Touchscreen in der Mitte des Armaturenbretts, der sogar Gesten versteht. Verfeinerte Sprachsteuerung und nahtlose Integration von Smartphones aller Marken kommen dazu – alles Teil der  ""Digitalisierungsoffensive "", die Müller konzernweit ausgerufen hat. 
Die nächsten technologischen Sprünge stehen hier bei VW bevor – das müssen sie auch, denn Renault-Nissan-Chef Carlos Ghosn etwa hat gerade angekündigt, bis Ende des Jahrzehnts City-Assistenten in alle Autoklassen zu bringen, die sich autonom durch den Innenstadtverkehr wühlen. Hier wird Volkswagen kontern und durch die Verquickung mit den Here-Services etwa beim Parken oder Navigieren noch etwas draufsetzen wollen. 
Der automatische Notruf E-Call und die City-Notbremse werden ebenfalls Standard. In den USA macht VW gerade massiv TV-Werbung für die automatische Notbremse im überarbeiteten Passat. 
Gerade in den USA hat VW ein Problem mit dem bisherigen Tiguan, der SUV ist für amerikanische Verhältnisse einfach zu klein. Vom Nachfolger wird es deshalb speziell mit Blick auf diesen Markt eine XXL-Version mit bis zu sieben Sitzen geben. Dazu kommt ab 2017 noch ein fast fünf Meter langer Midsize-SUV mit sieben Sitzen exklusiv für die USA. Der lange Tiguan jedenfalls könnte auch in Deutschland Fans gewinnen, böte er doch fast so viel Raum wie der Touareg, wäre aber billiger. Womit wir beim letzten Ansatz zur Krisenbewältigung wären. 
Langfristig will Volkswagen bei der preissensiblen Klientel aggressiver mitmischen. Dafür gibt es spezielle US-Modelle, die einfacher und damit kostengünstiger sind. Das Streichen einiger Modelle und eine konsequentere Nutzung der gemeinsamen Baukästen bei Karossen und Elektronik lassen Kosten sinken. Das wiederum kann generell günstigere Preise für die Kunden möglich machen, gerade wenn die Nachfrage mal schwächelt. 
Mehr Volkswagen fürs Geld: So könnte ein Weg aus der Krise aussehen. Aber zunächst muss Matthias Müller erst mal sein Hauptziel für 2016 erreichen. Und das hört sich einfach an, ist aber eine Herkulesaufgabe:  ""die Dinge in Ordnung bringen ""."	technik
"Deep Learning, neuronale Netze, maschinelles Lernen – das waren vor zwei Jahren noch Begriffe, die nur Fachleute interessiert haben. Heute sind es Modewörter, insbesondere im Silicon Valley. 
Dort ist ein Wettstreit um die klügsten Forscher entbrannt, die an künstlichen Intelligenzen (KI) arbeiten. Denn KI und artverwandte Techniken stecken schon jetzt in den Diensten von Google, Facebook, Microsoft und Yahoo. Sie helfen zum Beispiel bei der Bild- und Spracherkennung in virtuellen Assistenten und Fotoverwaltungssoftware. Die Unternehmen wollen ihren Maschinen aber noch viel mehr beibringen, weshalb sie jetzt versuchen, möglichst attraktiv für die wenigen Spezialisten auf dem Gebiet zu erscheinen. 
Dass sie Teile ihrer Werkzeuge, Algorithmen und sogar Hardware unter Open-Source-Lizenzen, also für alle frei nutzbar, veröffentlichen, gehört zur Strategie. Je mehr Menschen mit der Technik experimentieren, für deren Eigenentwicklung ihnen die Mittel fehlen, desto eher können Google und die anderen davon profitieren. Und ein wenig Angeberei dürfte auch dabei sein, wenn Yahoo offenbart, wie seine KI die gigantische Bildersammlung auf Flickr ordnet, oder wenn – wie diese Woche – Amazon zeigt, welche KI-Technik hinter seinen Produktempfehlungen steckt. Wer jetzt eigentlich was veröffentlicht hat, zeigen wir in unserer Kartengeschichte:"	technik
"Kaum ist der Wirbel um das Smartphone-Spiel Pokémon Go ausgebrochen, erhoffen sich die ersten Händler lukrative Geschäfte. Ein Wiener Autoteile-Händler gewährt 20 Prozent Nachlass auf Scheibenwischer oder Motoröle, wenn man auf dem Firmengelände ein virtuelles Monster fängt. Eine Saftbar im amerikanischen Gainesville schenkt Zockern einen Smoothie, wenn sie über die App weitere Spieler anlocken. In Deutschland sehen die Handelsverbände neue Chancen für Unternehmen und Ladenbesitzer. 
Virtuelle Monster fangen und sie gegeneinander kämpfen lassen: Pokémon Go knüpft an das Prinzip der beliebten Spieleserie an. Mit einem wesentlichen Unterschied: Um voranzukommen, müssen sich Spieler auf der Suche nach neuen Monstern, Prämien oder Kontrahenten in die Wirklichkeit begeben. Das Spiel basiert auf echten Geodaten. Bei eingeschalteter Kamera werden die Monster auf dem Smartphone-Bildschirm in die echte Umgebung eingeblendet. Straßen, Parks oder Einkaufszentren werden zum Jagdgebiet. 
Läden, Krankenhäuser, öffentliche Einrichtungen: Häufig besuchte Orte in der Realität verwandeln sich im Spiel zu Kampfarenen oder Plätzen zum Auffüllen der Ausrüstung (Pokéstops). Unklar ist noch, nach welchen Kriterien der Entwickler Niantic Labs und die Nintendo-Beteiligung Pokémon Company diese Orte gesetzt haben. Fest steht: Durch die App werden sie häufiger besucht. Einige Orte in Übersee wurden regelrecht überrannt. In den nächsten Wochen und Monaten wird es möglich sein, den Schöpfern neue Orte für Kampfarenen oder Pokéstops vorzuschlagen, heißt es aus Kreisen der Entwickler. 
Ein Berliner Souvenirladen in der Nähe des Brandenburger Tors ist in der virtuellen Spielwelt als Kampfarena markiert. Verkäufer Björn Rahn freut es:  ""Ich denke, wir können einen neuen Kundenstamm gewinnen. "" Besonders viele Pokémon-Spieler habe er vor oder in dem Geschäft noch nicht gesehen.  ""Einen Ansturm durch das Spiel wie in anderen Ländern haben wir noch nicht gemerkt "", sagt Rahn. 
In sozialen Netzwerken häufen sich Berichte, wonach Spieler ihre Restaurantsuche so abstimmen, dass sie möglichst viele Pokémon fangen. Gaststätten etwa in Australien haben darauf reagiert und legen zu festgelegten Zeiten Köder aus, die es gegen einen kleinen Geldbetrag innerhalb der App gibt, um seltene Pokémon anzulocken. Das soll Kunden animieren, beim Essen auf Monsterjagd zu gehen. Und glaubt man Nutzern auf Twitter, soll es bereits Beschwerden wegen fehlender Monster im Lokal gegeben haben. 
Für Marketing-Strategen eröffnet der Rummel um das Spiel ganz neue Möglichkeiten. Der Sprecher vom Handelsverband Deutschland, Stefan Hertel, sagt:  ""Die Beteiligung an Pokémon Go hat da angesichts des derzeitigen Hypes durchaus Potenzial, gerade jüngere und internetaffine Kundschaft in die Läden zu holen. "" Für Spieler steigere das das Einkaufserlebnis und die Verbundenheit zu den Geschäften vor Ort. Firmen könnten in der virtuellen Welt auf sich aufmerksam machen oder mit Gutscheinen oder Rabatten auf ihre Onlineshops hinweisen, sagt Martin Groß-Albenhausen vom Bundesverband E-Commerce und Versandhandel. 
 
Das US-Wirtschaftsmagazin Forbes erklärt bereits ganz konkret, wie Ladenbesitzer vom Hype profitieren können:  ""Weisen Sie sie nicht zurück, locken Sie sie an. "" Unternehmer sollen sich eine versteckte Werbemöglichkeit zunutze machen. Durch In-App-Käufe können Spieler Köder auf die Karte setzen, die Monster für 30 Minuten anlocken. Wenn Geschäfte das machen, könnte die Pokémon-Ansammlung in ihrer Nähe auch potenzielle Kunden anziehen. 100 Lockmittel sind aktuell im günstigsten Fall für rund 68 US-Dollar zu haben. 
Der Chef des Co-Entwicklers Niantic Labs, John Hanke, sagte jüngst der New York Times, in der Zukunft werde es für Geschäfte auch ganz offiziell die Möglichkeit geben, in dem Spiel mit gesponserten Punkten aufzutauchen. Sie können dafür bezahlen, dass ihre Location zu einem Pokéstop wird. Einen Zeitpunkt dafür nannte er nicht. Im Moment ist Niantic damit beschäftigt, seine Server unter dem Ansturm der Spieler nicht in die Knie gehen zu lassen. 
Zu einem Treffpunkt für Pokémon-Spieler ist auch eine Bäckerei in Berlin-Kreuzberg geworden. Vereinzelt stehen bereits Zocker vor dem Laden, wischen mit ihren Fingern über die Displays ihrer Smartphones und lassen ihre Monster gegeneinander kämpfen. Erkannt haben die Mitarbeiter das zunächst nicht. Künftig hofft Verkäuferin Kübra Sahin aber auf mehr Spieler:  ""Die kurbeln das Geschäft bestimmt an "", sagt sie. 
Doch Verbraucherschützer warnen: Ist Werbung als solche nicht gekennzeichnet, könnten Kunden getäuscht werden, erklärt Florian Glatzner vom Verbraucherzentrale-Bundesverband. Würden keine personenbezogenen Daten von Verbrauchern gesammelt, sei eine solche Praxis zumindest beim Datenschutz eher unkritisch. Dies sei aber nicht immer zu durchschauen:  ""Bei vielen Programmen hat der Nutzer kaum eine Chance zu sehen, welche Berechtigungen er freigibt und welche Daten fließen. "" Übrigens: An einer Außenstelle der Berliner Verbraucherzentrale ist ebenfalls eine Kampfarena. Die Mitarbeiter wussten davon nichts."	technik
"Ob die türkischen Putschisten Coup d'État – A Practical Handbook gelesen haben, ist nicht bekannt. Aber es ist zumindest ein Standardwerk. Der Politikwissenschaftler Edward N. Luttwak hat das Handbuch für politische Umstürze schon vor Jahrzehnten geschrieben. Falls die Aufständischen es doch auf ihrer Literaturliste hatten, dann wohl die veraltete Ausgabe von 1968. Es ist die ohne Internet. Wie wichtig es ist, während eines Putschversuchs neben den traditionellen Medien auch Internetprovider und soziale Medien zu kontrollieren, steht erst in der im April 2016 veröffentlichten überarbeiteten Fassung. 
Welche Rolle das Netz während des gescheiterten Putschversuchs in der Türkei spielte, hat Zeynep Tufekci in der New York Times beschrieben. Die in der Türkei geborene Soziologin lehrt an der Universität von North Carolina, außerdem ist sie Fellow am Berkman Center for Internet and Society in Harvard. Am Tag des versuchten Umsturzes war sie in Antalya und in den frühen Morgenstunden fiel ihr auf, dass ihr türkischer, der Regierung nahestehender Mobilfunkanbieter ihr monatliches Datenvolumen erhöht hatte. Genauer: Das Datenvolumen aller Kunden. 
In einem Land, in dem die Regierung in Krisenzeiten Internetverbindungen oft drosseln und soziale Netzwerke blockieren lässt, war das ungewöhnlich. Die eigentlich erwartete Sperre von Facebook, Twitter und YouTube für türkische Nutzer hatte es etwa laut Antizensur-Aktivisten zwar früh gegeben, aber die TIB, die zuständige Abteilung der Kommunikationsbehörde, hatte sie schnell wieder aufheben lassen. 
Sein ungewohnter – und bereits wieder zurückgenommener – Ansatz  ""Mehr Internet wagen "" half dem türkischen Präsidenten Recep Tayyip Erdoğan auf mehreren Ebenen: Er selbst konnte eine Verbindung zum iPhone einer Moderatorin im Studio von CNN-Türk aufbauen und per FaceTime-Videochat sowie später auch über Twitter die Bevölkerung zum Widerstand aufrufen. Die wiederum konnte sich über WhatsApp und Twitter binnen kürzester Zeit koordinieren und mobilisieren, nicht zuletzt mithilfe von Selfies, die sie vor den Panzern der Putschisten zeigten. 
Zudem hatten auch jene Journalisten Zugang zum Netz, die als regierungskritisch gelten. Ihre unabhängigen Berichte halfen der Bevölkerung, die Lage realistisch einzuschätzen, schreibt Tufekci. Und schließlich nutzten viele Bürger die Tools von Facebook und Twitter zum Livestreaming. Einige Abgeordnete übertrugen sogar, wie sie im Parlamentsgebäude ausharrten, als dieses unter Beschuss der Putschisten geriet. 
Es sei ein Fehler der Putschisten gewesen, nicht auch die türkischen Internetprovider unter ihre Kontrolle gebracht zu haben, schreibt ein Sicherheitsforscher, der unter dem Pseudonym The Grugq auftritt, auf Medium.com. The Grugq beschäftigt sich mit Operations Security (Opsec), Kommunikationssicherheit, Spionage sowie den Kommunikationsstrategien von Terroristen. 
Es wäre wichtig gewesen, schreibt er, die türkische Führung festzusetzen und ihr die Möglichkeit zu nehmen, eine Gegenerzählung zu verbreiten und den Widerstand zu organisieren:  ""Ein Putsch war erfolgreich, wenn die Bevölkerung glaubt, dass er erfolgreich war. Der FaceTime-Anruf von Erdoğan mit seinem Aufruf zum aktiven Widerstand und die Videos von Bürgern, die sich erfolgreich den Panzern entgegenstellen, waren der Anfang vom Ende "". 
In Anlehnung an Luttwaks Umsturz-Handbuch empfiehlt The Grugq nun vier Vorgehensweisen für Putschisten im Internetzeitalter: 
Allerdings benötigten die militärischen Anführer des versuchten Staatsstreichs selbst einen Internetzugang. Türkischen Medien zufolge koordinierten sie sich permanent über WhatsApp. 
 
Luttwak selbst ignoriert die Rolle des Internets in seiner ersten Analyse des gescheiterten Putschversuchs weitgehend. Gemäß der von ihm selbst aufgestellten Regeln in seinem Handbuch waren die beiden größten Fehler der Putschisten, die Panzer, Helikopter und Kampfjets jener Einheiten intakt zu lassen, die nicht zu ihnen gehörten – und Erdoğan nicht als erstes festzusetzen  ""oder wenigstens zu töten "". 
In der aktuellen Ausgabe von Coup d'État heißt es auch, trotz der Reichweite sozialer Medien und des Internets im Allgemeinen sei es zur Sicherung der eigenen Autorität am wichtigsten, die Radio- und Fernsehsender zu besetzen, denn  ""mit ihren Diensten wird die Stimme der Regierung assoziiert "". 
Aber an anderer Stelle räumt Luttwak dem Netz, seinen Diensten und den sozialen Medien durchaus einen hohen Stellenwert ein:  ""Wir müssen verhindern, dass die Opposition (gemeint ist hier die Regierung) ihre Kommunikationssysteme nutzt. Dadurch lähmen wir ihre Reaktion und hindern sie daran, jene Kräfte gegen uns einzusetzen, die sie noch kontrolliert. "" Das sei angesichts der Vielzahl von Kommunikationswegen zwar  ""kompliziert "", aber eben essenziell. Soziale Medien müssten jedenfalls gefiltert oder blockiert werden."	technik
"Die Superlative sparen wir uns. Downloadzahlen, Kursgewinne, Umsatzprognosen. Alles der Wahnsinn. Die Smartphone-App Pokémon Go bringt mehr Menschen auf die Straße als alle Protestparteien der Welt zusammen. Selbst die Begleiterscheinungen füllen Liveticker. Die ersten Unfalltoten, die Spielverbote in Gedenkstätten und die immer prominenteren Promis, die durch Tweets und Fotos belegbar  ""jetzt auch im Pokémon-Fieber "" sind. 
Hype heißt so was. Und schon rufen sich die Experten auf den Spielplan, erklären, analysieren, dekonstruieren ihn. Sie vergessen dabei, dass dieser nie ganz zu erklären ist – er ist ja über dem Normalen, über dem Erwarteten. Ließe sich ein Hype in seine Einzelteile zerlegen, wäre er – im Sinne des Reverse Engineering – auch erzeugbar. Und dann fehlte das Überraschende, das ihn konstituiert. Als Toyota einst alle größeren deutschen Städte mit Plakaten zukleisterte, bekamen sie die erwartete Aufmerksamkeit. Doch es ging nicht darüber hinaus. Sich Wassereimer über den Kopf schütten, verkleidet auf dem Bürotisch tanzen, die Feurigkeit eines nordirischen Auswechselspielers besingen – das Charmante des Hypes ist gerade seine scheinbare Zufälligkeit. 
Das soll die Leistung des Pokémon Go-Machers John Hanke nicht kleinreden. Ortsbasiertes Gaming, also das Spielen mithilfe von Geodaten, galt lange als Sorgenkind der Spieleentwickler. Das Potenzial war da, aber was auch immer sie versuchten, nichts wollte so richtig klappen. Ein Investor aus dem Silicon Valley bilanzierte: Wenn dein Spiel ortsbasiert ist, startest du beim 100-Meter-Lauf mit nur einem Bein. Zu faul seien die Menschen, wollten sich nicht bewegen, vor allem nicht beim Spielen. Jetzt: 
SIE:  ""Schatz, ich glaube, es sollte noch mal jemand mit dem Hund raus. "" 
ER:  ""Ja, stimmt, kann ich machen. "" 
SIE:  ""Nee, komm, bleib du ruhig hier, bei dem Regen. "" 
ER:  ""Macht echt nichts, ich wollte sowieso noch mal an die frische Luft. "" 
Und dann läuft er mit seinem Smartphone durch die Stadt, sieht dort seine reale Umgebung, Straßen, Wohnblöcke, Flüsse. Allerdings erweitert, augmented, ergänzt um Markierungen, die nur für das Spiel relevant sind. PokéStops, Arenen und natürlich die kleinen Taschenmonster, Poketto Monsutā, die namensgebenden Pokémon. Am PokéStop sammelt er Pokébälle ein und fängt damit die kuscheltierartigen Wesen, die dank Smartphonekamera wirklich vor ihm auf dem Bordstein zu kauern scheinen. Anschließend kann er sie in der Arena gegen andere antreten lassen. 
Das Spielprinzip ist weniger innovativ als es scheint, ähnliche Konzepte gibt es bereits seit Jahren. Neu ist der Erfolg. Den hatte Hanke, lange mitverantwortlich für Google Earth und Maps, mit dem Spiel Ingress bereits in der Nische. 2014 sah er, wie Google für einen Aprilscherz Pokémon-Monster bei Google Maps platzierte. Nur für einen Tag, aber es war ein Hit. Hanke ahnte, wie man den Durchbruch schaffen könnte: Technik plus Marke. Googles technisches Know-how gepaart mit der weltweiten Bekanntheit der Pokémon. So schaffte er es als Erster einbeinig ins Ziel. 
Aber wieso reißen sich nun Großstädter darum, im Regen Gassi zu gehen? Um das zu beantworten, brauchen wir eine Passantin, ebenfalls Spielerin, die ihm entgegenkommt, gedankenverloren auf ihr Smartphone starrt – und plötzlich auf seinen Hund zielt. Ihn mit dem Smartphone ins Visier nimmt, über das Display streicht, dann anfangen muss zu lachen. Sie sei verwirrt gewesen, habe den Hund für virtuell gehalten, wollte Pokébälle nach ihm werfen, ihn jagen und einfangen. Da wird dem Großstädter klar, um was es hier eigentlich geht: Beeren und Mammuts. 
Um Spiele zu untersuchen, gibt es in der Gamification-Theorie verschiedene Hilfsmittel. Die meisten basieren darauf, typische Elemente aus Spielen zu isolieren und ihre Ausprägung festzustellen. Dimension Anerkennung: Sehe ich als Spieler, dass ich vorankomme? Dimension Sozialer Einfluss: Interagiere ich mit anderen Spielern? Dimension Neugier: Hält das Spiel Überraschungen für mich bereit? Dimension Eigentum: Baue ich mir etwas auf, das ich als eigenen Besitz wahrnehme? Pokémon Go spricht die Spieler in verschiedenen Dimensionen an und macht folglich vieles richtig. 
 
Interessanter aber ist die Nähe zu dem, was die Psychologie menschliche Treiber nennt. Zahlreiche Modelle aus der Persönlichkeitsanalyse nutzen ähnliche Dimensionen wie in der Gamification-Theorie, nur werden eben keine Spiele untersucht, sondern Menschen und deren Bedürfnisse: Wie wichtig ist mir mein Status? Wie wichtig soziale Kontakte? Wie sehr strebe ich danach, Neues zu erfahren? Was bedeutet mir Besitz und das Anlegen von Vorräten? Anerkennung, Sozialer Einfluss, Neugier, Eigentum. Auch Macht gilt als Treiber, Idealismus, Zweckorientierung, Wettkampf. 
Was lässt sich aus dieser Kongruenz ableiten? Vielleicht ja, dass der Mensch spielend seine Bedürfnisse stillt. Und zwar gerade die, bei denen er das im normalen Leben nicht schafft. Nicht als Vorbereitung, nicht als Übung für ernsthafte Tätigkeit, wie es der Kulturhistoriker Johan Huizinga in seinem Modell des Homo ludens beschrieb. Sondern als Ersatzhandlung. 
Die menschlichen Treiber gelten als zeitlich wie räumlich universell. Die Menschen hatten sie schon immer und werden sie immer haben. Umgekehrt wird es ihnen in einer stets bequemer werdenden Welt nicht leichtgemacht, ihre Bedürfnisse zu befriedigen. Ihr Essen wird ihnen nach Hause geliefert, immer weniger Menschen arbeiten körperlich, Familienbindung nimmt ab, Prügeleien gelten als unschick. Was uns im echten Leben verwehrt bleibt, holen wir uns im Spiel zurück. 
Es war zufälligerweise im Rückblick auf ein Fußballturnier in Frankreich, als der Schriftsteller Paul Auster im Jahr 2000 in einem Beitrag von  ""Fußball als Kriegsersatz "" schrieb. Und der Philosoph Peter Sloterdijk äußerte sich einige Jahre später ähnlich:  ""Da wird nämlich das älteste Erfolgsgefühl der Menschheit reinszeniert: mit einem ballistischen Objekt ein Jagdgut zu treffen, das mit allen Mitteln versucht, sich zu schützen. "" 
Da sind wir wieder bei Pokébällen und Pokémon Go. Denn trotz blinkender Pixeltiere und bunter Farbverläufe machen die Spieler auch hier urmenschliche Erfahrungen: Sammeln, Jagen, Großziehen, Kämpfen. Sie holen sich semivirtuell zurück, was ihnen im Alltag verloren gegangen ist. Intensiver als beim Spielen zu Hause auf der Couch, weil näher dran am Original. 
Kulturpessimisten werden uns – wieder einmal – auf dem Weg in die Matrix sehen. Auf dem Weg zum nur noch der Energiegewinnung dienenden physischen Körper, dessen Geist in der Virtualität von der Wirklichkeit als Sklave abgelenkt wird. 
Die spielerische Ersatzhandlung ließe sich aber auch anders, nämlich positiver interpretieren. Als Chance, im echten Leben vom Fortschritt zu profitieren und doch die nicht abstellbaren Ur-Bedürfnisse zu stillen. Nach den ersten Unfällen durch von Pokémon Go abgelenkten Fahrern passt vielleicht sogar Austers pathetischer Schlusssatz:  ""Aber immerhin können wir nun die Opfer an den Fingern zweier Hände abzählen. Eine Generation früher zählten wir sie in Millionen. ""
""Das Europaparlament hat der Verordnung über Maßnahmen zum Zugang zum offenen Internet zugestimmt und alle Änderungsvorschläge abgelehnt – trotz aller Kritik und Bedenken, auch unter den Abgeordneten. Zwar ist grundsätzlich vorgesehen, dass Netzbetreiber alle Datenpakete gleichberechtigt durch ihre Leitungen schicken, egal woher sie stammen oder welchen Inhalt sie haben. Doch Kritiker befürchten, dass diese Netzneutralität wegen fehlender, schwammiger oder widersprüchlicher Formulierungen in der Verordnung aufgeweicht wird. 
Die Abstimmung war an ein weiteres Thema gekoppelt: die teilweise Abschaffung der Roaminggebühren im EU-Ausland. Zunächst sollen die Extragebühren ab dem 30. April 2016 erneut gedeckelt werden, sie dürfen dann für Gespräche 0,05 Euro pro Minute und 0,02 Euro pro SMS nicht mehr überschreiten. Bei Internetnutzung liegt die Höchstgrenze bei 0,05 Euro pro Megabyte. Ab dem 15. Juni 2017 sollen die Aufschläge dann grundsätzlich ganz wegfallen. Der Kompromiss enthält allerdings  ""Sicherungen "" für die Telekomfirmen, denen durch das Roaming Mehrkosten entstehen. Eine Fair-Use-Klausel sieht vor, dass die Befreiung von Roaminggebühren nur für eine  ""angemessene Nutzung "" des Handys im Ausland gilt. Anbieter können beispielsweise bestimmte Obergrenzen für die Dauer von Telefonaten und die Zahl der versandten SMS festsetzen. Die Einzelheiten über die  ""Klausel zur fairen Nutzung "" sollen die EU-Kommission und die zuständige Europäische Regulierungsbehörde (GEREK) bis Dezember kommenden Jahres festlegen. 
Somit bleibt nicht nur unsicher, wie weit die Abschaffung der Roaminggebühren für Endkunden tatsächlich gehen wird. Auch die Entscheidung zur Netzneutralität bleibt umstritten, in erster Linie, weil sie die Errichtung eines Zwei-Klassen-Internets vorantreiben könnte. 
Bisher gibt es für das  ""offene Internet "" keine europäischen Regeln, nur einzelne EU-Staaten haben Vorschriften. Doch die Datenmenge wächst und damit auch die Gefahr von Staus im Netz. Deshalb wurde diskutiert, ob in Sonderfällen nicht doch manche Internetdienste vorrangig behandelt werden können. 
Zwar solle sich niemand Vorrang im Internet erkaufen können, hatte die EU-Kommission beteuert. Sogenannte Spezialdienste wie Telemedizin oder auch hochauflösendes Fernsehen im Internet sollen andere Nutzungen nicht verdrängen und nur angeboten werden, wenn es genügend Kapazität gibt, so die EU-Kommission. Doch ist ein Verkehrsmanagement inklusive der Drosslung bestimmter Dienste oder Inhalte ausdrücklich vorgesehen, und zwar schon bei einer  ""drohenden Überlastung des Netzes "". 
Kritiker halten diese Formulierung für vage und fürchten, dass die Netzneutralität dadurch praktisch abgeschafft wird. Unmittelbar vor der Abstimmung forderten mehr als 30 Start-ups, Internetunternehmen und Investoren aus Europa und den USA Änderungen der Pläne. Sie befürchten, dass die Entwicklung innovativer Dienste behindert wird, wenn Internetprovider Überholspuren für bestimmte Daten einrichten und andere Daten ausbremsen dürfen. Sie wandten sich auch gegen den Vorschlag, dass der Datenverbrauch bestimmter Dienste wie Musik-Streaming aus den Datentarifen ausgeklammert wird (Zero-Rating heißt das), um diese Dienste gegenüber anderen zu bevorzugen. 
Der Erfinder des World Wide Web, Sir Tim Berners-Lee, hatte die Europaabgeordneten vor einer Sonderbehandlung von Spezialdiensten gewarnt. Es handle sich dabei um Überholspuren, für die man extra zahlen müsse. Betroffen davon seien Start-ups, kleine Unternehmen, Künstler, Aktivisten und Erzieher in Europa und der ganzen Welt. 
Update: Die Details zu den Roaminggebühren im zweiten Absatz haben wir ergänzt."	technik
"Der Diesel ist tot, es lebe der Diesel. So könnte man die Botschaft deuten, die in diesen Tagen aus Stuttgart-Untertürkheim zu hören ist. Dort, im Gebäude 120 des Daimler-Stammwerks, bereiten Ingenieure derzeit den Serienstart einer neuen Motorenfamilie vor, die nach den Worten von Peter Lückert alles bisher Bekannte auf den Kopf stellen soll.  ""Wir werden einen Quantensprung in Sachen nachhaltiger Emissionsreduzierung machen und Vorreiter zur Erfüllung zukünftiger Abgasvorschriften werden "", verspricht der Leiter der Dieselmotoren-Entwicklung. 
Um das zu erreichen, habe man  ""komplett neue Technologiekonzepte "" verwirklicht. Leichtbau und geringere Motorreibung, aber vor allem ein neues Brennverfahren und eine  ""innovative Abgasnachbehandlung "" würden  ""die technische Effizienz signifikant "" steigern, sagt der Motorenexperte. Das alles bringe die Agilität und den Fahrspaß des Dieselantriebs  ""in eine neue Dimension "". 
Große Worte erscheinen notwendig, um das Image des Dieselmotors wieder aufzupolieren und den Autokäufern deutlich zu machen, dass diese Antriebstechnik trotz des VW-Abgasskandals noch eine Zukunft hat. Und große Taten.  ""Die Autoindustrie befindet sich an einem Punkt, von dem aus es keine einfachen Lösungen mehr gibt "", beschreibt der Automobilverband VDA die Situation angesichts der strengeren Abgasvorschriften und der – an sich selbstverständlichen – Herausforderung, diese Limits auch auf der Straße tatsächlich erfüllen zu müssen. 
 ""Diesel 2.0 "", so nennt der VDA jene neuen Modelle, die mit SCR-Katalysator (Selective Catalytic Reduction), Harnstoffeinspritzung und AdBlue-Zusatztank den aktuellen Stickoxidvorschriften der Euro-6-Norm entsprechen. Der nächste Schritt wäre also der  ""Diesel 3.0 "" – ein Motor, der heutige und künftige Schadstoffvorschriften in allen Fahrsituationen erfüllt und damit wirklich sauber wird. 
Um das dazu erreichen, rüstet die Branche den Selbstzünder nochmals technisch auf.  ""Neben dem CO2-Ausstoß müssen auch die Rohemissionen weiter gesenkt werden "", beschreibt Rolf Bulander das Ziel. Der Bosch-Geschäftsführer macht deutlich, dass man sich bei zukünftigen Entwicklungen mehr auf die Vorgänge im Motor als auf die nachträgliche Reinigung der Abgase beschränken wird: Die Verbrennungsprozesse sollen so gesteuert werden, dass vor vornherein weniger Stickoxide entstehen. 
Bulander will deshalb das  ""Luftsystem des Motors "" verbessern: Turbolader und Abgasrückführung sollen präziser aufeinander abgestimmt und mittels Elektronik besser gesteuert werden als bisher. Dadurch soll es möglich werden, dass die Abgasrückführung im gesamten Betriebsbereich des Motors aktiv ist. Schadstoffhaltige Abgase werden dann permanent in die Brennräume zurückgeleitet und nochmals verbrannt. 
Außerdem will Bosch den Einspritzdruck weiter erhöhen, von derzeit 2.200 auf bis zu 2.700 bar. Dadurch könne man den Kraftstoff feiner in den Brennräumen zerstäuben und besser mit der Luft vermischen, erklärt das Unternehmen und spricht von einem  ""wesentlichen Faktor, um die Stickoxid- und Partikel-Rohemissionen zu senken "". Dem gleichen Ziel dient der Einsatz neuartiger Hightech-Injektoren, die der Auto-Zulieferer Delphi entwickelt. Sie sollen so leistungsfähig sein, dass der Kraftstoff millisekundenschnell bis zu neun Mal hintereinander eingespritzt werden kann und sich so während der Verbrennungsphase besser in den Zylindern verteilt. 
 
Doch solche Detailarbeit wird die Schadstoffemissionen nicht in großem Maß senken.  ""Bei schweren und großen Fahrzeugen ist eine alleinige Optimierung der Verbrennungsmotoren nicht ausreichend "", sagt Bulander. Man müsse solchen Selbstzündern künftig mit noch mehr Zusatztechnik auf die Sprünge helfen, damit sie die Stickoxidvorschriften in allen Fahrsituationen erfüllen – also auch beim Beschleunigen und auch bei hohem Autobahntempo. 
Die Fachleute sprechen vom  ""elektrischen Ablasten "" und meinen damit nichts anderes als den Einsatz eines zusätzlichen Elektromotors, der die abgastechnischen Dieselschwächen wettmachen soll. Das bedeutet: Beim Beschleunigen wird der Dieselmotor  ""abgelastet "", und eine Elektromaschine sorgt für die notwendige Kraft.  ""Dieser Effekt senkt die Emissionen bereits an der Quelle um zirka 10 bis 20 Prozent "", verspricht Bulander. 
Auch Continental in Hannover experimentiert mit der Elektrifizierung des Dieselantriebs. Dort ist man zu der Erkenntnis gelangt, dass es für eine akzeptable Stickoxid- und Rußbildung sinnvoll ist,  ""den Verbrennungsmotor möglichst oft abzuschalten "" und nur  ""unter günstigsten Bedingungen "" zu betreiben. 
So wird für den Pkw-Dieselmotor von morgen offenbar eine völlig neue Betriebsstrategie entwickelt, die das bisherige Konzept des von vielen Firmen favorisierten Hochdrehzahl-Dieselmotors über Bord wirft und sich wieder auf die prinzipbedingten Stärken des Dieselmotors besinnt. Statt die Triebwerke leistungsmäßig weiter hochzuzüchten, lässt man sie künftig konstant in niedrigen Drehzahl- und Lastbereichen arbeiten und gleicht die fehlende Durchzugskraft durch einen Elektromotor aus. Der Effekt: deutlich geringere Rohemissionen und bis zu 15 Prozent Verbrauchseinsparung. 
Der Aufwand für diesen Diesel-Hybrid ist allerdings groß. Um so viel elektrische Leistung zur Verfügung zu stellen, benötigen die Fahrzeuge ein separates elektrisches Bordnetz, das mit einer vierfach höheren Spannung arbeitet: 48 statt 12 Volt. Auch die Abgasreinigung wird noch komplizierter. Denn ein Dieselmotor, der oft stillsteht oder nur mit geringer Drehzahl arbeitet, kühlt dabei so stark ab, dass der nachgeschaltete NOx-Speicherkatalysator nicht anspringt und deshalb keine Stickoxide unschädlich machen kann.  ""Hier wird kontinuierlich heißes Abgas benötigt "", erklärt Conti-Experte Rolf Brück den Zielkonflikt. 
Die Lösung soll ein sogenannter E-Kat sein: ein elektrisch beheizter Metallkatalysator, der in das 48-Volt-Bordnetz integriert wird und von dort eine elektrische Leistung von bis zu 3.500 Watt abruft, um die Heizscheibe in seinem Inneren auf Temperatur zu bringen. 
Weil der abgelastete Dieselmotor von vorneherein weniger Schadstoffe erzeugt, könnte man den Wirkungsgrad dieser katalytischen Stickoxidminderung auf bis zu 80 Prozent steigern, versprechen die Entwickler. An dem nachgeschalteten SCR-System, das die Stickoxide mittels Harnstofflösung in Stickstoff und Wasserdampf umwandelt, führe aber trotzdem kein Weg vorbei. Der Verbrauch der Zusatzflüssigkeit könne angesichts der saubereren Rohemissionen allerdings verringert werden, so dass Autofahrer die Lösung seltener nachtanken müssten. 
Wohlgemerkt: Das alles ist Technik für die Zukunft. Der 48-Volt-Dieselhybrid existiert bisher nur im Prototypenstadium und muss noch viele Bewährungsproben bestehen – nicht nur beim Abgastest auf der Straße. Es ist gut möglich, dass der Dieselmotor mit Hilfe dieser Technik die europäischen NOx-Limits auch auf der Straße erfüllen wird, doch eines ist schon heute gewiss: Der Antrieb wird technisch komplexer, störanfälliger und vor allem teurer. 
Für Luxuslimousinen und große SUVs mag dieser Aufwand gerechtfertigt sein, doch für die große Zahl der Kompakt- und Mittelklassemodelle wird sich dieser Diesel 3.0 mit 48-Volt-Hybrid, SCR-System und anderen technischen Finessen nicht mehr lohnen. 
Höchste Zeit also, nach einer neuen Alternative zu suchen – nach einem Motor, der sauber, sparsam und zugleich bezahlbar ist. Tatsächlich erproben Daimler und Volkswagen eine Neuentwicklung in Forschungswagen, sprechen aber nur selten darüber: den Diesotto, also ein Mittelding aus Otto- und Dieselmotor. 
VW nennt es Combined Combustion System (CCS) und deutet damit das Ziel an: Man will die Benzin- und Dieseldirekteinspritzung zu einer Technologie verschmelzen. Das sei der Beginn einer  ""neuen Motor-Ära "".  ""Der Benzinmotor steuert das homogene Kraftstoff-Luft-Gemisch und die niedrigen Emissionen bei, der Diesel die Selbstzündung und den niedrigen Verbrauch "", erklärt ein VW-Ingenieur das Prinzip. 
Weil die Vorgänge in den Zylindern in jeder Phase genau kontrolliert und gesteuert werden, erziele man wie beim Benziner ein homogenes Gemisch, das sich bei hoher Verdichtung von selbst entzündet und bei niedriger Temperatur nahezu vollständig verbrennen soll. Damit wären die wichtigsten Voraussetzungen geschaffen, um die Ruß- und Stickoxidemissionen auf ein Minimum zu verringern, betont die VW-Forschung. 
In Wolfsburg und Stuttgart arbeitet man seit rund acht Jahren an dieser kombinierten Motortechnik. Bis 2020 soll sie serienreif sein. Es stimmt also wirklich: Der Dieselmotor hat Zukunft. Als Benziner."	technik
"Der Bundesnachrichtendienst (BND) hat jahrelang für die NSA in Europa spioniert. Er sammelte Daten aus Internetleitungen und Satellitenverbindungen und suchte darin mit Suchbegriffen der NSA nach Informationen. Er war ein  ""Wurmfortsatz "" der Amerikaner, wie es ein ehemaliger NSA-Mann im Untersuchungsausschuss des Bundestages nannte. Als Gegenleistung für diese Auftragsdatenverarbeitung bekamen die Deutschen die dafür notwendige Technik geschenkt und das Wissen, wie sich das Internet effektiv überwachen lässt. 
Das klingt nach einem guten Geschäft. Doch die Beamten stellten bald fest, dass sie missbraucht wurden – und ließen es zu. Die NSA ließ die Deutschen nämlich auch nach Informationen über europäische Politiker und Rüstungsunternehmen wie EADS suchen. Was sie fanden, gaben sie zum Teil an die NSA weiter – ohne dem Kanzleramt etwas von den Vorgängen zu erzählen, wie der Spiegel berichtet. 
Wie das Ganze technisch ablief, lässt sich anhand der Aussagen von BND-Mitarbeitern im NSA-Untersuchungssauschuss des Bundestages weitgehend rekonstruieren: 
Was sind Selektoren? 
Selektoren sind so etwas wie Suchbegriffe. Das können IP-Adressen, Telefonnummern, E-Mail-Adressen sein, genauso wie Geokoordinaten, MAC-Adressen, URLs. Aber auch einzelne Suchbegriffe können ein Selektor sein, also Namen oder Kürzel von Firmen und Behörden, oder Ausdrücke wie  ""Eurocopter "". 
In die Datenbanken des BND werden somit drei Dinge eingespeist: Die abgehörten Daten aus den Leitungen, die von der NSA gelieferten Selektoren und die Selektoren, die der BND selbst erstellt hat – denn auch er wühlt selbstverständlich in den Daten und sucht nach Interessantem. Als Ergebnis liefern die Rechner alle Informationen, die irgendetwas mit einem solchen Suchbegriff zu tun haben: Wen der Inhaber einer Telefonnummer angerufen hat, wer sich an einem bestimmten Ort aufgehalten hat und so weiter. Das sind die sogenannten Positiv-Selektoren, mit denen aktiv nach etwas gesucht wird. 
In der Sprache des BND gibt es aber auch noch Negativ-Selektoren, die wie vorgeschaltete Filter funktionieren. Tauchen die Negativ-Selektoren in einem Suchergebnis auf, soll die weitere Analyse an dieser Stelle abgebrochen werden. 
Wie hat die NSA ihre Selektoren an den BND übermittelt? 
Der Prozess lief vollautomatisch: Mehrmals am Tag hat sich ein BND-Server mit einem Server der NSA verbunden und neue Selektoren heruntergeladen. Das passiert  ""zwei-, drei- viermal täglich "", wie ein Zeuge sagte. 
Was hat der BND mit diesen Selektoren gemacht? 
Im NSA-Untersuchungsausschuss sagte der Zeuge W.K., Unterabteilungsleiter in der Abteilung Technische Aufklärung in Pullach, jeder einzelne Selektor sei darauf geprüft worden, ob er mit dem G-10-Gesetz und dem Kooperationsvertrag mit den USA vereinbar gewesen sei. Nur wenn das der Fall war, sei der Selektor wirklich verwendet worden. (Nachzulesen im Protokoll von netzpolitik.org im Abschnitt  ""Fragerunde 2: SPD "") 
Das klingt eindeutig. Doch es gibt auch abweichende Darstellungen. Klar ist: Den Selektoren war der sogenannte G-10-Filter vorgeschaltet. Das bedeutet, dass zuerst geprüft wurde, ob in den Daten Informationen von deutschen Staatsbürgern sind, die der BND nicht belauschen darf. War das der Fall, wurden sie aussortiert und gelöscht, erst danach wurde in den übrigen Daten nach den Selektoren gesucht. Klar ist auch, dass der G-10-Filter nicht perfekt funktionierte. Mehrere Beamte haben zugegeben, dass es Probleme damit gab und diese erkannt wurden. In dem bis heute geheimen  ""Schwachstellenbericht "" zum Projekt Eikonal ist das ebenfalls festgehalten. Klaus Landefeld, Chef des Internetknotens De-CIX, beschrieb im Ausschuss, es sei technisch nahezu unmöglich, Daten von Deutschen sauber zu filtern. 
War der G-10-Filter nun ein komplett automatisierter Vorgang oder nicht? Brigadegeneral Dieter Urmann, ehemals Leiter der BND-Abteilung Technische Aufklärung, sagte im Ausschuss, in manchen Operationen sei die G-10-Filterung nur händisch, in anderen maschinell mit zusätzlichen manuellen Stichproben durchgeführt worden. Dass dabei etwas durchrutschte, was nicht durchrutschen durfte, ist nicht auszuschließen. (Nachzulesen im Protokoll von netzpolitik.org im Abschnitt  ""Fragerunde 3: CDU/CSU "") 
In welches System hat der BND die Selektoren eingespeist? 
Die Datenbank heißt VeraS, das steht für Verkehrsanalysesystem. Sie beinhaltet Metadaten, also nicht die Inhalte von Gesprächen, E-Mails oder SMS, sondern alles, was als Daten um diese Kommunikation herum anfällt: Wer kommunizierte mit wem, wann und wo tat er das, wie lange und womit und so weiter. 
Die Daten für VeraS stammen aus verschiedenen Quellen des BND. Eine davon war der Internetknoten De-CIX in Frankfurt. Von den Antennen in Bad Aibling abgefangene Satellitenkommunikation eine zweite, in Ländern wie Afghanistan mitgeschnittene Kommunikation eine dritte. Es gibt aber noch weitere Quellen, so viel ist sicher. Welche genau, dagegen nicht. 
Was geschah nach der Datenbankabfrage? 
Die Ergebnisse, also die Rohdaten, wurden in die BND-Zentrale nach Pullach geschickt, wo sie ausgewertet und zum Teil auch an die NSA weitergeleitet wurden. 
Über wie viele Selektoren reden wir hier? 
Nach Informationen von ZEIT ONLINE hat sich der BND im Rahmen der Operation Eikonal insgesamt rund 800.000 Selektoren vom NSA-Server in seine Außenstelle in Bad Aibling geholt. 
Ob es weitere Operationen an den anderen Standorten des BND gegeben hat, in denen US-Geheimdienste und der BND ähnlich vorgegangen sind, ist unklar. 
Wann ist dem BND aufgefallen, dass Selektoren darunter waren, die sich auf Ziele in Deutschland und Europa beziehen? 
Das war erstmals im Jahr 2005 der Fall. Damals erkannte der BND Selektoren, die sich auf EADS und Eurocopter bezogen. 
2008 ist  ""einigen Mitarbeitern noch mehr "" aufgefallen, wie ZEIT ONLINE erfuhr. 
2013, nach Veröffentlichung der Snowden-Dokumente, stellte der BND eine Liste aller möglicherweise fragwürdigen Suchbegriffe zusammen. Sie umfasste 2.000 Selektoren. Die wurden allesamt auch eingesetzt, also nicht vorher aussortiert. 
Im März 2015, nach einem Beweisbeschluss des NSA-Untersuchungsausschusses, stellte der BND eine weitere Liste zusammen. Sie umfasste jene Selektoren, die der BND im Laufe der Jahre rechtzeitig als problematisch erkannt und aussortiert hatte. Problematisch meint: Ihre Verwendung war nicht vom G-10-Gesetz oder Datenschutzgesetzen gedeckt. Diese Liste umfasste 40.000 Selektoren. Ob das alle problematischen Selektoren waren, ist unklar. Es waren nur die, die dem BND aufgefallen sind, möglicherweise gab es mehr. 
Wann hat der BND die Bundesregierung informiert? 
Entweder hat er es bis vor wenigen Wochen gar nicht getan, was schlimm wäre. Es würde bedeuten: Der Nachrichtendienst hat drei Mal bemerkt, dass die NSA versucht, ihn für Spionagetätigkeiten zu instrumentalisieren, die den Interessen der Bundesrepublik zuwiderlaufen. Und er hat es trotzdem nicht für nötig gehalten, seine zuständige Aufsichtsbehörde, das Bundeskanzleramt, zu informieren. 
Oder der BND hat es der Bundesregierung doch früher gesagt, als diese bisher zugibt. Dann hätte die Regierung zugelassen, dass der BND gegen deutsche und europäische Interessen arbeitet."	technik
"Gadgets versetzen einen zurück in die Kindheit. Das macht einen Teil ihrer Faszination aus. Haben ihre Nutzer früher im Kreis der Familie die Weihnachtsgeschenke geöffnet, zelebrieren sie das Unboxing heute in YouTube-Videos. Das Gefühl beim Zusammenbau des Puppenhauses oder der Carrera-Bahn – es lässt sich mit dem Einrichten eines neuen Smartphones vergleichen und wer mit Lego, Fisher-Price oder Modellbausätzen gespielt hat, dürfte beim Anblick der neuen modularen, also frei kombinierbaren, Gadgets ein gewisses Kribbeln verspüren. 
So ließe sich jedenfalls der Erfolg erklären, den die Smartwatch Blocks oder das Handyprojekt RePhone in diesen Tagen auf der Crowdfunding-Plattform Kickstarter feiern. Blocks, ein Projekt aus Großbritannien, hat das gesteckte Finanzierungsziel in nur einer Stunde übertroffen und steht inzwischen bei fast 550.000 von anvisierten 250.000 Dollar. Das Projekt RePhone steht nach zwei Wochen bei gut 210.000 US-Dollar, obwohl die Initiatoren nur ein Viertel der Summe anpeilten. 
Beide Gadgets unterscheiden sich in Anspruch und Anwendungsbiet. Ihre Hersteller verfolgen dennoch dasselbe Ziel: Sie wollen austauschbare Hardware zum kleinen Preis anbieten, die individuelle Gestaltung fördern und nebenbei Teil einer größeren Bewegung sein. Aber dazu gleich mehr. 
Zunächst zur Smartwatch Blocks. Die unterscheidet sich von den Modellen der etablierten Hersteller, weil sie einen Teil ihrer Funktionen in die Glieder des Uhrenbandes auslagert. Die eigentliche Uhr enthält nur das Basismodul mit Akku, Prozessor, Sensoren und WLAN-Funktion. Alles weitere kann je nach Wunsch über Glieder im Uhrenband hinzugesteckt werden. Fünf solcher Module, darunter ein GPS-Modul, ein Pulsmesser und ein zusätzlicher Akku, erscheinen zum Verkaufsstart nächstes Jahr, weitere sollen durch Drittentwickler folgen. Der Preis für die Basisversion der Uhr beginnt bei knapp 200 US-Dollar, das Betriebssystem soll über eine App iOS und Android-Smartphones unterstützen. 
Während Blocks eine Alternative zu bestehenden Smartwatches sein soll, ist RePhone eher für Bastler interessant. Den Kern bildet ein Modul mit dem System und der Mobilfunkunterstützung, wahlweise für GSM oder 3G-Netze. Der Rest ist optional – ein Touchscreen, GPS-Module, Kopfhörerbuchsen, all das kann mit dem Kern verknüpft werden. Ein  ""Gehäuse "" aus Karton hält die Bauteile zusammen. Das ist schon von der Leistung und der Software her nicht als ernsthafter Ersatz für das tägliche Smartphone gedacht, sondern soll vor allem im vernetzten Haushalt helfen. Der Preis beginnt bei 40 US-Dollar. 
Blocks und RePhone sind Teil einer größeren Bewegung, die modulares Design in den Gadget- und Computerbereich bringen möchte. In den vergangenen Jahren sorgten mehrere Projekte für Aufsehen. 2013 machten etwa der Niederländer Dave Hakkens und sein Projekt Phonebloks Schlagzeilen, als er mit Google und Motorola das erste modulare Smartphone der Welt entwickeln wollte – ein Projekt, das inzwischen als Project Ara bekannt ist. Eigentlich sollte es in diesem Jahr in einer Testphase in Puerto Rico anlaufen, doch im August verschob Google das Projekt auf das kommende Jahr. Aus Finnland gibt es Pläne für das Puzzlephone und die für November angekündigte zweite Version des nachhaltig hergestellten Fairphones wirbt mit austauschbaren Einzelteilen. Xiaomis modulares Smartphone Magic Cube ist zurzeit nur ein Konzept. 
 
Es gibt gute Gründe für modulare Hardware. Elektroschrott ist ein Problem und austauschbare Bauteile könnten dafür sorgen, dass seltener komplette Geräte im Müll landen. Stattdessen werden einfach Module wie die Smartphone-Kamera erneuert. Reparaturen können die Nutzer einfacher selbst durchführen. Kooperationen zwischen Herstellern, die verschiedene, untereinander kompatible Module herstellen, könnten die Kosten für Produktion und Verbraucher senken. Und nicht zuletzt bieten modulare Gadgets eine gewisse Individualität: Das persönliche Smartphone nach dem Lego-Prinzip zusammenstecken – es ist und bleibt eine schöne Idee. 
So attraktiv sie ist, so problematisch ist die Umsetzung. Kleinere Hürden sind die Stabilität, der Schutz vor Schmutz und Wasser sowie die Kompatibilität zwischen bestehenden und vor allem zukünftigen Komponenten. Bei Project Ara etwa wird an einem neuen Verfahren gearbeitet, mit dem die Bauteile besser im Gerät haften sollen. Auch die Module der Smartwatch Blocks müssen noch zeigen, dass sie gleichzeitig einfach zu wechseln sind und das Uhrenband deshalb trotzdem stabil ist. 
Die größere Schwierigkeit besteht darin, dass modulare Gadgets in den besten Fällen auf Dritte angewiesen sind. Sowohl Project Ara als auch Blocks setzen auf ein offenes Entwicklersystem und hoffen, dass eines Tages Hardwarehersteller weitere Module liefern und die Auswahl somit attraktiver machen. Ein modulares Smartphone ist schließlich nur interessant, wenn es verschiedene Kameras, Prozessoren oder Speichermodule gibt, die sich qualitativ und preislich unterscheiden. Die Erfinder von Blocks haben nach eigenen Angaben bereits Anfragen einiger bekannter Hersteller wie dem Chip-Produzenten Qualcomm. 
Etablierte Smartwatch- und Handyhersteller zeigen derweil wenig Interesse für modulare Produkte. Sie sind darauf bedacht, ihr eigenes Ökosystem zu schützen und ihre bestehenden Geräte zu verkaufen. Dass es eines Tages modulare Hardware von Samsung, Apple oder Sony geben könnte, die im besten Fall problemlos untereinander funktioniert, ist praktisch undenkbar. Auch deshalb beschäftigen sich zurzeit vor allem Start-ups wie Blocks oder Fairphone mit dem Konzept; selbst Google hat Project Ara an das eher unbekannte US-Unternehmen Yezz ausgelagert. Eine  ""modulare Revolution "", wie die Macher von Blocks proklamieren, sieht anders aus. 
Das Nischendasein könnte für günstige Bastler-Produkte wie RePhone dennoch funktionieren. Der Erfolg von Mini-Rechnern wie dem Raspberry Pi oder Arduino hat gezeigt, wie die Maker-Szene schon jetzt mit modularer Technik arbeitet. Für die Smartwatch Blocks dürfte es weitaus schwieriger werden, jenseits der erfolgreichen Kickstarter-Kampagne zu bestehen. Selbst wenn Blocks es schafft, einen eigenen Store mit vielen Modulen anzubieten, muss sich die fertige Uhr gegen andere Smartwatches durchsetzen und zeigen, dass sie mehr bietet als die kindliche Faszination mit selbstzusammengesteckten Gadgets."	technik
"Apple hat am Montag kaum für Überraschung, aber ein bisschen für Verwirrung gesorgt. Vier große Themen arbeitete der US-Konzern in der Keynote seiner Entwicklerkonferenz WWDC ab: Das sind die kommenden Generationen der Betriebssysteme für den Mac, für die mobilen Geräte iPhone und iPad sowie für die Apple Watch – und ein neuer Musikdienst. 
Erst hat Apple den Radiostar gekillt, jetzt will es ihn wiederbeleben. Das Unternehmen ist bereit, sein einst marktumwälzendes, zuletzt aber leicht schwächelndes Geschäft mit Musik-Downloads zu kannibalisieren – so nannte es das Wall Street Journal – zugunsten eines Abo-Streamingmodells plus Radiosender plus iTunes-Playlisten plus Personalisierungswerkzeug plus Kommunikationskanal von Künstlern zu Konsumenten. Apple Music heißt das neue Angebot dann in der Summe. 
Für 9,99 US-Dollar im Monat bekommen dessen Nutzer den Onlinezugang zur Apple Music Bibliothek, die zum Start am 30. Juni aus etwa 30 Millionen Songs bestehen soll. Eine dreimonatige Testphase ist kostenlos, Familien können ein Paket für 14,99 Dollar kaufen, das sechs Menschen gemeinsam nutzen können. Euro-Preise wird Apple später bekannt geben. Eine kostenlose, werbefinanzierte Variante wie bei Spotify gibt es nicht. Das mag die Verbraucher enttäuschen (75 Prozent der Spotify-Nutzer wählen die Gratis-Variante), ist aber gut für Plattenfirmen, die nun auf Millionen neuer, zahlender Abo-Kunden hoffen dürfen. Wobei noch nicht bekannt ist, wie viel von dem Geld bei ihnen und bei den Künstlern landen wird. 
Aber Apple Music ist mehr als Streaming aus dem iTunes-Katalog. Zum Paket gehört auch ein Internet-Radiosender namens Beats 1 und der tut, was Radiosender eben tun: Er spielt eine Musik-Auswahl ab. Apple wäre aber nicht Apple, wenn es diese Auswahl nicht als absolut großartig bezeichnen würde, kuratiert von echten Kennern. Doch wer will das schon bewerten bei etwas so individuellem wie Musikgeschmack? Während der Keynote zeigten sich einige Beobachter jedenfalls eher unterwältigt bis ratlos. Ist es nicht ein wenig gaga, Internetradio als das nächste große Ding zu feiern? Möglicherweise ändert sich ihre Meinung, wenn der Dienst ausprobiert werden kann. Aber dazu muss Beats 1 muss mehr sein als ein aufgebohrtes iTunes Radio. 
Die Musik-App wird außerdem personalisierte Playlists zusammenstellen, basierend auf ein paar Vorgaben der Nutzer und lernfähiger Technik. Und es wird mit Connect einen Kanal geben, über den Künstler neue Songschnipsel, Texte oder auch Fotos mit ihren Fans teilen und so mit ihnen kommunizieren können. So etwas Ähnliches hat Apple schon einmal versucht. Erinnert sich noch jemand an Ping? Eben. 
Was an all dem wirklich innovativ sein soll, ist schwer zu sagen. Immerhin dürfte Apples Ziel klar sein: Die Nutzer immer schön im eigenen Musik-Universum halten. 
Bemerkenswert wäre noch, dass es Apple Music auch für Android geben wird. 
Die erste Smartwatch von Apple bekommt ein Betriebssystem-Update. Das watchOS 2 wird im Herbst verteilt. Dessen Neuerungen werden nun die Entwickler von Apps für sich nutzen. 
Bisher waren Drittanbieter-Apps für die Apple Watch nicht gerade ein Verkaufsargument für das Wearable. Da Apple den direkten Zugriff auf die Hardware noch nicht freigegeben hatte, waren alle 4.000 Apps auf der Watch nur erweiterte iPhone-Anwendungen mit entsprechenden zeitlichen Verzögerungen und beschränkten Funktionen. Das ändert sich jetzt: Apple erlaubt Entwicklern, Apps zu programmieren, die direkt auf der Watch laufen. 
Zugreifen dürfen sie auf Mikrofon und Lautsprecher, die Videowiedergabetechnik, den Beschleunigungssensor, den Pulsmesser, die digitale Krone und die Taptic Engine. Denkbar sind damit Apps, die Tonsignale von sich geben, sportliche Betätigung besser erfassen, mit der drehbaren Krone wie mit einem Scrollrad gesteuert werden oder neue Arten von Vibrationshinweisen verwenden. Denn die Taptic Engine ist ein besonderer Vibrationsmotor, der auch prägnante Klopfzeichen und eine Art Pulsieren beherrscht. Möglicherweise wird es schon bald Messaging-Apps geben, die auf Worte verzichten und stattdessen auf Klopfzeichen und Pulsmuster setzen – wie Emojis, die man nur fühlen kann. 
Keinen Zugriff erhalten Entwickler offenbar auf die Force Touch genannte Bedienfunktion. Durch einen kräftigen Druck auf das Display lassen sich bestimmte Einstellungen verändern, das ist auf einem so kleinen Display eine sinnvolle Funktion, weil sie nicht präzise sein muss. Entwickler dürften enttäuscht sein, dass sie sich das vorerst nicht zunutze machen können. 
Auch alternative Ziffernblätter von externen Anbietern wird es weiterhin nicht geben. Apple erweitert die Standardauswahl nur um Ziffernblätter aus den eigenen Fotos oder Fotoalben, wobei dann bei jedem Aktivieren der Uhr ein neues Bild aus dem Album als Hintergrund angezeigt wird. 
 
Das Zeitalter der Apps als voneinander getrennten Container ist beendet. Apples ab Herbst erhältliches mobiles Betriebssystem iOS 9 weicht die Grenzen zumindest zwischen den Anwendungen auf.  ""Proactive "", wie Apple die Technik nennt, handelt kontextabhängig und über App-Grenzen hinweg. Einladungen, die über E-Mail kommen, trägt sie zum Beispiel automatisch in den Kalender ein. Eine Funktion, von der man sich als Journalist nur wünschen kann, dass sie sich deaktivieren lässt. 
Ein zweites Beispiel: Wer auf dem iPhone nach Kontakten sucht, bekommt zunächst jene angezeigt, die etwas mit den anstehenden Terminen zu tun haben. Andere Apps drängen sich selbst je nach Ort oder Uhrzeit in den Vordergrund, oft spielt die Sprachsteuerung Siri eine wichtige Rolle. 
Intelligence features nennt Apple-Manager Craig Federighi das. Intelligence bedeutet nicht nur Intelligenz, sondern auch Geheimdienstinformationen. Das mag auch der erste Eindruck sein, der entsteht, wenn iOS 9 Informationen liefert, ohne dass ein Nutzer explizit danach gefragt hätte. Aber Federighi beteuert, dass Apple kein Interesse daran habe, seine Kunden auszuspionieren. Die Nutzerdaten würden nicht mit der einmaligen Apple-ID verknüpft und blieben auf dem jeweiligen Gerät. 
Das ist der wichtigste Unterschied zu Google Now, dem virtuellen Assistenten des Konkurrenten. Wer Google-Dienste nutzt, muss damit rechnen, dass Daten in einem Nutzerprofil gesammelt werden. Ansonsten hat Apple mit Proactive nur das iOS-Gegenstück zu Google Now entwickelt, das gerade erst eine mächtige Erweiterung namens Google Now On Tap bekommen hat. 
Außerdem stellte Apple für iOS eine App namens News vor. Sie zeigt speziell von den Anbietern aufbereitete magazinartige Artikel und erinnert an Facebooks jüngst gestarteten Versuch namens Instant Articles, Nutzer im eigenen Ökosystem zu halten. Die News-App wird es aber zunächst nur im englischsprachigen Raum geben. 
iOS 9 soll weniger Batterieressourcen verbrauchen und die Akkulaufzeit eines iPhones bei normaler Benutzung um eine Stunde verlängern. Ein spezieller Energiesparmodus soll für eine um drei Stunden verlängerte Laufzeit sorgen. 
Die nächste Version des Betriebssystems für den Mac wird OS X El Capitan heißen – benannt nach dem ikonischen Monolithen im Yosemite-Nationalpark. Apple hat nach eigenen Angaben vor allem Wert auf bessere Performance zum Beispiel für Gamer und eine intuitivere Bedienung gelegt. Im Herbst wird El Capitan als Update zur Verfügung stehen."	technik
"Die Kartellkammer des Berliner Landgerichts hat eine Klage von elf Verlagen gegen Google abgewiesen. Die Medienhäuser werfen Google vor, die kostenfreie Nutzung von Textausschnitten zu erzwingen. Das Gericht urteilte, dass der Konzern zwar der deutlich größte Suchmaschinenanbieter sei, einzelne Verlage jedoch nicht diskriminieren würde. 
Beide Seiten würden von Googles Vorgehen profitieren, sagte der Vorsitzende Richter Peter Scholz. Google leite Nutzer auf die Seiten der Verlage weiter, was zu mehr Seitenaufrufen und damit höheren Werbeeinnahmen führe. Profitieren würden auch die Nutzer, da es ihnen einfacher gemacht werde, relevante Inhalte zu finden. 
Die klagenden Verlagsgruppen, zu denen unter anderem der Axel-Springer-Verlag, Madsack und Dumont gehören, wollen prüfen, ob sie ihre Klage in der nächsthöheren Instanz einreichen. Im September waren sie bereits vom Bundeskartellamt zurückgewiesen worden. Die Behörde begründete ihre Entscheidung damit, dass es nicht um Kartellrecht gehe, sondern darum, wie weit das Leistungsschutzrecht reiche. 
Das Leistungsschutzrecht gilt seit August 2013. In ihm ist geregelt, dass Suchmaschinenbetreiber Geld an Verlage zahlen müssen, wenn sie deren Inhalte verwenden und dabei über  ""einzelne Worte oder kleine Textausschnitte "" hinausgehen. 
 
 
"	technik
"Wer zuerst kommt, mahlt zuerst: Nach diesem Prinzip hat die Bundesregierung eine Kaufprämie für Elektroautos aufgesetzt, die sie offiziell  ""Umweltbonus "" nennt. Die Subvention kann seit Anfang Juli beantragt werden. Ziel ist es, damit die Nachfrage nach elektrisch betriebenen Pkws anzukurbeln. Bislang scheint das selbst gesteckte Ziel, im Jahr 2020 eine Million Elektroautos auf deutschen Straßen zu haben, nämlich nicht zu erreichen worden zu sein. In Deutschland sind noch immer weniger als 50.000 Elektroautos zugelassen. 
Der Käufer eines reinen Batterie-elektrischen Neuwagens oder eines neuen Autos mit Brennstoffzelle erhält insgesamt einen Zuschuss von 4.000 Euro. Davon übernimmt der Bund die Hälfte, die andere Hälfte muss der Automobilhersteller dem Käufer als Nachlass gewähren. Für Plug-in-Hybride – also Fahrzeuge mit Verbrennungs- und Elektromotor, die sich an der Steckdose laden lassen – gibt es 3.000 Euro, ebenfalls je zur Hälfte von Bund und vom Hersteller getragen. Grundsätzlich fördert der Staat mit der Prämie Autos mit einem CO2-Ausstoß von unter 50 Gramm je Kilometer. 
Um den Vorwurf auszuräumen, der Zuschuss würde vor allem Luxusspielzeuge von Gutbetuchten unterstützen, hat der Bund eine Preisobergrenze festgelegt: Der Netto-Listenpreis des Basismodells darf 60.000 Euro netto nicht überschreiten. Das führt dazu, dass zum Beispiel ein Tesla Model S nicht förderfähig ist. Die komplette Liste aller Modelle, für die ein Umweltbonus beantragt werden kann, gibt es beim für die Prämie zuständigen Bundesamt für Wirtschaft und Ausfuhrkontrolle (Bafa). 
Außerdem werden nur Fahrzeuge gefördert, die seit dem 18. Mai 2016 gekauft wurden. Neben Privatpersonen können auch Unternehmen, Vereine, Körperschaften und Stiftungen die Prämie beantragen – nicht aber die öffentliche Hand sowie die Autohersteller selbst und deren Tochterfirmen. Das Fahrzeug muss mindestens sechs Monate auf den Antragsteller in Deutschland zugelassen sein. 
Insgesamt stehen 1,2 Milliarden Euro zur Verfügung – 600 Millionen Euro kommen vom Bund. Ist das Geld alle, endet automatisch die Laufzeit der Kaufprämie. Die Mittel reichen für bis zu 400.000 Fahrzeuge und damit laut Bafa voraussichtlich bis 2019. Die Regierung geht davon aus, dass der Topf aber schon früher leer sein dürfte. 
Bisher ist die Nachfrage nach der Prämie eher schleppend angelaufen. Beim Bafa sind seit 2. Juli bis Montag gut 700 Anträge eingegangen. Aufgeschlüsselt nach Herstellern liegt BMW mit 225 Anträgen vorne. Es folgen Renault (195), Volkswagen/Audi (73) und Mitsubishi (55). Die Antragszahl entspreche den Erwartungen, sagte Bafa-Präsident Arnold Wallraff. 
Das Bafa, das dem Bundeswirtschaftsministerium unterstellt ist, hat das Antragsformular online gestellt. Die Behörde hatte 2009 im Zuge der Wirtschaftskrise auch schon die sogenannte Abwrackprämie bearbeitet. Zusammen mit dem Antrag muss der Käufer den Kauf- beziehungsweise Leasingvertrag für das Elektroauto hochladen. 
 
Insgesamt listet das Bafa 81 Fahrzeugmodelle auf, für die die Kaufprämie auf Antrag gezahlt wird. Laut einem Bericht des Spiegel wurde der Bonus bisher besonders häufig für die Modelle Renault Zoe, BMW i3 und BMW 225xe beantragt. 
Der Renault Zoe ist der aktuelle Elektroauto-Bestseller in Europa. Er ist an einer heimischen Wallbox in einer guten Stunde vollgeladen, was für 100 bis 150 Kilometer mit dem 65-kW-Motor (88 PS) reicht. Zum Basispreis von 21.500 Euro addiert sich eine monatliche Batteriemiete von mindestens 49 Euro. 
Bei jeweils knapp 35.000 Euro beginnen die Preislisten von BMW i3 und Volkswagen e-Golf. Beide kommen von einer deutschen Marke, schaffen gut 150 Kilometer Aktionsradius, haben sonst aber wenig gemeinsam. Der vier Meter lange i3 ist optisch eigenständig. Für ein batterieelektrisches Auto ist er sehr leicht, hat einen starken Motor (125 kW/170 PS) und eine Optionsliste, mit der man den Preis problemlos über 50.000 Euro treiben kann. Der e-Golf (85 KW/115 PS) lässt sich für Laien kaum von anderen Golf-Modellen unterscheiden. Seine Serienausstattung ist mit vier Türen, Klimaautomatik und Navigationssystem fast vollständig. In Norwegen ist er das meistverkaufte E-Auto. 
Der Nissan Leaf ist mit über 200.000 Exemplaren der Weltverkaufsmeister unter den batterieelektrischen Autos. Er überzeugt durch Verlässlichkeit und die zuletzt auf 30 Kilowattstunden (kWh) gewachsene Speicherkapazität. Damit sind auf dem Papier bis zu 250 Kilometer Reichweite möglich; im Alltag sind abseits der Autobahn 200 Kilometer realistisch. Der Nissan Leaf, vom Format ungefähr so groß wie ein VW Golf und mit einem 80 kW (109 PS) starken Motor ausgestattet, kostet in der Version mit großer Batterie ab 34.385 Euro. Der Mindestpreis beträgt scheinbar geringe 23.365 Euro – dafür gibt es allerdings nur die Variante mit 24 kWh, und zusätzlich muss eine monatliche Rate als Leasing für die Batterie bezahlt werden. 
Daneben führt die Bafa-Liste auch einige rein elektrisch betriebenen leichten Nutzfahrzeuge auf, die ebenfalls förderfähig sind, beispielsweise der Kastenwagen Citroën Belingo Electric, der der Peugeot Partner Electric oder der Renault Kangoo Maxi Z.E. 
In jedem Fall nicht förderfähig ist, wie erwähnt, das luxuriöse Tesla Model S. Unter 82.700 Euro geht nichts. Wer zum Model S mit dem Zusatz P90D greift, muss mindestens 102.100 Euro zahlen. Dann fährt der Tesla 250 km/h schnell und beschleunigt in 4,4 Sekunden auf 100 km/h. Die Normreichweite von 557 Kilometern im offiziellen Testzyklus NEFZ reduziert sich auf der Autobahn auf unter 400 Kilometer. Die Nachfrage ist hoch, doch viele warten auf Teslas Model 3, das 2018 ab rund 40.000 Euro zu haben sein soll. 
Plug-in-Hybrid-Favorit in Europa ist aktuell der Mitsubishi Outlander PHEV. Wow, ein ökologisches SUV, könnte man bei seinem Anblick denken. Und der Normverbrauch klingt mit 1,8 Litern je 100 Kilometer auch phänomenal. Allerdings können es auch über zehn Liter Superbenzin werden – dann nämlich, wenn die Batterie erschöpft ist. Pluspunkte beim Outlander PHEV: Er ist schnellladefähig und mit Anhängerkupplung erhältlich. Für mindestens 39.990 Euro verkauft Mitsubishi eine Menge Auto, die Motoren bieten eine Systemleistung von 149 kW (203 PS). 
Technische Zwillinge sind der Audi A3 e-tron und der Volkswagen Golf GTE. Im Golf beginnt der elektrische Teilzeitspaß bei 36.900 Euro, im A3 bei 38.400 Euro. Die schwere Batterie hinter der Hinterachse schränkt das Kofferraumvolumen ein, und die elektrische Reichweite von 50 Kilometern schaffen selbst erfahrene Schleicher kaum. 
Aus dem VW-Konzern kommt außerdem der Volkswagen Passat GTE Variant. Er kostet mindestens 45.200 Euro. Sein härtester Konkurrent ist der TDI 2.0 mit 110 kW (150 PS) und Automatikgetriebe, der bei 36.600 Euro startet. Zwar ist der GTE zügiger unterwegs, mit den Kraftstoffkosten des TDI aber kann er nicht mithalten. So bleibt der Plug-in-Hybrid etwas für Leute, die unbedingt mal ausprobieren wollen, wie sich das Fahren mit Strom anfühlt. 
Der erste Van von BMW ist seit Frühjahr auch als Teilzeit-Elektroauto erhältlich. Der BMW 225xe, die Plug-in-Hybridversion des 2er Active Tourer, kombiniert einen Elektromotor mit einem vorn quer eingebauten Dreizylinder-Turbobenziner. Dabei treibt der Verbrenner die Vorderräder an, die Kraft des Elektromotors geht an die Hinterachse. Im Sportmodus leisten beide Antriebe zusammen 165 kW (224 PS). Laut BMW erreicht der 225xe (ab 38.700 Euro) einen Normverbrauch von 2,1 Liter pro 100 Kilometer. Die Batterie, deren Kraft für 41 Kilometer elektrisches Fahren reicht, ist in dem Kompaktvan unter der Rücksitzbank untergebracht. 
Schon länger auf dem Markt ist der Volvo V60 Plug-in-Hybrid (56.900 Euro). Der Mittelklasse-Kombi war das erste Fahrzeug, das Diesel- und Elektromotor verband. Zusammen kommen sie auf 206 kW (283 PS) und beschleunigen den Zweitonner bis auf 230 km/h. Bis zu 50 Kilometer soll der Steckdosen-Hybrid rein elektrisch schaffen, der Durchschnittsverbrauch auf 100 Kilometern wird mit 1,8 Litern angegeben. 
Der bekannte Toyota Prius Plug-in Hybrid (ab 36.600 Euro) basiert noch auf dem Vorgänger-Modell, die neue Version kommt erst Ende des Jahres auf den Markt. Arbeiten Verbrenner und E-Motor zusammen, ergibt sich eine Systemleistung von 100 kW (136 PS). Aufgeladen fährt der Kompakte rund 25 Kilometer rein elektrisch und erreicht eine Höchstgeschwindigkeit von 100 km/h. Der Verbrauch liegt bei 2,1 Litern. 
Bis Ende 2020 sind Elektroautos ab der Neuzulassung fünf Jahre von der Kfz-Steuer befreit. In einem Gesetz zur Förderung der Elektromobilität räumte die Regierung im vergangenen Jahr den Kommunen außerdem die Möglichkeit ein, Busspuren für Elektroautos zu öffnen, Durchfahrtsverbote für Elektroautos aufzuheben oder kostenlose Parkplätze für diese Fahrzeuge einzurichten. Bislang nutzen die Städte und Gemeinden diese Möglichkeit aber kaum. Daher bringt auch das eigens dafür eingeführte Kfz-Kennzeichen für Elektroautos, erkennbar am nachgestellten E (zum Beispiel B - PR 173E), kaum Vorteile. 
Neben der Kaufprämie beschlossen Vertreter der Autoindustrie und die Bundesregierung beim Autogipfel Ende April im Kanzleramt, dass zwischen 2017 und 2020 insgesamt 300 Millionen Euro in den Ausbau der vielerorts noch rudimentären Ladeinfrastruktur fließen sollen: 200 Millionen Euro für Schnellladesäulen, 100 Millionen Euro für normale Ladesäulen. Bundesverkehrsminister Alexander Dobrindt rechnet für diese Summe mit 15.000 zusätzlichen Ladepunkten. Sie sollen auch an Supermärkten oder Sportplätzen entstehen. 
Weitere 100 Millionen Euro sind dafür reserviert, mehr Elektroautos für die Fahrzeugflotten des Bundes anzuschaffen. Künftig soll jeder fünfte Wagen des Bundes mit Strom fahren."	technik
"Gut zwei Jahre ist es her, dass der CSU-Politiker Günther Beckstein mit einer Äußerung zum Thema Alkohol und Autofahren große Entrüstung auslöste. Zwei Liter Bier – oder zwei Maß, wie der Bayer sagt – innerhalb von sechs oder sieben Stunden getrunken, seien durchaus kompatibel mit einer anschließenden Autofahrt, hatte Beckstein, damals bayerischer Ministerpräsident, behauptet. Mit vehementem Protest reagierten nicht nur die Drogenbeauftragten und Suchtexperten der Republik. In der CSU passten Alkohol und Autofahren offensichtlich gut zusammen, empörten sich Kritiker, und die Diskussion um eine Null-Promille-Grenze war einmal mehr neu entfacht. 
Eine ganze Reihe von Gruppen unterstützt die Forderung nach einer schärferen Promille-Regelung. Dazu zählen der Automobil Club Europa (ACE), die Verkehrswacht und die Gewerkschaft der Polizei (GdP). Die Mehrheit der Bevölkerung haben sie dabei hinter sich. Zu diesem Ergebnis kommt zumindest eine repräsentative Umfrage, die kürzlich im Auftrag eines Versicherungsunternehmens gemacht wurde. Zwei von drei Befragten sprachen sich dort für ein absolutes Alkoholverbot im Straßenverkehr aus. Vor allem viele Frauen halten demnach die Null-Lösung für richtig. Aber auch bei den Männern sprach sich immerhin noch mehr als jeder Zweite dafür aus. 
Der ACE führt dazu konkrete Zahlen ins Feld. In den vergangenen zehn Jahren kamen rund 58.000 Menschen bei Unfällen auf deutschen Straßen ums Leben – rund 7100 Verkehrstote waren demnach bei Alkoholunfällen zu beklagen. Wie Statistiken zudem belegen, sind die Folgen von Unfällen, bei denen Alkohol im Spiel war, überdurchschnittlich schwer. Während bei 1000 Unfällen mit Personenschaden im Durchschnitt 13 Todesopfer zu beklagen sind, verlieren bei 1000 Alkoholunfällen durchschnittlich 25 Menschen ihr Leben. 
1998 wurde die Obergrenze in Deutschland von 0,8 auf 0,5 Promille gesenkt, nach einer Übergangszeit gilt der niedrigere Grenzwert seit 2001. Dass seit 1998 die Zahl der Alkoholunfälle, bei denen Personen verletzt oder getötet wurden, um 40 Prozent zurückgegangen ist, liefert Befürwortern und Gegnern einer weiteren Verschärfung Argumente. Das Bundesverkehrsministerium wertet die Bilanz als Erfolg und sieht derzeit keinen weiteren Handlungsbedarf.  ""Die 0,5-Promille-Grenze hat sich bewährt. Sie wird von allen Verkehrsteilnehmern akzeptiert und trägt wesentlich zu mehr Verkehrssicherheit bei "", meint Verkehrsminister Peter Ramsauer. 
Deutschlands größter Autofahrerclub, der ADAC, stößt ins gleiche Horn. Autofahrer, die sich durch den aktuellen Grenzwert und die drohenden Rechtsfolgen nicht von Alkoholfahrten abhalten lassen, könne auch ein schärferer Grenzwert nicht erreichen, argumentiert der ADAC. Die Mehrheit der Alkoholunfälle ereigne sich ohnehin jenseits der 1,5-Promillegrenze. Zudem zeige das Beispiel der Länder mit gesetzlichem Null-Promille-Limit – darunter Estland, Ungarn und Tschechien – dass ein striktes Alkoholverbot im Straßenverkehr nicht automatisch zu weniger Unfällen und Verkehrstoten führe. 
Die Befürworter einer schärferen Promille-Lösung ziehen ganz andere Schlüsse aus den Statistiken und den internationalen Vergleichen.  ""Die einzelnen Länder lassen sich nicht über einen Kamm scheren "", sagt Johannes Lindenmeyer. Er ist Direktor der Salus Klinik Lindow und beschäftigt sich im Rahmen von Sucht-Präventionsprogrammen auch mit der Problematik Alkohol im Straßenverkehr.  ""Internationale Studien haben sehr wohl gezeigt, dass ein Absenken der Alkoholgrenzwerte Wirkung zeigt "", sagt der Experte. In allen Ländern habe sich dadurch die Zahl alkoholbedingter Unfälle und Verkehrsopfer reduzieren lassen.  ""Schließlich ist das ja auch der Grund dafür, dass man hierzulande 2007 ein striktes Alkoholverbot für jugendliche Fahranfänger eingeführt hat. "" 
 
Zwar will sich der Präventionsexperte keinesfalls als puristischer Anti-Alkohol-Apostel verstanden wissen. Doch für eine klare Spielregel spricht er sich aus: null Alkohol im Straßenverkehr.  ""Es ist viel einfacher, gar nicht zu trinken, als wenig zu trinken "", ist Lindenmeyer überzeugt. 
Eine klare Linie mache es auch dem Umfeld leichter. So müssten Begleiter oder Gastwirte eingreifen, wenn sich jemand mit einem zu hohen Alkoholpegel im Blut hinters Steuer setzen will.  ""Doch wann man aktiv werden muss, wann man überhaupt aktiv werden darf, das lässt sich im konkreten Fall oft nur schwer entscheiden "", gibt Lindenmeyer zu bedenken. Verschiedene Menschen reagierten auf die gleiche Alkoholmenge ganz unterschiedlich. Gerade Vieltrinker hätten eine hohe Toleranz, sodass ihnen schwerlich anzumerken sei, wenn das gesetzlich tolerierte Level bereits überschritten ist. 
Auswirkungen auf die Fahrtüchtigkeit zeigen sich medizinischen Untersuchungen zufolge jedoch schon unterhalb der 0,5-Promille-Grenze. So verschlechtert sich die Adaptionsfähigkeit des Auges beim Hell-Dunkel-Sehen. Vor allem die Reaktionsfähigkeit wird schon durch einen moderaten Alkoholkonsum erheblich beeinträchtigt. 
Routinetätigkeiten – auch das Autofahren – könnten an Alkohol gewöhnte Menschen zwar noch ohne erkennbare Leistungseinbußen bewältigen, nachdem sie ein paar Gläschen getrunken haben, sagt Lindenmeyer. Doch sobald die Reaktion auf eine unerwartete Situation gefordert ist, schnitten sie schon nach dem Konsum geringer Alkoholmengen deutlich schlechter ab als im nüchternen Zustand. 
Tatsächlich gibt es laut dem Fachmann keine wissenschaftliche Basis für die aktuelle Promille-Regelung – genauso wenig, wie es sie für die 0,8 Promille gab. Der Grenzwert sei vielmehr ein politischer Kompromiss.  ""Das ist wie mit der Atomkraft "", sagt Lindenmeyer.  ""Ein gewisses Restrisiko bleibt. Die Gesellschaft muss entscheiden, ob sie das eingehen will. ""
""Die politischen Folgen der Snowden-Enthüllungen mögen bisher dürftig ausfallen. Aber auf der technischen Ebene tut sich etwas: Der verschlüsselte Datenverkehr im Internet hat sich im vergangenen Jahr weltweit mehr als verdoppelt. In Europa laufen sogar mindestens dreimal so viele Daten über verschlüsselte Verbindungen wie noch Anfang 2013, zeigt eine Statistik des kanadischen Netzwerkanbieters Sandvine. Verschlüsselung ist kein Allheilmittel gegen die Überwachungstechnik von NSA und GCHQ, sie macht es den Geheimdiensten aber zumindest schwerer. 
Die Zunahme liegt nicht zuletzt in den erweiterten Angeboten der großen Internetdienstleister begründet. Google, Yahoo oder Facebook hatten die Verschlüsselung ihrer Dienste nach den ersten NSA-Enthüllungen in der zweiten Jahreshälfte 2013 vorangetrieben. Die Statistik wird halbjährlich von Sandvine erhoben. 
Sandvine listet auf, auf welche Dienste sich der internationale Datenverkehr im Upload, Download und insgesamt verteilt. In Europa etwa entfällt in der Gesamtrechnung mit 17,4 Prozent der größte Teil auf YouTube. Zwar lässt sich Googles Videoplattform auch mit vorangestelltem https aufrufen, aber die verschlüsselte Verbindung wird noch nicht automatisch erzwungen. Deshalb zählt YouTube für Sandvine zu den unverschlüsselt genutzten Diensten. 
An zweiter Stelle liegt der Datendurchsatz zu unverschlüsselten Webseiten über das Protokoll http (16,3 Prozent), gefolgt von Daten, die über das File-Sharing-Protokoll Bittorrent (14,7 Prozent) laufen. An vierter Stelle dann steht der Datenverkehr über das Verschlüsselungsprotokoll SSL, er macht 6,1 Prozent des Gesamtverkehrs aus. Das ist zwar noch nicht viel, aber im Vorjahr waren es nur 1,5 Prozent. Der Anteil des verschlüsselten Datenverkehrs in Europa hat sich also vervierfacht. Der Anteil des verschlüsselten Datenverkehrs über den Mobilfunk nahm in Europa von 3,4 auf 6,2 Prozent zu. 
In den USA und Kanada fällt die Zunahme nicht ganz so hoch aus: aus 2,4 Prozent wurden 3,8 Prozent. In Lateinamerika aber stieg er von 1,8 auf 10,4 Prozent. Und auch in Afrika und Asien hat der verschlüsselte Datenverkehr sowohl im Festnetz als auch im Mobilfunknetz deutlich zugenommen. 
Eine Möglichkeit für Endnutzer, vermehrt SSL-gesicherte Verbindungen zu nutzen, ist die Browser-Erweiterung HTTPS Everywhere von der Electronic Frontier Foundation (EFF). Es gibt sie für den Firefox-Browser sowie als Betaversion für Android, Opera und Chrome. HTTPS Everywhere prüft anhand einer Liste, ob eine Website auch SSL unterstützt, aber nicht erzwingt. Wenn das der Fall ist, sorgt die Erweiterung dafür, dass die verschlüsselte Variante genutzt wird."	technik
"Im Tarifkonflikt zwischen der Deutschen Bahn und der Gewerkschaft Deutscher Lokomotivführer (GDL) ist die Schlichtung bis zum 25. Juni verlängert worden. Darauf haben sich die Beteiligten geeinigt. 
Die Schlichtung hatte am 27. Mai begonnen und war – mit der Option auf Verlängerung – auf drei Wochen angesetzt. Als Schlichter eingesetzt sind der Thüringer Ministerpräsident Bodo Ramelow (Linke) und der frühere brandenburgische Regierungschef Matthias Platzeck (SPD). 
Das Hauptproblem des seit einem Jahr andauernden Tarifkonflikts ist die Forderung der GDL, für jede der bei ihr organisierten Berufsgruppen einen eigenständigen Tarifvertrag abschließen zu dürfen. Die Bahn hat der GDL das zwar zugestanden, peilt aber Regelungen an, die widerspruchsfrei zu anderen Tarifverträgen sind, die mit der größeren Eisenbahn- und Verkehrsgewerkschaft (EVG) abgeschlossen wurden. Die Bahn strebt für ein und dieselbe Tätigkeit die gleiche Bezahlung und gleiche Arbeitszeiten an."	technik
"Als Yan Zhu 16 Jahre alt war, ging sie zur High School in St. Louis, Missouri. Dort langweilte sie sich so sehr, dass sie beschloss, sich direkt fürs College zu bewerben. Nicht für irgendeines, sondern für das berühmte MIT, das Massachusetts Institute of Technology. Sie wurde angenommen, und seither ist es mit der Langeweile vorbei. 
Yan Zhu, geboren in Peking und mit fünf Jahren in die USA umgezogen, ist jetzt 24 Jahre alt und hat eine Mission:  ""Ich helfe Menschen, sicher, privat und anonym das Internet zu nutzen "". Sie tut das, indem sie eine E-Mail-Verschlüsselung für Yahoo-Nutzer entwickelt, mit der Initiative Let's Encrypt und der Browsererweiterung HTTPS Everywhere sichere Internetverbindungen fördert, den elektronischen Briefkasten SecureDrop für Whistleblower verbessert und in einer von Tim Berners-Lee geleiteten Arbeitsgruppe des World Wide Web Consortiums (W3C) an der Web-Architektur der Zukunft arbeitet. Je weniger Daten unverschlüsselt, also als clear text über das Internet gesendet werden, desto zufriedener ist sie. 
Es ist eine erstaunliche Karriere für eine Mittzwanzigerin, die nach eigenen Angaben  ""erst mit 17 oder 18 angefangen hat, Computer wirklich zu benutzen "". Gleichzeitig könnte man Yan Zhu als Traditionalistin bezeichnen: Sie arbeitet mit Verschlüsselungsmethoden, die zum Teil älter sind als sie selbst und von fantasievollen Hackern zwar nicht grundsätzlich gebrochen, aber immer wieder umgangen oder ausgetrickst werden. Denn bessere gibt es noch nicht, jedenfalls nicht für den Masseneinsatz. 
Die Snowden-Enthüllungen haben der Welt vor Augen geführt, welches Ausmaß die Überwachung des Internets angenommen hat, welch einen Aufwand die NSA betreibt, um Datenpakete rund um den Globus abzufangen und auszuwerten. Doch nicht nur Geheimdienste nutzen die Schwachstellen der Technik und die Sorglosigkeit der Nutzer aus. Andere staatliche Stellen benutzen Überwachungstechnik zur Zensur des Netzes. Kriminelle versuchen, an Zugangsdaten oder andere verwertbare Informationen zu gelangen. Und nicht wenige Unternehmen nutzen die vielfältigen Analyseverfahren, um das Verhalten und die Interessen von Internetnutzern zu beobachten. 
Eine Möglichkeit, vieles davon zu verhindern oder zumindest erheblich zu erschweren, ist die Verschlüsselung von Nachrichten und Datenübertragungen. Dafür gibt es etablierte Protokolle und Programme wie PGP und SSL/TLS. 
Yan Zhu hat es sich zur Aufgabe gemacht, diese lange bekannte Technik möglichst weit zu verbreiten. Als Entwicklerin bei Yahoo und bei der Bürgerrechtsbewegung Electronic Frontier Foundation (EFF) betreut sie deshalb Projekte wie Yahoo End-to-End, Let's Encrypt und HTTPS Everywhere. 
Zum Artikel  ""Verschlüsselung: Kampf dem Klartext "" 
Heutige Verschlüsselungsverfahren wie PGP und SSL/TLS basieren auf jahrzehntealten Methoden und Algorithmen. Sie sind nicht perfekt, hinreichend gut ausgestatte und motivierte Angreifer können sie überwinden. Aber insbesondere die mühelose, passive Massenüberwachung im Internet durch Geheimdienste können sie weitgehend unmöglich machen. Jedenfalls solange, bis es Computer gibt, die heutige kryptografische Verfahren brechen. 
Quantencomputer werden das schaffen. Die Fachwelt rechnet damit, dass es in zehn bis 15 Jahren praxistaugliche Quantencomputer geben wird. Unternehmen wie IBM und Google arbeiten daran, aber auch die NSA versucht, einen solchen Computer zu entwickeln. Er ist ausdrücklich dafür gedacht, verschlüsselte Daten zu entschlüsseln. 
Tanja Lange sucht deshalb Algorithmen, die künftig zur Verschlüsselung eingesetzt werden können und die auch einem Quantencomputer standhalten würden. Die deutsche Mathematikerin an der Technischen Universität Eindhoven leitet dazu das von der EU-Kommission geförderte Projekt PQCRYPTO, das steht für Post-Quantum-Cryptography. 
Zum Artikel  ""Kryptografie: Verschlüsseln für den Tag X "" 
Eine dieser Techniken ist die Transportverschlüsselung mit SSL beziehungsweise dessen Nachfolger TLS. Mithilfe dieser Protokolle werden vor allem Internetverbindungen gesichert, bei denen sensible Daten wie Passwörter übertragen werden, also zum Beispiel beim Onlinebanking oder auf Shoppingseiten, aber auch bei der Anmeldung in sozialen Netzwerken. Nutzer erkennen eine SSL/TLS-Verbindung am  ""https "" in der Adresszeile ihres Browsers oder am Schloss-Symbol. 
Der Vorteil dieser Technik: Sie verbirgt, welche Daten ein Nutzer überträgt, und zwar vor dem eigenen Internetprovider, vor Schnüfflern im selben WLAN zum Beispiel im Café, und auch vor Geheimdiensten wie der NSA und deren britischem Gegenstück GCHQ, die transatlantische Glasfaserkabel anzapfen und massenweise Daten kopieren und speichern. 
Der Nachteil: Für Websitebetreiber ist es nicht ganz einfach, die Technik richtig zu implementieren. Zudem kostet es Geld, weil man dafür Zertifikate braucht, die den Nutzern zeigen, dass sie wirklich auf die gewünschte Website zugreifen und die von spezialisierten Unternehmen gegen Gebühr ausgestellt werden. 
An dieser Stelle setzt Yan Zhu an. Mit der Initiative Let's Encrypt, für die sie als Entwicklerin arbeitet, will sie die Implementierung von TLS stark vereinfachen:  ""Let's Encrypt ist eine neue Zertifikatsstelle, die kostenlose Zertifikate ausgeben wird an alle, die ihre Websites mit SSL verschlüsseln wollen "", sagt sie. Diese Woche will Let's Encrypt die ersten Zertifikate ausstellen. 
 
Als mögliche Zielgruppe von Let's Encrypt nennt Zhu Menschen, die eine kleine, private Website absichern wollen, aber wenig Ahnung von Web-Entwicklung haben.  ""So jemand braucht für die Implementierung von SSL mindestens ein bis zwei Stunden. Mit Let's Encrypt soll es weniger als eine Minute dauern. Es automatisiert die Implementierung in der sichersten bekannten Konfiguration. Und es ist kostenlos. "" 
Es gibt zwar auch andere Anbieter, die kostenlose Zertifikate ausgeben oder zumindest nicht mehr als zehn Dollar verlangen, aber dann müssen die Websitebetreiber immer noch selbst herausfinden, wie sie ihre Seite damit absichern. Noch ein Vorteil von Let's Encrypt: Die Zertifikate werden automatisch erneuert, wenn sie abgelaufen sind – etwas, das Administratoren sonst gerne mal vergessen. 
Weil ein komplett SSL/TLS-gesichertes Netz so etwas wie ein Lebenstraum von Zhu ist, hat sie auch das Browser-Add-on HTTPS Everywhere weiterentwickelt.  ""Die Erweiterung für Firefox, Firefox für Android, Chrome und Opera erzwingt verschlüsselte HTTPS-Verbindungen zu Websites, die das nicht standardmäßig unterstützen "", erklärt sie. 
Das Add-on wird also immer dann aktiv, wenn eine Seite über eine HTTPS-Verbindung erreichbar wäre, ein Nutzer dazu aber auch  ""https://... "" in die Adresszeile seines Browsers eingeben müsste, weil er andernfalls automatisch eine ungesicherte Verbindung aufbauen würde. Es basiert auf einer ständig aktualisierten Whitelist: Hunderte Freiwillige untersuchen Websites darauf, inwieweit sie HTTPS unterstützen, und schreiben entsprechende Regeln für das Add-on. 
HTTPS Everywhere ist ein Projekt der Bürgerrechtsbewegung Electronic Frontier Foundation (EFF). Für die arbeitet Zhu seit etwa drei Jahren. Nach ihrem MIT-Abschluss war sie nach Kalifornien gezogen, um in Stanford über Quantengravitation zu promovieren, hatte die Elite-Uni aber schnell wieder verlassen,  ""weil ich den Eindruck hatte, die akademische Kultur wäre nicht die richtige für mich "", wie sie sagt. Außerdem wollte sie an Dingen arbeiten,  ""die unmittelbar relevant für Aktivisten und Journalisten "" sind. So fand sie im berühmten Hackerspace Noisebridge und eben bei der EFF in San Francisco Gleichgesinnte. In diesem Umfeld entwickelte sie ihre technischen Fertigkeiten – und ihren sehr nerdigen Sarkasmus, der weniger im persönlichen Gespräch mit ihr, aber umso mehr in ihrem Twitter-Profil zum Vorschein kommt. 
Zhus day job, wie sie ihn nennt, ist aber nicht bei der EFF, sondern bei Yahoo. Das Unternehmen versucht gerade, sein mieses Image in Sicherheitsfragen loszuwerden. Yahoo nahm – wenn auch unfreiwillig – am Prism-Programm der NSA teil und galt lange Zeit als langsam und rückständig, wenn es um Datensicherheit und Verschlüsselung ging. 
Erst als Alex Stamos im März 2014 neuer Chef der intern  ""Die Paranoiden "" genannten Sicherheitsabteilung wurde, die Verschlüsselung von Nutzerdaten vorantrieb und sich öffentlich mit NSA-Direktor Michael Rogers anlegte, änderte sich das. Stamos, mittlerweile Sicherheitschef von Facebook, war es auch, der Yan Zhu zu Yahoo und den  ""Paranoiden "" holte. 
Dort leitet sie nun das Projekt Yahoo End-to-End, ein Plug-in für den Chrome-Browser, mit dessen Hilfe sich Yahoo-Nutzer verschlüsselte E-Mails senden können, ähnlich wie GMX und Web.de das seit Kurzem ermöglichen. Beide Lösungen basieren auf dem 25 Jahre alten Programm PGP. 
Die Abkürzung PGP steht für Pretty Good Privacy, und das trifft es im Kern ziemlich gut: PGP sorgt dafür, dass nur Sender und Empfänger eine Nachricht lesen können, selbst wenn sie unterwegs von einem Dritten abgefangen wird. Außerdem lässt sich mit der Signaturfunktion von PGP überprüfen, ob eine Nachricht wirklich vom vermeintlichen Absender kommt, oder ob jemand sie manipuliert hat. 
Es ist allerdings nicht ganz einfach, PGP richtig zu benutzen, weshalb es nie wirklich massentauglich geworden ist. Yahoo End-to-End soll die Benutzung von PGP erheblich erleichtern, damit die Hürde zum Verschlüsseln und Signieren sinkt und mehr Menschen es tun. 
Aber warum überhaupt diese alte, komplizierte Technik, die sich nie richtig durchgesetzt hat? Warum entwickelt Yahoo nichts Eigenes, Neues?  ""Der Vorteil von PGP ist: Es wurde ausführlich getestet und optimiert "", sagt Zhu.  ""Viele Menschen haben sich damit beschäftigt und sichergestellt, dass es wirklich ziemlich sicher ist. "" 
Yahoo nutzt als Basis für sein Add-on einen Code von Google, das schon früher angefangen hatte, an einem ganz ähnlichen Projekt zu arbeiten. Der Vorteil: Yahoo-Nutzer werden auch mit Gmail-Kunden verschlüsselt kommunizieren können, und auch mit Nutzern von GnuPG, einer freien Alternative zu PGP. 
Im Moment wird die Chrome-Erweiterung von Yahoo noch getestet, das Unternehmen hat den Quellcode veröffentlicht, damit externe Spezialisten ihn überprüfen können. Yahoo möchte die Erweiterung aber noch in diesem Jahr seinen Nutzern anbieten. 
PGP war sozusagen Zhus Einstiegsdroge: Ihr Weg von der High-School-Abbrecherin zur Hackerin begann nämlich mit einem  ""sehr paranoiden Freund am College "", sagt sie.  ""Er akzeptierte keine unverschlüsselten Mails von mir. Das motivierte mich, den Umgang mit PGP zu lernen. "" Heute hat Verschlüsselung für sie nichts mehr mit Paranoia zu tun, es ist zu einer Selbstverständlichkeit geworden. Für E-Mails, die sie unverschlüsselt schickt, entschuldigt sie sich mittlerweile."	technik
" ""Produktions-Weltrekord für Coupés "" und  ""Der Schlager auf dem Automarkt "" verkündeten die Schlagzeilen 1963, als der Karosseriebauer Karmann den 150.000. Volkswagen Karmann Ghia 1200 auslieferte. Was zehn Jahre zuvor in einer Pariser Garage mit einem geheimen Coupé-Konzept deutsch-italienischer Herkunft begonnen hatte, wurde zur bis dahin größten Erfolgsgeschichte erschwinglicher Sportcoupés. Bis zum Ende der Produktion im Sommer 1974 wurden insgesamt fast 540.000 Karmann Ghia aller Varianten gefertigt – fast 20 Jahre lang wurde der elegante Zweitürer auf Käfer-Basis im italienischen Ghia-Design gebaut. 
Die komplette Technik des intern Typ 14 genannten Karmann Ghia stammte vom Käfer, und sämtliche Entwicklungsschritte des Wolfsburgers wurden getreulich mitvollzogen. Dazu zählten Hubraum- und Leistungssteigerungen ebenso wie Cabriolets und Spitzenmodelle in Anlehnung an die großen VW 1500 und 1600. Eng verknüpft mit VW waren die Karmann-Ghia-Ableger ohnehin: Vertrieben wurden sie über das Netz der VW-Partner, die bereits mit dem ebenfalls bei Karmann gebauten Käfer Cabriolet beliefert wurden. 
Die Erfolgsgeschichte beginnt 1951. Damals führt Karossier Wilhelm Karmann erste Gespräche mit VW-Generaldirektor Heinrich Nordhoff über einen Roadster oder ein aufregend geformtes Sportcoupé auf Basis des Käfer. Das Auto soll das Volkswagen-Programm nach oben abrunden, in Amerika Erfolge einfahren und so zugleich die Karmann-Werkshallen auslasten. Auftragsproduktionen sind nicht leicht zu erhalten, das weiß man auch im 1901 gegründeten Karosseriebau-Unternehmen in Osnabrück aus Erfahrung. 
Zwei Jahre später ist es am Rande des Pariser Autosalons soweit: Im Oktober 1953 präsentiert Luigi Segre von Ghia Carozzeria seinem Freund Karmann im Pariser Vorort Neuilly in der Garage des französischen VW-Importeurs Ladouche einen ersten Prototyp des künftigen Karmann Ghia. 
Vorbild Chrysler-Coupé? 
Nur wenige Wochen später kommt Nordhoff zu Karmann nach Osnabrück, begutachtet den Prototypen und wird mit Wilhelm Karmann einig über eine Serienfertigung. Karmann kann Nordhoff vor allem mit einer Kalkulation überzeugen, wonach aus dem Coupé ein erschwingliches Großserienauto werden kann statt des von Nordhoff erwarteten kostspieligen Designerstücks. 
Die Zeit bis zum Produktionsanlauf im Sommer 1955 wird genutzt für reichlich Feinschliff. Das Coupé wird zum überraschend geräumigen 2+2-Sitzer, entwickelt mit viel Platz für zwei Erwachsene und zwei Kinder, aber auch größeres Reisegepäck – unter Wahrung der bei Ghia entwickelten Grundform. Darum, wie diese entstanden ist, ranken sich bis heute Legenden. Sicher ist: Der Karmann Ghia hat viele Väter, neben dem Designer Felice Mario Boano und seinem Sohn Gian Paolo soll auch Nachwuchsstar Sergio Coggiola beteiligt gewesen sein. 
Alle ziehen offenkundig Inspiration aus einem von Ghia für Chrysler gebauten Coupé. Dieser Chrysler D'Elégance wiederum ist ein Entwurf des US-Designgurus Virgil Exner, der sich später bei Ghia über die Designverwandtschaft des VW Coupés beschwert haben soll. Sei's drum. Letztlich ist der Volkswagen Karmann Ghia ein einzigartiger Entwurf, denn nur er passt ja auf die Käfer-Plattform und das Heckmotorlayout. 
 
Weitsichtig kommentiert die Presse die Medienpräsentation des Coupés in einem Hotel bei Osnabrück als  ""historischen Taufakt "". Tatsächlich übertreffen die einlaufenden Bestellungen für den Italiener deutscher Produktion die kühnsten Erwartungen. Statt 3.000 Einheiten liefert Karmann im ersten Jahr bereits 10.000 Coupés aus, und dennoch mangelt es an Produktionskapazitäten für den weit höheren Auftragsbestand. 
Auf der Frankfurter IAA zählt der Besuch des Karmann-Messestands nun zum Pflichtprogramm von Bundespräsident Theodor Heuss und Wirtschaftsminister Ludwig Erhard. Die IAA ist auch das perfekte Podium für die Premieren der nächsten Mitglieder in der Karmann-Familie: 1957 macht das Cabriolet Wilhelm Karmanns Traum vom Open-Air-Star wahr, 1961 folgt der ebenfalls von Ghia Carozzeria gezeichnete größere Typ 34 als Karmann Ghia 1500. Schon ein Jahr zuvor ist die Produktion bei Karmann Ghia do Brasil angelaufen. 
1965 wird das beste Produktionsjahr für die Karmann Ghia: Neben den 6.873 Exemplaren des größeren Typs 34 laufen 28.387 Coupés und 5.326 Cabriolets vom Band. Dabei sind die Käfer im verführerisch schönen Gala-Gewand nicht unumstritten. Manche Journalisten und Spötter tun die Wagen mit dem anfangs gerade einmal 30 PS starken Boxer-Vierzylinder als  ""Parodie eines schnellen Wagens "" oder als  ""Hausfrauen-Porsche "" ab. 
Schnelle Sportwagen wurden sie nie 
In der Tat ist der kleine Karmann Ghia 1200 mit einer Höchstgeschwindigkeit von 118 km/h nur unwesentlich schneller als der gleich stark motorisierte Käfer und deutlich träger als etwa das Auto Union 1000 Coupé oder Borgward Hansa Coupé, die ähnlich teuer sind wie das anfangs 7.500 Mark teure Coupé aus Osnabrück. Dafür kann der Karmann Ghia mit der Zuverlässigkeit und Langlebigkeit des Käfers glänzen. Die Käufer der Coupés und Cabrios profitieren zudem vom großen Wolfsburger Servicepartner-Netz. 
Karmann Ghia fahren, das bedeutet anfangs: schöne Autos mit rustikaler Technik, die immerhin stark genug war, um im Verkehr mitzuschwimmen. Dazu gehören auch das dünne Käfer-Lenkrad, Trommelbremsen ohne Servounterstützung und der stets vernehmbar arbeitende Boxermotor mit Luftkühlung. Das alles ändert sich im Lauf der Jahrzehnte synchron zur Käfer-Entwicklung. Aber auch mit vorderen Scheibenbremsen, moderner Schräglenker-Hinterachse und 50 PS bis kurzzeitig sogar 54 PS Leistung werden aus den Karmann Ghia nie schnelle Sportwagen. 
Die Käufer stört das nicht weiter. Alle Karmann Ghia schreiben ihre eigene Erfolgsgeschichte – nur für den größeren und arg teuren Typ 34, produziert bis Juni 1969, ist der Erfolg nicht ganz so groß. Dafür bleibt der Urtyp gut im Geschäft, bis Anfang 1974 der VW Scirocco schon kurz vor dem Golf das Ende der Heckmotor-Ära bei den kompakten Volkswagen ankündigt. Am 1. Juli 1974 wird die Produktion des Karmann Ghia eingestellt. Das brasilianische Werk bei São Paulo baut noch zwei Jahre länger das eigenständige Fließheckmodell TC. Dann ist auch dort Schluss."	technik
"ZEIT ONLINE: Herr Nestmann, über wenige Unternehmen wird in Deutschland zurzeit so kontrovers diskutiert wie über Uber. Wie gehen Sie damit um? 
Fabien Nestmann: Wir haben in der Tat eine Diskussion ausgelöst – nicht jeder findet Uber toll. Aber was in den Schlagzeilen über uns oft untergeht: Wir sind vor allem auch deshalb bekannt, weil wir etwas anbieten, das bei den Menschen extrem gut ankommt. 
ZEIT ONLINE: Wir verstehen Uber einfach nicht richtig? 
Fabien Nestmann: Das kann ich nicht beurteilen. In Europa ist man gelegentlich skeptisch gegenüber neuen Konzepten, die nicht aus Europa kommen. 
ZEIT ONLINE: In der Öffentlichkeit herrscht vor allem ein Eindruck vor: Uber ignoriert Gesetze und schafft seine eigene Realität. 
Nestmann: Wir haben in der Kommunikation sicher nicht alles richtig gemacht – gerade zu Beginn. Wir hätten deutlicher machen müssen, warum wir was tun. Aber: Wir haben uns immer an die geltenden Gesetze gehalten. 
ZEIT ONLINE: Ernsthaft? In den USA haben Sie Ihren Dienst auch in Miami und Austin angeboten, obwohl die dortigen Gesetze das nicht erlaubten. In Virginia bekamen Sie eine Unterlassungserklärung, machten aber weiter. 
Nestmann: Zu den USA kann ich nichts sagen. Ich bin für Deutschland zuständig. Hier haben wir immer alle Gesetze befolgt. Nach den Gerichtsurteilen in Berlin und Hamburg, durch die bestimmte Auflagen bestätigt wurden, haben wir unser Modell umgehend geändert. 
Richtig ist aber auch: Das Personenbeförderungsgesetz in Deutschland ist reformbedürftig – zumindest aber modernisierungswürdig. Alternative Transportmöglichkeiten sollten nicht einfach blockiert werden. Es fehlt eine pragmatische, nach vorne gerichtete Lösung. 
ZEIT ONLINE: Was heißt das konkret? 
Nestmann: Im Personenbeförderungsgesetz gibt es beispielsweise die Pflicht für Mietwagen, nach jedem Auftrag zum Unternehmenssitz zurückzukehren. Solche Leerfahrten belasten die Umwelt und sind schlicht sinnlos. Auch eine Prüfung über die Ortskenntnis ist in Zeiten von Navigationsgeräten sicherlich nicht mehr nötig – vor allem, wenn man feststellt, dass es durchaus auch Taxifahrer gibt, die sich in einer Stadt nicht genau auskennen. Damit stelle ich die herrschende Gesetzeslage nicht infrage. Aber Gesetze müssen angepasst werden können, wenn sich Technologien weiterentwickelt haben. Sie müssen einen Rahmen für das heute Mögliche schaffen. 
Das Start-up Uber aus San Francisco vermittelt Taxifahrten von Privatpersonen per Smartphone-App. Uber verdient über eine Provision mit und wirbt mit günstigeren Preisen. An der Firma, die mit mehr als 15 Milliarden Dollar bewertet wird, sind Konzerne wie Google und Goldman Sachs beteiligt. 
Der Dienst breitet sich rasant aus und wird weltweit in mehr als 70 Städten angeboten. Allerdings kommt es immer wieder zu Verboten. Auch die deutschen Taxifahrer wehren sich vor Gericht gegen die Konkurrenz. Eine Kritikpunkt: Fahrer müssen keine Lizenz zur Personenbeförderung besitzen. Die deutschen Verkehrsminister sind ebenfalls der Ansicht, dass Uber gegen das Personenbeförderungsgesetz verstößt. 
Nach Unternehmensangaben haben sich seit dem Start in Deutschland mehr als 12.000 Fahrer angemeldet. Die Zahl der tatsächlichen Fahrer veröffentlicht Uber ebenso wenig wie Daten zu Nutzern oder Fahrten. 
ZEIT ONLINE: Das deutsche Gesetz sieht allerdings auch vor, dass jeder einen Taxischein braucht, der Menschen für gewerbliche Zwecke durch die Gegend fährt. Sie behaupten, Fahrten über Uber seien gar nicht gewerblich. Die Fahrer hätten formal keinen Anspruch auf Bezahlung, sondern nur auf eine  ""freiwillige Servicepauschale "". Wollen Sie die Deutschen für dumm verkaufen? 
Nestmann: Theoretisch könnte man aussteigen, ohne zu bezahlen. Das machen allerdings die wenigsten. Es gibt ja glücklicherweise den Konsens: Ich bezahle, wenn ich etwas bekomme. Richtig ist: Uberpop ist sicher keine gewerbliche Aktivität, der man hauptberuflich nachgeht. 
 
ZEIT ONLINE: Jemand fährt jemanden gegen Geld von A nach B. Was soll daran nicht gewerblich sein? 
Nestmann: In den vergangenen Jahren haben sich Grenzen verschoben. Es gibt nicht mehr nur Arbeit und Freizeit. Neue Technologien ermöglichen es uns, auch in unserer Freizeit und mit unseren Freizeitbeschäftigungen Geld zu verdienen. Ich erziele einen eigenen Nutzen, gebe anderen aber auch etwas. Wir stellen die notwendige Plattform bereit, damit beide Seiten zueinander finden – also der, der teilen will, und derjenige, der dieses Angebot nutzen möchte. 
ZEIT ONLINE: Wie prüfen Sie Ihre Fahrer? 
Nestmann: Theoretisch kann jeder zum Uber-Fahrer werden. In der Praxis wollen wir ein sauberes Führungszeugnis, den Personalausweis und den Führerschein sehen. Wir führen mit jedem Fahrer ein Gespräch und nehmen das Auto in Augenschein. So wollen wir sicherstellen, dass alles, was auf der Plattform geschieht, in unserem Sinne ist. 
ZEIT ONLINE: Wer trägt das Risiko, wenn eine Behörde gegen Uber-Fahrer ein Bußgeld verhängt, weil er gegen geltendes Recht verstößt? 
Nestmann: Uber ist eine freiwillige Plattform. Niemand wird gezwungen, zum Uber-Fahrer zu werden. Aber wir stehen hinter unseren Fahrern. 
ZEIT ONLINE: In den Verträgen der Uberpop-Fahrer steht: Sollten Geldstrafen von Behörden verhängt werden,  ""tragen Sie diese Kosten selbst  "". 
Nestmann: Verträge werden zu Anfang gemacht, um sich möglichst gut abzusichern. Uber ist immer noch ein Start-up und wir entwickeln uns laufend weiter. Und die Verträge in Deutschland müssen wir an einigen Stellen noch einmal anpassen. 
ZEIT ONLINE: Können Sie verstehen, wenn die Taxifahrer in Ihnen nur eine Dumping-Konkurrenz sehen, die bestehende Regeln und Gesetze missachtet? 
Nestmann: Noch mal: Wir halten uns an alle Gesetze. Was wir aber auch machen: Wir sorgen in einem Markt, der lange unangetastet und extrem geschützt war, für Alternativen. Dass sich Taxifahrer dadurch angegriffen fühlen, ist verständlich. Allerdings basiert das auf einem Missverständnis: Es gibt viele Möglichkeiten, sich fortzubewegen. Uber ist eine legitime Alternative. Wir haben überhaupt nicht das Ziel, Taxen zu verdrängen. 
ZEIT ONLINE: Aber Sie nehmen den Taxis doch Fahrten weg. In San Francisco ist die Zahl der Taxifahrten durch Dienste wie Uber in den vergangenen 15 Monaten um mehr als 65 Prozent eingebrochen. 
Nestmann: Natürlich wollen wir wachsen. Aber: San Francisco ist eine Stadt in einem Land und nicht mit Hamburg oder München zu vergleichen. In San Francisco sind Taxen einfach schlecht – in Hamburg, München und den meisten deutschen Städten sind sie sehr viel besser. In New York fahren weiter Zehntausende von yellow cabs – die mit Uber verhältnismäßig friedlich koexistieren. Ich habe noch keine Daten gesehen, die nahelegen, dass wir den Taxen dort etwas wegnehmen. Wir machen den Kuchen nur größer. 
Fabien Nestmann ist seit September 2013 Deutschland-Chef des Fahrdienstes Uber. Der Betriebswirt arbeitete zuvor bei Identiv, einem US-Anbieter für Sicherheitstechnologie. Nestmann distanziert sich vom aggressiven Auftreten der Uber-Führung in den USA und bemüht sich um einen zurückhaltenderen Auftritt in Deutschland."	technik
"Die Bundesregierung will den  ""Cyber-Raum "" besser schützen. Gemeint ist damit das Internet und alle  ""durch das Netz erreichbaren Informationsstrukturen "", wie es in der neuen Cyber-Sicherheitsstrategie für Deutschland heißt. Weil das eine Menge zu schützender Dinge sind, hat die Bundesregierung am Mittwoch gleich zwei Gremien geschaffen. 
Das erste ist ein Nationales Cyber-Abwehrzentrum (NCAZ), das am 1. April 2011 in Bonn seine Arbeit aufnehmen soll. Zehn Beamte werden dort vor allem die Sicherheitslage im Internet beobachten und Empfehlungen aussprechen; sechs vom Bundesamt für Sicherheit in der Informationstechnik (BSI), zwei vom Verfassungsschutz und zwei vom Bundesamt für Bevölkerungs- und Katastrophenschutz (BBK). 
Die Aufgabe des NCAZ sei in erster Linie,  ""Informationen zusammenzutragen "", sagte Innenminister Thomas de Maizière bei der Vorstellung. Auch Bundeskriminalamt, Bundespolizei, Zollkriminalamt, Bundesnachrichtendienst und Bundeswehr sollen daran mitwirken. Vorbild ist das 2004 aufgebaute Terrorabwehrzentrum (GTAZ) . 
Das zweite beschlossene Gremium ist ein Nationaler Cyber-Sicherheitsrat. Er wird zum Bundeskanzleramt gehören und aus je einem Staatsekretär der verschiedenen Ministerien bestehen, also Vertreter vom Auswärtigen Amt, von Innen-, Verteidigungs-, Justiz-, Wirtschafts- und Finanzministerium und auch der Bundesländer versammeln. Geleitet wird dieser Rat von der IT-Beauftragten der Bundesregierung, Cornelia Rogall-Grothe . Anlassbezogen können andere Dienststellen hinzugezogen werden. 
Die Sicherheitsstrategie, das Abwehrzentrum und der Sicherheitsrat sind offensichtlich der Versuch, das Thema Sicherheit im Internet ernster zu nehmen als bisher und alle Bemühungen auf Bundesebene zu koordinieren. 
Die zur Vorstellung präsentierten Experten gaben sich alle Mühe, nicht den geringsten Zweifel daran zu lassen, dass das notwendig ist. Viele Zahlen wurden genannt und diverse Bedrohungsszenarien entworfen. Pro Sekunde würden irgendwo auf der Welt zwei neue Schadprogramme entwickelt, pro Minute in Deutschland zwei Identitäten gestohlen und pro Monat 30.000 Angriffe auf Netzwerke der Bundesregierung gestartet, sagte Hartmut Isselhorst, Abteilungsleiter beim BSI. 
Immer wieder war während der zweistündigen Veranstaltung die Rede davon, wie leicht es sei, im Internet Netzwerke anzugreifen, wie häufig es vorkomme und wie viel Geld sich damit verdienen lasse. 
 
Was allerdings zu der Frage führt, warum eine solche Struktur zur Abwehr – wenn sie so notwendig ist – nicht längst geschaffen worden ist. Immerhin klingt das Vokabular martialisch. Innenminister de Maizière sagte, Deutschland sei bisher bei dem Thema  ""gut aufgestellt "", auch im internationalen Vergleich, beispielsweise dank der Arbeit des BSI. Es mache nun nur  ""den nächsten Schritt "". 
Das führt zur Frage, warum die Notwendigkeit eines solchen Zentrums so stark betont wurde.  ""Durch ihr hektisches Vorgehen will die Bundesregierung offenbar die Versäumnisse der letzten Jahre beim Schutz kritischer Infrastrukturen kaschieren "", kritisierte zum Beispiel Konstantin von Notz, der innenpolitische Sprecher der Grünenfraktion im Bundestag. 
Auch beim Regierungspartner FDP ist man befremdet. Niemand bezweifle ja, dass es diese Bedrohungen gebe, heißt es dort. Doch gebe es diese schon länger und jeder, der es habe wissen wollen, hätte sich darüber informieren können. Wie jetzt darauf reagiert werde, wirke seltsam überstürzt. 
Die Cyber-Strategie ist im Übrigen größtenteils geheim. Nur ein kleiner Teil ist öffentlich, der weitaus größere gilt als  ""VS – Nur für den Dienstgebrauch "". Wobei dem Vernehmen nach darin nur steht, wie überrascht und aufgescheucht die Bundesregierung von dem Trojaner Stuxnet war. Was die These stützen würde, dass die Cyber-Strategie eher übereilt und plötzlich beschlossen wurde. 
Bei der Opposition und auch beim Regierungspartner FDP gibt es Befürchtungen, das in der Verfassung geforderte Trennungsgebot könnte verletzt werden. Das fordert, die Arbeit von Polizei und Geheimdiensten streng auseinander zu halten – dass also derjenige, der alles weiß, nicht alles darf und umgekehrt. 
FDP-Justizministerin Sabine Leutheusser-Schnarrenberger hat daher auf einer strikten Trennung der beteiligten Dienste im Cyber-Abwehrzentrum bestanden. Diese Trennung wurde auch in den Kabinettsbeschluss übernommen, doch ist nicht ganz klar, wie sie praktisch umgesetzt werden wird. Immerhin geht es darum, in dem Gremium Informationen auszutauschen, wobei Strafermittler auch Geheimdienstwissen erlangen könnten. 
Diese Frage hatte sich auch beim Terrorabwehrzentrum gestellt. Deswegen war im Koalitionsvertrag vereinbart worden, die Arbeit des GTAZ zu analysieren und zu prüfen, inwieweit dort das Trennungsgebot verletzt werde. Bislang hat es allerdings keine Prüfung gegeben. 
Die FDP-Abgeordnete Gisela Piltz hatte daher in Bezug auf das NCAZ gefragt , warum ein neues fachübergreifendes Zentrum aufgebaut werde, wo doch nicht einmal das bestehende GTAZ evaluiert worden sei. Eine Antwort darauf gibt es nicht. Allerdings hat das Innenministerium nun zugesagt, demnächst die Evaluierung des GTAZ durchzuführen. Wann genau, ist nicht bekannt."	technik
"Vor drei Jahren gründete der damalige Bundesinnenminister Hans-Peter Friedrich mit einigem Brimborium in Bonn ein Cyber-Abwehrzentrum, um die Nation vor den Gefahren des Internets zu schützen. Ein vertraulicher Bericht des Bundesrechnungshofes legt nun nahe, dass dort kaum mehr existiert als der schöne Name. Von Schutz und Abwehr könne kaum eine Rede sein, schreibt die Süddeutsche Zeitung, der der Bericht vorliegt. 
Die jetzige Konzeption des Abwehrzentrums sei  ""nicht geeignet, die über die Behördenlandschaft verteilten Zuständigkeiten und Fähigkeiten bei der Abwehr von Angriffen aus dem Cyberraum zu bündeln "", heißt es demnach in dem Papier. 
Das Zentrum soll eigentlich ein Ort sein, an dem diverse Behörden sich treffen, ihr Wissen über Netzangriffe austauschen und Gegenmittel beraten und koordinieren. Das Problem beginne damit, schreibt die Süddeutsche Zeitung, dass nicht einmal die wichtigsten drei Behörden – das Bundesamt für Sicherheit in der Informationstechnik (BSI), das Bundesamt für Verfassungsschutz (BfV) und das Bundesamt für Bevölkerungsschutz und Katastrophenhilfe (BBK) – regelmäßig an den Lagebesprechungen teilnähmen. Das BSI außerdem, dem die Leitung des Ganzen obliegt, hat bereits viel zu tun und gilt als mittlerweile überlastet mit seinen Aufgaben. 
Von anderen Diensten, die eigentlich mit dort sitzen sollten, fehle gleich jede Spur. Das Zollkriminalamt beispielsweise habe nur ein einziges Mal teilgenommen,  ""die vorgesehenen Einrichtungen der Bundeswehr "" seien bis auf den Militärischen Abschirmdienst MAD noch nie gekommen. Es sei darum  ""fraglich "", welchen Nutzen die Einrichtung überhaupt entwickeln könne, wenn sie selbst als Informationsplattform  ""nur geringe Akzeptanz "" finde, zitiert die Zeitung aus dem Bericht. 
Doch selbst wenn sie alle kämen, hätte der Rechnungshof demnach Zweifel an der Wirksamkeit des Abwehrzentrums. Der einzig vorgegebene Arbeitsablauf sei die tägliche Lagebesprechung im Zentrum. Handlungsempfehlungen, die das Zentrum vor allem geben solle, existierten nur im Jahresbericht. 
Der Rechnungshof empfiehlt, das Zentrum  ""mit eigenen Aufgaben und Kompetenzen für die Abwehr von Cyberangriffen "" auszustatten. Außerdem solle das Nebeneinander geordnet werden – immerhin gibt es inzwischen diverse Behörden, Dienststellen und Abteilungen hierzulande, die sich um das Internet kümmern."	technik
"Internetanbieter können nicht gezwungen werden, den Datenverkehr ihrer Netze inhaltlich zu filtern, um das illegale Herunterladen etwa von Musikdateien zu verhindern. Dies entschied der Europäische Gerichtshof (EuGH) im Fall eines belgischen Internetproviders. Eine solche Überwachung verstoße gegen die EU-Richtlinie über den elektronischen Geschäftsverkehr und sei zudem mit der Grundrechtecharta der EU unvereinbar. 
Internetanbieter dürfen dem Urteil zufolge deshalb keine allgemeinen Überwachungspflichten auferlegt werden. Zudem müsse das Recht auf freien Datenaustausch gewahrt bleiben, entschied der EuGH. (Az: C 70/10) 
Eine solch aktive Prüfung sämtlicher Daten komme einer  ""allgemeinen Überwachung "" gleich, die mit der EU-Richtlinie über den elektronischen Geschäftsverkehr unvereinbar sei. Eine solche Pflicht würde auch zu einer Beeinträchtigung der unternehmerischen Freiheit des Providers führen, da er ein kostspieliges und allein auf seine Kosten betriebenes Überwachungssystem einrichten müsste. Zudem verstoße die Kontrolle gegen Grundrechte der EU: den Schutz von persönlichen Daten und die Weitergabe persönlicher Informationen. 
Im vorliegenden Fall hatte die belgische Gesellschaft zur Verwertung von Musikrechten, das Gema-Pendant SABAM, geklagt, weil Internetnutzer Musik über den Provider Scarlet heruntergeladen hatten, ohne dafür zu bezahlen. Sie nutzten ein sogenanntes Peer-to-Peer-Programm. Damit können Nutzer direkt Verbindung zu Rechnern anderer Nutzer aufnehmen, um mit diesen Daten auszutauschen. Der Provider war deshalb von einem belgischen Gericht verpflichtet worden, elektronische Sperrfilter einzubauen. Scarlet legte Berufung ein. 
Solche Filtermechanismen werden auch in Deutschland diskutiert. Möglich sind sie problemlos. Das eingesetzte Verfahren nennt sich Deep Packet Inspection (dpi) und wird beispielsweise genutzt, um Spam zu erkennen und zu entfernen. 
Zugangsanbieter wollen es auch nutzen, um verschiedene Inhalte voneinander unterscheiden und anschließend verschieden behandeln zu können. Sie argumentieren, dass beispielsweise Videoseiten für so viel Datenverkehr sorgten, dass die Leitungen verstopft würden. Mittels dpi könnten sie, so die Behauptung, wichtige Inhalte bevorzugt weiterleiten und andere bremsen. 
Kritiker sehen darin jedoch eine Verletzung verschiedener Grundrechte. Könne dann doch jeder Netzbetreiber entscheiden, wer sich wie informieren kann. Der freie Austausch von Daten sei damit gefährdet, die sogenannte Netzneutralität nicht mehr gewahrt. Das EuGH-Urteil bestätigt diese Argumentation."	technik
"Die Website des französischen Senders TV5Monde ist wieder gestört. Nach dem Hacker-Angriff von Dschihadisten auf den Sender am Mittwochabend war die Seite am Morgen erneut nicht zu erreichen. Am Donnerstagnachmittag war die Website zunächst wieder zugänglich gewesen. Die Gründe für die erneuten Zugangsprobleme sind noch unklar. 
Durch die Cyberattacke war der Fernsehsender stundenlang lahmgelegt worden, auf der Website sowie den Facebook- und Twitter-Seiten des Senders wurden Drohungen der Terrormiliz  ""Islamischer Staat "" (IS) gegen Frankreich gezeigt. 
Der Hacker-Angriff begann am Mittwochabend um 22 Uhr. Anstelle des üblichen TV5Monde-Programms waren nur noch schwarze Fernsehbildschirme zu sehen. Erst am frühen Donnerstagmorgen konnte der Sender, der in mehr als 200 Staaten und Regionen ausgestrahlt wird und wöchentlich 35 Millionen Zuschauer hat, wieder eigene Bilder senden. Ab dem späten Nachmittag lief der Sendebetrieb wieder normal. 
Die Hacker kaperten auch die Internetauftritte des Senders, der von öffentlich-rechtlichen Sendern in Frankreich, Belgien, Kanada und der Schweiz betrieben wird. Auf der Facebook-Seite von TV5Monde wurden Dokumente veröffentlicht, bei denen es sich um Ausweise und Lebensläufe von Familienmitgliedern von französischen Militärangehörigen handeln soll, die an Einsätzen gegen den IS beteiligt sind. 
Laut dem französischen Innenminister Bernard Cazeneuve deutet vieles auf einen  ""Terrorakt "" hin. Senderchef Yves Bigot sprach von einer  ""in der Fernsehgeschichte beispiellosen "" Attacke. Der Cyberangriff sei  ""extrem gezielt und mächtig "" gewesen. Demnach attackierten die Hacker den Sender über die sozialen Netzwerke. 
Die EU-Kommission will nun für mehr Cyber-Sicherheit sorgen. Dazu müsse es eine enge Zusammenarbeit mit allen Mitgliedsländern und anderen Verbündeten geben, schrieb der Vizepräsident der EU-Kommission, Frans Timmermans, auf seiner Facebook-Seite. Aus EU-Kreisen verlautete, dass die Kommission Ende des Monats eine Sicherheitsagenda verabschieden will, die sich hauptsächlich mit der Cyber-Sicherheit befasse."	technik
"Eine italienische Ikone feiert in diesen Wochen ihr Comeback – auf der Plattform des Mazda MX-5. Der neue Fiat 124 Spider wird sogar gemeinsam mit dem Japan-Roadster im Mazda-Werk in Hiroshima gebaut. Ob er die Erfolgsgeschichte seiner Vorgänger fortschreiben kann, muss der neue Nippon-Fiat erst noch zeigen. Der vor 50 Jahren vorgestellte, offene Sportwagen galt damals als Fiats Antwort auf den zur gleichen Zeit präsentierten Alfa Spider: zwei 2+2-Sitzer, die beide von Pininfarina gezeichnet worden waren und sich doch deutlich differenzierten. 
Im Gegensatz zum potenten Alfa musste sich der Fiat Spider anfangs mit einem 66 kW (90 PS) starken Vierzylinder begnügen. Leistungszuwächse und siegreiche Rallyeversionen gab es erst für die 1970er Jahre. Was den Turiner nicht daran hinderte, die Amerikaner ins Herz zu treffen, die letztlich drei Viertel der Produktion kauften. Als Fiat 1975 seinen Spider in Europa still und leise zu Grabe trug, freute sich Alfa bereits über eine Alleinstellung. Zu früh – denn Pininfarina ließ seinen größten kommerziellen Erfolg nicht sterben, schenkte ihm stattdessen ein zweites Leben unter eigenem Label. Erst 1985 rollte der letzte Pininfarina Spidereuropa vom Band, als mit 200.000 Einheiten meistgebauter Italo-Spider. 
Schon die Premierenfeier des sportlichen Bestsellers war spektakulär, denn sie war Bestandteil eines Jahrhundertfestes. 1966 wurde schließlich die 100. Wiederkehr des Geburtstages von Giovanni Agnelli zelebriert, des Unternehmers also, der die Fabbrica Italiana Automobili Torino (Fiat) gegründet hatte. 
Passend zu diesem Jubiläum übernahm der Enkel, Advokat Giovanni Agnelli, die Macht im Haus und richtete Fiat strategisch neu aus. Die Modelle wurden nun internationaler vermarktet und nicht mehr nach der Hubraumgröße benannt. Erstes Modell der neuen Nomenklatur war der Fiat 124, der auch als sowjetisches Volksauto Lada gebaut wurde und in insgesamt drei Kontinenten vom Band rollte. Nur die exklusivste Version des Fiat 124, der Spider, kam ausschließlich aus italienischen Werken. Verkauft wurde er dennoch weltweit, mit dem Hauptmarkt in Nordamerika. 
Enthüllt wurde das zunächst Sport Spider genannte Cabrio auf der damals wichtigsten Designmesse, dem Turiner Salon. Es basierte auf einer verkürzten Bodengruppe der Limousine und hatte eine Karosserie, die anfangs ausgewogener gezeichnet war als der gleichfalls von Pininfarina karossierte Alfa Spider mit Rundheck. So jedenfalls die Bewertungen zeitgenössischer Kritiker, die den Alfa erst in der 1969 eingeführten Fastbackversion uneingeschränkt bejubelten. Der Fiat Spider verriet sein Baujahr dagegen stets nur durch Details. Etwa die Mitte der siebziger Jahre eingeführten Stoßstangen für US-Sicherheitsnormen. Andererseits kündete noch in den achtziger Jahren ein klassisches Armaturenbrett mit Holzfurnier vom Zeitgeist der Swinging Sixties. 
Damals beerbte der Fiat Spider nicht nur seinen Cabrio-Vorgänger vom Typ 1600, sondern konkurrierte gleich mit einem Dutzend offener Europäer. Allerdings fehlte es diesen fast immer am damals boomenden Dolce-Vita-Feeling, an einer Italianità, die besonders in Amerika gefragt war. Auf diesem weltweit größten Sportwagenmarkt gelangen dem 124 Spider deshalb von Beginn an seine größten Erfolge. 
Die Karosserie des 124 Spider wurde bei Pininfarina gefertigt und erst anschließend zur Endmontage in die Fiat-Fabrik geliefert. Trotz dieses Aufwandes und des anspruchsvoll konstruierten Motors mit Leichtmetallzylinderkopf und Zahnriemenantrieb für die zwei Nockenwellen war der Fiat von Beginn an nicht übermäßig teuer. Mit 10.980 Mark kostete der offene 124 deutlich weniger als ein Alfa Spider oder der Triumph TR 4 – ein weiterer Grund für den Erfolg des Fiat. 
 
Verblüffend schnell war er obendrein. Auf ersten Testfahrten bescheinigte die Presse dem Fiat Spider eine Höchstgeschwindigkeit von fast 200 km/h, jedoch war die Werksangabe von 174 km/h im Alltag realistischer. Deshalb rüstete Fiat mit dem ersten Facelift nach und lancierte 1970 einen größeren Vierzylinder mit 81 kW (110 PS), dem wiederum drei Jahre später eine Version mit 87 kW (118 PS) folgte. Noch kräftiger war ab Ende 1972 eine Sonderserie für Fia-Rallyefahrzeuge. Der Fiat 124 Abarth Rally mit verbreiterten Kotflügeln hatte in der Straßenversion 94 kW (128 PS) unter der mattschwarzen Haube, die Wettbewerbsversion bringt es auf bis zu 220 PS. Heute zählt der 124 Abarth Rally zu den von Sammlern am höchsten dotierten Spielarten des Spider. 
Im Jahr 1975 erging es den Straßenversionen des Abarth aber erst einmal wie allen anderen Fiat Spider: Mit dem neuen Mittelklassemodell 131 Mirafiori beendete der Turiner Hersteller die Ära 124. Allerdings nur in Europa, denn die Amerikaner bezogen die Spider nach wie vor. Einzelne US-Fiat fanden zudem weiterhin den Weg nach Deutschland, belieferte doch bis 1980 eine Vertriebsfirma hier lebende US-Bürger mit der Sportwagenspezialität. 
1982 begann das zweite Leben des Spider: Fiat ließ es damals zu, dass Pininfarina die Produktion des nun Pininfarina Spidereuropa genannten Sportwagens in Eigenregie fortführte, mit einem zwei Liter großen Vierzylinder, der 99 kW (135 PS) leistete. Das war genug Biss, um den einzig verbliebenen Alfa-Rivalen bis 1985 in der Leistung ebenso zu übertrumpfen wie in den verkauften Stückzahlen. Dann war der Fiat beziehungsweise Pininfarina endgültig reif fürs Werksmuseum, nach 198.020 Exemplaren, von denen 170.000 in Nordamerika verkauft wurden. 
Bis zum Spätherbst 2015: Im November zeigte Fiat auf der Los Angeles Auto Show einen neuen 124 Spider, der in einem Joint-Venture mit Mazda gebaut wird. Fiat konnte kaum etwas Besseres passieren, als den 124 Spider auf Basis des MX-5 und dessen klassischer Roadsterarchitektur zu revitalisieren."	technik
"Der Tesla-Autopilot dient zum assistierten Fahren, erfordert aber ständige Reaktionsbereitschaft vom Fahrer. Denn wie ein Fahrer in der Schweiz feststellen musste, kann zu großes Vertrauen zur Gefahr werden, auch wenn das Auto scheinbar alle Alltagssituationen meistern kann. 
Der Fahrer eines Model S von Tesla Motors veröffentlichte selbst ein Video auf Youtube, in dem sein Fahrzeug auf einen Lieferwagen auffährt, der auf der linken Spur der Autobahn wegen zähem Verkehr anhalten muss. Kurz zuvor zog ein Wagen, dem der Tesla mit dem adaptiven Spurhaltesystem folgte, auf die rechte Spur, um weiterzukommen. Der Tesla folgte nicht, weil sich auf der rechten Spur ein weiterer Pkw dazwischen schob, und blieb links. Dort stand der Lieferwagen mit eingeschalteter Warnblinkanlage. Das Model S bremste nicht, sondern fuhr dem Hindernis ins Heck, was nach Angaben des Fahrers zu einem beträchtlichen Schaden führte. Laut einem Artikel auf Fortune hat der Fahrer nach dem Unfall Tesla kontaktiert, doch das Unternehmen fand keine Fehler in dem Fahrzeug. 
Der Fahrer behauptet, dass er Teslas Adaptive Cruise Control genutzt habe, das System hätte aber nicht wie sonst gebremst. Er berichtet, das Fahrzeug habe sogar noch leicht beschleunigt, bevor er selbst auf die Bremse getreten habe. Mit letzterer Aktion schaltet sich die Autopilotfunktion ab. 
Sie ist übrigens keineswegs mit einer autonomen Fahrfunktion zu verwechseln. Der Wagen hält damit die Spur, bremst entsprechend zum Verkehr und kann beschleunigen, um den Abstand zum Vordermann zu halten. Dabei werden Verkehrsschilder erkannt und Geschwindigkeitseinschränkungen berücksichtigt. Wenn der Fahrer die Spur wechseln will, muss er den Blinker manuell betätigen und das Auto sucht sich eine Lücke, um selbstständig auszuscheren. Doch diese Technik ist nicht perfekt, wie schon einige Fahrer erleben mussten. 
Die Autopilotfunktion wurde im Tesla Model S im Oktober 2015 durch das Softwareupdate 7.0 kostenpflichtig aktiviert. Die Aktivierung des Autopiloten soll 3.300 Euro kosten, als Extra beim Neukauf 2.700 Euro. Tesla bietet auch einen kostenlosen Probemonat für die Funktion an. 
Die notwendigen Sensoren für die Assistenzsysteme hatten die Autos bereits eingebaut bekommen. Aus rechtlichen Gründen müssen die Fahrer jedoch jederzeit in der Lage sein, das Lenkrad zu übernehmen. Tesla musste den Autopiloten nach der Einführung auf eine geringere Geschwindigkeit drosseln, weil einige Fahrer das System überreizten. 
Der Fahrer des Unfallwagens gab zu, dass er früher hätte reagieren können, er habe jedoch auf das System vertraut, nachdem das Fahrzeug tausende Male selbst gebremst habe. Teslas Bedienungsanleitung warnt ausdrücklich vor einem solchen Szenario:  ""Traffic-Aware Cruise Control bremst oder verringert die Geschwindigkeit nicht immer bei stehenden Fahrzeugen, besonders wenn man über 80 km/h fährt und ein Fahrzeug, dem man folgt, aus Ihrem Fahrweg herausfährt und ein stehendes Fahrzeug oder Objekt vor Ihnen steht. "" Tesla-Fahrer sollten stets vorbereitet sein, dann korrigierend einzugreifen. 
Derzeit gibt es etwa 70.000 Teslas, die mit der für Landstraßen und Autobahnen geeigneten Selbstfahrfunktion ausgerüstet sind. Sie sollen schon 160 Millionen Kilometer per Autopilot gefahren sein, teilte der Leiter des Programms, Sterling Anderson, auf der Konferenz Emtech Digital in San Francisco jüngst mit."	technik
"Bing!, erklingt der freundliche Signalton aus den Lautsprechern des Tesla Model S, dann ist der Autopilot eingeschaltet. Und das heißt: Jetzt beginnt die Zukunft. Der Wagen hält nun automatisch den Abstand zum Vordermann, gleichzeitig lenkt er sicher zwischen den Fahrspuren der Autobahn. Das wirkt keineswegs wie von Geisterhand, sondern erstaunlich natürlich. Das Gefühl, von einer Maschine fremdgesteuert zu werden, bleibt aus. Stattdessen steigt der Komfort. 
Mit der Autopilotfunktion hat Tesla einen Vorsprung gegenüber der Konkurrenz. Keinen großen, aber einen spürbaren. Und das US-Unternehmen geht damit auch ein Risiko ein, das die Wettbewerber noch scheuen. 
Eins vorweg: Der Begriff Autopilot ist irreführend. Er suggeriert, dass hier bereits das autonome Fahren im Sinn eines Roboterautos funktioniert. Die Bezeichnung gehört zur bei Tesla üblichen und perfekten Selbstinszenierung. Tatsächlich handelt es sich um ein sogenanntes teilautomatisiertes System. Das heißt: Es übernimmt die Quer- und Längsführung des Autos, also im Wesentlichen das Lenken und Bremsen. Dabei muss der Fahrer das System durchgehend überwachen. Macht das Auto einen Fehler, ist der Mensch in der Pflicht. 
Doch worin liegt nun Teslas Vorsprung? Zunächst einmal unterscheidet sich die Hardware für den Autopiloten nicht oder nur marginal von dem, was selbst für einen VW Polo erhältlich ist: Ein Radar überwacht den Abstand, eine Frontkamera erkennt Fahrspuren und andere Verkehrsteilnehmer, und je sechs Ultraschallsensoren vorne und hinten helfen beim Einparken und messen während der Fahrt Distanzen im Zentimeterbereich. Das war es. 
Diese Technik ist in allen aktuellen Tesla Model S serienmäßig eingebaut. Wer sie nutzen will, muss allerdings einen Aufpreis zahlen. Er liegt beim Kauf des Wagens bei 2.800 Euro. Wenn ein Käufer sich erst nach der Auslieferung für die Fahrautomatisierung erwärmt, kann er sie nachträglich freischalten lassen – dann für 3.300 Euro. Das erscheint angesichts der niedrigen Kosten der Bauteile frech. Doch wenn Volkswagen in einem Golf 3.025 Euro für das Fahrassistenzpaket verlangt, zeigt das, wie einig sich die weltweiten Autohersteller bei der Preisgestaltung sind. 
Zudem liegt der Wert des Tesla-Systems nicht in der Hard-, sondern in der Software. Die ist ohnehin per Definition eine Stärke des US-Unternehmens: Etliche Mitarbeiter rekrutieren sich aus dem Silicon Valley. Die Software wird regelmäßig per Wlan aktualisiert. Es ist unverständlich, dass andere Hersteller das nicht oder selten tun. 
Bei nüchterner Betrachtung grenzt sich Teslas Autopilot in zwei Punkten vom Wettbewerb ab. Die aber bewirken einen deutlichen qualitativen Unterschied zur Konkurrenz. 
Der erste Punkt hat mit dem Festhalten des Lenkrads zu tun. Dem Fahrassistenten etwa in einem Mercedes-Benz der C-Klasse könnte man wohl genauso bei der Arbeit zusehen wie einem Tesla Model S – wenn die sogenannte Hands Off Detection im Mercedes nicht nach rund zehn Sekunden dazu mahnen würde, wieder das Steuer zu ergreifen. Bei Tesla lässt man es laufen. Das klappt auf der Autobahn dermaßen überzeugend, dass man sich unwillkürlich fragt, warum es nicht bei allen so geht. 
Dankenswerterweise wurde das Wetter während unseres Tests immer schlechter. Erst fing es ein wenig an zu regnen, dann immer stärker. Irgendwann war es für das menschliche Auge schwer, den Unterschied zwischen betongrauem Asphalt und Fahrbahnmarkierung zu erkennen. Die Kamera des Autopiloten von Tesla indes arbeitet exzellent, sie orientiert sich an den weißen Streifen und anderen Verkehrsteilnehmern. Und der Fahrer kann im Zentraldisplay zusehen, wie es Autos und Lkws identifiziert. Schön. 
 
Später auf der A7, bei extrem eingeschränkter Sicht, war der Autopilot doch überfordert. Ein Warnton macht darauf aufmerksam, wenn die Software an Grenzen gerät: Piep, piep, bitte übernehmen Sie das Lenkrad. Wer auf die sich verstärkenden akustischen Mahnungen nicht reagiert, wird automatisch entschleunigt: Das Model S wird langsamer, und die Warnblinkanlage schaltet sich ein. 
Kurzum: Der Autopilot funktioniert auf der Autobahn unter den meisten Bedingungen sehr gut und geradezu faszinierend. Und wenn er überfordert ist, übergibt er zurück an den Menschen, der hoffentlich aufmerksam ist. 
Denn klar ist: Zeitungslesen, Rasieren oder Schminken sind ein Spiel mit dem Feuer, das jeder unterlassen sollte. Tesla geht das kalkulierte Risiko ein: Man gewöhnt sich schnell an das System, und das Vertrauen wächst in kurzer Zeit, obwohl die Rechtslage eindeutig ist – das Auto assistiert lediglich, die Verantwortung trägt der Fahrer. Daran lässt das Unternehmen übrigens keinen Zweifel: Tesla spricht von  ""Komfortmerkmalen "", weil es weiß, dass man eben noch kein System hat, bei dem sich der Besitzer vorübergehend komplett abmelden kann. Das geschieht erst mit den nächsten Entwicklungsstufen, dem hoch- und vollautomatisierten Fahren. 
Tesla empfiehlt den Einsatz der Teilautomatik noch nicht für die Stadt. Ausprobieren kann man es trotzdem. Und es ist verblüffend, wie reibungsarm auch das schon funktioniert. Das Model S ist ein sehr langes (4,98 Meter) und breites (1,96 Meter) Auto, das den Fahrer in einer Metropole wie Hamburg zur dauernden Aufmerksamkeit zwingt. Im Autopilot-Modus zirkelt das Elektroauto – wenn man es lässt – dennoch präzise durch den Verkehr. 
Meistens jedenfalls. Verwirrend wird es, wenn viele Fahrspuren sich überkreuzen, zum Beispiel bei der Einfahrt in einen sechsspurigen Kreisel. Dann kann das passieren, was in vielen YouTube-Videos zu sehen ist: Das Auto nimmt den falschen Weg, und der Fahrer muss sofort korrigieren. 
Hier aber beginnt die zweite Stärke und qualitative Abgrenzung des Model S. Denn Teslas Autopilot lernt. Alle Model S sind online und geben ihre Positionen anonymisiert in eine Datencloud. Wenn nun alle Tesla-Fahrer vor der immer gleichen Kurve von 100 auf 70 km/h bremsen, wird der Autopilot das verinnerlichen. 
Diese Schwarmintelligenz mit gleichzeitiger Aktualisierung der Karten wird ein wesentlicher Bestandteil jedes kommenden Automatisierungsgrads sein – bei Tesla ist das schon heute Realität. 
Daraus zu schließen, dass die Konkurrenz abgehängt wäre, ist allerdings falsch. Wenn von 2018 an der automatische Notruf eCall gesetzlich verpflichtend in jedes neue Auto eingebaut werden soll, erfordert das die Ausrüstung mit einem GPS-Sensor und einem GSM-Modul. Alle neuen Pkws bis zum kleinsten Importkleinwagen hätten so mit der anfangs beschriebenen Hardware die technischen Voraussetzungen für das, was der luxuriöse und teure Tesla schon heute kann. 
Immerhin, an einer Stelle hat eine deutsche Software einen Vorteil: Der Autopilot von Tesla überholt auf der Autobahn auch rechts. Vielleicht ist das System zu sehr auf US-Highways getrimmt, wo das üblich und außerdem erlaubt ist. Ein zuletzt von uns gefahrener VW Passat GTE ging dagegen automatisch vom Gas und unterließ das Rechtsüberholen. Tesla, ein Update für die Germans, bitte!"	technik
"Schon vor einigen Jahren hat die Nationale Plattform Elektromobilität (NPE) – ein von der Bundesregierung 2010 initiiertes Gremium mit Vertretern aus Industrie, Wissenschaft und Politik – das Ziel ausgegeben, Deutschland müsse Leitanbieter bei Elektrofahrzeugen werden. Parallel dazu ist das selbstständig fahrende Auto längst ein wichtiges Zukunftsthema geworden, das neue Player wie Google und Apple, die bisher mit Automobilbau nichts zu tun hatten, angelockt hat. 
Wie sind die deutschen Autohersteller Volkswagen, Daimler, BMW und Opel für die Zukunft aufgestellt? Welche Antriebe jenseits des Verbrennungsmotors bieten sie schon jetzt? Wie sieht die Forschung der Technik aus, die es künftig Insassen erlauben wird, die Hände vom Lenkrad zu nehmen? 
BMW: Mit der Kraft des Karbons 
Wie kaum ein anderer deutscher Konzernchef der Branche hat der scheidende BMW-Vorstandsvorsitzende Nobert Reithofer auf die Elektromobilität gesetzt – und ist dabei auch Risiken eingegangen. BMW hat nicht wie andere Hersteller ein existierendes Modell genommen, den Verbrennungsmotor rausgeworfen und einen elektrischen Antriebsstrang eingebaut. Stattdessen entwickelte BMW ein völlig neues Auto mit einer Karosserie aus karbonfaserverstärktem Kunststoff. Schon diese Hülle war etwas Besonderes: Sie macht den BMW i3 leicht – gerade bei einem reinen batterieelektrischen Fahrzeug zählt für die begrenzte Reichweite des Akkus jedes Gramm. 
Allerdings hat der Extremleichtbau seinen Preis. Die Herstellung der Karbonfasern ist kostenintensiv, was den Preis des Fahrzeugs in die Höhe treibt. In Deutschland verlangt BMW mindestens 34.950 Euro. Doch mit dem i3 zählt BMW zu den innovativsten Autoherstellern weltweit, mit einer eigenen Submarke BMW i. 
Daneben setzen die Münchner wie andere Anbieter auch auf die Plug-in-Hybridtechnik, also die Kombination aus Verbrennungs- und Elektromotor plus der Möglichkeit, den Akku an der Steckdose wieder aufzuladen. Die deutsche Autoindustrie braucht die Technik, da sie ohne die teilelektrischen Fahrzeuge nur schwerlich die künftigen CO2-Grenzwerte der EU einhalten kann. 
Erst steckte BMW den Plug-in-Hybridantrieb in einen etwas exotischen Sportwagen namens i8 – ebenfalls mit Karbonkarosserie – und in Modelle aus dem hochpreisigen Segment. Doch künftig gibt es von den Münchnern diese Technik auch in tiefer angesiedelten Fahrzeugklassen, etwa im 3er BMW. Medienberichten zufolge plant BMW für 2018 auch mit einem weiteren i-Modell, dem i5 als Plug-in-Hybrid. 
Beim Wasserstoffantrieb setzte BMW lange auf die Entwicklung von Motoren, in denen das flüchtige Gas verbrannt wird. 2009 stellte der Konzern die Forschung ein – allerdings ohne nun intensiv in die Forschung zur Brennstoffzelle einzusteigen, in der Wasserstoff mit Sauerstoff reagiert und dabei Strom für einen Elektromotor entsteht. Während Konkurrenten längst Testwagen mit Brennstoffzelle fahren lassen, geht BMW zögerlicher vor. Vertriebschef Ian Robertson erklärte das in einem Interview Ende 2014 indirekt mit den erwarteten technologischen Sprüngen bei den Akkus. 
Gleichwohl kooperiert BMW bei der Brennstoffzelle mit Toyota. Die Japaner haben auf dem Heimatmarkt bereits ein Serienauto mit der Wasserstoff-Technologie herausgebracht. BMW könnte diese Technik also relativ leicht für eigene Fahrzeuge übernehmen. Im Herbst wurde in Fachmedien vermutet, dass der künftige i5 mit Strom aus einer Brennstoffzelle fahren würde. Das dementierte der Konzern allerdings. 
Auch bei Assistenzsystemen sind die Bayern vorn dabei. So führten BMW-Ingenieure Anfang des Jahres auf der Consumer Electronics Show (CES) in Las Vegas vor, wie ein i3 ohne Mensch an Bord von allein über ein Parkdeck fährt, eine Lücke zwischen zwei Autos findet und selbstständig einparkt. 
Beim autonomen Fahren insgesamt versucht BMW, Daimler auf den Fersen zu bleiben. Auch BMW testet seit Mitte 2011 mit Forschungsfahrzeugen das automatisierte Fahren auf der Autobahn; der neue 7er, der im September auf der Internationalen Automobil-Ausstellung IAA in Frankfurt Weltpremiere haben wird, soll zumindest den Stop-and-go-Verkehr bis 60 km/h automatisch beherrschen. Weiter ist auch Mercedes in erhältlichen Neuwagen nicht. Und bei einer Technik der Zukunft kooperieren BMW und Daimler sogar: Gemeinsam entwickeln sie ein System für das kabellose Laden von Elektroautos über Induktion. 
 
Lange galt Daimler als weltweiter Pionier bei alternativen Antrieben, vor allem bei der Brennstoffzelle. Schon Mitte der 1990er Jahre gab es erste Prototypen mit der auf Wasserstoff basierenden Technik, und im Jahr 2000 kündigte der damalige Daimler-Chef Jürgen Schrempp gar die Markteinführung für 2004 an: das Necar 5 ( ""New Electric Car "") auf Basis der Mercedes A-Klasse. Daraus wurde bekanntlich nichts, und inzwischen hat Toyota den Wettkampf gewonnen: Seit Dezember 2014 ist in Japan das erste serienmäßige Auto mit Brennstoffzelle, der Toyota Mirai, auf dem Markt. 
Noch voriges Jahr war Daimler auf Augenhöhe, hatte bereits 2011 mit zwei auf Wasserstoffbetrieb umgerüsteten B-Klasse-Autos die Welt umrundet, damit die Zuverlässigkeit des Antriebs belegt und für 2014 den Serienstart angekündet. Doch dann verschob Daimler den Start auf 2017. Allerdings werden die Schwaben auch dann noch zu den Vorreitern gehören. 
Deutlich weniger stark forcierte Daimler den batterieelektrischen Antrieb. Zwar experimentiert der Konzern auch damit schon lange, doch am Markt beschränkte sich der Antrieb bei der Hauptmarke Mercedes-Benz erst mal auf Spielereien wie 2013 den Sportwagen SLS AMG Electric Drive mit vier Elektromotoren für 416.500 Euro. Ganz anders bei der Tochter Smart: Deren Kleinwagen war in den Neunzigern von Nicolas Hayek ohnehin als Elektroauto konzipiert worden. Seit Spätsommer 2012 gibt es den Smart nun auch als Variante electric drive. 
Die Kernmarke Mercedes-Benz beschränkt sich beim Stromern derweil weitgehend auf die Teilvariante: Als Plug-in-Hybride sollen bis 2017 mindestens zehn Modelle kommen. Die große S-Klasse machte den Anfang, inzwischen gibt es auch die C-Klasse mit Steckdosen-Anschluss, und selbst SUV wie der eben in New York vorgestellte Nachfolger der M-Klasse, der GLE, erhalten einen Elektromotor. Einziger Mercedes mit reinem Strombetrieb ist auf absehbare Zeit eine B-Klasse mit dem Antriebsstrang, den Daimler von Tesla übernommen hat. 
Verzockt hat sich der Konzern bei der Batterietechnik für E-Autos. Während andere deutsche Hersteller von Beginn an auf Zellen aus dem Ausland setzten – Asien ist bei der Batteriezellenfertigung führend –, wollte Daimler auch die Zellen selbst fertigen. Zusammen mit dem Chemieunternehmen Evonik baute der Konzern in Sachsen eine Fabrik für Lithiumzellen auf, die im Elektro-Smart eingesetzt wurden. Doch die Fertigung bei Li-Tec Battery war nicht wirtschaftlich, Evonik stieg im Frühjahr 2014 aus und inzwischen hat auch Daimler beschlossen, die Zellenfertigung aufzugeben und für den künftigen E-Smart ab 2016 Zellen aus Südkorea zu beziehen. 
Ähnlich wie bei der Brennstoffzelle verläuft der Wettkampf zwischen den Autoherstellern beim (teil-)autonomen Fahren. Auch Daimler lässt schon seit einigen Jahren seine Testfahrer die Hände vom Lenkrad nehmen, weil die Technik im Fahrzeug zumindest auf bestimmten Autobahnstrecken den Wagen allein steuert. Zumindest für das, was gesetzlich derzeit erlaubt ist, bietet Daimler seit Mitte 2013 in der S-Klasse technische Hilfe: In Staus kann sich der Wagen bei langsamem Tempo selbst bewegen und steuern. 
Was darüber hinaus schon jetzt alles technisch machbar ist, beweist Daimler in einem Prototypen der S-Klasse mit Technologie, die der Hersteller  ""intelligent drive "" nennt: Mit Kameras, Sensoren und viel Elektronik im Kofferraum lenkt sich der Prototyp über Ampelkreuzungen, durch Kreisverkehre und Ortsdurchfahrten. 
Und wie der Innenraum künftig aussehen kann, hat Daimler auch schon präsentiert. Das Konzeptfahrzeug F 015 erzeugt mit vier drehbaren Sesseln eine Lounge-Atmosphäre, bei der sich die Insassen gegenüber setzen können, während das Auto einen von A nach B bringt. Doch bis es so weit ist, werden nach Einschätzung der Experten noch 15 Jahre vergehen. 
Das Image ist dröge: Nix los in Wolfsburg – weder bei der Elektrifizierung noch beim automatisierten Fahren haben die VW-Ingenieure etwas auf die Räder gestellt. Man wartet wie immer ab, was die anderen machen, um nach der Identifikation eines Erfolgsmodells mitzumachen und das Feld von hinten aufzurollen. Follower-Strategie nennt man das. Langweilig, aber sicher. 
Die Fakten sprechen bei genauer Betrachtung eine andere Sprache. Zwar ist Volkswagen nicht überall an der Spitze. Die Innovationskraft ist trotzdem stark, allen Unkenrufen zum Trotz. So verkauft Volkswagen mit dem e-Golf eines der aktuell erfolgreichsten Batterie-elektrischen Autos. In Norwegen, dem staatlich massiv geförderten Paradies der Stromfahrzeuge, liegt er mit Abstand auf Platz 1 der Zulassungsstatistik. 
Das Rezept dazu ist das Gegenteil des BMW i3: Statt auf ein eigenes Design und ein völlig neues Konzept setzt man auf optisch Bewährtes – der Golf ist auch in seiner Variante mit Verbrennungsmotor sehr beliebt bei den Skandinaviern. Die Kritik, den e-Golf nicht als reines Elektroauto konzipiert zu haben, weist Volkswagen zurück. Der Vorteil: Der Wagen wird auf der gleichen Fertigungslinie produziert wie die Versionen mit Benzin- und Dieselmotoren. Man kann flexibel auf jede Nachfrage reagieren, und das finanzielle Risiko ist überschaubar. 
Die Premium-Tochter Audi kommt derweil wieder zurück in die Spur. Vor zwei Jahren lief es unter Entwicklungschef Wolfgang Dürheimer deprimierend schlecht: Angeschobene Projekte wie der rein elektrisch fahrende A1 e-tron wurden wieder abgeblasen. Jüngst hat Audi den R8 e-tron vorgestellt, dessen Batteriekapazität mit 92 Kilowattstunden wohl nicht zufällig marginal über der des Branchenprimus Tesla Model S (85 kWh) liegt. Die Stückzahlen des Elektro-Sportwagens werden minimal sein, und den vermutlich horrenden Preis hat Audi noch nicht veröffentlicht. Aber es tut sich etwas, der R8 e-tron kommt entgegen den vielen Ankündigungen tatsächlich – und gerade noch rechtzeitig, um den Slogan  ""Vorsprung durch Technik "" nicht endgültig ad absurdum zu führen. 
Hinzu kommen im VW-Konzern etliche Plug-in-Hybridautos von den Porsches 918, Panamera und Cayenne über die Volkswagen Golf und Passat GTE bis zu den Audis A3 und Q7 e-tron. Zögerlich ist der Volkswagen-Konzern lediglich bei den Brennstoffzellenautos. Außer auf acht Prototypen für die USA (vier US-Passat, vier Audi A7 h-tron) bleibt die Aktivität auf die Forschung beschränkt. Hier wird man nachziehen müssen – oder man überlässt der asiatischen Konkurrenz von Hyundai über Toyota bis Honda das internationale Feld. 
Rückenwind von der Politik erhoffen sich die Entwickler derweil für das automatisierte – oder, wie es im Audi-Jargon heißt: pilotierte – Fahren. Damit ein teilweise oder vollständig selbstfahrendes Auto wenigstens ausprobiert werden kann, muss Verkehrsminister Alexander Dobrindt mehr rechtlichen Freiraum schaffen. Auf der Autobahn A9 soll das bald der Fall sein. In den USA hat Audi im Januar einen selbstlenkenden A7 Sportback schon von Kalifornien zur Messe CES nach Las Vegas fahren lassen, rund 900 Kilometer. Und auch in Deutschland experimentierte Audi mit einem umgerüsteten RS7. 
Ausgenommen von vielen neuen Technologien sind zurzeit Seat und Škoda. Hier kommt an, was bei anderen Konzerntöchtern etabliert ist. Die Innovation findet bei der Kernmarke VW und bei den Renditebringern Audi und Porsche statt. 
Umparken im Kopf: Dieser Werbeslogan soll das Wiedererstarken einer einstmals extrem beliebten Automarke dokumentieren. Mit Modellen wie dem Adam, dem Corsa und dem SUV Mokka trifft Opel den Geschmack der Kunden. Nur mit dem zukunftsträchtigen Fortschritt ist es bei den Rüsselsheimern nicht so weit her. Zu groß ist der Zwang zum Geldverdienen bei den gut gemachten Brot- und Butter-Autos, und zu wenig lässt die US-amerikanische Mutter General Motors (GM) die deutsche Tochter an den elektrischen Entwicklungen des Konzerns teilhaben. So sind die Autos mit dem Blitz zwar wieder gefällig, aber für Strom und Ladung steht das Markensymbol nicht. 
Dabei hatte Opel durchaus einen guten Weg eingeschlagen: Der Ampera war ein konsequent gemachter Plug-in-Hybrid. Kein anderes Modell dieses Konzepts hatte eine so hohe elektrische Reichweite. Lag es am Preis oder am Zeitgeist – vielleicht war er einfach zu früh da? –, Opel hat den Ampera wegen Erfolglosigkeit ersatzlos eingestellt. Und das, obwohl der baugleiche Chevrolet Volt jetzt einen Nachfolger hat, der in den USA zu kaufen ist. Mit größerer Batteriekapazität, leicht gesunkenem Verbrauch und zu einem niedrigeren Preis. 
Die Managemententscheidung ist trotzdem eindeutig: Den neuen Chevrolet Volt wird es nicht mit Opel-Logo geben. Mutmaßlicher Grund ist das Geld. Die Homologation, also die Zulassung, in Europa ist aufwändig und teuer, dazu kommen die Kosten für Vertrieb und Marketing. Ein Zuschussgeschäft, auf das man sich offenbar nicht einlassen wollte. Ebenfalls nicht als Opel zu haben ist der batterieelektrische Kleinwagen Chevrolet Spark ( ""Funke "") EV. Der Chevrolet Bolt ( ""Bolzen "") wiederum soll ein ernst zu nehmendes Kompaktauto mit Ladestecker werden. Eine Zukunft als Opel ist möglich, aber ungewiss. 
General Motors ist wie alle weltweit operierenden Autokonzerne aktiv bei Brennstoffzellenfahrzeugen. Der GM HydroGen4 ist in Deutschland seit 2006 als Opel unterwegs; einige Exemplare finden sich in Versuchsprojekten. Weiter ist man aber noch nicht – von einer konkreten Markteinführung ist erst recht nicht die Rede. Druck könnte aus Südkorea kommen, wo man generell weniger Angst vor Elektroautos hat als hier. 
Die Realität von Opel im Antriebsbereich ist also bodenständig. Keine Experimente, lautet die Devise. Die Verbrennungsmotoren sind gut gemacht, vom leisen Diesel mit 1,6 Litern Hubraum bis zum Dreizylinder-Benziner mit Turboaufladung. Für Spielchen mit Strom ist kein Platz. Auch nach Besonderheiten im Bereich des automatisierten Fahrens sucht man vergebens. Opel bietet, was andere ebenfalls tun. An die Spitze setzen kann sich die Traditionsmarke nicht. Warum sollte sie auch? Die Konkurrenz bei Škoda oder Peugeot geht ähnlich vor. 
Das Risiko, dass Opel durch den Rost fällt, ist jedenfalls gering: Wenn es drauf ankommt, wird GM die Innovationen liefern können, die auch deutsche Ingenieure konstruieren könnten und sicher gerne würden – für die aber aus Kostengründen zu wenig Raum ist."	technik
"Am autonomen Fahren kommt kaum ein Autohersteller mehr vorbei. Es wird geforscht, getüftelt und erprobt, was das Zeug hält. Dass die Technik inzwischen reif genug ist, unter bestimmten Voraussetzungen ein Auto allein von A nach B zu bringen, haben mehrere Unternehmen bereits bewiesen: Als erster deutscher Hersteller ließ Mercedes eine S-Klasse von Mannheim nach Pforzheim fahren; BMW versucht sich auf der Strecke vom Hauptsitz zum Münchener Flughafen; Audi demonstrierte seinen Fortschritt unter anderem in den USA. 
Nissan denkt schon einen Schritt weiter. Denn damit das autonome Fahren Realität wird, braucht es mehr als nur ein paar Sensoren und schnelle Rechner. Auf der Automesse in Tokio präsentierte der japanische Hersteller kürzlich die selbstfahrende Studie IDS und legte einen erstaunlich konkreten Zeitplan vor: 2016 will der Hersteller einen Assistenten bringen, der in Staus dem vorausfahrenden Fahrzeug folgt; zwei Jahre später sollen automatische Spurwechsel auf der Autobahn möglich sein. Und schon 2020 sollen Nissan-Modelle in der Stadt selbstständig über Kreuzungen rollen können. 
Dabei stellt vor allem der Stadtverkehr die Forscher vor Herausforderungen. Die Technik tut sich schwer, rote Ampeln eindeutig zu erkennen – das rote Licht könnte ja auch von anderen Fahrzeugen kommen. Außerdem stören zum Beispiel Zäune, durch die wir Menschen problemlos hindurchblicken, das künstliche Kameraauge ungemein. Und dann fehlen vor allem auf Kreuzungen auch noch die Spurmarkierungen, an denen sich der Wagen sonst so gerne orientiert. 
Das alles wäre nicht so schlimm, wäre da nicht noch der Mensch. Hohe Priorität hat für die Nissan-Forscher deshalb zum einen die Beziehung des Autos zum Fahrer, zum anderen die zu den Passanten – beide müssen sich auf das Fahrzeug verlassen. 
Um das Vertrauen des Dann-nicht-mehr-Fahrers zu gewinnen, entwickelt Nissan ein ausführliches Informationssystem, das offenlegt, was die Technik gerade macht und als nächstes vorhat. Ein Prototyp zeigt im Kombi-Instrument an, welche Sensoren gerade was überwachen, wo sie Hindernisse oder Gefahren erkennen und welches Manöver ansteht, zum Beispiel Überholen oder Abbiegen. Außerdem soll sich das Auto den Stil des Fahrers abschauen, wenn dieser im manuellen Modus unterwegs ist. Nach aktuellem Stand ist die Technik nach etwa zehn Fahrten in der Lage, das Beschleunigungs- und Bremsverhalten ihres Besitzers zu kopieren, und schafft mit diesem gewohnten Fahrgefühl zusätzlich Vertrauen. 
Deutlich komplexer gestaltet sich das Zusammenleben mit der Außenwelt. Auch hier setzt Nissan auf Kommunikation: Über ein blaues LED-Licht, das sich um den ganzen Wagen zieht, signalisiert der Prototyp den Passanten, dass gerade der Autopilot den Wagen steuert. Außerdem kommuniziert das Fahrzeug über ein Display hinter der Windschutzscheibe mit den Fußgängern und fordert sie auf, über die Straße zu gehen. 
Diese Unterhaltungen zwischen Mensch und Maschine wollen die Forscher deutlich ausbauen. Es sei wichtig, dass das Auto berechenbar bleibt, sagt Marteen Sierhuis, der für Nissan im Silicon Valley an der Technik tüftelt. Denn: Auch der Mensch ist berechenbar. Wir haben gelernt, andere Personen einzuschätzen, weil wir uns entweder über Blicke verständigen oder aber bekannte Verhaltensmuster wiedererkennen. Sieht ein Fußgänger zum Beispiel, dass ein Autofahrer an der Kreuzung einen Schulterblick macht, liegt es nahe, dass er abbiegen will. 
Umgekehrt muss aber auch das Auto lernen, menschliches Verhalten, das es über Kameras wahrnimmt, zu interpretieren und darauf sein Handeln abzustimmen. Sonst führt das autonome Fahren nicht zu einer Verkehrsentlastung, sondern zum Chaos. Man stelle sich nur einen Zebrastreifen vor, an dem zufällig ein Fußgänger steht, der aber gar nicht über die Straße will – momentan würde das autonome Auto wohl nicht weiterfahren. 
Um den Verkehr flüssig zu halten, ist es manchmal sogar nötig, sich nicht ganz regelkonform, aber gesellschaftlich akzeptiert zu verhalten. Sierhuis' Paradebeispiel ist die typische amerikanische Kreuzung, an der immer das Fahrzeug Vorfahrt hat, das als erstes kommt. Sieht man nun aber, dass dieses Fahrzeug abbiegen will, kann man auch als Nummer zwei sofort weiterfahren, ohne ihm die Vorfahrt zu nehmen. Solange nicht auch selbstdenkende Autos diese Entscheidungen treffen können, wird der Verkehr an der Kreuzung immer wieder unnötig ins Stocken geraten. 
Um solche Situationen zu erforschen, ist Sierhuis mit seinem Team auf der ganzen Welt unterwegs. In bestimmten Teilen Brasiliens etwa ist es quasi erlaubt, an roten Ampeln weiterzufahren, da Stehenbleiben zu gefährlich wäre. Außerdem unterscheiden sich auch verschiedene Gesellschaftsgruppen stark voneinander: Banker rund um die Wall Street verhalten sich im Straßenverkehr ganz anders als die Studenten auf dem Campus der Stanford University. 
Doch die Forscher sind guter Dinge, Verhaltensmuster zu definieren, die auf der ganzen Welt gleich sind, und diese das Auto zu lehren. Denn so gern wir selbst unsere Unterschiede betonen – am Ende sind wir doch alle gleich. Sonst könnten wir ja nie im Ausland Auto fahren."	technik
"Die Software eines selbstfahrenden Autos von Google hat einen Unfall verursacht. Mitte Februar waren ein Google-Auto und ein Linienbus in der kalifornischen Stadt Mountain View bei San Francisco zusammengestoßen. Google nahm in dem am Montag veröffentlichten Untersuchungsbericht die Schuld auf sich.  ""In diesem Fall tragen wir eindeutig eine gewisse Verantwortung, denn es hätte keinen Zusammenstoß gegeben, wenn sich unser Auto nicht bewegt hätte "", sagte Google dem TV-Sender CNBC. 
Es ist das erste Mal, dass Google einen Softwarefehler als Verursacher eines Unfalls einräumt. Bei vergangenen Unfällen mit Google-Autos galt stets menschliches Versagen als Unfallursache. Der Roboterwagen hatte die Spur wechseln wollen, um ein Hindernis zu umfahren. Er ließ zunächst mehrere Fahrzeuge passieren und fuhr dann los, obwohl sich ein Bus näherte. Die beiden Fahrzeuge stießen mit geringer Geschwindigkeit frontal zusammen, verletzt wurde niemand. 
Der Google-Mitarbeiter am Steuer des umgerüsteten Lexus habe das Manöver nicht aufgehalten, weil er davon ausgegangen sei, dass die Zeit ausreiche oder dass der Bus notfalls bremse, teilte Google mit. Die Software sei nach dem Unfall angepasst worden. Die Autos berücksichtigten jetzt, dass es weniger wahrscheinlich sei, dass Busse und andere große Fahrzeuge sie durchließen. 
Das Google-Auto ist nach Angaben des Konzerns zum Zeitpunkt der Kollision etwa drei Kilometer pro Stunde gefahren, der Bus fuhr gut 20 Kilometer pro Stunde. Von der Polizei liegt bisher keine Einschätzung zu den Umständen des Unfalls vor. 
Die selbstfahrenden Autos mit Google-Software sind in mehr als sechs Jahren über zwei Millionen Kilometer gefahren und waren in gut ein Dutzend kleinerer Unfälle verwickelt. In den meisten Fällen fuhren andere Autofahrer von hinten auf die Google-Autos auf. 
Am Steuer jedes Google-Autos sitzt ein Mensch, der die Software überwachen und eingreifen soll, wenn er den Eindruck hat, dass sie überfordert ist. Kritiker hatten in der Vergangenheit immer wieder bemängelt, die Google-Autos seien zu vorsichtig unterwegs und behinderten den Verkehr. Die Polizei hatte einen Wagen herausgewunken, weil er zu langsam gefahren war."	technik
"Die Technik war bislang den Gesetzen voraus. Ob von BMW, Daimler oder ausländischen Automobilherstellern: Schon länger fahren Testautos auf den Straßen, in denen kein Fahrer die Hände am Lenkrad hat. Zumindest zeitweise. Dafür braucht es jeweils eine Sondergenehmigung, etwa die  ""Innovationscharta für das Digitale Testfeld Autobahn "". Mit dieser gab Bundesverkehrsminister Alexander Dobrindt (CSU) im Herbst 2015 die Autobahn A 9 in Bayern für Autohersteller, Zulieferer und Forschungseinrichtungen frei, damit diese dort ihre automatisierten und vernetzten Autos im Realbetrieb testen können. 
Das Roboterauto ist also schon heute keine Utopie mehr – und zwar nicht nur bei Google im Silicon Valley. In begrenztem Rahmen ist halb automatisiertes Fahren auch schon in bestehende Automodelle eingezogen. Das sogenannte Intelligent Drive bei Mercedes-Benz erlaubt, im Stau auf der Autobahn an einen elektronischen Fahrassistenten zu übergeben, der bis Tempo 30 den Wagen steuert. Allerdings funktioniert das System nur, wenn der Fahrer regelmäßig das Lenkrad berührt. Das verlangen die aktuellen Gesetze. 
Die fußen auf dem sogenannten Wiener Übereinkommen über den Straßenverkehr, einem internationalen Vertrag aus dem Jahr 1968 zur weltweiten Standardisierung grundlegender Verkehrsregeln. Die Übereinkunft schreibt vor:  ""Jeder Fahrzeugführer muss unter allen Umständen sein Fahrzeug beherrschen, um den Sorgfaltspflichten genügen zu können und um ständig in der Lage zu sein, alle ihm obliegenden Fahrbewegungen auszuführen. "" Eine Ergänzung erlaubt seit Herbst 2014 Systeme, mit denen ein Fahrzeug autonom fährt, sofern sie jederzeit vom Fahrer übersteuert oder ausgeschaltet werden können. 
Jetzt hat die Bundesregierung diese Änderung der Wiener Konvention in deutsches Recht umgesetzt, damit Deutschland dem internationalen Regelwerk auch in seiner neuesten Form folgt.  ""Das automatisierte und vernetzte Fahren ist die größte Mobilitätsrevolution seit der Erfindung des Autos "", sagt Verkehrsminister Dobrindt.  ""Mit unserem Gesetzentwurf machen wir den Weg frei, damit automatisierte Fahrsysteme immer mehr Fahraufgaben selbstständig übernehmen können. "" Der Entwurf schafft somit Rechtssicherheit für Spurhalteassistenten oder das beschriebene Stop-and-go-System von Mercedes. 
Die Neuregelung bedeutet allerdings, dass nach wie vor der Fahrer die Verantwortung behält und jederzeit die Kontrolle über das Auto übernehmen können muss. Ein Google-Auto ohne Lenkrad und Pedale ist also weiterhin nicht zugelassen. Welche Systeme tatsächlich unter die neuen Freiheiten fallen, muss eine Arbeitsgruppe der UN-Wirtschaftskommission für Europa (UNECE) erst noch im Detail festlegen. Die Ideen der Autohersteller und Zulieferer gehen über die kleine Änderung am Wiener Abkommen jedenfalls längst weit hinaus: Innenraumdesigns etwa mit Drehsitzen, sodass sich alle Insassen einander zuwenden können, während der Wagen selbstständig von A nach B fährt. 
Der Automobilverband VDA unterscheidet bei selbstfahrenden Autos drei Stufen: 
Die Technologie selbst fahrender Autos basiert vor allem auf Sensoren und Kameras, die die Umgebung erfassen und deren Daten Rechner schnell verarbeiten. Einparkhilfen oder Stauassistenten funktionieren bereits so. Autobauer setzen diese Technologie zunächst in Luxusfahrzeugen ein, die zumindest mit teilautomatisierten Elementen noch in diesem Jahrzehnt auf dem Markt erwartet werden. 
Technisch wird es noch einige Jahre dauern, bis solche Utopien Wirklichkeit und auch die Fahrer zu Beifahrern werden. Doch bis dahin müssen Wiener Übereinkunft und nationale Gesetze weiter modifiziert werden. Die Bundesregierung treibe dafür die Öffnung internationaler Abkommen voran, so Dobrindt. Eines seiner wichtigsten Ziele: Im Wiener Übereinkommen soll die Begriffsdefinition des Fahrers so erweitert werden, dass dem Menschen künftig automatisierte Systeme, die eine volle Kontrolle über ein Fahrzeug haben, gleichgestellt werden. 
Dann müsste ein Fahrer die Technik nicht mehr permanent überwachen. Erst eine solche Neuregelung würde etwa einen Autobahnpiloten erlauben, der zwischen Auf- und Abfahrt dem Nutzer das Autofahren komplett abnimmt. Dafür sind jedoch noch eine ganze Reihe weiterer Punkte zu klären. Wer haftet, wenn das Roboterauto einen Unfall baut – der Besitzer oder der Autohersteller? Wie wird ein solches Fahrzeug überhaupt versichert? Und wie sehen die Sicherheitsstandards aus, damit nicht Hacker von außen ins System eingreifen können? 
Dafür hat Dobrindt schon 2014 auf nationaler Ebene einen runden Tisch  ""Automatisiertes Fahren "" ins Leben gerufen, bei dem zuständige Ministerien und Behörden sowie Industrie, Forschungseinrichtungen und Überwachungsorganisationen wie der TÜV fachübergreifend technische wie rechtliche Fragen erörtern. So plädiert eine Arbeitsgruppe des runden Tisches dafür, dass voll automatisierte Autos einen Unfalldatenspeicher besitzen müssen, um künftig Haftungsfragen klären zu können. Allerdings stellt sich da sofort die nächste Frage: die nach dem Datenschutz. 
International wird es im September wieder spannend. Dann findet in Japan das nächste Treffen der G-7-Verkehrsminister statt, auf dem die Regeln für das automatische Fahren weiter diskutiert und vorangebracht werden sollen. Etwas Zeit bleibt den Ministern noch: Zwar bauen in den nächsten Jahren immer mehr Autohersteller teilautonome Systeme in Autos ein – autonom fahrende Autos, die komplett die Kontrolle übernehmen, werden aber vermutlich erst um 2025 auf dem Markt sein."	technik
"Die Gegenoffensive von Google ließ nicht lange auf sich warten. In einem seitenlangen Beitrag auf der Plattform Medium erklärte der Konzern Mitte Dezember ausführlich, warum das, was in Kalifornien passiert, Innovation und Fortschritt im Kern bedrohe.  ""Wir laufen Gefahr, im Status quo zu verharren, und machen es unmöglich, das volle Potential dieser Technologie zu entfalten "", hieß es in dem offenen Brief mit dem Titel Die Perspektive vom Fahrersitz. Der Schritt, schlussfolgerte Chris Urmson stellvertretend in dem Text, sei schlicht  ""verblüffend "". 
Urmson ist Chef der Google-Schmiede für selbstfahrende Autos. Und genau die hat der bevölkerungsreichste Bundesstaat jetzt ins Visier genommen. Mitte Dezember legte die dortige Straßenverkehrsbehörde einen ganzen Satz an Regeln vor, die den Betrieb und den Verkauf von autonomen Fahrzeugen kontrollieren sollen. Noch sind das nur Vorschläge, die frühestens in einem Jahr in Kraft treten würden. Doch die Branche schlägt schon jetzt Alarm, schließlich dient der Heimatstaat von Tesla und Google als Blaupause für die Regulierung der Autobranche. 
Stein des Anstoßes ist vor allem eine Anforderung, die Googles Vision vom fahrerlosen Auto mit einem Schlag unbrauchbar machen könnte. Denn um überhaupt im Straßenverkehr zugelassen zu werden, müssten die Fahrzeuge in Kalifornien jederzeit mit einem Fahrer unterwegs sein, der in der Lage ist, im Notfall einzugreifen. Selbst Wirtschaftsblätter und große Tageszeitungen wie die LA Times warfen den Behörden daraufhin ungewöhnlich deutlich vor, die Technologie damit uninteressant zu machen. 
Niemanden würde der Vorstoß so sehr treffen wie den Konzern aus Mountain View. Google testet seit 2009 fahrerlose Autos – derzeit noch mit Begleitpersonen aus dem eigenen Haus – auf den Straßen rund um den Hauptsitz. Wohin es langfristig gehen soll, zeigt ein im Sommer eingeführtes Modell, das komplett ohne Fahrer auskommen und gleich ganz auf das Lenkrad verzichten soll. Verlaufen die Tests erfolgreich, will Google seine Zweisitzer in einer ersten Phase zum Beispiel für die Lieferung von Paketen einsetzen. 
Doch die Ideen der Branche gehen längst weiter. Vollkommen fahrerlose Wagen sollen in Zukunft neue Alternativen zu gängigen Transportmitteln schaffen und Verkehrsprobleme lösen. Millionen von Amerikanern, heißt es auch in dem Brief von Google, könnten so mobiler werden, selbst wenn sie selbst keinen Führerschein hätten, wegen Krankheit oder Alter nicht mehr fahrtüchtig seien oder sich ein eigenes Auto schlicht nicht leisten könnten. Autonome Lkw sollen Frachttransporte sicherer und effizienter machen und zumindest zeitweise den Fahrer ablösen. Längst sind neben Technologieunternehmen wie Apple und Tesla auch die traditionellen Autobauer, darunter Daimler und Audi, in das Geschäft eingestiegen – und haben sich mit Forschungsstellen im Silicon Valley angesiedelt, um ihre Visionen voranzutreiben. 
Jetzt fühlt sich die Branche in ihren Ambitionen vom Staat ausgebremst. Denn auch die übrigen Vorschläge aus Kalifornien stellen die Hersteller vor Schwierigkeiten. Die Verkehrsbehörde will für den Betrieb etwa ein spezielles Training für die Fahrer verlangen, zudem sollen die Autos von den Herstellern nur geliehen, nicht gekauft werden können. Die Firmen müssen die Wagen ununterbrochen überwachen und monatliche Berichte abliefern.  ""Ich hoffe, die endgültigen Regeln werden deutlich anders aussehen "", sagt angesichts dieser Vorschläge Bryant Walker Smith von der University of South Caroline School of Law, der sich mit der noch jungen Branche seit Jahren beschäftigt. Die Vorschläge in Kalifornien gäben der Industrie eine  ""äußerst frustrierende Perspektive "". 
Kalifornien gilt dank des Silicon Valley nicht nur als einer der führenden Tech-Staaten im Land, zugleich werden hier oft die Regeln für die gesamte Autobranche geformt, weil nirgends mehr Fahrzeuge unterwegs sind. Wie wichtig der Markt für die Industrie ist, zeigte sich erst vor wenigen Wochen, als entschieden wurde, die Sammelklagen gegen Volkswagen in dem Westküstenstaat zu bündeln. Zwar werde Kalifornien die Regeln für autonomes Fahren nicht alleine machen, sagt Walker Smith.  ""Aber die anderen werden genau schauen, was die dortigen Regulierer machen. "" 
Dass die Ideen in jetziger Form bestehen bleiben, halten Beobachter aber für unwahrscheinlich. Denn der Druck auf die Gesetzgeber, die Straßen für die Wagen freizumachen, kommt von allen Seiten.  ""Es wird jede Menge Druck geben, öffentlich und hinter verschlossenen Türen "", sagt Walker Smith. In der nun laufenden zweimonatigen Kommentarphase würden Firmen wie Google und Tesla öffentlich für lockere Vorschriften trommeln. Die Branche werde versuchen, über die Behörde hinweg direkt beim Gesetzgeber in Kalifornien und Washington Einfluss zu üben und auf ein Einschreiten zu drängen. Die US-Regierung hält sich bislang zurück, denn die Regeln für Amerikas Straßen werden auf Staatenebene gemacht. Nur vier Bundesstaaten haben überhaupt explizit Regeln für den Betrieb von autonomen Fahrzeugen eingeführt, im Rest des Landes bleibt die rechtliche Lage unklar. 
Viele der Bundesstaaten könnten die Debatte in Kalifornien nun nutzen, um ihrerseits stärker um die Firmen zu buhlen.  ""Die Vorschläge aus Kalifornien werden vor allem anderen Staaten wie Texas zugute kommen "", glaubt Walker Smith. Schon jetzt fahren die Google-Wagen im texanischen Austin, der Heimat des jährlichen Technologie-Mekkas South by Southwest, weil der Bundesstaat den Unternehmen mehr Freiheiten gibt. Bürgermeister Steve Adler sei dem Projekt so sehr verpflichtet, unkten viele, dass er bei seiner Rede zum Startschuss Zitate verwendet habe, die ein Google-Lobbyist geschrieben hatte. 
Neben Texas kämpft auch Nevada darum, zu den Vorreitern im autonomen Fahren zu gehören. Bereits 2012 hatte der Bundesstaat grünes Licht für die Tests neuer Google-Modelle auf allen öffentlichen Straßen gegeben – als erster Bundesstaat im Land. Und im Frühjahr startete hier die erste Testfahrt für den autonomen Lkw von Daimler. 
Dass die Entwicklung durch den Vorstoß in Kalifornien wirklich zurückgeworfen werden könne, glaubt Walker Smith deshalb nicht. Sollten die Regeln in der jetzigen Form umgesetzt werden, so der Jurist,  ""werden Google und Co. sich noch aggressiver als bisher nach anderen Standorten umsehen "". Man sei optimistisch, hieß es von Google schlicht, dass man gemeinsam mit der Behörde an einer besseren Lösung arbeiten könne."	technik
"Beim Cloud Computing hat der Einzelne keine Kontrolle über seine Daten und was mit ihnen geschieht. Diese Erkenntnis ist nicht neu und Datenschützer warnen schon lange vor den Gefahren dieser Dienste. Doch haben die Warnungen bislang nicht dazu geführt, dass sich die europäische Politik darauf einstellt und versucht, das zu Problem zu lösen. Das schreiben sechs Autoren in einer Studie, die sie im Auftrag des EU-Parlaments verfasst haben. 
Die Politik der Europäischen Union gehe beim Thema Netzpolitik von falschen Voraussetzungen aus und schaue in die falsche Richtung, argumentieren sie in der Untersuchung mit dem Titel  ""Fighting cyber crime and protecting privacy in the cloud "". 
Vor allem ein Aspekt werde von der Europäischen Kommission viel zu wenig berücksichtigt, heißt es: das sogenannte Cloud Computing. Im Bereich Internet werde das eigentliche Problem  ""unterschätzt, wenn nicht gar ignoriert "", nämlich die Herausforderungen, die sich beim Verarbeiten von Daten in der Cloud für Datenschutz und Datenkontrolle ergäben. 
Cloud ist bedrohlicher als Kriminalität 
Nicht Betrug und Identitätsdiebstahl sind demnach die größten Gefahren für EU-Bürger, wenn sie sich im Netz bewegen. Sondern die Risiken, die sich ergeben, wenn Daten in anderen Ländern verarbeitet werden. Denn EU-Bürger hätten keine Rechtssicherheit, wenn sie Cloud-Angebote wie die von Google, Amazon oder Facebook nutzen. Sie könnten ihr Recht auf Datenschutz und auf Kontrolle der eigenen Daten nicht durchsetzen. 
In der Studie wird auch die Ursache dafür analysiert. Drei Interessenbereiche definieren nach Meinung der Autoren den Markt der Cloud-Anbieter: die Interessen von Unternehmen, die von Staaten und die Interessen, die aus zwischenstaatlichen Beziehungen entstehen, also beispielsweise aus Verhandlungen zwischen den USA und der EU. 
Dieses  ""diplomatische Dreieck "", wie sie es nennen, könne aus zwei Perspektiven betrachtet werden. Aus der Perspektive der einzelnen Nutzer und aus der Perspektive, das Internet auf globaler Ebene regulieren zu wollen. Die Politik der EU sei derzeit vor allem auf die globale Regulierung ausgelegt. Ein Beispiel dafür ist der Versuch, mit Acta ein internationales Handelsabkommen zwischen den USA, Europa und Asien zu verhandeln. 
Allerdings, so schreiben die Autoren, sei bei dieser Betrachtung der einzelne Nutzer das schwächste Glied in der Beziehungskette. Acta hat genau das gezeigt. Normale Nutzer des Internets fühlten sich durch den geplanten Vertrag gegängelt und überwacht. Er war allein aus Sicht der Industrie formuliert und gedacht, Interessen der Nutzer spielten darin keine Rolle. Sie gingen dagegen auf die Straße. In diesem Fall erfolgreich. 
 
Bei anderen Punkten haben sie aber sehr viel weniger Einfluss auf die Probleme. Die Autoren führen dafür den Umgang amerikanischer Behörden mit Daten an. Gesetze wie der US Patriot Act und der gerade verlängerte FISA Amendments Act erlauben es Polizei und Geheimdiensten in den USA, bei Anbietern wie Google oder Facebook gespeicherte Daten ohne Kenntnis der Betroffenen zu sehen. Das betrifft auch Daten, die von Bürgern aus anderen Ländern stammen, beispielsweise eben aus Europa. 
Laut FISAAA, wie das Gesetz genannt wird, dürfen amerikanische Ermittler heimlich jede Kommunikation abhören, wenn sie vermuten, dass einer der Beteiligten Amerikaner ist. Nach Einschätzung der Autoren stellt diese Erlaubnis ein größeres Risiko für die Souveränität europäischer Daten dar,  ""als jedes Gesetz, über das europäische Politiker jemals nachgedacht haben "". 
Den bisherigen Bemühungen der EU, solche Themen mit anderen Staaten zu verhandeln, stellen die Autoren ebenfalls ein schlechtes Zeugnis aus. So hätten die Politiker es versäumt, bei internationalen Verträgen adäquate Datenschutzstandards zu fordern. Als Beispiel nennen sie das sogenannte Safe-Harbor-Abkommen mit den USA. Das erlaubt es, personenbezogene europäische Daten auch in den USA zu verarbeiten – wenn das dortige Unternehmen die Richtlinien des Abkommens einhält. Amazon, Google und Facebook sind dort beispielsweise Mitglied. 
Blind für Risiken pseudonymer Daten 
Jedoch habe Safe Harbor große Lücken, heißt es in der Studie. Es gelte beispielsweise nicht für normale Telekommunikationsanbieter, die inzwischen auch Cloud-Dienste bereitstellten. Außerdem sei das Abkommen angesichts der Überwachungsgesetze wie eben FISAAA nicht mehr viel wert. Und es ignoriere sogenannte pseudonyme Daten, Informationen also, die einzeln nicht dazu dienen können, jemanden zu identifizieren. Die jedoch zusammengefasst mit anderen das Zusammensetzen von Profilen erlauben und damit sehr genaue Aussagen über ein Individuum. 
Die Blindheit für die Risiken pseudonymer Daten werde leider auch nicht durch die aktuelle Datenschutzreform der EU-Kommission beseitigt, sagt Caspar Bowden, einer der sechs Autoren der Studie. Zumindest werde sie wohl nicht dazu führen, dass die Industrie solche Daten künftig besser schützen muss. Eine Befürchtung, die auch in der EU-Politiker teilen und daher fordern, dass beispielsweise das pseudonyme Datum IP-Adresse geschützt werden soll. 
 ""Unglücklicherweise waren die Lobbyisten zuerst da, der Reformentwurf enthält genau die gleichen Lücken – die allerdings weniger Lücken sind, sondern vielmehr die entscheidende Maginot-Linie bilden, die damit leider an der falschen Stelle gebaut wurde. "" Ja möglicherweise könnte der Reformentwurf sogar noch zu einer Verschlechterung führen, sagt Bowden. Denn beispielsweise fordere Großbritannien, nur solche Daten sollten als persönliche Daten geschützt werden, die jemanden  ""leicht identifizieren "", also Name, Adresse et cetera. Die Briten wollten auch die bisher im Entwurf stehende Warnung vor Profilen entfernen. Quelle seiner Sorge ist eine bei Statewatch geleakte Einschätzung des Entwurfs durch den EU-Rat. 
Angesichts solcher Entwicklungen sollte die EU ihren Fokus verändern, fordern die sechs Autoren. Europäische Internetpolitik müsse stärker den Nutzer in den Mittelpunkt stellen. Denn er und seine Daten seien im Moment am stärksten bedroht."	technik
"Am 15. September feierte die Bittorrent-Seite The Pirate Bay neunten Geburtstag : Gegründet 2003 von Mitgliedern der schwedischen Urheberrechtsgegner Piratbyrån, war die  ""Bay "" in wenigen Jahren zur größten Börse für sogenannte Torrents geworden. Torrents sind kleine Dateien, mit denen Nutzer über spezielle Programme Inhalte wie Musik, Spiele oder Filme tauschen konnten. Heute gehört Pirate Bay zu den 100 meistbesuchten Websites im Netz und gilt als wichtiger Ort in der Filesharing-Szene. 
Das ist insofern bemerkenswert, als dass es die Seite eigentlich gar nicht mehr geben sollte. Am 31. Mai 2006 stürmten Polizisten mehrere Rechenzentren in Schweden und nahmen die Inhaber des Webhosters PRQ fest: die Programmierer Gottfrid Svartholm Warg und Fredrik Neij, gemeinhin als das  ""Gehirn "" und die  ""Hände "" von Pirate Bay bezeichnet. Sie wurden in den folgenden Monaten zusammen mit dem  ""Mund "", dem Pressesprecher Peter Sunde, und Carl Lundström, Wasa-Erbe und Geldgeber, wegen Beihilfe zur Urheberrechtsverletzung angeklagt. 
Seitdem hat sich einiges getan – und auch wieder nicht. Zwar wurden die vier Angeklagten nach einem langwierigen Prozess und einem ähnlich langen Berufungsverfahren im November 2010 zu mehrmonatigen Haftstrafen und Schadensersatzzahlungen in Höhe von 6,5 Millionen Euro verurteilt. Doch nur Lundström verbüßte seine Strafe über Sozialstunden und Hausarrest und hat mittlerweile Privatinsolvenz in der Schweiz angemeldet. Zahlungen gab es bis heute keine. Und Pirate Bay ist weiter online. 
Das hat vor allem zwei Gründe: ein unsichtbares Netzwerk aus Betreibern und Gönnern, sowie technische Entwicklungen, die es den Verfolgern immer schwerer machen, den Dienst dauerhaft abzuschalten. 
Den Verfolgern immer einen Schritt voraus 
Die neuste dieser Entwicklungen hat die Plattform gerade im offiziellen Blog vorgestellt: Pirate Bay verlegt fast ihre gesamte Infrastruktur in die Cloud. Statt klassische Server in Rechenzentren zu mieten, verteilt sich das Netzwerk künftig auf mehrere virtuelle Server bei Anbietern in mehreren Ländern. Lediglich ein Router und Load-Balancer zur gleichmäßigen Verteilung des Datenverkehrs bleiben als Hardware übrig. Fallen diese in die Hände von Dritten, führen sie bloß zu verschlüsselten Daten. Nutzer und Betreiber seien außerdem geschützt, weil selbst die Hoster nicht wüssten, wessen Daten sie eigentlich verteilen, heißt es im Szeneportal Torrentfreak . Fällt ein Anbieter aus, wird der Service innerhalb von acht Stunden automatisch auf einen anderen verlagert. 
Es ist nicht der erste Versuch der Plattform, den Filesharing-Jägern ein Schnippchen zu schlagen. Ende 2009 schaltete sie den hauseigenen Tracker ab, jenen Server, der den Verkehr zwischen den Nutzern im Bittorrent-Netzwerk koordiniert und deshalb oft als größte Schwachstelle des Protokolls gilt. Im Februar dieses Jahres kündigte Pirate Bay an, statt auf Torrent-Dateien künftig auf sogenannte Magnet-Links zu setzen. Sie enthalten zwar die gleichen Informationen wie Torrents, sind aber keine Dateien zum Herunterladen, sondern Hyperlinks – ein wichtiger Unterschied bei der Verfolgung von Urheberrechtsverletzungen, weil der Nachweis, dass Dateien verbreitet oder angeboten wurden, schwieriger wird. 
Überhaupt hat es das Team der Piratenbucht nahezu perfektioniert, schnell auf juristische und technische Angriffe zu reagieren. Nach der Razzia im Mai 2006 hatte es nur drei Tage gedauert, bis der Service wieder erreichbar war. Kaum länger dauerte es bei einer zweiten großangelegten Aktion im September 2010. Und obwohl britische Provider im Frühjahr dieses Jahres den Zugriff auf den Service sperrten, hat sich der Peer-to-Peer-Datenverkehr in Großbritannien längst wieder stabilisiert ; zu schnell sorgten die Betreiber von Pirate Bay für alternative Zugänge. Auch die Erfahrungen aus Ländern wie Belgien oder den Niederlanden zeigen : Weder Netzsperren noch Razzien konnten dem dezentralisierten und gut organisierten Netzwerk etwas anhaben. 
 
Ähnlich schwer greifbar wie die Infrastruktur von Pirate Bay sind auch die Menschen dahinter. Sowohl Sunde als auch Svartholm und Neij haben sich inzwischen von Pirate Bay distanziert. Das müssen sie auch, denn mit der Bestätigung des Gerichtsurteils wurde ihnen jeglicher Kontakt mit dem Netzwerk untersagt. 
Wer stattdessen hinter dem oft kollektiv auftretenden Pirate Bay Team steckt, ist unklar. Die letzte brauchbare Spur führt zu einer Briefkastenfirma auf den Seychellen namens Reservalla. Nach Angaben Sundes übertrugen die Gründer bereits 2006 Pirate Bay einem unbenannten Dritten, bevor sie von Reservalla gekauft wurde. Die heutigen Besitzer des Unternehmens sind unbekannt. Allerdings brachten Ankläger immer wieder Fredrik Neij ins Gespräch, der weiterhin als Inhaber der Domain thepiratebay.org registriert ist – eine reine Formalität, wie er sagt. 
Das Fehlen von Betreibern macht es für die Behörden schwierig, den Service dauerhaft abzuschalten. Razzien, wie vor einigen Wochen beim Webhoster PRQ , brachten zwar kleinere Torrent- und Streamingseiten zu Fall, nicht aber Pirate Bay. Und auch Untersuchungen von Netzprovidern wie dem nicht allzu ernst auftretenden Serious Tubes Networks führten zu keinem Erfolg. Sie stellen zwar die Internet-Anbindung der Plattform sicher, besitzen aber keinerlei Zugriffe auf die Inhalte. Fast scheint es, als hätte Pirate Bay tatsächlich ihre  ""irdische Form "" abgelegt, wie es im jüngsten Blogeintrag heißt. 
Die schwedische Justiz bewegt sich – langsam 
Angesichts dieser Handlungsohnmacht konzentrieren sich die Behörden aktuell wieder auf die bereits verurteilten Menschen. Im Frühjahr zog die schwedische Botschaft in Bangkok den Ausweis des in Laos lebenden Fredrik Neij ein. Neij legte Einspruch ein mit der Begründung, er müsse seine Frau und Kinder regelmäßig zu Ärzten ins benachbarte Thailand begleiten, seine Verurteilung in Schweden rechtfertige nicht den Entzug seines Ausweises. Er hatte damit Erfolg – Anfang Oktober bekam der 34-Jährige die Papiere zurück . 
Nicht ganz so gut sieht es für Gottfrid Svartholm Warg aus. Der Programmierer wurde kürzlich in seiner Wahlheimat Kambodscha festgenommen. Er soll an einem Hack gegen das schwedische Unternehmen Logica beteiligt gewesen sein, bei dem mehrere Tausend Steuernummern öffentlich gemacht wurden. Zwar hat Kambodscha kein Auslieferungsabkommen mit Schweden, doch Wargs abgelaufenes Visum führte zu seiner Abschiebung . Seit Mitte September sitzt Svartholm Warg in schwedischer Untersuchungshaft. 
Peter Sunde dagegen sucht sein Glück in der Offensive. Der eloquente Sprecher lebt inzwischen als IT-Berater in Berlin und besucht regelmäßig Konferenzen in Europa . Entsprechend argwöhnisch beobachten die Kläger, allen voran Film- und Musikverlage, seine Finanzen. Schließlich schuldet er ihnen Ersatzzahlungen in Millionenhöhe. Sunde bestreitet, bei den von ihm gegründeten Bezahl-Diensten Flattr und Kvittar angestellt zu sein, auch wenn er offenbar über Drittfirmen Anteile besitzt. Aber prinzipiell sei er nicht bereit, auch nur einen Cent zu zahlen. Bei einem jüngeren Gnadengesuch bezeichnete er das Verfahren gegen Pirate Bay und seine Person einmal mehr als unrechtmäßig. 
Viel Erfolg dürften Sunde und seine früheren Mitstreiter mit ihren Plädoyers allerdings nicht haben.  ""Aus rechtlicher Sicht ist das Spiel für die drei gelaufen "", sagt der schwedische Rechtsprofessor Mårten Schultz ,  ""das Urteil ist gefallen und früher oder später werden sie ihre Haftstrafen antreten müssen. "" 
Es wäre eine späte Genugtuung für die Kläger und gleichzeitig kaum mehr als ein Pyrrhussieg: Denn ihr ursprüngliches Angriffsziel, Pirate Bay mit all ihren Inhalten, wird es weiter geben. Und mit dem Umzug in die Cloud ist sie besser geschützt denn je."	technik
"Der Chaos Communication Congress (C3) hat sich entspannt. Das jährliche Treffen von Dateninteressierten und Hackern aus aller Welt fand bislang im Berliner Congress Centrum (BCC) statt, das nur mit Mühe die bis zu 4.000 Menschen fasste. Nun ist der Kongress des Chaos Computer Clubs nach Hamburg gezogen. Schätzungsweise 6.000 Besucher sind gekommen und trotzdem muss niemand Atemnot fürchten. 
Das WLAN läuft stabil, es gibt genug Sitzmöglichkeiten und sogar genug Steckdosen und Toiletten für alle. Entsprechend gelöst ist die Atmosphäre. In Berlin hatte die Veranstaltung immer etwas Wuseliges, zu viele drängten sich in den zwei Etagen des Kongresszentrums am Alexanderplatz und versuchten, noch in einen der überfüllten Säle zu kommen. In Hamburg wird eher flaniert. 
Überall sitzen kleine Grüppchen, plaudern, arbeiten, basteln, surfen. Die sogenannten Hackcenter allein bieten nun auf drei Etagen Platz für unzählige Projekte, von Lichtinstallationen über Quadrocopter bis hin zu Workshops wie Arduino für absolute Anfänger. 
Umzug geglückt 
Im Club ist man über den gelungenen Neustart erleichtert, der Plan war umstritten.  ""Ich war ja nicht so für den Umzug "", sagt Felix von Leitner, der vor allem unter seinem Avatar fefe bekannt ist.  ""Aber der Kongress hier ist gut. Im BCC herrschte einfach Platzmangel. Hier verläuft sich das. Und das Hackcenter riecht auch nicht mehr wie ein Pumakäfig, weil die Leute nicht mehr so eng aufeinander hocken. "" 
Nicht, dass die Stimmung in Berlin je aggressiv gewesen wäre. Nerds sind eher friedvolle Mitbürger. Die Hamburger Polizei beispielsweise stuft das Gefährdungspotenzial der Veranstaltung wohl noch unter der eines Kirchentages ein . Aber die Freude an den neuen, sehr viel größeren Räumen ist vielen anzumerken. Auch wenn zwei der drei Säle ab und zu dann doch überfüllt waren. 
Es ist das zweite Mal, dass der Club umziehen musste. Schon von 1984 bis 1997 wurde der Chaos Communication Congress in Hamburg veranstaltet. Die Stadt ist der Ursprung des Clubs, dort begann seine Geschichte. Doch das Eidelstedter Bürgerhaus, wo der C3 seinen Anfang nahm, wurde zu klein, genau wie nun das BCC. Denn der Einfluss des Clubs und das Interesse an ihm sind gewachsen. 
Und sie wachsen weiter. 2012 war ein gutes Jahr für den Club, er hat für seine Projekte viel Aufmerksamkeit und Lob bekommen. Vor Kurzem erst waren mehrere Mitglieder mal wieder vom Bundesverfassungsgericht als Gutachter geladen. Sie sollten ihre Einschätzung zur Antiterrordatei abgeben . 
Nicht alle sind glücklich damit. Einige von denen, die sich vor allem für das Basteln an Geräten und Software interessieren, sehen das wachsende politische Engagement mit Skepsis. Doch es sind letztlich beide Fähigkeiten – technischer Sachverstand und politischer Einsatz für Datenschutz und Bürgerrechte – die den Chaos Computer Club (CCC) für viele der Besucher interessant machen. Und es waren viele gekommen, viele zum ersten Mal. 
 
 ""Der Umzug war die beste Entscheidung überhaupt "", sagt Constanze Kurz, eine der Sprecherinnen des CCC.  ""Wir haben genug Platz für alle, wir müssen keinen mehr wegschicken. "" In den vergangenen Jahren wurden die Eintrittskarten in einer Art Verlosung verkauft und waren binnen Minuten vergeben. Zum Ärger derjenigen, die draußen bleiben mussten. 
Doch ging es bei dem Umzug nicht nur um mehr Platz.  ""Es musste eine Veränderung her, ich betrachte es als eine Art Häutung "", sagt Kurz. An seinem alten Ort habe der Kongress viel zu reibungslos funktioniert, man habe etwas Neues ausprobieren wollen, denn es habe keine Überraschungen mehr gegeben, steht im offiziellen Blog des C3 . 
Angst vor Veränderung sei völlig normal, sagt von Leitner und dass die Organisation an neuem Ort mit neuen Leuten auch hätte  ""total nach hinten losgehen können. Aber das Flair kommt von den Leuten, und es war relativ schnell klar, dass die auch nach Hamburg kommen. "" 
Initiative gegen Diskriminierung und Belästigung 
Eine Überraschung zumindest gab es. Eine neue Gruppe interessiert sich offensichtlich für den CCC: Frauen. Sie sind eine Minderheit bei der C3, noch zumindest. Und das liegt nicht unbedingt an den Frauen selbst, sondern vor allem an den Männern. 
Frauen und Informatik ist ein Thema, vor allem in Deutschland. Das belegte ein Vortrag von Britta Schinzel . Die emeritierte Professorin hat sich lange mit dem Geschlechterverhältnis in ihrem Fachgebiet beschäftigt. An Beispielen wie der Universität Rostock und Carnegie Mellon zeigte sie, dass nur in Deutschland Frauen die Informatik meiden. In anderen Ländern sind mindestens die Hälfte der für Informatik Immatrikulierten weiblich. In der DDR waren es sogar sehr viel mehr. Ende der achtziger Jahre waren es an der Uni Rostock 65 Prozent, nach der Wende fiel dieser Wert auf nur noch acht Prozent . 
Die Ursache ist laut einer von Schinzel zitierten DFG-Studie das Frauenbild der Männer. Sie würden Informatikerinnen nicht als richtige Frauen und nicht als richtige Informatikerinnen wahrnehmen und sie so abschrecken. 
Auf dieses Problem wurde beim C3 nicht nur mit einem Vortrag aufmerksam gemacht. Schon immer betrachtete sich der CCC als Verein, der für jede Lebensform offen ist. Für den Kongress in Hamburg wurde zusätzlich eine Anti-Harassment-Policy verabschiedet – das Versprechen also, gegen jede Bedrohung, Belästigung und Diskriminierung vorzugehen. 
Gruppen wie die Ada Initiative kritisieren, dass bei den oft männlich dominierten Hackerkonferenzen vor allem in den USA regelmäßig Frauen belästigt würden . Um auf das Problem aufmerksam zu machen, verteilten einige Aktivisten und Aktivistinnen grüne, gelbe und rote Karten auf dem C3. Wer sich belästigt fühle, solle dem Belästiger eine gelbe oder rote Karte übergeben, so die Idee. Da das eher symbolischen Charakter hat, wurde auch ein Notruftelefon eingerichtet. 
Die Karten verursachten etwas Verwirrung und wurden von einigen Teilnehmern offensichtlich nicht sonderlich ernst genommen. Doch sie sorgten immerhin für Aufmerksamkeit für das Problem. Auf dem C3 ist der Frauenanteil vergleichsweise gering, Gruppen im CCC wie die Häcksen konnten daran bislang wenig ändern. Vielleicht fühlen sich Frauen künftig weniger abgestoßen. Dem Wachstum des Kongresses und der Hackerszene kann es nur nützen. 
Der Autor hat auf dem 29C3 unentgeltlich einen Vortrag gehalten."	technik
"Das schwedische Parlament hat am Mittwoch mit großer Mehrheit beschlossen, die EU-Richtlinie zur Vorratsdatenspeicherung umzusetzen. Abgeordnete der bürgerlichen Regierungsparteien und der Sozialdemokraten haben dafür gesorgt. 
Der Abstimmung war eine jahrelange, oft leidenschaftliche Debatte über Datenschutz und Demokratie in der schwedischen Öffentlichkeit vorausgegangen. Die Regierung hat lange mit der Umsetzung gezögert. Schon im Februar 2010 hatte der Europäische Gerichtshof – nach einer Klage der EU-Kommission – festgestellt, dass Schweden gegen den EU-Vertrag verstößt. Eine solche Klage droht auch Deutschland , wenn es die Richtlinie nicht bald umsetzt. 
Schweden riskierte mit der Nichteinführung der Vorratsdatenspeicherung eine Geldstrafe: Die Kommission forderte eine Strafzahlung in Höhe von 9.597 Euro pro Tag nach dem Urteil im EU-Gericht. Dazu braucht es aber ein weiteres gerichtliches Urteil. Kritiker der Datensammlung finden, so viel sollte den Schweden die Abwendung des Überwachungsstaats wert sein. 
Im März 2011 verzögerte eine Minderheit im Parlament die Abstimmung trotz des EuGH-Urteils um ein weiteres Jahr. Sie beriefen sich dabei auf eine Verfassungsbestimmung, laut der die Verabschiedung eines Gesetzes aufgeschoben werden muss, wenn ein Sechstel der Abgeordneten gegen dieses Gesetz ist. 
Was Vorratsdaten über uns verraten: 
 
Am Mittwoch aber ist diese Frist abgelaufen. Die Debatte im Parlament war dann relativ kurz und wurde von nur wenigen Abgeordneten verfolgt – die Mehrheit war schon vor der Abstimmung sicher. In der Debatte verwies die Sozialdemokratin Elin Lundgren auf die tagesaktuelle Situation in Frankreich : Die Polizei sei dem mutmaßlichen Mörder von Toulouse durch dessen Internet-Aktivitäten auf die Spur gekommen. 
Schweden wird die Verkehrsdaten seiner Bürger künftig für sechs Monate speichern, die Mindestlänge in der Richtlinie. Die Umsetzung in Schweden soll aber in einigen Punkten weitergehen, als sie müsste. So sollen auch Daten zu nicht beantworteten Telefonanrufen gespeichert werden. Das entsprechende Gesetz soll ab dem 1. Mai gelten. Die Regierung soll jetzt Vorschriften zu den technischen Details erarbeiten, und auch diese müssen vom Parlament gebilligt werden. 
Einige schwedische Politiker hoffen, dass die Datenspeicherung durch kommende Entwicklungen in der EU wieder aufgehoben werden kann. So fordern die liberalen Parteien in Europa , die Umsetzung der Richtlinie für die Mitgliedsstaaten freiwillig zu machen. Das jedenfalls sagte Johan Linander, Sprecher der Zentrumspartei, der Zeitung Dagens Nyheter . 
 
Diesen Vorschlag lehnt die EU-Kommission ab.  ""Nein, es gibt keinen Grund, darauf zu hoffen. Wir können keine Gesetzgebung à la carte haben, so funktioniert es nicht "", sagte Maria Åsenius, eine Mitarbeiterin der EU-Kommissarin Cecilia Malmström . Die Mitgliedsländer hätten keine Lust, die Richtlinie zu streichen, sagt Åsenius. 
Malmström arbeitet seit längerer Zeit an einer Überarbeitung der Richtlinie. Ihre für den Sommer erwarteten Vorschläge sollen laut Åsenius  ""den Schutz der persönlichen Integrität stärken "". Die Neuauflage der Richtlinie solle präziser definieren, was als  ""schweres Verbrechen "" gelten soll – die Bedingung dafür, dass die Polizei Daten ausgeliefert bekommt. Die Kommission plane zudem, die Mindestspeicherzeiten zu verkürzen. 
In Schweden wird der Beschluss des Parlaments die Kritiker der verdachtsunabhängigen Speicherung von Telekommunikationsdaten kaum stoppen. Die Jugendorganisation der Linkspartei bietet jetzt  ""digitale Selbstverteidigungskurse "" an. Von Peter Sunde , dem bekannten Gründer von The Pirate Bay und Flattr, sollen die Teilnehmer lernen, wie sie kommunizieren, ohne digitale Spuren zu hinterlassen ."	technik
"Die Vorratsdatenspeicherung ist in Deutschland derzeit ausgesetzt, und doch speichern Telekommunikationsunternehmen Unmengen an Daten ihrer Kunden, oft für viele Monate . Manche davon benötigen sie zur Abrechnung und für den Betrieb der Dienste. Doch braucht es dazu längst nicht alle, die gespeichert werden und es braucht sie meistens auch längst nicht so lange. Der Bundesdatenschutzbeauftragte Peter Schaar hat nun gemeinsam mit der Bundesnetzagentur einen Leitfaden veröffentlicht , nach dessen Vorgaben die Telefonfirmen künftig nur noch speichern sollen, was notwendig ist. 
Bei Kontrollen würden Datenschützern immer wieder Firmen auffallen, so heißt es in einer Erklärung Schaars, die Verkehrsdaten zu lange speicherten, weil die gesetzlichen Regelungen zu großzügig ausgelegt würden. Wie viele Daten dabei aufbewahrt werden, hatte eine Studie der Bundesnetzagentur gezeigt , die im Juni vom Arbeitskreis Vorratsdatenspeicherung (AK Vorrat) veröffentlicht worden war . 
Die entsprechenden Gesetze sind nicht klar formuliert und legen nicht eindeutig fest, welche Daten der Kunden wie lange gespeichert werden dürfen. Im Telekommunikationsgesetz (TKG) heißt es hauptsächlich, Verkehrsdaten dürften  ""im erforderlichen Maß und im dazu erforderlichen Zeitraum "" verwendet werden. Allerdings ist Erforderlichkeit rechtlich nicht eindeutig definiert. 
 
Schaar hat daher für jede Datenkategorie angegeben, was wie lange maximal gespeichert werden darf: 
Das Ganze ist auch insofern relevant, als sämtliche dieser Daten für Ermittler interessant sind. Zum Thema Vorratsdatenspeicherung und Telekommunikationsüberwachung enthält der Leitfaden einen kurzen Absatz. Die Essenz: Ermittler und Geheimdienste dürfen ausschließlich die Daten bekommen, die entsprechend der obigen Kriterien rechtmäßig gespeichert wurden. Mehr als diese dürfen sie nicht abgreifen. 
 
Der bereits erwähnte AK Vorrat jedoch ist mit Schaars Empfehlung nicht einverstanden. Die dort aufgeführten Fristen seien zu lang, findet die Vereinigung.  ""Sensible Daten wie etwa Funkzellen sollen überhaupt nicht erhoben werden, wenn dies nicht für ortsgebundene Tarife notwendig ist "", heißt es in einer schriftlichen Erklärung . 
Die Anbieter erheben und speichern die sogenannte Cell-ID mit der Begründung, dies sei notwendig, um Dienste wie beispielsweise eine Home-Zone abzurechnen. Bei denen erhalten die Kunden an einem bestimmten Ort verbilligte Tarife. Doch nutzen längst nicht alle Kunden solche ortsgebundenen Angebote. Und selbst wenn sie es tun, muss nicht jede Funkzelle gespeichert werden, bei der sich ein Mobiltelefon einloggt. Auch die Speicherung der Gerätekennung IMSI lehnt der AK Vorrat ab. Mit ihr ist es möglich, ein Mobiltelefon auch dann wiederzuerkennen, wenn die SIM-Karte gewechselt wurde. Für die Abrechnung oder den Betrieb wird diese Nummer aber nicht zwingend gebraucht. 
Nicht rechtsverbindlich 
Die Grünen kritisieren einen anderen Aspekt an Schaars Konzept. Das Instrument des  ""rechtlich nicht verbindlichen Leitfadens "" zeige,  ""wie zurückhaltend die Datenschutzaufsicht in Sachen Telekommunikation in Deutschland verläuft "", sagte Konstantin von Notz, der innen- und netzpolitische Sprecher der Grünenfraktion. 
Das Blog Netzpolitik kritisiert , dass auch die von Schaar gewählten Speicherfristen teilweise zu lang sind. Zitat:  ""So wird es 'in der Regel als ausreichend angesehen', Rufnummern von Anrufer und Angerufenen bis drei Monate nach Rechnungsversand zu speichern. Dass Vodafone diese bisher nur für eine Woche speichert, zeigt, dass es auch mit weniger geht. "" 
Notz appelliert an alle Bürgerinnen und Bürger,  ""ihre Vertragsabschlüsse im Bereich Telekommunikation ganz bewusst auch davon abhängig zu machen, dass Provider mit ihren Daten, speziell ihren Verkehrsdaten, nachweislich datenvermeidend und datensparsam umgehen "". 
Der Leitfaden von Peter Schaar wäre natürlich in dem Moment hinfällig, in dem das Gesetz zur Vorratsdatenspeicherung wieder in Kraft tritt."	technik
"Sie suchten doch braune Wanderschuhe? Oder vielleicht einen passenden Rucksack? Eine praktische Regenjacke? – Man könnte es das Zalando-Gefühl nennen. Kaum hat man ein wenig ziellos in einem Online-Shop wie eben Zalando gestöbert, schon scheint das ganze Internet aus persönlichen Werbebannern zu bestehen. Und die zeigen immer genau die Produkte, für die man sich kurz vorher interessiert hatte. Gegenwehr ist möglich, aber mitunter aufwendig. 
Re-Targeting heißt das im Fachjargon der Werbebranche. Gemeint ist, dass dem potenziellen Kunden nach einer bereits erfolgten Suche immer wieder das vermeintliche Objekt seiner Begierde unter die Nase gehalten wird. Es funktioniert, indem Shops und Suchmaschinen Cookies in den Browser des Nutzers setzen, also kleine Datenkrümel, anhand derer der Kunde später wiedererkannt werden kann. Dass die Werbewirtschaft mit diesen Krümeln nicht knauserig ist, zeigen Programme wie Ghostery , die alle Cookie-Annäherungsversuche von sogenannten Drittanbietern sichtbar machen. 
Das Ergebnis ist beeindruckend: Schon nach einer Stunde normalem Alltags-Surfen – ein paar Newsseiten, bei Buchhändlern, Suchmaschinen, Shops, Blogs – hätte sich der Browser Dutzende neuer Cookies eingefangen. Auch ZEIT ONLINE verwendet Cookies, wie in unserer Datenschutzerklärung beschrieben . Etliche der im Selbsttest entdeckten Cookies gehören zu Facebook und Google , andere zu Dienstleistern und Marketingfirmen wie Webtrekk oder nugg.ad, das der Deutschen Post DHL gehört. Was wollen die alle? 
 ""Für die Nutzer ist es fast unmöglich "", sagt Dan Auerbach von der Electronic Frontier Foundation (EFF),  ""überhaupt noch zu durchschauen, welche Cookies wofür benötigt werden. "" Zwar lassen sich die Cookies im Browser mit wenigen Klicks löschen, aber was genau sie wem bereits mitgeteilt haben oder welche Super-Cookies im Hintergrund trotzdem weiter die Wege des Nutzers durchs Netz protokollieren, bleibt völlig undurchsichtig. Die EFF, die sich nicht nur in den USA für Verbraucherrechte einsetzt, fordert deshalb, dass Browser die Privatsphäre besser schützen sollten, zum Beispiel durch den Do-Not-Track-Modus , der Webseiten mitteilt, dass der Nutzer nicht verfolgt werden möchte. 
Kleine Textdateien, in denen in erster Linie Datum und Uhrzeit gespeichert werden, wann ein Nutzer eine Seite besucht hat. Besucht er diese Seite erneut und hat den Cookie bis dahin nicht gelöscht, wird er von dem Server der Seite ausgelesen und verrät damit über längere Zeiträume, wie oft jemand wiederkommt. Notwendig sind sie beispielsweise bei Onlineshops, da nur so sicher gestellt werden kann, dass der Nutzer, der auf Seite A die Bestellung eingab der gleiche ist, der anschließend auf Seite B seine Bezahlinformationen eingibt. 
Dateien, die Angaben über den Rechner eines Nutzers enthalten und von Seiten mit Flash-Animationen erzeugt werden. Während klassische Cookies nur für einen bestimmten Browser wie Firefox gelten, sind Flash-Cookies browserunabhängig. Außerdem sind sie in der Größe nicht begrenzt. Normale Cookies können nur vier Kilobyte groß sein, für Flash-Cookies gilt diese Grenze nicht, weswegen sie sehr viel mehr Informationen übertragen können. Sie können nur über den Adobe-Einstellungsmanager oder über spezielle Programme und Add-ons wie Better Privacy gelöscht werden. 
Eine besonders hartnäckige Cookie-Version sind die sogenannten Zombie-Cookies oder Evercookies. Sie kopieren sich an mehrere Stellen, auch in die Ordner für Flash-Cookies. Um sie zu entfernen, müssen sie in allen Ordnern gleichzeitig gelöscht werden. Andernfalls verteilen sie sich beim nächsten Aufruf der Website, von der sie stammen, wieder in alle vorherigen Ordner. 
Auch Zählpixel genannt, sind in Webseiten oder E-Mails eingebettete Bilddateien, die nur ein Pixel groß sind. Über die Kommunikationsdaten des Internetprotokolls verrät ihr Aufruf Uhrzeit, Browser, Betriebssystem und IP-Adresse des Nutzers. Hilfsprogramme können sie sichtbar machen oder gänzlich blockieren. 
Die permanenten Cookies lassen sich in zwei Gruppen unterteilen, in Erstanbieter- und Drittanbieter-Cookies: Erstanbieter-Cookies stammen von der Website, die ein Nutzer selbst angesteuert hat. Drittanbieter-Cookies werden von den Anzeigenkunden dieser Seite gesetzt. Und zwar auch ohne dass der User die Anzeige überhaupt angeklickt hat. 
Den deutschen Datenschützern geht das nicht weit genug. Ihnen sind alle Cookies, die nicht lediglich dem reibungslosen Funktionieren einer Website dienen, ein Dorn im Auge. Die Unternehmen hätten sich längst angewöhnt, die Nutzer mit ausgefeilten Methoden systematisch auszuspionieren. Das fange bei Zählpixeln an und höre bei Predictive Behavioral Targeting (also dem zwecks Kaufprognose geschickten Kombinieren von Einzelnutzerdaten mit Umfragen und Registrierungsdaten) auf.  ""Das Ergebnis sind Nutzerprofile, die zur gezielten Ansprache dienen oder lukrativ an Dritte verkauft werden "", stellte der Verbraucherzentrale Bundesverband (vzbv) kürzlich noch einmal klar. 
Der Verband drängt deshalb darauf, dass die EU-Richtlinie 2009/136/EG endlich in Deutschland umgesetzt und das Tracking strenger reguliert wird. Zukünftig solle nicht mehr opt-out gelten, sondern opt-in . Dass die Bundesregierung das mit Hinweis auf den ohnehin strengen deutschen Datenschutz bislang ablehnt, findet vzbv-Referentin Michaela Zinke kein hinreichendes Argument:  ""Die EU-Richtlinie sagt ganz klar, dass es eine Einwilligung geben muss, bevor ein Cookie zur Profilbildung gesetzt werden darf. "" 
Genau hier beginnt die Haarspalterei. Wer sagt überhaupt, dass die durch Cookies übermittelten Informationen  ""personengebundene Daten "" seien, argumentiert die Werbewirtschaft. Alexander Gösswein, Managing Director bei der Agentur Criteo, die für Onlineshops personalisiertes Re-Targeting umsetzt, betont, dass seine Firma  ""keinerlei Rückschlüsse auf die Person hinter einem bestimmten Browser "" ziehen könne.  ""Weder Name, Wohnort, Geschlecht oder Interessen noch sonst irgendwelche personenbezogenen Daten werden von uns gespeichert oder verwendet. "" Außerdem lassen sich die personalisierten Criteo-Anzeigen sehr einfach deaktivieren, eine kleine Infoschaltfläche, die in jedes Banner eingebaut ist, macht es möglich.  ""Wir sind stets ein Vorreiter in Sachen Transparenz und Datenschutz gewesen "", sagt Gösswein. 
 
Dass nicht neue staatliche Regularien, sondern transparente Praktiken die Lösung sind, betonen auch andere in der Branche.  ""Unsere Analysemethoden gehen mit dem deutschen Datenschutz konform "", sagt Alexander Schreiber, Produktmanager bei Mindlab, einer Software-Firma aus Stuttgart , die sich auf Webanalyse spezialisiert hat. Das Cookie ist dabei nur ein Baustein, Mindlab könnte notfalls auch ohne auskommen.  ""Wir haben ein patentiertes Verfahren entwickelt "", erklärt Schreiber,  ""bei dem eine Software-Komponente vor den Webserver geschaltet wird, der den Content bereit stellt "". Die Software schreibt den gesamten eingehenden und ausgehenden Datenstrom mit und wertet ihn aus, ohne dass IP-Adressen gespeichert oder Cookies gesetzt werden. Stattdessen setzt Mindlab eine sogenannte Reverse-Proxy-Technologie und das  ""URL-Rewriting "" ein, bei der die Session eines einzelnen Nutzers direkt in den URL eingeschrieben wird. 
Dass es dennoch Grauzonen und fragwürdige Praktiken gibt, bestreiten auch Werbefachleute nicht. Nugg.ad-Geschäftsführer Stephan Noller, der außerdem bei der EU in Brüssel die Interessen der Internetwirtschaft vertritt, hält das Cookie grundsätzlich für ein  ""spannendes Instrument, auch weil es der User kontrollieren und löschen kann "". Die im Verborgenen agierenden Super-Cookies oder Flash-Cookies dagegen seien inakzeptabel.  ""Deshalb hat sich der Dachverband der europäischen Internetindustrie IAB davon auch klar distanziert. "" 
Trotzdem, räumt er ein, gäbe es die Problematik der unerlaubten Datenverknüpfung und möglichen Profilbildung im Zusammenhang mit Kundentracking.  ""Die gemeinsame Aufgabe von Politik, Datenschützern und Industrie "", meint Noller,  ""wird darin bestehen, die Grenzen dessen, was erlaubt ist, sehr genau zu beschreiben "". Und die Gesetze müssten dann für alle gelten, für europäische Unternehmen genauso wie für amerikanische. 
Nicht nur über Cookies ist ein Wiedererkennen möglich 
In den USA ist die Tracking-Debatte mittlerweile in der Öffentlichkeit angekommen. Erst vor einigen Wochen hatte Google Schlagzeilen gemacht, weil es das Drittanbieter-Cookie-Verbot von Apple im Safari-Browser zu umgehen versuchte.  ""Einerseits gibt es immensen Druck im Markt, immer mehr Daten über die Kunden zu sammeln "", sagt Dan Auerbach.  ""Andererseits fürchten Firmen wie Google durchaus, dass Anti-Tracking-Gesetze verabschiedet werden könnten. "" 
Dabei geht es nicht allein um Cookies. Die EFF hat vor einiger Zeit schon darauf hingewiesen, dass die Wiedererkennung eines einzelnen Browsers auch anhand eines  ""Browser-Fingerabdrucks "" möglich ist, allein aus den zahlreichen Infoschnipseln, die der Browser bei jedem Seitenaufruf mitliefert.  ""Es ist schwer zu sagen, was genau welche Werbenetzwerke tatsächlich auswerten, aber die technischen Möglichkeiten gibt es auf jeden Fall. "" 
Das Internet sei ein datengetriebenes Medium, sagt Lobbyist Noller,  ""und alle funktionierenden Geschäftsmodelle haben mit Werbung zu tun "".  ""Für werbetreibende Unternehmen "", ergänzt Manager Gösswein,  ""ist es dabei natürlich wichtig zu wissen, dass sie ihre Zielgruppe mit ihrer Botschaft auch erreichen können "". Nur unter diesen Umständen seien sie bereit, weiterhin in Online-Werbung zu investieren.  ""Dem Nutzer muss klar sein, dass er mit Daten zahlt "", sagt auch Verbraucherschützerin Michaela Zinke.  ""Aber ich möchte entscheiden können, ob ich den Preis, den ich zahlen soll, noch angemessen finde. "" 
In dieser Legislaturperiode sei vermutlich nicht mehr mit einem Cookie-Gesetz zu rechnen, meint Zinke. Vorerst bleibt dem Verbraucher damit nur der Selbstschutz. Möglichkeiten gibt es: Browsereinstellungen ändern, Plug-ins nutzen, Säuberungsprogramme wie BetterPrivacy installieren. Die TU Berlin hat außerdem eine Cookie-Suchmaschine aufgesetzt, dort kann man die Adresse einer Website eingeben und sich anzeigen lassen, wie viele Cookies diese Seite setzen will. Der Test mit ZEIT ONLINE ergibt: 1 Erstanbieter-, 5 Drittanbieter-Cookies. Bei Zalando sind es 5 Erstanbieter- und stolze 45 Drittanbieter-Cookies."	technik
"Die deutschen Geheimdienste filtern Millionen E-Mails nach verdächtigen Begriffen und unterliegen dabei weder der Kontrolle von Richtern noch von Datenschützern. Das sagte eine Sprecherin des Bundesdatenschützers Peter Schaar auf Anfrage von golem.de . Grundlage der E-Mail-Überwachung nach Suchbegriffen ist das G-10-Gesetz , in dem die Befugnisse der Geheimdienste zum Eingriff in das nach Artikel 10 des Grundgesetzes garantierte Brief-, Post- und Fernmeldegeheimnis geregelt ist. 
Die Sprecherin erklärte:  ""Die rechtliche Grundlage für eine solche strategische Telekommunikationsüberwachung finden Sie in den Paragrafen 5 und folgende des G-10-Gesetzes. Die Kontrolle obliegt dem Parlamentarischen Kontrollgremium des Deutschen Bundestags. "" Der Bundesdatenschutzbeauftragte  ""besitzt hier keine Befugnisse "", sagte sie. 
Laut einem Bericht des Parlamentarischen Kontrollgremiums (PKG), haben die Geheimdienste Verfassungsschutz, Bundesnachrichtendienst und Militärischer Abschirmdienst (MAD) im Jahr 2010 die Inhalte von Millionen E-Mails durchsucht und dabei in über 37 Millionen elektronischen Nachrichten verdächtige Suchbegriffe gefunden. 
Die Versechsfachung gegenüber dem Vorjahr sei der Zunahme von Spam geschuldet, hieß es zur Begründung. Gesucht wurde nach rund 15.300 Begriffen aus den Bereichen Terrorismus, Massenvernichtungswaffen und Schleusung. In nur 213 Fällen ergaben sich durch die millionenfache E-Mail-Überwachung verwertbare Hinweise für die Geheimdienste. 
Ströbele: Kaum Bundesbürger betroffen 
Der Landesdatenschützer von Schleswig-Holstein , Thilo Weichert , sagte golem.de zum Thema Richtervorbehalt:  ""37 Millionen Richterbeschlüsse – das ginge auch nicht. Aber auch die Endselektion läuft nicht über einen Richter. Die Kontrolle obliegt vielmehr der sogenannten G-10-Kommission. "" Zwischen G-10-Kommission und PKG gebe es eine Arbeitsteilung zwischen Genehmigung und Kontrolle für die Filterung der E-Mails, erklärte Weichert. 
Laut Piratenpartei war bei den im Jahr 2010 verschickten 40 Milliarden E-Mails 2010, die kein Spam waren, etwa jede tausendste E-Mail betroffen. Piratenpartei-Chef Sebastian Nerz schreibt in seinem Blog :  ""Der Vorfall zeigt, dass deutsche Geheimdienste tun und lassen können, was sie wollen. Obwohl das Parlamentarische Kontrollgremium über diese Grundrechtsberaubung informiert war, hat es nichts dagegen getan. "" Die Geheimdienste betrieben eine verfassungswidrige elektronische Rasterfahndung, von der jeder Bürger betroffen sei. Nerz riet dazu, E-Mails zu verschlüsseln. 
Die Abgeordneten Michael Hartmann ( SPD ) und Christian Ströbele (Grüne) wiesen darauf hin, dass von der Maßnahme wohl kaum Bundesbürger oder deutsche Firmen betroffen sein dürften, da es vor allem um eine  ""strategische Überwachung der gebündelten Funkübertragung etwa über asiatischen oder afrikanischen Ländern "" gehe, wie Ströbele laut heise.de sagte. Falls doch Bundesbürger betroffen seien,  ""gilt für sie prinzipiell der Schutz des Grundgesetzes mit der Pflicht zur sofortigen Datenlöschung "". 
Es gehe um Überwachung von internationaler Kommunikation, bestätigte Hartmann, der die laufende Debatte deshalb als  ""Sturm im Wasserglas "" bezeichnete. Aus dem Bundeskanzleramt hatte es geheißen, die Überwachung diene gerade auch dem Schutz militärischer und ziviler deutscher Kräfte im Ausland sowie der Abwehr von terroristischen Gefahren im Inland. 
Erschienen bei golem.de , aktualisiert von ZEIT ONLINE"	technik
"Misstrauen ist das erste, was der Spieler in Antichamber lernt. Verlass dich auf nichts – selbst wenn du es mit eigenen Augen gesehen hast. Denn dort, wo du gerade noch gestanden hast, klafft im nächsten Moment ein Abgrund. Die Wand, die dir eben noch den Weg versperrte, ist mit einem Mal nicht mehr da. 
Wer Antichamber zum ersten Mal spielt, fühlt sich in einen bizarren Albtraum versetzt. Das Labyrinth, aus dem der Spieler entkommen muss, ist scheinbar willkürlichen Gesetzen unterworfen. Das Spiel zertrümmert systematisch alle Gewissheiten der herkömmlichen Welt, um dann etwas Neues auf diesen Trümmern zu bauen. Statt also ein Gerüst aus Regeln zu errichten, wie dies andere Spiele tun, trainiert Antichamber hypothetisches Denken und macht ganz nebenbei das Lernen selbst zum Thema. Der Spieler ist dabei das Kaninchen im Versuchslabor. 
Geschaffen hat diese Welt der Australier Alexander Bruce. Ursprünglich war Antichamber als reines Geometrie-Puzzle geplant, doch im Lauf der knapp siebenjährigen Entwicklungszeit fanden immer mehr psychologische Elemente den Weg in das Spiel. Für die Konzeption und die technische Umsetzung gewann Antichamber mehrere Preise, etwa bei der Penny Arcade Expo und dem Independent Games Festival. Nun ist es auf Steam für PC erhältlich. 
Als Inspirationsquelle nennt Alexander Bruce unter anderem die Werke des niederländischen Künstlers M.C. Escher. Der bediente sich bei der nicht-euklidischen Geometrie und ihren gekrümmten Räumen. 
Was nach schwergängiger Theorie klingt, ist im Spiel erstaunlich leichtfüßig umgesetzt. Der Spieler beginnt in einem dunklen, hohen und vollkommen leeren Raum. Aus dem hell erleuchteten Nebenraum, der durch eine Glasscheibe abgetrennt ist, lockt eine Tür mit der Aufschrift  ""Exit "". Sogleich fällt auch die Digitaluhr an der Wand auf, die erbarmungslos neunzig Minuten herunterzählt. Die Eile, in die man unfreiwillig gerät, ist allerdings nur der Vorgeschmack auf eine ganze Reihe Pawlowscher Reflexe, die Antichamber gezielt vorführt. Schon bald wird deutlich, dass neunzig Minuten knapp bemessen sind und nur künstlichen Druck erzeugen. Die meisten Spieler werden ein Vielfaches dieser Zeit benötigen, um den Ausgang zu erreichen. 
Der dunkle Raum vom Anfang dient in Antichamber als zentraler Knotenpunkt. Man kann sich dorthin per Escape-Taste zurückbeamen, wenn man einmal nicht mehr weiter weiß oder – was durchaus vorkommt – alle Rückwege verbaut hat. Der Spieler stirbt also nicht, sondern muss allenfalls von vorn beginnen. 
Im Ausgangsraum befindet sich auch die Karte des Labyrinths, die mit jedem Fortschritt weiter wächst. Die einzelnen Abschnitte lassen sich per Mausklick direkt ansteuern. Was auf der Karte nicht angezeigt wird, sind Höhenunterschiede und Hindernisse. Antichamber erfordert also, dass man sich die Eigenheiten der Räume gut einprägt; besonders auch deshalb, weil sich manche Rätsel über mehrere Kartensegmente erstrecken. 
 
Zu den beeindruckendsten Leistungen von Antichamber zählt, wie mühelos es gedankliche Werkzeuge vermittelt. Um nicht zu viel zu verraten, sei hier nur ein leichtes Rätsel aus dem frühen Spielverlauf erwähnt: Man betritt einen Raum, der augenscheinlich eine Sackgasse ist. In der Decke befindet sich eine Kachel mit der Aufschrift  ""Don't look down "". Folgt man der Anweisung und blickt nur nach oben, passiert – wie zu erwarten – nichts. Blickt man hingegen nach unten, löst sich der Zimmerboden auf, und man stürzt in einen tiefen Schacht. Was also tun? Wer Antichamber auch nur kurz gespielt hat, wird merken, dass sich Räume nicht nur durch Blicke, sondern auch durch Bewegung verändern lassen, und er wird daraus den richtigen Schluss ziehen. 
Spätestens beim Stichwort  ""Versuchslabor "" wird der eine oder andere an das Spiel Portal gedacht haben. Die Unterschiede zu dem höchst erfolgreichen Puzzle-Platformer der Entwickler von Valve sind allerdings groß. Portal und Portal 2 reihen Testkammern in weitgehend linearer Reihenfolge aneinander, während Antichamber einen in alle Richtungen wuchernden Irrgarten bietet. Darüber hinaus macht Portal gleich zu Beginn mit grundlegenden Spielmechaniken wie den Teleportern vertraut. In Antichamber hat der Spieler zu Beginn nichts als die eigene Beobachtungsgabe. Erst zu einem späteren Zeitpunkt erhalten Spieler ein Werkzeug, mit dem sie etwa Blöcke vor Schiebetüren und Lasern platzieren können. 
Hinweise erst nach dem Lösen eines Rätsels 
Antichamber unterscheidet sich aber noch in einem weiteren Punkt wesentlich von herkömmlichen Spielen: Es gibt keine Rahmenhandlung oder künstliche Intelligenz wie bei Portal. Stattdessen läuft der Spieler vollkommen allein in kargen Gängen und Schächten herum. Die Räume wechseln zwischen reinem Weiß und stechenden Schockfarben. Verstärkt wird das Gefühl der Verlorenheit noch von dem Soundtrack, der mal aus sphärenhaftem Rauschen, mal aus Vogelstimmen besteht. 
Der einzige Weg, auf dem Antichamber direkt mit uns kommuniziert, sind Wandtafeln mit Zeichnungen und Sprüchen wie  ""Mastering a skill requires practice "" (Eine Fähigkeit zu erlangen, erfordert Übung) oder  ""To get past a problem, you might just need to keep pushing through it "" (etwa: Um ein Problem zu überwinden, musst du dich vielleicht einfach durchboxen). Die Sprüche wirken zunächst wie Plattitüden, oft enthalten sie jedoch Hinweise auf Lösungen. Bezeichnend für das Konzept von Antichamber ist, dass es die Tafeln meist nicht vor, sondern hinter den Rätseln platziert. Wissen gibt es in diesem Spiel nicht geschenkt. Die Tafeln dienen vor allem dazu, Rätselstrategien zu verinnerlichen. 
Die große Stärke von Antichamber ist zugleich auch seine große Schwäche: Manche Passagen sind so komplex geraten, dass sie einen für Stunden in Rätselstarre versetzen. Nicht jeder Spieler wird die Geduld aufbringen, die Rätsel ohne Zuhilfenahme von Anleitungen aus dem Netz zu meistern. Wer die Sache dennoch alleine durchzieht, wird mit großen Triumphmomenten belohnt. Antichamber strahlt eine konzeptionelle Frische aus, die es in anderen Games nur ganz selten gibt. Seine Vagheit mag verstören, seine Tricks und Kniffe verwirren. Und doch gehört dieses Labyrinth zu den Orten, die Spieler unbedingt besucht haben sollten."	technik
"Verbindungs- und Standortdaten von Telefon- und Internetnutzern müssen bis auf weiteres zur Strafverfolgung gespeichert werden. Mit der großen Mehrheit der Fraktionen von Union und SPD beschloss der Bundestag das Gesetz, das die Telekommunikationsunternehmen verpflichtet, Kommunikationsdaten ihrer Kunden zehn Wochen lang zu speichern. Standortdaten von Handys müssen vier Wochen lang aufgehoben werden. 
404 Abgeordnete stimmten dafür – 99 weniger als die Koalition Stimmen hat. 148 stimmten dagegen, 7 enthielten sich. Besonders in der mitregierenden SPD gibt es viele Kritiker der Datenspeicherung. Der Netzpolitiker Lars Klingbeil etwa stimmte mit Nein. 
Zur Vorratsdatenspeicherung gehören die Rufnummern der beteiligten Anschlüsse, Zeitpunkt und Dauer der Anrufe sowie die IP-Adressen von Computern. E-Mail-Verkehrsdaten sind ausgenommen. Wie kurz vor der Abstimmung bekannt wurde, speichern die Mobilfunkbetreiber auch die Inhalte von SMS-Nachrichten, weil sie sich nicht von den Stammdaten trennen lassen. 
Der Versuch der Regierung, die Daten zur Verfolgung schwerer Straftaten zu speichern, ist nicht der erste. Aufgrund einer Richtlinie der EU-Kommission hatte die Bundesregierung ein entsprechendes Gesetz erlassen, das 2010 vom Bundesverfassungsgericht gekippt wurde. Auch die Richtlinie wurde später vom Europäischen Gerichtshof für unzulässig erklärt. 
In der vorangegangenen Debatte im Bundestag lobten Vertreter der Koalitionsfraktionen das Gesetz als ausgewogen,  ""grundrechteschonend "", angemessen und verfassungskonform. Die Oppositionsfraktionen von Bündnis 90/Die Grünen und der Linken stimmten dagegen. Der Grünen-Netzpolitiker Konstantin von Notz kritisierte das Speichergesetz als erneuten  ""rechtsdogmatischen Dammbruch "". Da derzeit gar keine EU-Richtlinie zur Datenspeicherung existiert, sei das Gesetz unnötig. Er kündigte an, vor dem Bundesverfassungsgericht dagegen zu klagen. 
Auch Politiker von FDP und Piraten haben angekündigt, in Karlsruhe Beschwerde einzulegen. Zum Teil sind das dieselben Akteure, die auch erfolgreich gegen das erste Gesetz geklagt hatten. 
Zusätzliche Brisanz erhält das Vorhaben der Koalition durch ein weiteres Detail: Laut einem Bericht der Süddeutschen Zeitung werden bei SMS-Nachrichten entgegen der datenschutzrechtlichen Vorschriften auch die Inhalte der Kurznachrichten bei den Telekommunikationsunternehmen gespeichert. Allerdings würden sich die Kommunikationsunternehmen strafbar machen, wenn sie die Inhalte aufgrund des Vorratsdatengesetzes weitergäben. Sofern es also den Unternehmen nicht gelingt, die Daten zu trennen, dürfte die Information, wer wann wem eine SMS schickte, also zunächst nicht herausgegeben werden. Für ein Mitlesen oder -hören der Kommunikationsinhalte müssten sich die Ermittler von Polizei und Justiz durch Richter eine Überwachungsmaßnahme genehmigen lassen. 
Schon bisher speichern die Provider Verbindungsdaten mehrere Wochen lang, aber nur zum Zweck der Abrechnung und nicht so detailliert. 
"	technik
"Zwei Jahre ist es her, da offenbarte sich die dunkle Seite der Digitalisierung. Edward Snowden hatte seine Dokumente vorgelegt. Jeder kann jetzt wissen, wie leichtfertig Geheimdienste mit Bürgerrechten umgehen. Der Europäische Gerichtshof sprach seither mehrere maßgebliche Urteile zum Schutz digitaler Grundrechte. Einer, der von Amtswegen unsere digitale Zukunft gestalten soll, ist Günther Oettinger, EU-Kommissar für Digitale Wirtschaft und Gesellschaft. Grund genug, ihn zu fragen, wie das Netz künftig gestaltet und reguliert werden soll. 
ZEIT ONLINE: Herr Oettinger, Sie haben gerade die Chance, die Welt zu verändern. Werden Sie es tun? 
Oettinger: Wir wollen die digitale Welt europäisch gestalten und von den nationalen Regelwerken wegkommen hin zu einer europäischen Rechtsetzung. Und wir wollen die Wettbewerbsfähigkeit Europas entscheidend verbessern. 
ZEIT ONLINE: Das klingt reichlich passiv, wenn man bedenkt, in welcher Position Europa gerade ist. Beispiel Datenschutz: Das Safe-Harbor-Abkommen ist gefallen. Nun muss neu geregelt werden, welche Daten europäischer Bürger amerikanische Dienstleister wie Google oder Facebook verarbeiten dürfen. Gleichzeitig treten die Verhandlungen zur neuen Datenschutz-Grundverordnung in eine entscheidende Phase. Sie könnten also auch sagen: Europa wird Maßstäbe für die Welt setzen! 
Oettinger: Noch sind wir in einer defensiven Position. Wir haben in Europa 28 fragmentierte Datenschutzgesetze. Die Folge ist, dass sich so mancher große Dienstleister genau überlegt, wo der Datenschutz am einfachsten einzuhalten ist. Doch das wollen wir ändern und in die Offensive gehen. Dem dient die neue Datenschutzgrundverordnung. Sie schafft einen direkten und überall gleich anwendbaren Standard für immerhin 510 Millionen Menschen und für alle digitalen Dienstleister – Europäer wie Nicht-Europäer – die in diesem Binnenmarkt wirtschaften wollen. 
Ich könnte das natürlich viel pathetischer verkaufen. Aber wir müssen erst einmal liefern. In Zeiten der Renationalisierungstendenzen tut man gut daran, Schritt für Schritt vorzugehen, wenn man im Ministerrat Mehrheiten gewinnen will. Aber wir haben ein klares Ziel: Wir wollen eine Digitalunion und die Vollendung des digitalen Binnenmarktes. 
ZEIT ONLINE: An Standards, die für mehr als 500 Millionen Menschen gelten, kommt niemand mehr vorbei, der digital wirtschaften will. Ist das nicht die Chance, Regeln zu machen, denen die ganze Welt auf Jahre hin folgen muss, auch die großen amerikanischen Dienstleister? 
Oettinger: Für die nächsten ein oder zwei Jahrzehnte ist unsere Marktmacht sicherlich so, dass kein Big Player auf unseren Markt verzichten will. Klar ist aber umgekehrt: Die Interessen Europas auf einen Nenner zu bringen, ist gar nicht so einfach, auch im digitalen Sektor. 
Hier spiegelt sich das Dilemma Europas. Eine kleine Behörde in Kalifornien setzt gerade Standards dafür, wie viel Schadstoffe Dieselmotoren ausstoßen dürfen. Genauso könnte die EU bestimmen, wie auf der Welt mit privaten Daten umgegangen werden muss. Dem steht jedoch das starke Interesse auch der europäischen Industrie entgegen, die möglichst viele Daten frei erheben, nutzen und weitergeben will. Oettinger könnte mit seiner Zuständigkeit für digitale Wirtschaft und Gesellschaft solche Standards entwickeln. Aber er soll auch zwischen den Gegensätzen vermitteln, soll europäische Industrie und europäische Bürger gleichermaßen vertreten. 
ZEIT ONLINE: Ein digitaler Binnenmarkt braucht eine gut funktionierende Infrastruktur. Dazu gehört die Netzneutralität, also die Möglichkeit für alle Internetanbieter, jedes Datenpaket gleichberechtigt schnell zu übertragen, egal woher und von wem es stammt oder welchen Inhalt es hat. Sie halten die neue Verordnung für eine Garantie, dass alle Daten gleich behandelt werden. Kritiker weisen jedoch auf zahlreiche Unklarheiten hin, die die Neutralität aushebeln. 
Oettinger: Europäisches Recht ist nicht ganz so konkret wie ein Mustermietvertrag oder die Schneeräumpflicht von Kleinkleckersdorf. Aber wir sind der erste Kontinent, der einen offenen Zugang zum Internet und Diskriminierungsfreiheit gesetzlich regelt. In Europa hat man jetzt ein sehr hohes Maß an Planungssicherheit. 
ZEIT ONLINE: Die Deutsche Telekom hat sogleich Pläne gemacht mit ihrem Gedankenspiel, bei Start-ups für eine konkurrenzfähige Übertragungsqualität zu kassieren. Was haben Sie gedacht, als Sie davon erfuhren? 
Oettinger: Wenn im Netz niemand benachteiligt wird, also die Geschwindigkeit im offenen Netz gleich bleibt, dann kann die Telekom ruhig on top für Spezialdienste besondere Angebote machen, egal ob für Start-ups oder andere industrielle Anbieter. Dass die Geschwindigkeit nicht für andere gedrosselt wird, muss von der Bundesnetzagentur kontrolliert werden. 
ZEIT ONLINE: Noch ist unklar, wie genau diese Kontrolle ausgestaltet sein wird. Was erwarten Sie konkret von der Bundesnetzagentur und den anderen europäischen Regulierungsbehörden? 
Oettinger: Wir entwickeln gerade Leitlinien mit den für uns sehr wichtigen nationalen Regulierungsbehörden. Denn für die Marktkontrolle haben wir keinen eigenen Unterbau. Missbrauch und Rechtswidrigkeit müssen die Regulierer erkennen. 
ZEIT ONLINE: Wie stark wollen Sie diese Leitlinien selbst definieren? 
Oettinger: Darüber verhandeln wir jetzt. Ich will es so konkret wie möglich, aber ich brauche dafür alle 28 Kontrollbehörden, die wiederum die Unterstützung ihres jeweiligen Aufsichtsministeriums benötigen. Das ist ein dynamischer Prozess. Internetnutzer haben jetzt ja auch eine Klagemöglichkeit. Damit wird das Thema erstmals justiziabel. Ich erwarte, dass es Musterklagen geben wird. 
Wer hat Vorfahrt im Netz? Das ist eine der wichtigen Fragen, die gerade weltweit verhandelt werden. Die EU sucht nach einem Kompromiss, sie will niemanden benachteiligen und es trotzdem möglich machen, dass sich für Geld Vorteile kaufen lassen. Ob der Gesetzgeber eine taugliche Antwort auf dieses Dilemma gegeben hat, muss sich erst noch erweisen. Im Zweifel müssen das Gerichte übernehmen. 
ZEIT ONLINE: Der gleiche Gestaltungsspielraum, den Sie im Datenschutz haben, öffnet sich gerade auch beim Thema Sicherheit. Seit Edward Snowden wissen wir, was in der Arbeit der Geheimdienste schief läuft. Spätestens seit den Anschlägen von Paris ist aber auch klar, dass die Geheimdienste nicht so aufklären, wie es nötig wäre. Nochmal: Ist das nicht ein guter Zeitpunkt, um die Welt zu verändern? 
Oettinger: Es geht erst einmal um Grundvertrauen oder Grundmisstrauen. Deutsche Geheimdienste oder auch solche in London oder Paris sind Behörden, deren Mitarbeiter einen Amtseid auf ihr Land geschworen haben. Für mich ist der Geheimdienst im Auftrag des Gemeinwesens unterwegs, er ist nicht ein Gegner des Gemeinwesens. Mit diesem Grundvertrauen müssten wir daher zuerst definieren: Was dürfen die Geheimdienste, wo sind die Grenzen, wo ist die Privatsphäre zu achten und dürfen die Dienste auf Daten von Privatunternehmen zugreifen? 
Oettinger spielt darauf an, dass in den USA Unternehmen verpflichtet sind, ihre Nutzerdaten mit Geheimdiensten zu teilen. Für den Europäischen Gerichtshof war das ein Grund, um das Safe-Harbor-Abkommen über den Datenaustausch zwischen Europa und den USA für unwirksam zu erklären. 
ZEIT ONLINE: Wenn man bedenkt, was wir darüber erfahren haben, was Geheimdienste so tun, nicht nur amerikanische, sondern auch europäische, dann kann man allerdings Zweifel daran haben, ob diese Dienste tatsächlich zum Wohl des Gemeinwesens arbeiten. 
Oettinger: In Deutschland zweifelt man am Geheimdienst in London – aber die Regierung in London zweifelt nicht an ihm. 
ZEIT ONLINE: Die Regierungen zweifeln selten an Geheimdiensten, die Bürger schon eher. 
Oettinger: Aber die britischen nicht. Am deutschen Geheimdienstverständnis wird sich Europa nur eingeschränkt orientieren. Wir haben nun einmal eine Geschichte, in der die Privatsphäre in den Jahren 1933 bis 1945 und in der DDR schrecklich verletzt wurde. Diese Geschichte haben andere Länder nicht. In Großbritannien ist das Verständnis ein ganz anderes. Geheimdienste haben dort einen viel umfangreicheren Auftrag, mehr Instrumente, und auch einen größeren Rückhalt als bei uns. 
ZEIT ONLINE: Aber die Attentate von Paris haben die Dienste trotzdem nicht verhindern können. Da fragt man sich als deutscher Bürger schon, warum wir Standards wie in Großbritannien oder Frankreich akzeptieren sollten. 
 
Oettinger: Wir sind von außen betrachtet nicht in der Lage, zu überschauen, wie gut die Dienste in Frankreich gearbeitet haben. Klar ist nur, diese Debatte findet in Frankreich nicht statt. Deshalb rate ich dazu, sie nicht von Deutschland nach Frankreich zu exportieren. Wir werden bestimmte Datenschutzgrundsätze europäisieren, wir werden die digitale Privatsphäre im nächsten Jahr neu definieren. Aber für die Geheimdienstpraxis wird es so schnell keinen europäischen Standard geben. Das werde ich nicht mehr erleben. 
Ein typischer Reflex von Sicherheitspolitikern nach Ereignissen wie in Paris ist es, zu fordern, dass mehr Daten von Privatpersonen gespeichert und den Sicherheitsbehörden zugänglich gemacht werden. Diese Vorratsdatenspeicherung hatte die EU in einer eigenen Richtlinie festgeschrieben. Das Gesetz wurde jedoch vom Europäischen Gerichtshof mit dem Argument für ungültig erklärt, dass die Verhältnismäßigkeit zwischen den Sicherheitsinteressen des Staates und dem Schutz der Privatsphäre der Bürger nicht gewahrt worden sei. Den Nationalstaaten steht es nun frei, eigene Vorratsdatengesetze zu schaffen, so wie es Deutschland gerade tat. 
ZEIT ONLINE: Im Ministerrat gibt es nun wieder viele, die sich eine neue Richtlinie zur Vorratsdatenspeicherung wünschen, in der Kommission auch. Gehören Sie dazu? 
Oettinger: Wir hatten einen europäischen Text, der wurde vom Europäischen Gerichtshof verworfen. Die Kommission hat danach entschieden, abzuwarten und zu beobachten, wie einige Mitgliedsstaaten ihre Überzeugungen umsetzen. Das ist der Status quo. Im Jahresarbeitsprogramm der Kommission für 2016 ist eine neue Gesetzgebungsinitiative nicht enthalten. 
ZEIT ONLINE: Es mag also im Ministerrat diskutiert werden – in der Kommission aber nicht? 
Oettinger: Es kann sein, dass der Ministerrat sagt, jetzt haben einige Länder Erfahrungen mit eigenen Regeln gesammelt, jetzt wäre eine Harmonisierung angezeigt. Wenn der Rat die Kommission dann um einen Vorschlag bittet, wird die Kommission das sicherlich prüfen. Aktuell aber ist nichts geplant. 
Die Digitalisierung sei zu einem  ""Totalphänomen "" geworden, schrieb Bundesjustizminister Heiko Maas in dieser Woche in der ZEIT. Jedoch fände eine Technikgestaltung durch Recht kaum statt. Es stellt sich also die Frage, wie das Netz und seine Akteure demokratisch kontrolliert werden können. Martin Schulz, Präsident des Europäischen Parlaments, rief deshalb in der ZEIT dazu auf, eine Charta der digitalen Grundrechte zu schreiben. 
ZEIT ONLINE: Was halten Sie von der Idee einer digitalen Grundrechte-Charta, wie Martin Schulz sie vorschlägt? 
Oettinger: Wir haben Grundrechte, die stehen zum Beispiel im Vertrag von Lissabon. Es geht hier nicht um neue Grundrechte. Es geht nur um die Anwendung der bekannten Grundrechte in den digitalen Technologien. 
ZEIT ONLINE: Aber ändert die Digitalisierung nicht die Grundlage für diese Grundrechte, kommen nicht neue Felder hinzu? Über den Datenschutz haben die Väter des Grundgesetzes vor fast siebzig Jahren beispielsweise nicht nachgedacht. 
Oettinger: Das ist nur eine Frage der Quantität. Ein Finanzamt hat früher die Daten auch erhoben, aber in Papierakten hinterlegt. Jetzt werden sie digital erfasst. Der Unterschied ist, dass man jetzt sehr viel mehr Daten erheben kann. 
ZEIT ONLINE: Aber genau das ist ja das Problem, oder nicht? 
Oettinger: Nein, denn es ist ja weniger eine Frage der Qualität, sondern der Quantität. Früher stand der Aktenordner des Steuerzahlers im Finanzamt – heute haben wir eine elektronische Übertragung. Deshalb müssen wir regeln, welche Daten das Amt erheben und speichern darf. Aber das sind alles Fragen der Technik, nicht der Verfassung. Der Schutz der Privatsphäre, der Schutz des Eigentums ändert sich durch die digitalen Technologien in der Qualität nicht. 
ZEIT ONLINE: Trotzdem sagt niemand: Ich alleine habe das Recht an meinen Daten und darf bestimmen, was damit passiert. 
Oettinger: Aber es sind ja gar nicht alles Ihre Daten. Ihr Lebensalter, ob Sie verheiratet sind und welche Steuerklasse Sie haben, das sind nicht Ihre Daten. Weil Sie verpflichtet sind, sie dem Finanzamt zu nennen. Diese Daten sind Sache des Finanzamts. Wenn Sie eine Steuererklärung fälschen, können Sie sogar strafrechtlich verfolgt werden. Es gibt also Daten, die durch Sie entstehen, die aber diesen Behörden gehören. 
ZEIT ONLINE: Gibt es einen Unterschied zwischen Behörden und Privatunternehmen, die Daten erheben? 
Oettinger: Ja. Allerdings sind Unternehmen Vertragspartner, und deshalb haben sie auch Rechte, Informationen von Ihnen zu verlangen. Wenn Sie eine Wohnung mieten, hat der Vermieter auch das Recht zu wissen, wer Sie sind. Aber niemand verlangt, dass Sie einen Vertrag eingehen. Nur dem Staat müssen Sie sich unterordnen, er darf bestimmte Daten nach klaren Regeln sammeln, auch wenn Sie das nicht wollen. Deshalb haben Sie auch kein Recht darauf, dass diese Daten gelöscht werden. 
ZEIT ONLINE: Das stimmt nicht ganz. Denn es gibt ja Regeln, wann Akten vernichtet werden müssen. Meine Abiturarbeiten sind beispielsweise nach zehn Jahren zerstört worden. Bei vielen Privatunternehmen sind die Regelungen eher vage oder gar nicht vorhanden. 
Oettinger: Der Staat hat ein Recht, im Rahmen seiner Aufgaben Daten zu speichern. Die Privatwirtschaft muss sich an öffentliches Datenschutzrecht halten. Wenn also ein Recht auf Löschen kommt, dann muss sich auch Google daran halten. Sonst könnte es im schlimmsten Fall sogar bestraft werden und aus dem europäischen Markt fliegen. 
ZEIT ONLINE: Sie sagen also, für die Digitalisierung braucht man keinen neuen Katalog an grundlegenden Regeln? 
Oettinger: Ich sehe keinen Bedarf. Die Grundrechte bleiben gleich, die Menschenrechte auch. Die werden auch in dreißig Jahren noch aktuell bleiben. Es geht nur um die Anwendung in einem sich täglich verändernden technischen Feld. Die Einführung des Computers war auch eine Revolution, aber niemand kam auf die Idee, ein Computer-Grundgesetz zu schaffen. 
Das stimmt nicht ganz. Es gibt ein Grundsatzurteil, das umgangssprachlich als Computer-Grundrecht bezeichnet wird. Das Grundrecht auf Gewährleistung der Vertraulichkeit und Integrität informationstechnischer Systeme wurde 2008 vom Bundesverfassungsgericht formuliert. Demnach sind dem Staat Eingriffe in private Computer nur mit engen Begrenzungen erlaubt, da jeder Mensch seinem Computer vertrauen können muss und da er sich kaum dagegen wehren kann, heimlich und präventiv ausgespäht zu werden. 
ZEIT ONLINE: Das Recht auf informationelle Selbstbestimmung wurde gerade deshalb als Grundrecht verankert. 
Oettinger: Ja, aber das ist dann ein Absatz und es geht nicht um eine neue Grundrechte-Charta. Menschenwürde, Unversehrtheit, Meinungsfreiheit, freies Wahlrecht, Diskriminierungsverbot, Sozialstaat – alle diese Dinge sind seit Jahrzehnten aktuell. 
ZEIT ONLINE: Wir müssen also keine neue Welt erfinden? 
Oettinger: Nein, es geht vielmehr um Modernisierung, um Aktualisierung. Aber es bedarf keiner neuen Welt der Rechtsetzung. Sondern es geht um die Ergänzung unseres Regelwerks und um eine Europäisierung."	technik
"Wie nach früheren Attentaten haben die Vertreter von Sicherheitsbehörden auch nach den schweren Terroranschlägen von Paris bessere Ermittlungsmöglichkeiten gefordert. Der stellvertretende Vorsitzende der Gewerkschaft der Polizei (GdP), Jörg Radek, verlangte am Wochenende eine längere Speicherung von Verbindungsdaten.  ""Das eng gefasste Gesetz zur Vorratsdatenspeicherung muss überdacht werden "", sagte Radek der Rheinischen Post. Erst vor wenigen Tagen haben Bundestag und Bundesrat beschlossen, dass die Telekommunikationsprovider die Verbindungsdaten ihrer Kunden zehn Wochen lang speichern müssen. 
Das ist der Polizei ohnehin immer zu wenig gewesen. Während Radek keinen genauen Mindestzeitraum für die Speicherung angibt, geht der GdP-Landesvorsitzende von Nordrhein-Westfalen, Arnold Plickert, schon weiter. Die Speicherfristen müssten bei mindestens einem Jahr liegen, sagte er dem Blatt. Mit der Begründung:  ""Wir können damit möglicherweise zukünftige Terroranschläge verhindern, weil wir so an Informationen über die Terroristen kommen, an die wir sonst nicht gelangen. "" 
Ähnlich argumentierte Radek in einer Mitteilung:  ""Ein geplanter Terroranschlag dieser Größenordnung und Brutalität ist mit einem normalen Polizeischutz nicht zu verhindern. Die einzige Chance, ein solches Attentat zu verhindern, ist, den Tätern bereits bei der Planung auf die Spur zu kommen. Das bedeutet eine intensive nachrichtendienstliche und polizeiliche Überwachung potenzieller Gefährder. "" Zwar sei die deutsche Polizei nach den Anschlägen vom Januar 2015 in Paris verstärkt worden. Dennoch sei eine bessere Informationsgewinnung und -auswertung angesichts der terroristischen Bedrohung für die Polizei nach wie vor dringend notwendig, sagte Radek. 
Auch in anderen Ländern ist nach den Anschlägen die Diskussion um Maßnahmen wie die Vorratsdatenspeicherung neu entbrannt. Dabei gelten in Ländern wie Frankreich schon länger Speicherfristen von zwei Jahren. Zudem hat das Land nach den Anschlägen vom Januar auf das Satiremagazin Charlie Hebdo mit zwölf Toten die Überwachungsmöglichkeiten stark erweitert. Im Juli wurde das entsprechende Gesetz im Senat angenommen. 
Demnach wird in Frankreich die Vorratsdatenspeicherung auf fünf Jahre ausgedehnt. Erlaubt wird der Einsatz von Abhörgeräten, Keyloggern und IMSI-Catchern gegen Verdächtige ohne eine richterliche Zustimmung. Zudem dürfen die Behörden bei den Telekommunikationsanbietern Geräte, sogenannte Boîtes Noires, schwarze Kisten, installieren, die Metadaten in Echtzeit analysieren, um verdächtige Kommunikationsmuster zu erkennen. Die Analyse von Metadaten soll auch genutzt werden, um Profile potenzieller neuer Terroristen zu erkennen. Dies zielt vor allem auf Rückkehrer aus Syrien und dem Irak ab, die vor ihrer Abreise noch keine verdächtigen Aktivitäten erkennen ließen. Dazu sollen nun alle Kontakte von bereits verdächtigen Personen analysiert werden dürfen. 
Der Erfolg dieser Mittel ist nicht nur nach den jüngsten Anschlägen umstritten. Dennoch dürften sich die Befürworter in ihrem Vorhaben gestärkt sehen – und sie richten sich auf weitere  ""blinde Flecken "" in der Überwachung: US-Sicherheitsexperten verwiesen jüngst darauf, dass die Kommunikation von Terrororganisationen wie dem IS zunehmend außerhalb der Reichweite von Geheimdiensten liege. Der frühere Direktor der US-Antiterror-Zentrale NCTC, Michael Olsen, machte laut Yahoo Newsmehrere Gründe dafür aus. Zum einen würden Terrorverdächtige die von der NSA überwachten, bestehenden Kommunikationskanäle gar nicht mehr nutzen. 
Zum anderen würden sie beispielsweise von amerikanischen zu russischen Service-Providern wechseln, verschlüsselt kommunizieren oder Messenger-Apps wie Whatsapp oder iMessage benutzen. In diesem Zusammenhang ist US-Medien aufgefallen, dass die Behörden die Überwachung von Nachrichten über Facebook und WhatsApp stark ausgedehnt haben. Während es im ganzen Jahr 2014 nur neun Anfragen gab, waren es im ersten Halbjahr 2015 bereits 201 Anfragen. 
Der belgische Innenminister Jan Jambon verwies in der vergangenen Woche, noch vor den Anschlägen, auf die Nutzung der Playstation 4 als Kommunikationsmedium durch die Terroristen. Dies sei noch schwieriger zu verfolgen als beliebte Messenger WhatsApp. Die New York Times schrieb am Montag unter Berufung auf europäische Ermittlerkreise, dass die Angreifer von Paris verschlüsselte Kommunikation genutzt haben sollen. Eine Bestätigung dafür gibt es noch nicht. 
Zuletzt verzichtete die US-Regierung darauf, von den IT-Firmen Hintertüren in ihre Verschlüsselungsprogramme zu verlangen. Nach den Anschlägen vom Januar hatte sich US-Präsident Barack Obama noch hinter entsprechende Forderungen des britischen Premierministers David Cameron gestellt, der als Gegner verschlüsselter Kommunikation gilt. Erst vor zwei Wochen legte die britische Regierung ein überarbeitetes Gesetz für eine  ""Supervorratsdatenspeicherung "" vor, die nun noch schneller umgesetzt werden soll. Hintertüren für Verschlüsselung soll das Gesetz aber vorerst nicht enthalten. 
Am Montag erklärte Cameron zudem, dass man bis zu 1.900 neue Stellen in den Geheimdiensten GCHQ, MI5 und MI6 schaffen wolle. Zusätzlich soll das Budget für die Flugsicherheit verdoppelt werden und vor allem Flughäfen im Ausland besser kontrolliert werden. Nach dem Flugzeugabsturz in Ägypten hatte Großbritannien bereits Flüge von und nach Scharm al-Scheich ausgesetzt."	technik
"Wer nicht paranoid ist, muss verrückt sein. Unter diesem Titel hat der US-Schriftsteller Walter Kirn gerade einen lesenswerten Aufsatz über den Alltag in Zeiten einer mal mehr, mal weniger subtilen Überwachung veröffentlicht. Er schreibt darin über die mitunter gruseligen Analysefähigkeiten von Internetunternehmen, aber auch über das gigantische Rechenzentrum der NSA in Utah.  ""In unseren Maschinen stecken so viele Geister – ihr genauer Aufenthaltsort so versteckt, ihre Methoden so raffiniert, ihre Motive so undurchsichtig – dass man nicht wach sein kann, wenn man sich nicht von ihnen verfolgt fühlt "", findet Kirn. 
Dieses Gefühl kann auch die Vorratsdatenspeicherung auslösen, die heute vom Bundestag als  ""Gesetz zur Einführung einer Speicherpflicht und einer Höchstspeicherfrist für Verkehrsdaten "" beschlossen wurde. Denn sobald das Gesetz in Kraft tritt, müssen die Telekommunikationsanbieter in Deutschland das Kommunikationsverhalten aller Bürger protokollieren, egal, ob sie einer Straftat verdächtig sind oder nicht. Die schwarz-rote Koalition bemüht sich, die Auswirkungen des Gesetzes kleinzureden und trifft auf Verständnis bei jenen, die ohnehin glauben, dass sie nichts zu befürchten haben. Doch viele der Verharmlosungen stimmen schlicht nicht oder nur eingeschränkt: 
Das ist doch gar keine Überwachung. 
Technisch richtig, sinngemäß aber geradezu gefährlich verfälschend. Ein Unionspolitiker im Rechtsausschuss des Bundestages hat gesagt, man solle vorsichtiger sein mit Begriffen im Zusammenhang mit Vorratsdaten, das sei  ""keine Überwachung "". Die Vorratsdaten werden bei den Anbietern gesammelt, also bei der Telekom, Vodafone et cetera. Solange sie nur dort liegen, findet tatsächlich keine Überwachung statt. In dem Moment aber, in dem jemand darauf zugreift und sie nutzt, ändert sich das. 
Streng genommen ist die Vorratsdatenspeicherung also die Basis, um eine flächendeckende Überwachung aller Bürger erst zu ermöglichen. Zu behaupten, sie wäre  ""keine Überwachung "", ignoriert die Gefahr, die allein schon in der Datensammlung liegt. Und die Aussage leugnet, dass es ohne Vorratsdaten diese Überwachung gar nicht geben könnte. 
Es sind doch nur Metadaten. 
Falsch. Erstens stimmt es faktisch nicht, weil die Mobilfunkanbieter – aus derzeit nicht zu ändernden technischen Gründen, wie sie sagen – mit den SMS-Verbindungsdaten auch die Inhalte der SMS speichern. Das berichtet die Süddeutsche Zeitung heute. Einziger Trost: Diese Inhalte dürfen sie Strafverfolgern im Rahmen der Vorratsdatenspeicherung nicht aushändigen. 
Zweitens kann von  ""nur "" keine Rede sein. Metadaten verraten, wer wann mit wem kommunizierte, wo und wie lange. So werden Netzwerke, Beziehungen und Tagesabläufe – sprich: Profile – sichtbar. Nicht umsonst sind Metadaten auch für Geheimdienste wie die NSA die wertvollsten Daten. 
Die Strafverfolger brauchen die Vorratsdatenspeicherung. 
Unklar. Es gibt keine empirischen Belege dafür. Nicht in Deutschland und auch nicht in irgendeinem anderen EU-Staat. Politiker von Union und SPD nennen immer wieder Einzelfälle, in denen die Vorratsdatenspeicherung geholfen hat oder hätte – und immer wieder stellt sich heraus, dass ihre Behauptungen falsch sind. 
Außerdem gibt es durchaus Strafverfolger, die dem Instrument kritisch gegenüber stehen, ja die sogar sagen, sie brauchen es nicht. Denn bei schweren Straftaten hat der Staat längst diverse Möglichkeiten, auf Daten zuzugreifen und tut das ausgiebig. So ausgiebig, dass die Ermittler mit der Auswertung der gesammelten Daten oft völlig überfordert sind. 
Die schwarz-rote Koalition will in eineinhalb Jahren anfangen, das Gesetz zu evaluieren. Der zuständige Sachverständige soll vom Bundestag bestellt werden, also mit der Regierungsmehrheit. 
Wer nichts verbrochen hat, hat doch nichts zu verbergen. 
Falsch. Das Recht auf Privatsphäre ist ein Menschenrecht. Wer nichts verbrochen hat, muss deshalb noch lange nicht offenlegen, was er getan hat oder mit wem er kommuniziert. Ein gutes Gleichnis dafür, wie gefährlich diese Behauptung ist, hat Edward Snowden geliefert. Wer so argumentiere, sagte er, der könne auch behaupten: Ich brauche keine Meinungsfreiheit, ich habe ja nichts zu sagen. 
Es wird doch nur zehn bzw. vier Wochen lang gespeichert. 
Einerseits richtig. Strafverfolger hätten gerne längere Speicherfristen. Andererseits ist auch dieses  ""nur "" irreführend. Nach zehn beziehungsweise vier Wochen sind ja nicht alle Daten weg. Ein Datensatz, der ein präzises Bewegungs- und Kommunikationsprofil ergibt, ist jederzeit gespeichert. Wer seine Kommunikationspartner und seinen Bewegungsradius einigermaßen konstant hält – was für die meisten Menschen gelten dürfte – hat keinen Vorteil durch die kurze Speicherfrist. 
Die Daten dürfen doch nur zur Aufklärung  ""besonders schwerer Straftaten "" verwendet werden. 
Richtig. Im Gesetzentwurf enthalten ist eine Liste von Straftaten, die nicht alles umfasst, was im eigentlich zugrundeliegenden Paragrafen 100a StPO Absatz 2 aufgezählt wird. Es fehlen zum Beispiel die in der Strafprozessordnung genannten Betrugsdelikte. 
Allerdings zeigt die Erfahrung, dass es nicht lange dauern wird, bis erste Rufe nach einer Ausweitung zu hören sein werden. Die gab es sowohl nach der ersten Einführung der Vorratsdatenspeicherung in Deutschland 2007 als auch in anderen EU-Mitgliedsstaaten als Reaktion auf die mittlerweile vom EuGH gekippte Vorratsdatenrichtlinie. Der Bund Deutscher Kriminalbeamter hatte die Ausweitung der heute beschlossenen Vorratsdatenspeicherung bereits im Sommer gefordert. 
 
Wenigstens ist nicht anzunehmen, dass die Bundesregierung dem in absehbarer Zeit nachkommen wird. Aktivisten und Oppositionspolitiker haben bereits Verfassungsbeschwerden angekündigt – und bevor das Bundesverfassungsgericht die neue Vorratsdatenspeicherung nicht abgesegnet hat, wird die Regierung es kaum wagen, sie zu verschärfen. 
Es gilt doch der Richtervorbehalt. 
Theoretisch ja. Nur haben Richter bei diesem Thema offenbar nie Vorbehalte. Das jedenfalls sind die Erfahrungen des Berliner E-Mail-Providers Posteo. In dessen Transparenzbericht heißt es:  ""In der Praxis werden offenbar alle Anträge auf Überwachungsmaßnahmen bewilligt. "" Posteo beruft sich auf Zahlen aus Berlin, dem einzigen Bundesland, das erfasst, wie viele Anträge auf Überwachungsmaßnahmen von Richtern abgelehnt werden. Und abgelehnt wurden nach 2007 genau: null Anträge. 
Die Daten von Journalisten, Ärzten, Anwälten, Geistlichen und Beratern in Hilfseinrichtungen für Schwangere oder Drogenabhängige dürfen doch nicht verwendet werden. 
Richtig, aber das nützt nicht viel. Die Daten werden erst einmal unterschiedslos gespeichert. Informanten, Patienten und Klienten müssen darauf vertrauen, dass Strafverfolger eine Kommunikation mit Journalisten, Ärzten oder Anwälten als solche erkennen und dann nichts von dem verwenden, was sie bei dieser Analyse erfahren haben. Wie das praktisch umgesetzt und überprüft werden kann, ist völlig unklar. 
Bundesverfassungsgericht und EuGH erlauben die Vorratsdatenspeicherung doch grundsätzlich. 
Richtig. Das Bundesverfassungsgericht hat in seinem Urteil von 2010 die erste Version der Vorratsdatenspeicherung nur deshalb für verfassungswidrig erklärt, weil darin die Datensicherheit, die Grenzen des Zugriffs, die Transparenz desselben und der Rechtsschutz der Betroffenen nicht den verfassungsmäßigen Vorgaben entsprachen. 
Allerdings gaben die Richter zu bedenken, dass die anlasslose Speicherung der Daten aller Bürger dazu geeignet ist,  ""ein diffus bedrohliches Gefühl des Beobachtetseins hervorzurufen, das eine unbefangene Wahrnehmung der Grundrechte in vielen Bereichen beeinträchtigen kann "". 
Der Europäische Gerichtshof verlangte, die Vorratsdatenspeicherung  ""auf das absolut Notwendige "" zu beschränken und erklärte die entsprechende EU-Richtlinie für ungültig, weil der Gesetzgeber  ""die Grenzen überschritten hat, die er zur Wahrung des Grundsatzes der Verhältnismäßigkeit einhalten musste. "" Auch der EuGH störte sich sehr wohl an der unterschiedslosen Datenspeicherung aller Bürger. Sein Urteil fiel in dieser Hinsicht so eindeutig aus, dass die EU-Kommission die Lust verlor, eine neue Richtlinie zu erarbeiten. Mit anderen Worten: Die höchsten Gerichte haben Vorratsdaten als gefährliches Problem erkannt und viel strengere Grenzen dafür gefordert, als das Gesetz nun vorsieht."	technik
"Ist das noch Silicon-Valley-Sprech oder schon US-Militärdoktrin?  ""Our policy is we try things and we celebrate our failures. "" –  ""Unsere Strategie sieht so aus: Wir probieren Dinge aus und feiern unsere Misserfolge. "" 
Es ist ein Zitat von Eric Schmidt aus dem Jahr 2010, dem damaligen Google-CEO, heutigen Vorstandsvorsitzenden von Googles Mutterkonzern Alphabet und künftigen Leiter des Defense Innovation Advisory Board im Pentagon. Schmidt soll dieses neue Beratungsgremium aufbauen, um Denkweisen und Technik aus Kalifornien nach Washington zu bringen. Mehr Valley im Verteidigungsministerium sozusagen. 
Der 60-Jährige ist dafür der wohl naheliegendste Kandidat. Er vermittelt ohnehin seit Langem zwischen den Technikunternehmen und der Regierung der USA. Bereits 2009 holte Präsident Obama ihn in sein Beraterteam. Egal, ob es darum geht, die Unternehmen in den Kampf gegen Terrorismus und Propaganda einzubinden oder nur die Website HealthCare.gov zu reparieren: Schmidt sitzt mit am Tisch, wenn die Regierung Internetnachhilfe braucht. 
Nach Angaben des Verteidigungsministeriums soll Schmidt helfen, den Einsatz unter anderem von Cloud-Technologie, Apps und Rapid Prototyping voranzutreiben. Verteidigungsminister Ash Carter und Schmidt werden dafür etwa ein Dutzend Vertreter des öffentlichen und des privaten Sektors um sich versammeln, wohl nicht zuletzt, um die seit einiger Zeit eher als angespannt geltenden Beziehungen zwischen beiden Seiten öffentlichkeitswirksam zu pflegen. Seit den Snowden-Enthüllungen und nun erneut durch den Fall FBI versus Apple misstrauen die Unternehmen ihrer eigenen Regierung. 
Möglicherweise ist die Wiederannäherung sogar das eigentliche Ziel des Verteidigungsministeriums. Mit der Defense Advanced Research Projects Agency (Darpa) hat es schließlich schon eine eigene, finanziell und personell bestens ausgestattete Forschungsbehörde zur Verfügung, die sich um die Entwicklung neuer Technologien für den militärischen Einsatz kümmert. 
Einblicke in militärische Operationen oder Strategien sollen die Mitglieder des Defense Innovation Advisory Board nicht erhalten, versichert das Pentagon. 
Gleichzeitig kündigte das Ministerium an, ein sogenanntes Bug-Bounty-Programm aufzusetzen. Hacker sollen bestimmte Teile des Pentagon-Netzwerks angreifen, um Sicherheitslücken aufzudecken. Wer mitmachen will, muss sich anmelden und wird dann erst einmal einem Sicherheitscheck unterzogen. Erfolgreichen Angreifern winken Geldprämien oder  ""anderweitige Anerkennung "". Der Name des Programms lautet schlicht Hack the Pentagon. 
Das Verteidigungsministerium ist die erste amerikanische Regierungsorganisation, die ein solches Programm ins Leben ruft. In der Wirtschaft ist das Modell bereits weitverbreitet, Unternehmen wie Facebook, Microsoft und die Deutsche Telekom betreiben es seit Längerem. Allein Facebook hat seit 2011 mehr als vier Millionen US-Dollar an Hacker ausgezahlt, die Sicherheitslücken meldeten. 
Natürlich könnte man lästern, dass dieser Wettbewerb inoffiziell schon seit Jahren läuft. Tatsächlich aber ist die Idee sinnvoll und würde zum Beispiel auch dem Deutschen Bundestag gut zu Gesicht stehen."	technik
"Auf dem diesjährigen SXSW-Festival in Austin gab es eine Paneldiskussion mit dem Titel HAL to Her: Wie Sprache Technik menschlicher macht. Der Weg vom fiesen Supercomputer HAL im Film 2001: A Space Odyssey vor knapp 50 Jahren hin zur freundlichen künstlichen Intelligenz Samantha im Film Her vor zwei Jahren diente als Analogie, um die Entwicklung auf dem Gebiet kommunizierender Technik zu illustrieren. Zu den Vortragenden gehörte auch Leslie Carmichael von Microsoft, die davon überzeugt war, dass wir künftig mit unseren Computern sprechen – und diese mit uns. 
Wie die größeren Pläne von Carmichaels Arbeitgeber aussehen, wurde am Mittwochabend deutlich. Auf der hauseigenen Entwicklerkonferenz Build in San Francisco präsentierte Microsoft nicht nur kommende Funktionen für Windows 10 wie biometrische Logins, die Zusammenführung von Games in Windows und der Xbox und die Entwicklerversion der Datenbrille HoloLens. Im Mittelpunkt stand ein Ökosystem sogenannter Chatbots: Anwendungen, die auf Text- und Spracheingaben reagieren. 
Microsofts CEO Satya Nadella nannte das conversation as an interface – Unterhaltungen als Benutzeroberfläche. Er gab zu, das jüngste Experiment mit dem Chatbot Tay auf Twitter sei nicht ganz optimal verlaufen. Eigentlich sollte Tay von anderen Nutzern lernen, wie junge Menschen sprechen. Doch die Nutzer fütterten den Chatbot vor allem mit rassistischen Inhalten, die Tay aufgriff und wiederholte, so dass Microsoft das Experiment schnell beendete und nun überarbeitet. Weitere Bots sollen dagegen bald in Windows und in Microsofts Smartphone-Apps auftauchen. 
Zum Beispiel in Skype: Die kommende Version des Messengers wird sowohl Microsofts digitale Assistentin Cortana enthalten als auch Entwicklern ermöglichen, eigene Bots zu programmieren, die wiederum mit Cortana verknüpft sind. Ein Beispiel: Cortana erkennt während eines Gesprächs, dass der Nutzer eine Konferenz besuchen möchte. Automatisch blockiert sie den jeweiligen Zeitraum im Kalender des Nutzers und aktiviert den Bot der US-Hotelkette Westin. Der schlägt dem Nutzer anschließend Zimmer vor, inklusive Bildern und Preisen, und reagiert auf Rückfragen. Der Nutzer kann seine Reise planen, ohne jemals Skype verlassen zu müssen. 
In Satya Nadellas Vision der Zukunft geht es also um drei handelnde Akteure: Den Nutzer, den virtuellen Assistenten und die Bots von Drittanbietern. Assistenten wie Cortana, Apples Siri oder Amazons Alexa kennen die Vorlieben und persönlichen Daten ihrer Nutzer und greifen je nach Bedarf auf weitere Bots zurück.  ""Bots sind wie neue Anwendungen, mit denen wir kommunizieren "", sagte Nadella. Und das nicht nur per Texteingabe, sondern immer häufiger über Sprache. 
Über die neue Skype Bot Platform können Entwickler eigene Bots für Skype erstellen. Doch das ist nur ein Teil der Strategie: Mit dem Bot Framework bietet Microsoft eine zweite, allgemeinere Plattform an, die über Bots in Skype hinausgeht. Wer sie nutzt, kann Bots auch für weitere Anwendungen programmieren, die diese unterstützen. Microsoft Office etwa oder die Messenger Slack und Telegram. Das geht zwar auch schon jetzt, aber wer Microsofts Bot Framework nutzt, kann gleichzeitig auf die Entwicklungen des Unternehmens zurückgreifen, wie die automatische Übersetzung in 30 Sprachen oder neuronale Netzwerke, die Inhalte auf Fotos erkennen können. Einige dieser sogenannten Cognitive Services sind für Bot-Anbieter ab einer bestimmten Zahl an Einsätzen allerdings kostenpflichtig. 
 
Mit seinen auf der Build-Konferenz vorgestellten Plänen reagiert Microsoft auf zwei Entwicklungen im Computer- und Smartphone-Bereich. Zum einen passen Bots hervorragend zum Branchentrend der Automatisierung, mit deren Hilfe die Nutzer mit immer weniger Klicks immer komplexere Aufgaben bewältigen sollen. Zum anderen glauben viele, das Zeitalter der Apps nähere sich schon wieder seinem Ende. Statt für jeden einzelnen Dienst eine eigene App zu installieren, gehe der Trend dahin, das einzelne Apps selbst zur Plattform werden, wie es der Branchenanalyst Benedict Evans in seinem Blog beschreibt. 
Vor allem Facebook hat das mit seinem Messenger bereits demonstriert. Dort gibt es nämlich bereits jetzt Chatbots von ausgewählten Drittanbietern und es wird spekuliert, dass Facebook auf seiner kommenden Entwicklerkonferenz die Messenger-Plattform noch weiter öffnen könnte und möglicherweise sogar einen eigenen Bot-Store ins Leben ruft. Auch von Google wird gemunkelt, das Unternehmen arbeite an einen neuen Messenger mit Bots und künstlicher Intelligenz. Und es dürfte wohl nur eine Frage der Zeit sein, bis Apple mit Siri nachlegt. 
Es bleibt die Frage, ob Microsoft nicht zu spät kommt, zumal das Unternehmen im Gegensatz zu Google oder Apple keine erfolgreiche Smartphone-Plattform hat (Windows Phone wurde während der Build-Keynote nicht einmal erwähnt) und auch Skype fällt mit etwa 300 Millionen Nutzern im Vergleich zu Facebooks Messenger mit 800 Millionen deutlich ab. Im mobilen Bereich dürfte es Microsoft deshalb schwer haben, eine eigenständige Plattform für Bots zu etablieren. 
Aber womöglich plant Microsoft ohnehin den Weg durch die Hintertür, wie es Casey Newton von The Verge beschreibt: Microsoft will in erster Linie nicht die Plattform sein, auf der die Bots der Zukunft laufen, sondern die Plattform, mit der sie entwickelt werden. Sozusagen der stille Dienstleister im Hintergrund. 
Satya Nadella jedenfalls glaubt an den Erfolg der Bots:  ""Es ist ein einfaches, aber mächtiges Konzept. Wir wollen eine Intelligenz erschaffen, die menschliche Fähigkeiten ergänzt "", sagte Nadella zum Abschluss der Keynote in San Francisco.  ""Es geht nicht um Mensch gegen Maschine, sondern um Mensch mit Maschine. "" Oder anders gesagt: Die Bots der Zukunft sind tatsächlich mehr Her als HAL."	technik
"Der Bundesnachrichtendienst (BND) steht im Zentrum eines Skandals, der zur Stunde die deutsche Politik erschüttert. Laut einem Spiegel-Bericht hat der BND jahrelang dem US-Geheimdienst NSA geholfen, europäische Unternehmen auszuspionieren – und auch europäische Politiker, wie ZEIT ONLINE erfuhr. Dem Kanzleramt sagte der Nachrichtendienst davon lange nichts. BND-Präsident Gerhard Schindler gerät nun unter erheblichen Druck. 
Dem Bericht zufolge lieferte die NSA dem BND zehn Jahre lang sogenannte Selektoren, das können zum Beispiel IP-Adressen, E-Mail-Adressen oder Telefonnummern sein. Diese gab der BND in seine weltweiten Überwachungssysteme und Datenbanken ein, die Ergebnisse gingen wiederum zum Teil an die NSA. 
Spätestens im Jahr 2008 sei BND-Mitarbeitern aufgefallen, dass diverse Selektoren nichts mit seinem gesetzlichen Auftrag zu tun hatten und auch nicht vom sogenannten Memorandum of Agreement abgedeckt waren, auf dessen Basis die Bundesrepublik und die USA die gemeinsame Terrorbekämpfung betrieben. Es handelte sich stattdessen um Selektoren, die europäische Unternehmen wie EADS und Eurocopter, aber auch Politiker betrafen. Doch statt das Kanzleramt zu informieren, half der BND den amerikanischen Kollegen zunächst bei der Spionage gegen Europa und Deutschland. Zu groß war die Angst, von den Erkenntnissen und der Hilfe der NSA abgeschnitten zu werden. 
Einige Aspekte des Spiegel-Berichts waren bereits bekannt: Schon im Oktober 2014 hatte die Süddeutsche Zeitung berichtet, dass der BND bemerkt hatte, was die US-Kollegen da versuchten. Gestoppt habe er es nicht, weil er vonseiten der NSA ohnehin nur Ausreden erwartete. Im März 2015 hatte der ehemalige Leiter der Abteilung Technische Aufklärung zudem im NSA-Untersuchungsausschuss bestätigt, dass der BND bestimmte Selektoren erkannt und abgelehnt hat. 
Der Spiegel beschreibt nun das Ausmaß der Bespitzelung der europäischen Partner. Demnach habe eine Projektgruppe des BND aufgrund eines Beweisantrags des NSA-Untersuchungsausschusses untersucht, wie viele Selektoren gegen europäische und deutsche Interessen gerichtet waren. Das Ergebnis: Es waren wohl bis zu 40.000. Allerdings hatte der BND nicht alle davon tatsächlich verwendet, zumindest zum Teil hatte er sie von vornherein aussortiert. 
Insgesamt hat der amerikanische Geheimdienst den Deutschen rund 800.000 Selektoren geschickt, wie ZEIT ONLINE erfuhr. Mehrmals am Tag hatte der BND von einem Server der NSA Selektoren heruntergeladen und sie in seine Datenbanken und Systeme eingegeben, hieß es dazu mal im Untersuchungsausschuss. Anschließend wurden die Ergebnisse zur BND-Zentrale nach Pullach zur Auswertung geschickt und von dort aus zum Teil auch weiter an die NSA. 
Besonders brisant: Schon 2013 hatte der BND laut Spiegel intern eine Liste der problematischen Selektoren erstellt. Er war damals auf etwa 2.000 Selektoren gekommen, die er auch allesamt benutzt hatte. Dem für die Geheimdienstaufsicht zuständigen Kanzleramt hat er das alles angeblich nicht gemeldet. Dort soll man erst vor wenigen Wochen von der neuen, erheblich längeren Liste erfahren haben. Ob das wirklich so war oder die Bundesregierung nicht doch früher von dem Problem erfahren hat, ist aber ungeklärt. 
Am Mittwochabend unterrichtete Kanzleramtsminister Peter Altmaier (CDU) die Mitglieder des Parlamentarischen Kontrollgremiums und des NSA-Ausschusses darüber. BND-Präsident Gerhard Schindler sei von der Sitzung explizit ausgeschlossen worden, schreibt der Spiegel. Er wird nun möglicherweise zurücktreten müssen. 
Die Mitglieder des NSA-Untersuchungsausschusses sind überzeugt, dass es Kreise im BND gibt, die sich jeglicher Kontrolle entziehen, dass sie belogen wurden und dass es mindestens  ""Probleme "" mit der fachlichen Aufsicht über den BND durch das Kanzleramt gibt. Einer politische Bewertung haben sich die meisten Ausschussmitglieder bisher enthalten. Sie wollen zunächst so schnell wie möglich Akten sehen und Zeugen befragen, um den Sachverhalt aufklären zu können. 
Das Parlamentarische Kontrollgremium (PKGr) will sich baldmöglichst vom Ausschuss informieren lassen und dann zu ausfühlichen Bewertungen kommen, hieß es im Anschluss an eine kurzfristig angesetzte Sitzung des Gremiums. Der PKG-Vorsitzende André Hahn von der Linken sprach von einem gravierenden Vorgang, der  ""mehr als eine Panne "" sei. Es gehe um die Sicherheit der Bundesrepublik und die Arbeit der Geheimdienste. 
Seine Fraktionskollegin Martina Renner sagte:  ""Für uns ist das ganz klar Spionage, möglicherweise unter Hilfe des BND "". Sie fordert den Rücktritt des BND-Chefs:  ""Schindler ist nicht mehr tragbar. "" Zur Frage, ob auch der frühere Kanzleramtsminister Steinmeier als Oberster Aufseher der Geheimdienste verantwortlich gemacht werden müsse, blieb sie unkonret. 
Hans-Christian Ströbele von den Grünen sprach von  ""einem zweiten Skandal "". Zu dem von Edward Snowden öffentlich gemachten Überwachungsskandal komme nun die Wirtschaftsspionage der NSA, die der Dienst und auch US-Präsident Barack Obama immer bestritten hätten."	technik
"Begonnen hat sie als simple Rangierhilfe beim Rückwärtsfahren. Längst aber hat sich die Kamera im Auto zum Multifunktionsassistenten aufgeschwungen. Vor allem in den kleinen Klassen übernimmt sie als kostengünstige Alternative zunehmend Aufgaben, die bislang Radartechnik vorbehalten war. 
Königsdisziplin des Radars war lange Zeit die Abstandsmessung zu Hindernissen. Assistenzsysteme, die bei einem drohenden Unfall selbstständig eine Vollbremsung einleiten, gibt es schon seit rund einem Jahrzehnt. Lange waren sie allerdings Autos aus den oberen Fahrzeugsegmenten vorbehalten. Kamerabasierte Technik macht die Notbremsassistenten mittlerweile auch in den kleinen Klassen verfügbar. 
Besonders genau bei der Abstandsmessung sind Stereokameras, wie sie etwas Subaru im Offroad-Kombi Outback anbietet. Nach dem Vorbild des menschlichen Auges beobachten zwei Kameras in leicht unterschiedlichen Winkeln die Straße und errechnen aus den kleinen Unterschieden im Bild die Entfernung von Objekten. Nähert sich das Auto etwa einem festen Hindernis, drosselt der Notbremsassistent bei einer drohenden Kollision zügig die Geschwindigkeit. Der Zulieferer Bosch bietet mittlerweile ein ähnliches System an, das seit Kurzem beim SUV-Hersteller Land Rover eingesetzt wird. Bosch rühmt sich, besonders wenig Platz zu benötigen. Denn der große Raumbedarf spricht neben den relativ hohen Kosten gegen Stereokameras. 
Die günstigere und platzsparende Alternative sind Monokameras. Auch diese sind in der Lage, Abstände zu Hindernissen zu ermitteln. Sie können sie nicht wirklich messen, aber mit Hilfe von Bilderkennungsprogrammen und Bewegungsmustern zumindest relativ genau schätzen. Das braucht Zeit und ist nicht immer ganz exakt – zumindest für City-Notbremssysteme bei geringen Geschwindigkeiten reicht das aber. 
Genau das verhilft der Kamera wohl endgültig zum Siegeszug. Denn beim Crashtest von Euro NCAP bekommen Modelle ohne zumindest optional erhältliche Notbremsassistenten die volle Wertung mittlerweile nicht mehr. Und die Kriterien werden weiter verschärft, sodass auch Kleinwagen künftig ein ganzes Arsenal an Helfern haben müssen, um die imageträchtigen fünf Sterne der Tester zu erhalten. 
Hier hilft die Kamera weiter. Von ihrem Platz hinter der Windschutzscheibe aus kann sie auf das Halten der Fahrspur achten, Verkehrszeichen erkennen und eben Auffahrunfälle verhindern. Im Heck oder in den Spiegeln fungiert sie zudem als Totwinkelwarner oder Rückfahrhelfer. In dieser Funktion wird sie in den USA von 2018 an sogar obligatorisch; der Gesetzgeber will damit die hohe Zahl tödlicher Rangierunfälle senken. Und wenn die Kamera eh schon an Bord ist, kann man sie auch gleich intelligent machen und mit den anderen Videoaugen vernetzen. 
Die simple Ausbaustufe der Umfeldbeobachtung beherrscht die Kamera schon geraume Zeit. Ein Beispiel: Beim Nissan-System mit dem Namen Around View berechnet ein Computer aus vier Kameras eine virtuelle Draufsicht, die dem Autofahrer im Borddisplay seinen Wagen aus der Vogelperspektive zeigt. Parkrempler sind so fast ausgeschlossen – insbesondere, wenn das Auto mit Hilfe der Kamerabilder selbstständig ohne menschlichen Eingriff rangiert. Mittlerweile beherrschen die Kameras aber noch mehr Funktionen, etwa die Warnung vor Querverkehr beim Rückwärtsausparken oder das Um-die-Ecke-Spähen beim Fahren durch enge Torausfahrten. 
Komplett ersetzen lässt sich der Radar durch die Kamera aber auf absehbare Zeit nicht. Ein Grund dafür ist schon, dass das Videoauge durch Starkregen, tief stehende Sonne oder Dunkelheit schnell irritiert ist. Zudem ist der Radar bei höherer Reichweite schneller und genauer als Kamerasysteme, vor allem auf Autobahnen und Schnellstraßen ist das wichtig. Vor allem in Fahrzeugen höherer Klassen sorgt darum häufig ein Radarsystem für Redundanz bei der Umfeldbeobachtung. Und auch die autonomen Autos werden auf den Radar und weitere Sensorsysteme wie Lidar und GPS kaum verzichten können."	technik
"Twitter hat derzeit ein kleines Problem mit seinen eigenen Richtlinien.  ""Wenn jemand private Informationen über dich gepostet hat, sende uns bitte ein Hilfe-Ticket. Wenn du aktiv bedroht wirst, rufe die Polizei "", heißt es in den Twitter-Regeln zur Sicherheit . Außerdem steht auf der Seite der Satz:  ""Denke immer daran, dass wir nicht die Polizei sind. "" Nun hat Twitter doch einmal selbst Polizei gespielt – und prompt gab es Ärger. 
Was war passiert? Guy Adams, ein US-Korrespondent der britischen Tageszeitung The Independent , machte auf Twitter seinem Unmut über die zeitversetzte Ausstrahlung der olympischen Spiele auf dem Sendernetzwerk von NBC Luft:  "" The man responsible for NBC pretending the Olympics haven't started yet is Gary Zenkel. Tell him what u think! Email: Gary.zenkel@nbcuni.com  "" ( ""Der Mann, der dafür verantwortlich ist, dass NBC so tut, als hätten die Spiele noch nicht angefangen, heißt Gary Zenkel "", gefolgt von der Aufforderung, ihm per E-Mail die Meinung zu sagen.) NBC ist einer der exklusiven Medienpartner des Großereignisses, gleichzeitig kooperieren Twitter und NBC als Werbepartner während der Spiele. 
Guy Adams' Nachricht traf den Nerv vieler Nutzer. Sie verteilten den Tweet weiter und versahen ihn mit dem Hashtag #nbcfail. Twitter-Mitarbeiter nahmen Notiz von den Nachrichten und benachrichtigten NBC. Sie empfahlen dem Sender, die Sperrung von Guy Adams' Account bei Twitter zu beantragen. Der Sender stellte diesen Antrag, und zwar mit der Begründung, er wolle die Privatsphäre seines Mitarbeiters Zenkel schützen. 
Andere Twitter-Mitarbeiter, die nicht wussten, dass ihre eigenen Kollegen das Ganze ins Rollen gebracht hatten, entsprachen diesem Antrag von NBC und sperrten den Account. So erklärte es Twitter wenige Stunden später in einem Blogpost . 
Verstoß gegen eigene Regel 
Einige Nutzer warfen dem Unternehmen daraufhin Willkür und Zensur vor. Denn in früheren, vergleichbaren Fällen habe Twitter anders gehandelt . Der Regisseur Spike Lee zum Beispiel hatte die Adresse eines Pärchens veröffentlicht, von dem er sich bedroht gefühlt hatte – ohne dass er dafür von Twitter dafür gemaßregelt wurde. Und auch Justin Biebers Account wurde nicht gesperrt, als der Teeniestar die Telefonnummer eines Mannes postete, von dem er sich gemobbt fühlte.  ""Klar, das waren ja auch keine Twitter-Werbekunden "", spottete ein Twitter-Nutzer und sprach damit den Kern der Kritik aus: Das Unternehmen behandele Nutzer nicht unparteiisch, wie es das in seinen eigenen Regeln vorgibt, sondern ausgerichtet an den Einnahmen, die sie dem Unternehmen bringen. 
Dieser Verdacht muss bei näherer Betrachtung relativiert werden: Es stimmt, Twitter hat versagt, als es NBC selbst auf den kritischen Tweet hinwies und damit gegen seine eigene Regel verstieß, Inhalte nicht aktiv zu überprüfen. Aber die Sperrung des Accounts, basierend auf einem Tweet, der private Informationen preisgab, entsprach geltendem Twitter-Regelwerk. Die Sonderbehandlung war also eher das Nicht-Sperren von Accounts, über die private Informationen Dritter preisgegeben wurden. 
Werden Regeln auf Twitter nicht durchgesetzt, beruht das auf dem Prinzip: Wo kein Kläger, da kein Richter. Das bedeutet aber nicht, dass es niemals Kläger und niemals Richter geben wird. 
 
Im Fall Adams monierten die Twitter-Kritiker, der Vorfall sei gar kein Regelbruch gewesen, da die veröffentlichte E-Mail-Adresse die Geschäftsadresse des NBC-Mitarbeiters war und somit keine private Information. Doch das ist Auslegungssache. In welcher Art E-Mail-Adressen, die ein Unternehmen vergibt, auch privat genutzt werden können, ist Sache des betreffenden Unternehmens. Entsprechend war es das gute Recht von NBC, die Privatsphäre seines Mitarbeiters schützen zu wollen. 
Bleibt ein weiteres Argument der Kritiker: Twitter sei als Zensor aufgetreten und sei daher kein geeignetes Werkzeug mehr für Journalisten. Der Blogger, Autor und Journalistik-Dozent Jeff Jarvis forderte , Twitter müsse eine Grenze zwischen Sponsoren und sich ziehen, vergleichbar mit dem Trennungsgebot von Anzeigen und redaktionellen Inhalten in Nachrichtenmedien. Jarvis spielte damit auf den Verdacht der Sonderbehandlung an. 
Keine Sonderrechte für Sponsoren – das mag Journalisten nützen und gut klingen – folgerichtig ist es nicht. Denn Twitter ist nach seinem Selbstverständnis ein soziales Netzwerk und bietet eine Technologie zum Teilen von Informationen. An keiner Stelle gibt Twitter vor, ein Qualitätsmedium zu sein, das Anforderungen wie dem Trennungsgebot unterliegt, um eine unabhängige Berichterstattung zu garantieren. 
Twitter existiert, um profitabel zu sein 
Die Förderung von Meinungsfreiheit ist nicht die Daseinsberechtigung von Twitter, sondern bestenfalls eine Begleiterscheinung seines Erfolgs. Mit anderen Worten: Twitter existiert nicht, um die Welt zu besser zu machen und demokratische Willensbildung zu ermöglichen, sondern um profitabel zu arbeiten . 
Könnte sich das Unternehmen also mehrere solcher Vertrauensbrüche leisten, ohne dass ihm wirtschaftlicher Schaden entsteht? Ja, könnte es. Von den offiziell 100 Millionen aktiven Nutzern würde nur eine Minderheit den Dienst wegen solcher Vorfälle verlassen – nämlich diejenigen, die mit besonders hohen Erwartungen an Transparenz , Rechtschaffenheit und werbliche Unabhängigkeit an den Dienst herangehen. Ihnen gegenüber stehen aber all die Nutzer, für die Twitter nur ein Mittel ist, um mit Freunden in Kontakt zu bleiben. Für sie sind Tweets nicht mehr als SMS, die sie öffentlich versenden. Dass einem sozialen Netzwerk diese vergleichsweise unkritische Nutzergruppe genügt, um zu wachsen, zeigt Facebook."	technik
"Twitter hat in der Türkei den Zugang zu den Tweets mehrerer Journalisten und Medien gesperrt. Unter anderem könnten Tweets der Kolumnistin Nazlı Ilıcak und des Reporters Fatih Yağmur nicht abgerufen werden. Auch Tweets der Zeitung Bugün und des Portals Gerçek Gündem seien gesperrt worden, berichtet Hürriyet. 
Ein Gericht in Istanbul hatte zuvor einer Klage des Richters Bekir Altun stattgegeben. Altun hatte angegeben, dass hunderte Tweets auf einen Artikel verlinkten, der seine Persönlichkeitsrechte verletze. In dem Artikel schrieb Fatih Yağmur, Altun habe illegale Abhörmaßnahmen der Polizei bewilligt. 
Im Juli 2014 waren mehr als 50 ranghohe Polizisten festgenommen worden. Ihnen wurde Korruption und Amtsmissbrauch vorgeworfen. Unter anderem sollen sie seit 2010 hunderte Personen abgehört haben, gegen die es keine Vorwürfe gab. Im Zuge des Prozesses musste unter anderem Altun als Richter zurücktreten. 
"	technik
"Das Umweltbundesamt (UBA) bringt sein eigenes Messprogramm für den Schadstoffausstoß von Dieselfahrzeugen voran. Man befinde sich  ""kurz vor der Ausschreibung "", sagte ein Sprecher des Amts und bestätigte damit einen Bericht von Süddeutscher Zeitung, NDR und WDR. Geeignete Firmen sollen demnach künftig die Abgase auf der Straße messen – bislang wird dies nur im Labor gemacht. 
Für die Zulassung eines Autos entscheidend ist das Ergebnis von Tests unter Laborbedingungen. Zuständig in Deutschland ist das Kraftfahrt-Bundesamt (KBA). Ab September 2017 sind nach Vorgaben der EU Tests unter realen Fahrbedingungen vorgeschrieben. 
UBA-Präsidentin Maria Krautzberger sagte dem Rechercheverbund, in den hoch belasteten Innenstädten würden Diesel-Fahrzeuge  ""sicher keine Zukunft "" haben.  ""Ich bin erleichtert, dass diese Botschaft zumindest in Teilen der Autoindustrie endlich anzukommen scheint. "" Vor wenigen Tagen hatte Volkswagenchef Matthias Müller die Dieseltechnologie offen in Frage gestellt. Er zweifle daran, dass sein Unternehmen  ""noch viel Geld für die Weiterentwicklung des Diesels in die Hand nehmen "" werde, sagte Müller. 
Das KBA hatte nach Bekanntwerden des Manipulationsskandals bei Volkswagen mehr als 50 Dieselfahrzeuge untersucht. Die Gebrauchtwagen wurden im Labor und auf der Straße untersucht, und zwar vor allem auf den Ausstoß von gesundheitsschädlichen Stickoxiden (NOX). Wegen teils hoher Überschreitung der Grenzwerte kündigten Volkswagen, Audi, Porsche, Opel und Mercedes an, sie würden insgesamt 630.000 Autos für Nachbesserungen in die Werkstätten holen."	technik
"Bei Google haben sie sich offenbar einen großen Spaß daraus gemacht, Apple in den Tagen vor der Entwicklerkonferenz WWDC maximal zu trollen. 
Erst veröffentlichte Google am vergangenen Dienstag die App Motion Stills für iOS, die aus den meist verwackelten Live Photos saubere GIFs macht. So eine App hätte Apple selbst längst anbieten müssen, um aus seinem gehypten iPhone-Kamerafeature endlich eine sinnvolle Anwendung zu machen. 
Nur einen Tag später gab Google eine Version seiner Machine-Learning-Software TensorFlow für iOS frei. Damit können iOS-Entwickler Apps bauen, die auf Googles künstlicher Intelligenz (KI) beruhen. Apple selbst hat bisher nichts vorgestellt, was da mithalten könnte. 
Am Donnerstag kam – bestimmt ganz zufällig – heraus, dass Google-Mitgründer Larry Page an zwei Unternehmen beteiligt ist, die fliegende Autos entwickeln. Fliegende Autos! 
Apple hingegen gab kurz vor Beginn der WWDC bekannt, dass es im App Store für iOS bald Abomodelle und Werbung in der Suche gibt. Und wenn Nutzer einen Dienst länger als zwölf Monate abonnieren, senkt Apple seine Beteiligung an den Einnahmen von 30 auf 15 Prozent. In den Worten von Homer Simpson: laaaangweilig! Zumal es nur wenige Stunden später hieß (wenn auch bislang nur inoffiziell), dass Google das gleiche tun werde, sogar ohne die Zwölf-Monate-Beschränkung. 
Kommt da noch was, Apple? Oder wird die WWDC 2016 die unspektakulärste Entwicklerkonferenz, die das Silicon Valley seit Langem erlebt hat? Am Montag ab 19 Uhr deutscher Zeit werden Apple-CEO Tim Cook und seine Topmanager diese Frage beantworten. Apple hat für die Keynote das große Bill Graham Civic Auditorium in San Francisco gemietet, wie schon bei der Vorstellung des iPhone 6s im vergangenen Herbst. Der Saal fasst 7.000 Menschen, denen muss das Unternehmen etwas bieten. 
Muss? Das ist Ansichtssache. Die Erwartungen jedenfalls aus der Techjournalismus-Blase an Apple sind fast schon dreist. Sie lassen sich wie folgt zusammenfassen: Weil Microsoft, Facebook und zuletzt Google auf ihren Entwicklerkonferenzen ein neues Computerzeitalter mit künstlicher Intelligenz (KI) und superhilfreichen, alles wissenden Chatbots ausgerufen haben, müsse Apple das jetzt auch tun. 
Beispielhaft zeigt sich das im Beitrag des ZDNet-Chefredakteurs Larry Dignan.  ""Apple hat sich in eine Big-Data- und KI-Ecke manövriert und ist hinter Amazon, Google und andere zurückgefallen "", heißt es da,  ""es ist Zeit für einen neuen Pakt zwischen Apple und seinen Kunden, für ein KI-Comeback. "" Dignan fordert Apple auf, das Thema Privatsphäre weniger dogmatisch zu sehen und lieber das grenzenlose Vertrauen seiner Kunden zu nutzen, um irgendwas mit Siri, KI und Nutzerdaten zu machen. Auf Prinzipien pfeifen as a service, sozusagen. 
Dignan und andere wollen etwas Großes sehen, etwas Spektakuläres. Vielleicht sind sie auch angestachelt von Analysen wie dieser, laut der Apple mehr Geld denn je in die Entwicklung neuer Produkte steckt als je zuvor, was nur bedeuten könne, dass eine radikale Neuausrichtung bevorstehe. Dass die möglicherweise noch Jahre dauern wird, dass es Apple vielleicht erst einmal wichtiger ist, den Entwicklern etwas Handfestes zu geben als etwas Virtuell-Visionäres – geschenkt. 
 
Ganz unschuldig ist Apple nicht am Erwartungsdruck. Das Unternehmen inszeniert seine Entwicklerkonferenzen ähnlich wie seine Produktvorstellungen als Medienevents. Es geht nicht mehr nur um neue Schnittstellen und Frameworks für Entwickler, es geht um eine Vorschau auf das, was mit den nächsten iPhones und Macs möglich sein wird. Es geht darum, Produkte zu bewerben, die es nicht vor dem Herbst zu kaufen geben wird. Google und die anderen haben das Prinzip einfach übernommen und in diesem Jahr auf die vorläufige Spitze getrieben, mit ihren Visionen von etwas, das weit über die kommenden zwölf Monate hinausgeht. 
Wenn die Gerüchte stimmen, werden Apples Ankündigungen in diesem Jahr vergleichsweise bescheiden ausfallen. Siri dürfte im Mittelpunkt stehen: Es gilt als gesichert, dass iOS-Entwickler ihre Apps künftig mit Siri verknüpfen können. Ein Uber oder eine Pizza bestellen, Tweets diktieren, Fahrpläne abfragen, YouTube-Videos heraussuchen – so etwas wäre im kommenden Betriebssystem iOS 10 per Sprachbefehl und Sprachausgabe möglich, wenn die Entwickler denn darauf anspringen. 
Die virtuelle Assistentin soll zudem endlich auch auf dem Mac zur Verfügung stehen und dort auf Zuruf die gleichen Aufgaben erledigen wie auf dem iPhone oder iPad. Vorstellbar ist ebenfalls, dass Apple gleich Nägel mit Köpfen macht und Entwicklern mehr Spielraum beim Einsatz von Siri auch auf der Apple Watch und im Apple TV gibt. Wer nicht gerne mit seinen Geräten redet, hätte wenig von solchen Erweiterungen. 
Aber Apple dürfte ein gesteigertes Interesse daran haben, im – wann auch immer – kommenden Zeitalter der conversation as an interface eine marktreife Lösung zu haben, die auf einer Vielzahl von Geräten und in einer Vielzahl von Sprachen funktioniert, als verbindendes Element in Apples Hardware-Ökosystem. 
Die meisten anderen Gerüchte zur WWDC betreffen mehr oder weniger umfangreiche kosmetische Änderungen, neue Hardware ist eher nicht zu erwarten und wäre eine Überraschung. Dafür soll Apple Music eine neue Oberfläche bekommen, iTunes ebenso. Beide hätten es bitter nötig. Ob iOS 10 neben Siri und ein paar neu gestalteten Icons weitere größere Umbauten beinhaltet, ist noch eine der spannenderen Fragen. 
OS X wird angeblich umbenannt in MacOS oder macOS, damit es besser zu iOS, watchOS und tvOS passt, aber das ist nur Marketing und wird weder Entwickler noch sonst jemanden vor Begeisterung vom Sitz reißen. 
Apple Pay wird möglicherweise auch auf Websites und in iMessages intergriert, aber so lange der Dienst in Deutschland nicht zur Verfügung steht, hätten Apple-Kunden hierzulande nichts davon. 
Ein Gerücht immerhin gibt es, das ein wenig nach Zurücktrollen gegen Google klingt: Demnach wird Apple seinen SMS-Ersatz iMessage künftig auch in einer Android-Version anbieten. Es wäre ein direkter, bekannter und weit verbreiteter Konkurrent zu Googles kommendem Messenger Allo – inklusive standardmäßiger Ende-zu-Ende-Verschlüsselung."	technik
"Die Fahrräder parken zu Dutzenden draußen, drinnen brandet Jubel und Applaus auf im Restaurant Ampelmann in Berlin-Mitte. Die Anhänger der Initiative für ein Fahrrad-Volksbegehren in Berlin sind begeistert, als Denis Petri die Zahl der Unterschriften verkündet.  ""Mit einem solch hohen Ergebnis hätte ich im Traum nicht gerechnet "", sagt Petri, einer der Initiatoren des Projekts. 
105.425 Bürger unterschrieben in den vergangenen Wochen für den Antrag auf ein Volksbegehren, das dem Land Berlin ein  ""Gesetz zur Förderung des Radverkehrs "" bringen soll. Mehrere Aktenordner voller Unterschriften haben die Aktivisten im Restaurant nebeneinander aufgereiht, sie sollten noch am gleichen Tag der Berliner Landesregierung übergeben werden. 
Die von Petri, Heinrich Strößenreuther und weiteren Mitstreitern auf den Weg gebrachte Initiative Volksentscheid Fahrrad hat über fünfmal so viele Unterschriften eingesammelt wie nötig gewesen wären. Im ersten Schritt waren 20.000 Signaturen gefragt, damit das Volksbegehren die erste Hürde nimmt, von der Politik geprüft wird und womöglich im nächsten Jahr als eigentlicher Volksentscheid den Berliner Bürgern vorgelegt wird. 
Tatsächlich war die Resonanz enorm, wie Strößenreuther und Petri berichten. Allein bei der traditionellen Sternfahrt des Allgemeinen Deutschen Fahrrad-Clubs am ersten Juniwochenende hätten 15.000 Menschen auf den Listen unterzeichnet. Die Initiative feiert Berlins schnellsten Volksentscheid, denn die gut 100.000 Unterschriften kamen in nur dreieinhalb Wochen zusammen – die Vorgabe geht von 20.000 in sechs Monaten aus. 
Der Berliner Senat habe sich völlig verkalkuliert, sagte Petri bei der Vorstellung der Zahlen. Die Anhänger des Projekts unterstellen dem Senat, mit seiner amtlichen Kostenschätzung für das Vorhaben bewusst übertrieben zu haben, um Befürworter abzuschrecken. 2,1 Milliarden Euro setzt die Landesregierung über einen Zeitraum von acht Jahren an, wenn man die Forderungen der Initiative umsetze. Diese wiederum geht von weitaus geringeren Kosten aus: Die direkten Aufwendungen belaufen sich laut Strößenreuther auf 392 Millionen Euro – und darin seien Einsparungen noch nicht gegengerechnet. Unterm Strich komme man auf 320 Millionen Euro an Kosten. 
Ziel der Initiative Volksentscheid Fahrrad ist, dass sich Radfahrer in Berlin sicher und komfortabel fortbewegen können und so noch mehr Berliner aufs Fahrrad umsteigen. In Zusammenarbeit mit dem Allgemeinen Deutschen Fahrrad-Club (ADFC) und anderen Verbänden wurde der Entwurf eines Radgesetzes mit zehn Kernmaßnahmen entwickelt, die innerhalb von acht Jahren umgesetzt werden sollen: 
- ein Netz aus 350 Kilometern sicheren Fahrradstraßen 
- zwei Meter breite Radwege oder Schutzstreifen entlang jeder Hauptstraße 
- mindestens 100 Kilometer Radschnellwege für den Rad-Pendlerverkehr in und durch die Stadt 
- jedes Jahr 75 gefährliche Kreuzungen sicher umgestalten 
- schnelle und effektive Beseitigung von Mängeln an Radwegen und Fahrradstraßen 
- auf Hauptstraßen 50 grüne Wellen für Radfahrer, Fußgänger und den öffentlichen Nahverkehr 
- 200.000 Abstellmöglichkeiten an ÖPNV-Haltestellen und an Straßen; Einrichtung von Fahrradparkhäusern und Fahrradstationen an Regionalbahnhöfen 
- Fahrradstaffeln bei allen Polizeidirektionen und Ordnungsbehörden, Einrichtung einer Ermittlungsgruppe für Fahrraddiebstähle bei der Berliner Polizei 
- personell gut ausgestattete und vernetzte Verwaltungseinheiten für Radverkehrsbelange 
- Förderung des Radverkehrs durch Öffentlichkeitsarbeit; alle Verkehrsteilnehmer für ein besseres Miteinander sensibilieren 
Ab einer Schwelle von 20.000 gültigen Unterschriften muss die Berliner Senatsverwaltung die Zulässigkeit eines Antrags auf ein Volksbegehren prüfen. Dafür müssen die Initiatoren aber zugleich einen fertigen, inhaltlich begründeten Gesetzentwurf vorlegen, mit dem sich das Abgeordnetenhaus den Befürwortern zufolge befassen soll. 
Als nächstes überprüft die Senatsverwaltung die Gültigkeit der Unterschriften. Nach Angaben der Initiative Volksentscheid Fahrrad erweist sich erfahrungsgemäß ein Viertel als ungültig – etwa weil Menschen mehrmals unterschreiben oder die Idee unterstützen, obwohl sie nicht in Berlin wohnen. 
Die Initiative geht außerdem davon aus, dass bis Jahresende der Senat die rechtliche Prüfung des Radgesetz-Entwurfs abgeschlossen haben wird. Im nächsten Jahr könnte dann – falls nicht der Senat beziehungsweise das Abgeordnetenhaus von sich aus das Gesetz annehmen – das Volksbegehren folgen: Es würde das Berliner Landesparlament dazu auffordern, den Gesetzentwurf innerhalb einer Frist zu behandeln und zu billigen. Dafür müssten aber erneut Unterschriften gesammelt werden, nämlich 175.000 Stück binnen vier Monaten. Zum eigentlichen Volksentscheid käme es erst, wenn das Abgeordnetenhaus den Entwurf ablehnt. Dann würden die Berliner Bürger direkt entscheiden. 
Die Initiatoren sehen für die enorme Diskrepanz zwischen ihrer Rechnung und der des Senats einen wesentlichen Grund: Der Senat kalkuliere mit erheblich mehr Hauptstraßen, an denen der Volksentscheid breite Radwege fordere – dadurch komme man auf eine höhere Zahl, deren Bau natürlich viel mehr Geld kostet. Zumal der Senat auch mit wesentlich höheren Standards für die Radwege rechne als die Initiative – die hier von einem  ""Deluxe-Aufschlag "" spricht. 
 ""Der Senat plant eine Fünf-Sterne-Fahrradstadt, wir wären mit einer Drei-Sterne-Fahrradstadt zufrieden "", fasst Strößenreuther zusammen. Es sei zu begrüßen, wenn der Senat über zwei Milliarden Euro für Radverkehr ausgeben wolle – er solle diese Summe aber nicht weiterhin als die Kosten des Radgesetzes bezeichnen, verlangte der Mitinitiator.  ""Und wenn der Senat dafür doch keine zwei Milliarden ausgeben will, dann soll er unser Radgesetz annehmen und umsetzen. "" Mit 392 Millionen Euro sei eine gute, sichere Radinfrastruktur in Berlin zu schaffen. 
Das Volksbegehren fällt in der Hauptstadt in eine politisch hochspannende Zeit: Für Mitte September ist die Wahl zum Abgeordnetenhaus angesetzt, Strößenreuther geht davon aus, dass Radverkehr  ""eines der Top-Wahlkampfthemen "" sein wird. Einen entscheidenden Anstoß dazu hat er mit seiner Initiative gegeben. Zumal er nicht müde wird hervorzuheben, dass die Berliner CDU das Thema entdeckt und selbst bereits eine bessere Radinfrastruktur angemahnt habe – während die SPD, die neben dem Regierenden Bürgermeister Michael Müller auch den Verkehrssenator Andreas Geisel stellt, sich bisher eher abwehrend verhalte. Er wolle  ""lieber mehr Rad-Genossen als Auto-Sozen "", sagt Strößenreuther süffisant. Er setzt darauf, dass die SPD jetzt die Flucht nach vorn startet, angetrieben nicht zuletzt durch die hohe Zahl an Unterschriften. 
Immerhin: Senator Geisel hat zugesagt, sich mit den Initiatoren zum Gespräch zu treffen. Am 18. Juli werden er, sein Staatssekretär Christian Gaebler (SPD) sowie der in der Senatsverwaltung für Verkehr zuständige Burkhard Horn sich mit der Initiative Volksentscheid zusammensetzen. Ob diese bereit sei zu Kompromissen, also auch zu Abstrichen in ihrem Radgesetz? Dazu will sich Strößenreuther derzeit nicht äußern: Der Senat sage bisher nicht konkret, was er bis wann umsetzen wolle, er höre  ""immer nur ein Rumheulen und Klagen "", was alles nicht gehe, sagt Strößenreuther. An Geld mangele es der Stadt nicht: In den letzten fünf Jahren sind laut der Initiative in Berlin 4,6 Millionen Euro an eingeplantem Radverkehrsbudget gar nicht ausgegeben worden, weil es Berlin an Personal fehle und Senat und Bezirke nicht richtig zusammenarbeiten würden. 
Eines ist für Strößenreuther deshalb klar: Ohne ein bindendes Gesetz wird es nicht gehen. Denn es verpflichtet den Senat und die untergeordneten Behörden zur Umsetzung beschlossener Maßnahmen in einem festgelegten Zeitraum. Ein solches Radgesetz – es wäre das erste in Deutschland – hätte außerdem Vorbildcharakter. Die Initiative hat nach eigener Aussage bereits Anfragen aus anderen Städten erhalten, wie man einen solchen Volksentscheid startet. Die Stimmung auf den Straßen drehe sich – nicht nur in Berlin."	technik
"Acht Jahre hatte die Menschheit, um zu lernen, was Apps sind: Voneinander getrennte Programme, die sich einzeln mit einer Fingerberührung vom Homescreen eines Smartphones oder Tablets starten lassen, wenn sie gerade benötigt werden. 2007, mit dem ersten iPhone, wurde das Modell massentauglich. Nun wird es Zeit, es wieder zu vergessen. 
Denn  ""das Ende von Apps, wie wir sie kennen "" naht, wie manche es ausdrücken. Gar vom  ""Tod des Startbildschirms "" ist die Rede. Vielleicht nicht ausnahmslos und innerhalb weniger Monate. Aber Apple und Google arbeiten zumindest daran. Ihr Ziel: Apps werden nicht mehr die Benutzeroberfläche des Betriebssystems sein, sondern umgekehrt. Android und iOS drängen sich in den Vordergrund und degradieren Apps zu Datenlieferanten. Das soll den Nutzern, die immer mehr auf ihren mobilen Geräten erledigen, dabei helfen, Zeit zu sparen und nicht mehr ständig zwischen verschiedenen Apps hin- und herwechseln zu müssen. 
Die folgenden sechs Ansätze werden noch in diesem Jahr zu Bestandteilen von Android und iOS – oder sind es bereits. 
Googles aufgebohrte Version seines virtuellen Assistenten Google Now wird eine neue Benutzungsoberfläche des für den Herbst angekündigten Betriebssystems Android M. Das Prinzip: Google Now on tap versucht zu verstehen, was ein Nutzer gerade auf seinem Smartphone- oder Tabletdisplay sieht, um ergänzende Informationen aus anderen Apps hinzuzufügen. 
Zwei Beispiele: Wer von Freunden per E-Mail gefragt wird, ob er abends mit ins Kino kommt, um Mad Max zu schauen, muss nur den Homebutton seines Smartphones gedrückt halten, um Informationen über den Film angezeigt zu bekommen – ohne die E-Mail-App zu verlassen. Und wer eine Kurznachricht erhält, in der jemand ein Restaurant vorschlägt, bekommt erste Informationen aus Yelp und anderen thematisch passenden Apps eingeblendet – ohne dass diese geöffnet werden. 
Google Now on tap ist nicht auf den Homebutton beschränkt. Wer auf seinem mobilen Gerät gerade Musik hört, braucht nur zu sagen:  ""OK Google, wie heißt der Sänger dieser Band? "", und schon bekommt er die gewünschte Information angezeigt. Dank der Kombination aus Spracheingabe und Googles Fähigkeit, Bildschirminhalte zu erkennen, ist es nicht mehr nötig, die Musik-App zu verlassen und über den Startbildschirm die Suchmaschinen-App aufzurufen und dort den Namen der Band einzutippen. 
Ein zweites Anwendungsbeispiel demonstrierte Google auf einer Konferenz in Paris: In einer App war ein Tennisspieler zu sehen, ein Google-Manager fragte nur  ""OK Google, wie viele Grand Slams hat er gewonnen? "" Der virtuelle Assistent verstand, dass sich das  ""er "" in der Frage auf den Tennisspieler bezog und gab die Antwort. 
Google Now on tap ist außerdem in Lage, aus dem Standort des mobilen Geräts auf die Umgebung zu schließen und Fragen zu beantworten wie  ""Wie heißt diese Kirche? "" oder  ""Wie hoch ist dieser Turm? "". 
Die entsprechenden Gebäude und Geschäfte müssen allerdings in Google Maps verzeichnet sein. Und die Nutzer müssen GPS aktiviert haben. Das aber frisst Akku und kreiert ein exaktes Bewegungsprofil. 
 
In iOS 9 wird Apple den Standort ebenfalls für kontextabhängige Aktionen nutzen. Der neue proactive assistant – in Deutschland sagt Apple dazu proaktive Hilfestellung – merkt sich, wo ein Nutzer am liebsten welche Musik hört. Stöpselt der Nutzer dann an diesem Ort die Kopfhörer ins iPhone, startet diese Musik umgehend. Die Musik-App aufzurufen, ist nicht mehr nötig. 
Apples lernende Maschine unterstellt damit allerdings, dass ein Mensch immer nach demselben Muster handelt. Wer mal schlechte Laune hat und etwas ganz anderes hören will als sonst, muss die ungefragt startende Musik erst stoppen und sich dann selbst die passende in der Musik-App suchen. 
Apple wird in iOS eine Alternative zum Startbildschirm anbieten: die erweiterte Suche. Das Unternehmen bittet App-Entwickler, ihre Anwendungen zu indizieren und durchsuchbar zu machen, damit die neue Suche in iOS 9 – ähnlich wie Google Now on tab – Informationen aus mehreren Apps zusammenstellen und geballt anzeigen kann. 
Entwickler müssen davon ausgehen, dass Nutzer die eigentliche App dadurch zwar so häufig wie bisher nutzen, aber seltener aufrufen. Wessen Geschäftsmodell auf In-App-Werbebannern beruht, dürfte das wenig attraktiv finden. 
Benachrichtigungen über eingehende Kurznachrichten oder Eilmeldungen waren in der Vergangenheit meist nur ein Hinweis, dass in einer App etwas passiert ist. Wer auf eine WhatsApp-Notification tippte, landete in WhatsApp. Die aktuellen Versionen von Android und iOS aber bieten schon interactive notifications in Form sogenannter Karten. Sie umfassen den eigentlichen Inhalt aus der App wie eben den Text einer Kurznachricht, sowie die Möglichkeit, direkt darauf zu antworten. Die eigentliche App müssen Nutzer für diese Aktionen nicht aufrufen."	technik
"Geht es nach deutschen Großstädten, könnten umweltschädliche Dieselfahrzeuge bald mit einem gezielten Fahrverbot belegt werden. Als Möglichkeit für die Durchsetzung eines Verbots sehen etwa Berlin, München, Bremen und Stuttgart die Einführung der blauen Umweltplakette, wie eine Umfrage der Deutschen Presse-Agentur ergab. Durch den Aufkleber könnten Autos mit hohem Ausstoß von gesundheitsschädlichen Stickoxiden (NOx) aus Städten und Ballungsräumen ausgeschlossen werden. Gerade dort werden Grenzwerte immer wieder deutlich überschritten. 
Die Großstädte weisen jedoch auf die bislang fehlende gesetzliche Grundlage für eine blaue Umweltplakette hin. Deswegen rechnet etwa Bremen mit einer Einführung nicht vor dem Jahr 2018. Die Plakette wurde im Frühjahr durch Bundesumweltministerin Barbara Hendricks (SPD) ins Gespräch gebracht. Daran gab es jedoch viel Kritik – vor allem aus der Union und von Automobilverbänden. 
Städte wie München oder Berlin warnten vor sozialer Härte bei der Einführung des Aufklebers. Es bedürfe Übergangsfristen und Ausnahmeregelungen. Beispielsweise für Anwohner oder Betriebe, hieß es aus den Verwaltungen. Eine Nachrüstung entsprechender Dieselautos, wie etwa zur Reduzierung von Feinstaub mit einem Partikelfilter, sei nicht möglich, darauf wies München hin. 
Immer wieder werden in Städten und Ballungszentren EU-Grenzwerte für Stickoxide deutlich überschritten. Beim besonders gesundheitsschädlichen Gas Stickstoffdioxid (NO2) stellte das Umweltbundesamt im vergangenen Jahr an rund 60 Prozent aller Messstationen an stark befahrenen Straßen Überschreitungen fest. 
Der von der EU festgelegte Grenzwert von 40 Mikrogramm Stickstoffdioxid pro Kubikmeter im Jahresdurchschnitt wurde 2015 besonders deutlich in Stuttgart gerissen. An der Straße am Neckartor lag die Konzentration des lungenschädlichen Gases bei durchschnittlich 87 Mikrogramm. An der Landshuter Allee in München waren es 84 Mikrogramm pro Kubikmeter. Auch an einzelnen Messstationen in Köln, Kiel, Heilbronn, Hamburg und Darmstadt wurden Werte von über 60 Mikrogramm pro Kubikmeter festgestellt. 
Weil die Grenzwerte in Deutschland seit Jahren überschritten werden, hatte die EU-Kommission gegen Deutschland im vergangenen Jahr ein Verfahren eröffnet. Immer wieder warnen auch Umweltorganisationen vor gesundheitlichen Risiken durch Stickstoffdioxid. 
Konkrete Pläne für Diesel-Fahrverbote gibt es in den größeren Städten Deutschlands aber noch nicht. In Düsseldorf wollen die Behörden abwarten, welche Rahmenbedingungen für die blaue Plakette gelten sollen. In Dortmund sagte eine Sprecherin der Stadt, dass erhöhte NO2-Belastungen ausschließlich in Straßennähe nachweisbar seien. Bereits nach kurzer Distanz lägen die Belastungen unterhalb der Grenzwerte. 
In München will die Verwaltung zunächst auf Alternativen setzen, um Stickoxid-Werte zu senken. Etwa mit dem Ausbau von Ladestationen für Elektroautos oder der Förderung von Elektroautos und -fahrrädern für Handwerker, Vereine oder Lieferdienste. In Berlin verweist man auf die Anschaffung von neuen Linienbussen. Dadurch seien in bestimmten Straßen der Hauptstadt Stickoxidemissionen um mehr als zehn Prozent zurückgegangen."	technik
"Glaubt man der PR aus Brüssel, hat Europa jetzt seine eigenen Avengers: Die neue Superheldentruppe besteht aus der EU-Kommission und den vier Plattformanbietern Google in Form von YouTube, Facebook, Microsoft und Twitter. Gemeinsam wollen sie Hetze im Internet stärker bekämpfen, die Ausbreitung von Hassbotschaften unterbinden, Rassismus und Fremdenfeindlichkeit in sozialen Netzwerken stoppen und nebenbei noch Freiheit und Toleranz im Netz fördern. 
So steht es in einem neuen Verhaltenskodex, den die vier Unternehmen und die EU-Kommission am gestrigen Dienstag in Brüssel vorstellten. Demnach verpflichten sie sich,  ""klare und wirksame Verfahren "" für die Prüfung von Hasskommentaren einzurichten. Anträge sollen binnen 24 Stunden geprüft und die entsprechenden Inhalte gegebenenfalls gelöscht werden. Gleichzeitig sollen sie künftig enger mit nationalen Kontaktstellen und Partnerorganisationen zusammenarbeiten. 
Gemeinsam im Kampf gegen den Hass – das ist natürlich eine gute Schlagzeile. Sie passt auch hervorragend in den Diskurs, der seit einigen Monaten die öffentliche Debatte über soziale Netzwerke bestimmt: Rassistische Kommentare auf Facebook. Terrorpropaganda auf Twitter. Hatespeech auf YouTube und Reddit. All das und  ""die jüngsten Terroranschläge "" hätten der EU-Kommission vor Augen geführt,  ""wie dringend gegen illegale Hetze im Internet vorgegangen werden muss "", sagte die verantwortliche EU-Kommissarin Věra Jourová. Sie begrüßt deshalb die Selbstverpflichtung der vier großen IT-Unternehmen. 
Und die begrüßen freundlich zurück:  ""Wir freuen uns auf die gemeinsame Arbeit mit der EU-Kommission "", heißt es vonseiten Googles.  ""Wir freuen uns auf einen weiteren konstruktiven Dialog "", sagt die Europachefin von Twitter. Und Monika Bickert von Facebook ist froh, dass man die eigenen Anstrengungen zur Bekämpfung von Hasskommentaren mit europäischer Unterstützung fortsetzen könne. 
So viel Harmonie herrscht selten zwischen beiden Seiten. Wäre die Beziehung zwischen den führenden US-Technikunternehmen und der EU-Kommission nämlich ein Facebook-Status, könnte man sagen: Es ist kompliziert. Google steht aufgrund der Gestaltung seiner Suchergebnisse und der marktbeherrschenden Stellung des mobilen Betriebssystems Android derzeit im Fokus von Wettbewerbsverfahren. Die Streamingdienste von Amazon und Netflix sollen künftig mehr europäische Filme und Serien zeigen, Microsoft wurde vor drei Jahren zu Bußgeldzahlungen in Höhe von mehr als 560 Millionen Euro verdonnert. 
Die Debatte um Hasskommentare und der nun vorgestellte Verhaltenskodex bieten den Unternehmen nun die seltene Möglichkeit, die Wogen zumindest etwas zu glätten und der EU-Kommission politisch entgegenzukommen. Und das Beste aus ihrer Sicht: Groß verbiegen müssen sie sich dafür nicht. Denn vieles von dem, was die Selbstverpflichtung enthält, tun sie ohnehin bereits. Mit dem Verhaltenskodex bekräftigen sie nur ihre Bemühungen. 
 
So gibt es auf allen Plattformen bereits Verfahren, über die Nutzer auffällige Inhalte melden können. Es gibt Community-Guidelines, in denen auf verbotene Inhalte hingewiesen wird, auf Twitter etwa auch explizit auf terroristische Propaganda. Facebook und Google setzen sich schon jetzt für einen aktiven Gegendiskurs auf ihren Plattformen ein und sponsern europaweite Initiativen. Mit Strafverfolgungsbehörden arbeiten sie ebenfalls bereits zusammen, wie einige Verurteilungen in den vergangenen Monaten gezeigt haben. 
Auch die Selbstverpflichtung, illegale Inhalte binnen 24 Stunden zu prüfen und nationale Gesetze zu befolgen, gibt es bereits, wenn auch nicht europaweit. Im Dezember hatte Justizminister Heiko Maas (SPD) eine Arbeitsgruppe mit Facebook, Twitter und Google ins Leben gerufen, damit Inhalte schneller gelöscht werden, nämlich im besten Fall innerhalb von 24 Stunden. Facebook hat seit Anfang des Jahres eine eigene Task Force in Deutschland, die aber nach Einschätzung des Justizministeriums noch immer viel zu langsam arbeitet. Trotzdem könnte das deutsche Modell demnächst auch europaweit angewandt werden. 
Ob sich mit dem Verhaltenskodex aber wirklich viel ändert, ist zweifelhaft. Zumal selbst die Formulierungen in dem Dokument aus Brüssel reichlich Spielraum für Interpretationen lassen, was einige deutsche Politiker bereits kritisieren. Beispielsweise müsse bloß  ""die Mehrheit der gültigen Meldungen "" innerhalb von 24 Stunden zu prüfen sein – ob das nun nur für Meldungen von NGOs gilt oder für sämtliche Nutzermeldungen, ist unklar. Die Unternehmen müssen sich des Weiteren  ""um Partnerschaften aus der Zivilgesellschaft "" bemühen und die  ""Zusammenarbeit untereinander verbessern "". Wie das genau aussieht und wie das überprüft werden soll, bleibt unerwähnt. Vieles ist abstrakt, wenig konkret. 
Den IT-Unternehmen kann das nur Recht sein. Denn auch wenn sie zweifelsfrei daran interessiert sind, Hasskommentare auf ihren Plattformen zu minimieren, wollen sie sich zum einen auch nicht zu sehr einmischen; die Linie zwischen Zensur und freier Meinungsäußerung ist nicht zuletzt auch durch verschiedene nationale Gesetzeslagen dünn. Zum anderen laufen sie Gefahr, polizeiliche oder hoheitliche Aufgaben zu übernehmen, indem sie in letzter Instanz entscheiden, wann welcher Kommentar tatsächlich gegen geltendes Recht verstößt. Diesen Aspekt kritisieren deshalb Bürgerrechtler und Netzaktivisten an dem neuen Verhaltenskodex: Statt den Behörden übernehmen die Plattformen die führende Rolle, heißt es. Das könne dazu führen dass kontroverse, aber möglicherweise legale Inhalte ebenfalls gelöscht werden. 
In den kommenden Monaten wollen die EU-Kommission und die Unternehmen weiter an gemeinsamen Lösungen arbeiten. Die jetzige Selbstverpflichtung ist in erster Linie eine politische Geste: Microsoft, Facebook, YouTube und Twitter zeigen der EU-Kommission damit, dass sie das Problem mit der Hetze im Internet erkannt haben, sich geschlossen für Lösungen einsetzen und den Verantwortlichen in Brüssel entgegenkommen. In einer Zeit, in der die EU-Kommission gerne die Peitsche schwingt, kann ein bisschen Zuckerbrot schließlich nicht schaden. 
Korrektur: In einer früheren Fassung des Artikel hieß es, Facebook hat mit der NGO Access Now zusammengearbeitet. Tatsächlich war Access Now zunächst an den Gesprächen zwischen den IT-Unternehmen und der EU-Kommission beteiligt, hat diese aber mittlerweile verlassen und kritisiert den vorgestellten Verhaltenskodex scharf."	technik
"Der Konflikt auf den Straßen scheint ein immerwährender – und oft beruht er auf Missverständnissen und Unkenntnissen. Zum Beispiel, wenn Autofahrer glauben, dass Radfahrer niemals nebeneinander fahren dürfen. Oder wenn Radler meinen, sie dürften an einer roten Ampel mal eben auf den Gehweg ausweichen. 
Die Straßenverkehrsordnung (StVO) hält auch manche Überraschung parat. Nicht jeder Autofahrer weiß offensichtlich, dass es auch Radwege gibt, deren Benutzung freiwillig ist – eine Regel, die übrigens schon seit den späten 1990er Jahren existiert. Und viele Pkw-Nutzer sind sich augenscheinlich auch nicht im Klaren darüber, welche Rechte in sogenannten Fahrradstraßen gelten, die als solche ausgeschildert sind: Autos sind dort nur in ausdrücklich gekennzeichneten Ausnahmen erlaubt, zum Beispiel wenn es sich um Fahrzeuge von Anwohnern handelt. 
Manches Verhalten ist nicht geregelt, sollte aber dem gesunden Menschenverstand nach logisch sein, zum Beispiel, dass es riskant ist, mit Einkaufstüten am Lenker Rad zu fahren. Wie gut wissen Sie über Verkehrsregeln Bescheid, die Radfahrer betreffen? 
"	technik
"Kaum ein Tag vergeht, an dem Volkswagen nicht mit neuen Vorwürfen im Abgasskandal konfrontiert wird. Wurde die Manipulation der Dieselmotoren noch verfeinert, als die US-Behörde Ende 2014 bereits gegen VW ermittelte? Hat VW in den USA Beweismaterial vernichtet, wie ein geschasster Mitarbeiter behauptet? 
Antworten auf diese und andere Fragen gibt es nicht – Wolfsburg schweigt. Der Konzern verweist auf die laufenden Ermittlungen, erst müssten diese abgeschlossen sein. Selbst die Jahrespressekonferenz wurde auf Ende April verschoben, weil die von VW beauftragte US-Kanzlei Jones Day, die im Konzern den Skandal aufarbeitet, erst in der zweiten April-Hälfte Bericht erstatten will. Der Konzern betont, dass das sichergestellte Datenmaterial 102 Terabyte umfasse, ein riesiger Wust. 
Aber muss das alles wirklich so lange dauern? Am Freitag ist es genau sechs Monate her, dass die US-Umweltbehörde Epa den Betrug mit manipulierten Dieselfahrzeugen publik machte. Über die Gründe ist bislang wenig bekannt. Die dürren Mitteilungen des Konzerns erwecken oftmals den Eindruck, VW wolle bewusst Antworten schuldig bleiben. 
Noch im Herbst räumte der Konzern ein:  ""Wir haben das wichtigste Teil unserer Autos kaputt gemacht: Ihr Vertrauen. "" Seither hat VW aber nur wenig Schuldbewusstsein an den Tag gelegt. Beispielhaft ist dafür das blamable Radiointerview, das VW-Chef Matthias Müller Mitte Januar in den USA gab, in dem er den Skandal als  ""technisches Problem "" herunterspielte und behauptete, VW habe nicht die Epa belogen, sondern  ""zunächst die Frage nicht verstanden "". Im Konzern wird der Skandal nüchtern  ""Dieselthematik "" genannt. Die Ingenieure bezeichnen den Rückruf, bei dem seit Ende Januar in Deutschland nach und nach die Fahrzeuge mit manipulierten Motoren korrigiert werden, als  ""Diesel-Projekt "". 
Auch bezüglich des Rückrufs von Millionen Fahrzeugen zeigt sich das Unternehmen schmallippig. Bekannt ist nur, dass laut VW bei den 1,2- und 2,0-Liter-Varianten des betroffenen Dieselmotors EA 189 das Aufspielen eines Software-Updates genügt, bei den 1,6-Liter-Motoren soll in der zweiten Jahreshälfte zusätzlich ein Plastikbauteil, ein Strömungsgleichrichter, eingebaut werden. Doch was die Software beim Motormanagement ganz genau verändert, ist bisher unklar.  ""Wir wünschen uns, dass Volkswagen transparenter wird, was die Details des Rückrufs betrifft "", sagt Reinhard Kolke, der Leiter des ADAC-Technikzentrums in Landsberg am Lech. 
Er hat derzeit allerdings auch ein wenig Verständnis für Volkswagen:  ""Im Moment hat das Kraftfahrt-Bundesamt (KBA) die vom Unternehmen vorgesehenen Maßnahmen für den Passat noch gar nicht freigegeben. Da ist es nachvollziehbar, dass der Hersteller noch keine Details bekannt geben will. "" Der Rückruf war Ende Januar mit dem in Deutschland seltenen VW-Pick-up Amarok gestartet. Nach und nach sollen die betroffenen Modelle in die Werkstätten gerufen werden. Für die vielen Modellvarianten muss VW unterschiedliche Lösungen entwickeln, die jeweils vom KBA genehmigt werden müssen. 
Kritischer ist Gerd Lottsiepen, verkehrspolitischer Sprecher des ökologisch ausgerichteten Verkehrsclubs Deutschland (VCD). Er wirft Volkswagen vor, die Öffentlichkeit an der Nase herumzuführen.  ""Der Konzern tut so, als würde er zu Kreuze kriechen – tatsächlich aber schweigt er weiter, fährt eine Salami-Taktik und versucht alles, um auch künftig halblegale Schlupflöcher zu nutzen und Abgaswerte schönzurechnen. "" Tatsächlich vertritt VW sogar die Ansicht, nach den europäischen Regelungen sei die Manipulationssoftware gar keine verbotene Abschalteinrichtung. 
Die dürftigen Informationen über den Rückruf schüren die Skepsis gegenüber VW. Wenn für einen Großteil der Motoren ein schlichtes Software-Update reicht, das lediglich die Kennfelder der Motorsteuerung ändert, fragt man sich, warum der Dieselmotor nicht von vornherein so eingestellt war und es überhaupt einer Manipulation bedurfte. Auch diese Frage beantwortet VW bislang nicht. 
Das Unternehmen verspricht, dass die Autos nach dem Update nicht nur die geforderten Grenzwerte erreichen, sondern auch eine identische Fahrleistung aufweisen. Man habe gegenüber dem KBA und einem unabhängigen technischen Dienst nachgewiesen,  ""dass die bisherige Motorleistung, das maximale Drehmoment sowie die bisherigen Geräuschemissionswerte unverändert bleiben. Auch die ursprünglich angegebenen Kraftstoffverbrauchswerte und damit die CO2-Emissionen wurden bestätigt. "" 
 
Man ändert die Motorsteuerung – aber es bleibt alles beim Alten? Das klingt nach der Quadratur des Kreises. ADAC-Technikchef Kolke hält es zumindest für  ""nicht zwingend unrealistisch "". Ein Vorher-nachher-Test der Zeitschrift auto motor und sport (ams) hat jedoch gezeigt, dass es nicht so einfach ist, wie VW behauptet. Die beiden getesteten VW Amarock enthielten einen manipulierten Dieselmotor 2.0 TDI, bekamen das Software-Update und verbrauchten laut ams anschließend deutlich mehr als vor dem Rückruf. Kolke hält den ams-Test auf der Straße indes für problematisch: Die Verbrauchsunterschiede könnten sich auch durch unterschiedliche Fahrweisen der Testfahrer vorher und nachher erklären lassen. Der ADAC will, sobald VW den Rückruf der Passats begonnen hat, unter Laborbedingungen, aber mit realitätsnäheren Fahrzyklen seines seit Jahren angewandten Eco-Tests Passat-Exemplare von ADAC-Mitgliedern vorher und nachher untersuchen. 
Und was die einfach klingende Software-Lösung betrifft:  ""Man mag denken, dass VW von Anfang an den Motor so hätte einstellen und auf die Abschalteinrichtung hätte verzichten können "", sagt Kolke.  ""Doch in den USA, wo der Skandal den Anfang nahm, haben die Behörden diese Lösung offenkundig ja nicht akzeptiert. "" 
Tatsächlich streitet VW mit dem US-Umweltamt Epa und der kalifornischen Carb bis heute über die richtigen Maßnahmen zur Korrektur der betroffenen Autos.  ""Das große Problem liegt für Volkswagen in den USA, denn dort ist der Grenzwert erheblich niedriger "", erläutert Kolke.  ""Und die Amerikaner erwarten dauerhaft geringe Abgase nicht nur im Labor, sondern auch auf der Straße. Das war in Europa nach Ansicht des Automobilherstellers nicht die Aufgabenstellung. "" 
VW verweist darauf, dass auf dem Rollenprüfstand vor dem Update wie auch danach die offiziellen Grenzwerte eingehalten würden – nur nach dem Rückruf eben ohne die Manipulation. Insofern würden die betroffenen Autos jetzt in den  ""ordnungsgemäßen Zustand "" versetzt. Ziel des hiesigen Rückrufs ist es also nicht, dass die korrigierten Autos künftig im Alltag nicht mehr Stickoxide ausstoßen als in den Vorgaben festgelegt. 
Das zeigt auch der ams-Test mit den Amaroks: Die stießen auf der Straße vorher wie nachher pro Kilometer rund 1,5 Gramm NOx aus, also mehr als das Achtfache des Pkw-Grenzwerts. Das empört VCD-Experte Lottsiepen:  ""Es kann ja wohl nicht sein, dass auch nach dem Austausch der Software der Grenzwert im Realverkehr weiterhin eklatant überschritten und damit die Gesundheit der Bürger geschädigt wird – nur, dass das jetzt vom Kraftfahrt-Bundesamt für legal erklärt wird. "" VW steht auf dem Standpunkt, dass es für den Alltagsbetrieb derzeit keine Grenzwerte gebe und die Fahrzeuge lediglich die Abgaslimits im offiziellen Prüffahrzyklus erfüllen müssten. 
So lässt also nicht nur die Aufklärung des Betrugs durch VW auf sich warten, auch der Rückruf steht durch die ams-Messungen zumindest in einem fahlen Licht. In Wolfsburg war man ziemlich verärgert über den Bericht in der Zeitschrift, ist zu hören. Das Kraftfahrt-Bundesamt sieht indes kein Problem: VW habe alle Anforderungen beim Amarok erfüllt, die europäischen Genehmigungsvorschriften würden im Labortest eingehalten und es sei auch  ""nachgewiesen, dass keine unzulässige Abschalteinrichtung vorhanden ist "". 
 ""Das KBA handelt scheinbar rechtskonform, aber auf jeden Fall schizophren "", sagt VCD-Experte Lottsiepen. Er beklagt auch, dass die Ergebnisse der KBA-Messungen von mehr als 50 Dieselfahrzeugen verschiedener Hersteller bis heute nicht veröffentlicht wurden. Bundesverkehrsminister Alexander Dobrindt (CSU) hatte nach dem Auffliegen der VW-Manipulationen die Messungen in Auftrag gegeben. Umweltverbände gehen davon aus, dass ähnlich, wie es bei VW ans Tageslicht kam, auch andere Autobauer die Abgastests manipulieren. 
Und VW? Tut sich weiter schwer damit, die Fehler einzuräumen. Auch weil die Klagen von Fahrzeughaltern, Anlegern und der US-Regierung vor Gericht noch gar nicht laufen. Da will sich VW nicht frühzeitig durch zu viel Schuldeingeständnis in eine schlechte Position bringen."	technik
"Die Deutsche Akkreditierungsstelle mit Sitz in Berlin bemängelt laut Spiegel die Arbeit von Organisationen wie TÜV, Dekra und GTÜ. Die Stelle teilte den Prüfeinrichtungen mit, dass sie zum 10. Dezember für alle amtlich anerkannten Überwachungsorganisationen die Akkreditierung ausgesetzt habe, zitierte der Spiegel aus einem internen Protokoll. 
Hintergrund der Entscheidung seien Vorwürfe, die Prüfer hätten ihre Arbeit mit Messgeräten nicht ordnungsgemäß dokumentiert. 
Sobald der Bescheid bestandskräftig ist, müssen die Bundesländer laut dem Bericht den Prüforganisationen die Anerkennung entziehen. Laut dem internen Protokoll würde ein Widerruf der amtlichen Anerkennung  ""das System der Hauptuntersuchungen zusammenbrechen lassen "". Damit wäre offen, wer die nächsten Hauptuntersuchungen durchführen darf. Zudem wäre zu klären, ob den Fahrzeughaltern bereits zugeteilte Prüfplaketten gültig bleiben. 
Das verantwortliche Verkehrsministerium wollte sich nicht dazu äußern. Der Spiegel schreibt von großer Nervosität in den Behörden. 
Im Sommer hatte der Chef der Akkreditierungsstelle, Norbert Barz, dem Bericht zufolge zahlreiche Verkehrsminister der Länder in einem Brief auf die Probleme hingewiesen. Unter dem Betreff  ""Keine zuverlässigen Hauptuntersuchungen gewährleistet "" habe er mitgeteilt, Besuche bei den Prüfdiensten hätten ergeben, dass  ""in erheblichem Umfang "" Messgeräte eingesetzt würden, die nicht nach den  ""einschlägigen Anforderungen "" und dem  ""Stand der Technik kalibriert sind "". 
Die betroffenen Prüfgesellschaften wiesen dies später zurück. Der Vorwurf, die Zuverlässigkeit der Hauptuntersuchung sei nicht mehr gewährleistet, entbehre  ""jeglicher Grundlage "", zitierte der Spiegel aus einer gemeinsamen Erklärung. Alle Messgeräte seien ordnungsgemäß  ""kalibriert, geeicht oder stückgeprüft "". 
Die Deutsche Akkreditierungsstelle ist ein 2010 gegründetes Unternehmen und zu zwei Dritteln in Staatsbesitz. Sie beaufsichtigt gemäß europäischen Bestimmungen Dienstleister in Handel und Wirtschaft."	technik
"In Deutschland leben mehr als 380.000 Menschen in Gebieten, in denen die Grenzwerte für gesundheitsschädigende Stickoxide überschritten werden. Das geht aus einer Antwort des Bundesumweltministeriums auf eine Anfrage der Grünen im Bundestag zurück. Danach wohnen in Deutschland 382.213 Menschen in Straßen, in denen teilweise deutlich überhöhte Werte gemessen wurden. 
Allein in Hamburg gehe man von 221.780 Betroffenen aus. In Berlin lebten 2012 laut einer vom Ministerium zitierten Statistik 64.300 Menschen in stark belasteten Straßenzügen.  ""Ich gehe von nahezu einer Million betroffenen Bürgern aus, wenn die Ergebnisse der Messstellen richtig hochgerechnet werden "", erklärte die Grünen-Umweltpolitikerin Bärbel Höhn. 
In einem aktuellen Bericht des Umweltministeriums heißt es, um eine Einhaltung der geltenden Emissionsbegrenzungen zu erreichen, müssten künftig mehr  ""unabhängige behördliche Kontrollen "" von Diesel-Fahrzeugen stattfinden. Die Kosten für diese Kontrollen müssten die Hersteller tragen. 
Der Volkswagen-Konzern hat eingeräumt, die Abgaswerte von Dieselautos manipuliert zu haben. Der umweltpolitische Sprecher der Grünen-Bundestagsfraktion, Peter Maiwald, sagte:  ""Ein Gutes hat der Abgasskandal. Endlich wurde der Nebel gelichtet und die Frage beantwortet, wieso zwar Pkw Schadstoffgrenzwerte theoretisch einhalten, aber praktisch die Stickoxid-Emissionen in den Städten nicht abnehmen. "" Die Europäische Kommission hatte am 18. Juni wegen der Grenzwertüberschreitungen ein Vertragsverletzungsverfahren gegen Deutschland eingeleitet. 
Gesundheitsschädliche Stickoxide wie etwa Stickstoffmonoxid und -dioxid kommen in der Natur nur in winzigen Mengen vor. Sie stammen vor allem von Autos, aber auch aus Kohle-, Öl- und Gaskraftwerken. Dieselmotoren stoßen viel mehr Stickoxide aus als Benziner. Die Stoffe können Schleimhäute angreifen und so zu Husten, Atembeschwerden und Augenreizungen führen. Sie können auch Herz und Kreislauf beeinträchtigen."	technik
"Facebooks Umgang mit Hasskommentaren beschert dem Unternehmen erneut juristischen Ärger. Die Staatsanwaltschaft Hamburg hat nun Ermittlungen gegen den Nordeuropa-Chef von Facebook wegen des Verdachts der Volksverhetzung eingeleitet. Das bestätigte eine Sprecherin der Behörde ZEIT ONLINE. Zuvor hatte Spiegel Online darüber berichtet. 
Auslöser für die Ermittlungen sei eine Anzeige des Rechtsanwalts Chan-jo Jun, sagte die Sprecherin weiter. Das Landeskriminalamt prüfe nun, ob sich die Vorwürfe bestätigen lassen und wer innerhalb des Unternehmens gegebenenfalls die Verantwortung trägt. Erst danach erfolge eine rechtliche Bewertung. Die Strafanzeige richte sich gegen den Nordeuropa-Chef Martin Ott. 
Die Behörde hat wegen ähnlicher Vorwürfe bereits Untersuchungen gegen drei andere Manager des Unternehmens eingeleitet. Auch hier hatte der Rechtsanwalt Jun Strafanzeige wegen Beihilfe zur Volksverhetzung erstattet. 
Der Anwalt schrieb damals auf seiner Website, dass er inzwischen 61 Fälle von nach seiner Auffassung rechtswidrigen Inhalten schriftlich an Facebook gemeldet habe. Davon seien zum 2. Oktober 27 Inhalte gelöscht und 37 weiter veröffentlicht gewesen. Facebook erwecke durch die Weigerung, die Inhalte zu löschen, den Eindruck, ohne Gegenwehr zu Hass und Gewalt aufrufen zu können, schreibt der Anwalt weiter. Das vergifte die Stimmung im Land. 
Facebook steht schon länger in der Kritik, nichts gegen Hasskommentare zu unternehmen. Bundesjustizminister Heiko Maas bat Facebook-Vertreter deshalb bereits zum Gespräch. Der SPD-Politiker forderte von dem Unternehmen, die eigenen Gemeinschaftsstandards strikter durchzusetzen – also fremdenfeindliche und rassistische Äußerungen im Zweifel lieber zu löschen, als sie stehen zu lassen. 
Zweitens sollte Facebook ein deutschsprachiges Moderatorenteam in Deutschland aufbauen, das die Beschwerden deutscher Nutzer bearbeitet. Zudem sollte das Unternehmen offenlegen, wie viele Nutzerbeschwerden es erhält, wie lange diese bearbeitet und wie viele Inhalte daraufhin gelöscht werden."	technik
"Wer sich ein Fitness-Armband oder einen anderen Bewegungstracker zulegt, will damit Informationen über sich gewinnen, speichern und nutzen. Die Geräte sind keine Datenvermeidungsapparate. Aber deshalb müssen sie noch lange keine Datenschleudern sein. Eine Untersuchung der auf Malware-Erkennung spezialisierten Firma AV-Test jedoch zeigt, wie fahrlässig manche Hersteller mit den Daten ihrer Kunden umgehen. 
AV-Test hat neun in Deutschland erhältliche Fitnesstracker und die dazugehörigen Apps auf Datensicherheit geprüft: Werden Daten verschlüsselt vom Gerät zum Smartphone übertragen? Wie sicher ist das Pairing-Verfahren, mit dem Tracker und Smartphone gekoppelt werden? Wie gut sind die Daten vor dem Zugriff durch Dritte geschützt? Elf solcher Probleme hatte die Firma formuliert. Das im Vergleich sicherste Gerät hatte nur eine Schwachstelle, das unsicherste neun. 
Getestet wurden die Fitnessarmbänder 
Die erfreulichste Erkenntnis der Tester: Alle Smartphone-Apps für die jeweiligen Tracker kommunizieren verschlüsselt mit dem Internet, wenn sie Nutzerdaten an die Firmenserver senden. Zur Qualität der Verschlüsselung macht AV-Test zwar keine Angaben, aber zumindest scheint der Datentransport bei allen Anbietern soweit abgesichert zu sein, dass heimliches Mitlesen nicht ohne Weiteres möglich ist. 
Problematischer sind die Bluetooth-Einstellungen und die Apps selbst. Bleibt Bluetooth die ganze Zeit aktiviert, kann das ein Weg für Dritte sein, auf die Daten des Nutzers zuzugreifen. Das gilt insbesondere dann, wenn ein Fitnesstracker sich mit jedem Smartphone in Bluetooth-Reichweite verbinden kann, ohne dass der Besitzer das bemerkt – wenn das Pairing also ohne ordentliche Authentifizierung stattfindet. 
Das ist der Fall beim FitBit Charge. Die Tester schreiben:  ""Jedes Smartphone mit Bluetooth ist bei dem Fitness-Tracker willkommen. Es fragt nicht nach einer PIN oder anderen Authentifizierungen – es verbindet sich einfach und übergibt freiwillig alle seine Daten. Diese werden auch nicht verschlüsselt oder anderweitig geschützt. "" 
Auf Nachfrage präzisierte AV-Test die Angaben: Wenn der FitBit-Tracker gerade nicht mit einem Smartphone kommunizierte, konnten ihn die Tester mit ihrer eigenen App dazu bringen, Nutzerdaten an ein heimlich gekoppeltes Smartphone zu übertragen. Dafür mussten sie zwar in Bluetooth-Reichweite sein, also im Umkreis von wenigen Metern, aber ganz unrealistisch ist das nicht. FitBit hat auf eine E-Mail-Anfrage von ZEIT ONLINE bisher nicht reagiert. 
Die Geräte von FitBit und Acer waren die einzigen im Test, die Drittanbieter-Apps den Zugriff auf die Fitnessdaten erlauben. Präparierte, etwa als Spiele getarnte Apps können die Daten also heimlich abgreifen und per Internet versenden. Mit einer manipulierten App gelang es den Prüfern zudem, Daten aus dem Acer-Armband nicht nur abzufangen, sondern auch zu verändern und zurück zum Gerät zu schicken. Interessant könnte das für Nutzer sein, die Boni oder Rabatte von ihrer Krankenkasse bekommen, wenn sie sich nachweislich sportlich betätigen: Wer seine Fitnessdaten derart manipulieren kann, kann sich die Bewegung sparen. 
FitBit und Acer landen mit acht beziehungsweise neun (mehr oder weniger schwerwiegenden) Mängeln auf den letzten Plätzen des Vergleichs. Testsieger ist das Sony Smartband. Einziges Manko nach den Kriterien von AV-Test ist hier die fehlende Option, Bluetooth direkt am Armband deaktivieren zu können. 
Die Missbrauchsszenarien mögen etwas weit hergeholt sein. So nennt AV-Test noch einen anderen Weg, die Krankenkassen zu betrügen: Wer die Daten eines gleichaltrigen, aber deutlich sportlicheren Nachbarn abgreifen und in sein eigenes Profil übernehmen kann, muss sich nicht selbst bewegen. Er müsste sich allerdings regelmäßig in Bluetooth-Reichweite von eben jenem Nachbarn begeben, wenn der gerade seinen Tracker trägt. Angesichts der bisherigen Verbreitung von Fitnesstrackern und vor allem den wenigen entsprechenden Krankenkassen-Angeboten dürften solche Fälle äußerst selten vorkommen. 
Zumindest aber zeigen solche Tests, dass manche Hersteller keinen gesteigerten Wert auf die Sicherung von Nutzerdaten legen. Vertrauen in das kommende Internet der Dinge entsteht so nicht. 
Update: Die PR-Firma von FitBit in Deutschland hat mittlerweile reagiert und ein Statement von FitBit zur angeblich mangelhaften Authentifizierung angekündigt."	technik
" ""Hast du das noch im Griff, Mark? "" fragte der ZEIT-Autor Johannes Gernert vergangene Woche in einem Brief an den Facebook-Gründer Mark Zuckerberg. Darin ging es um die tägliche Hetze in den Timelines gegen Flüchtlinge, um den Hass, der Asylsuchenden inzwischen unter fast jedem Beitrag zum Thema entgegenspringt – und den Facebook bislang größtenteils ignorierte, was zuletzt zu zahlreichen Debatten über die Verantwortung und Pflichten der Plattform führte. 
Jetzt hat Facebook angekündigt, gegen Drohungen und Hass-Postings stärker vorzugehen. In Zukunft würden in Deutschland  ""Androhungen von physischer Gewalt als glaubhafte Drohungen eingeschätzt und entfernt "", teilte das Unternehmen am Dienstag mit und erwartet, dass damit  ""deutlich mehr kontroverse Inhalte "" gesperrt werden.  ""Die Toleranz gegenüber leichtfertig dahingeschriebenen Kommentaren mit fremdenfeindlichem Unterton wird eingeschränkt "", hieß es. 
Eingeschränkte Toleranz also. Oder anders gesagt: Neue Regeln gibt es zwar nicht, die Betreiber wollen die alten aber etwas stärker anwenden. Konkret betrifft das die Gemeinschaftsstandards, auf die sich Facebook gerne bezieht. Darin wird geregelt, dass Facebook Hassbotschaften entfernt, die Personen unter anderem  ""aufgrund ihrer Rasse, Ethnizität und nationaler Herkunft direkt angreifen "". An anderer Stelle heißt es:  ""Wir entfernen glaubwürdige körperliche Bedrohungen, die sich an einzelne Personen richten. "" 
Wer Sätze wie diese mehrmals durchliest, erkennt schnell das Problem: Wann ist eine Bedrohung glaubwürdig? Wann ein Kommentar ein direkter Angriff? Wann handelt es sich tatsächlich um einen Witz oder Satire, so geschmacklos sie sein mag? Zudem sind Flüchtlinge in der Reihe der  ""geschützten Gruppen "" nicht vorhanden. Facebook hält damit offen, sich hinter den bewusst schwammigen Richtlinien zu verstecken. 
Bislang konnten die Facebook-Nutzer Inhalte, die ihrer Meinung nach gegen die Gemeinschaftsstandard verstoßen, an das Netzwerk melden. Der Prozess dauert lang und endete in vielen Fällen damit, dass Facebook sich gegen eine Löschung entschied. Auch wenn der jüngste Bericht des Netzwerks zeigt, dass Facebook mehr illegale Inhalte denn je löscht. Allerdings bezieht sich die Zahl auf Inhalte, die tatsächlich gegen lokale Gesetze, etwa Verleugnung des Holocaust, verstoßen, nicht auf Verstöße gegen die Gemeinschaftsstandards. In schwerwiegenden Fällen bittet Facebook seine Nutzer, die Polizei einzuschalten. 
Nun scheint sich die Einstellung zu ändern, jedenfalls in der Außendarstellung:  ""Uns ist bewusst, dass einige Menschen Dinge auf Facebook gepostet haben, die Flüchtlinge bedrohen "", schreibt der Facebook-Politikdirektor in Europa, Richard Allen, in einem Blogeintrag. In Zukunft werden deshalb  ""in Deutschland Androhungen von physischer Gewalt als glaubhafte Drohungen eingeschätzt und entfernt "". 
Ein Beispiel: Ein Kommentar laut dem Flüchtlingsheime brennen sollen und der mit Gewalt gegen Migranten kokettiert, gilt gemäß der neuen Interpretation als glaubhafte Drohung – ganz abgesehen davon, ob es nun ironisch oder als schlechter Witz gemeint war. Demnach sollten Postings wie diese auch schneller aus den Timelines verschwinden. Jedenfalls theoretisch. In jedem Fall soll es die Arbeit des Prüfteams leichter machen, sagt Allen. 
 
Ob sich tatsächlich etwas ändert, dürfte sich erst in den kommenden Wochen und Monaten zeigen. Die intransparente Arbeit des Prüfteams wurde immer wieder kritisiert. Facebook selbst gab keine Auskunft darüber, wie viele Menschen tatsächlich die Meldungen überprüfen. Künftig aber wolle man konkrete Zahlen zum Ausmaß von Hasskommentaren und Umgang damit herausgeben. Das hat die deutsche Facebook-Sprecherin Tina Kulow angekündigt. 
Die neuen Richtlinien seien in Zusammenarbeit mit der Freiwilligen Selbstkontrolle Multimedia-Diensteanbieter (FSM) verbessert worden, sagte sie weiter. Im September hatte sich Justizminister Heiko Maas mit dem Online-Netzwerk auf die Bildung einer gemeinsamen Task-Force gegen Hassbotschaften im Internet verständigt. Wie die Zusammenarbeit zwischen Facebook und der FSM genau aussieht, sagten die Verantwortlichen nicht. Letztlich seien die jüngsten Vorstöße Facebooks erst mal nur ein Versuch, sich zu bessern, sagen die Kritiker. 
In den vergangenen Monaten wurde die Kritik an der passiven Haltung des Netzwerks bezüglich Hasskommentaren lauter, sogar die Bundeskanzlerin nahm zwischenzeitlich an der Debatte teil. Derweil gab es Fälle, in denen die Staatsanwaltschaft eingriff und Nutzer wegen Volksverhetzung verurteilt wurden. Erst vor zwei Wochen leitete die Hamburger Staatsanwaltschaft Ermittlungen gegen den Nordeuropa-Chef Martin Ott ein – der Rechtsanwalt Chanjo Jun hatte Strafanzeige gestellt – und im Oktober wurde eine Berlinerin zu einer Bewährungsstrafe verurteilt. 
Weitere Nachrichten wie diese würde sich das Unternehmen gerne ersparen. Ein bisschen weniger Toleranz ist möglicherweise ein Weg. Fakt ist aber auch: Selbst mit neuen Richtlinien lässt sich dem Hass im Netz nur schwer beikommen."	technik
"Die EU-Kommission will Reisenden die Nutzung von Streamingdiensten wenigstens während Kurztrips ins europäische Ausland erleichtern. Mit einer neuen Portabilitätsverordnung sollen EU-Bürger im Urlaub oder auf Geschäftsreisen TV-Serien auf Netflix oder Fußball-Übertragungen auf Sky sehen können, wenn sie sich zuvor bei den jeweiligen Anbietern registriert haben. EU-Digitalkommissar Günther Oettinger hofft auf eine schnelle Einigung mit EU-Parlament und Ministerrat, damit die Verordnung 2017 in Kraft treten kann. 
Die Kommission geht damit ansatzweise das sogenannte Geoblocking an, bei dem bestimmte Internetinhalte aus Lizenzgründen in anderen EU-Staaten nicht angeboten werden. Die Neuregelung soll aber nicht für Bürger gelten, die dauerhaft im EU-Ausland leben. Auch urheberrechtlich geschützte Inhalte wie etwa Sportübertragungen, die ohne Registrierung verfügbar sind, werden von der Regelung ausgenommen. 
Julia Reda, die Abgeordnete der Piraten im Europaparlament, schreibt deshalb:  ""Wer heute bereits Sport- oder Filmangebote im Netz abonniert hat, und darauf lediglich auch aus dem Urlaub zugreifen möchte, wird die Verordnung begrüßen. Für diesen Anwendungsfall ist sie auch geeignet und sollte zügig verabschiedet werden. Aber eines muss klar sein: Dass 'dieser Inhalt in deinem Land nicht verfügbar' ist, trifft vor allem jene, die Angebote nutzen wollen, die in ihrem Herkunftsland noch nicht einmal gegen Bezahlung angeboten werden. ... Wir brauchen mehr als nur Roaming für Netflix. "" 
Es war der einzige Richtlinienentwurf, den Oettinger am Mittwoch vorstellte. Für andere Themen rund ums Urheberrecht stellte er lediglich einen groben Fahrplan vor, konkrete Vorschläge der Kommission soll es demnach in den kommenden sechs Monaten geben. So will die EU-Kommission zunächst den Zugang zu Onlineinhalten bei Bildung und Forschung sowie für Menschen mit Behinderungen erleichtern. Zudem will die Kommission verstärkt gegen illegale Kopien geschützter Inhalte vorgehen und dazu unter anderem den Geldfluss an Unternehmen unterbrechen, die mit solchen Kopien Geld verdienen. 
Reda, von Oettinger kürzlich  ""die vermutlich sachkundigste Abgeordnete "" im Bereich Urheberrecht genannt, deren Position er aber nicht teile, hatte im Vorfeld durchaus Schlimmeres befürchtet. Im November hatte das Portal IPKat ein Positionspapier der Kommission geleakt, das Reda als  ""Frontalangriff auf den Hyperlink "" interpretierte. In einem darin enthaltenen Absatz zu  ""neuen Formen der Online-Distribution urheberrechtlich geschützter Werke "" hieß es:  ""In diesem Zusammenhang wird die Kommission untersuchen, ob Handlungsbedarf besteht bei der Definition der Rechte zur 'Kommunikation mit der Öffentlichkeit' und zum Begriff 'verfügbar machen' "". Reda hatte das mit früheren Aussagen von Oettinger abgeglichen und hatte daraus abgeleitet, die Kommission erwäge, das reine Verlinken von urheberrechtlich geschützten Inhalten lizenzpflichtig zu machen, also zum Beispiel jeden Link auf Presse-Artikel im Netz. 
In der Pressemitteilung der Kommission heißt es nun aber:  ""Wir beabsichtigen nicht, Hyperlinks zu 'besteuern'. Nutzer werden nicht zahlen müssen, wenn sie Links auf urheberrechtlich geschütztes Material teilen "". 
Auch Kommissionsvizepräsident Andrus Ansip versuchte in der Pressekonferenz, diese Bedenken zu zerstreuen:  ""Ich möchte eindeutig klarstellen: Diese Kommission hat keinerlei Pläne zur Besteuerung von Hyperlinks. Nichts dergleichen. "" Allerdings räumte er ein:  ""Wir müssen klarer machen, wo wir über Hyperlinking sprechen und wo wir über neue, wertschöpfende Produkte sprechen. Wo wir nicht von neutralen Vermittlern sprechen können, sondern von aktiven Verkäufen, Geldverdienen und so weiter. "" Was bdeutet, dass Betreiber von gewinnorientierten Suchmaschinen und Aggregatoren möglicherweise doch für Links zahlen sollen. 
So klingt es auch, wenn es um die Verlinkung kurzer Textauszüge, sogenannter Snippets, durch Aggregatoren wie Google News geht. Die Kommission werde, so steht es auf Seite 10 des vollständigen Aktionsplans,  ""prüfen, ob bestimmte Maßnahmen hinsichtlich Newsaggregatoren nötig sind "". 
Zusammengefasst bedeutet das: Ein europäisches Leistungsschutzrecht für Presseverleger ist nicht vom Tisch.  ""Damit ignoriert die Kommission explizit die Ablehnung solcher Pläne durch das EU-Parlament "", schreibt Till Kreutzer von der Initiative gegen ein Leistungsschutzrecht (Igel). 
In Deutschland und Spanien gibt es entsprechende Gesetze bereits. Bislang machen sie alle betroffenen Seiten unglücklich: Die Verlage, die Aggregatoren und die Nutzer. Reda schreibt:  ""Kommissar Oettinger muss endlich begreifen: Eine schlechte Idee wird nicht dadurch besser, dass man sie großflächiger umsetzt. "" 
Mit Material von Reuters"	technik
"1.200 Millimeter Spurbreite, 2.500 Millimeter Radstand, 26 PS Leistung und acht Liter Verbrauch – das sind die Eckdaten eines Autos, das Ferdinand Porsche entwickeln soll. Er nennt es  ""einen Gebrauchswagen mit möglichst narrensicheren Einrichtungen "". Und davon ist Adolf Hitler begeistert. Der Reichskanzler träumt von der  ""Volksmotorisierung "": Jeder Deutsche soll sich ein eigenes Auto leisten können – einen  ""Volkswagen "", der nicht mehr als ein Motorrad kostet.  ""Machen Sie das "", sagt Hitler im Frühjahr 1934 zu seinem Landsmann Porsche und betont:  ""Jeder Preis ist gut – jeder unter 1.000 Mark. "" Dem wagt der Autokonstrukteur nicht zu widersprechen. 
Auf Druck der Naziregierung muss der Reichsverband der Automobilindustrie (RDA) mit Porsche einen Vertrag abschließen, der die Termine und den späteren Verkaufspreis des Autos vorschreibt, nicht aber dessen Technik.  ""P. hat die gestellte Aufgabe innerhalb von zehn Monaten zu lösen "", lautet die Zeitvorgabe. Es war der Beginn einer Zusammenarbeit, die eine neuere VW-Chronik als  ""zweideutiges Vergnügen "" charakterisiert. 
Tatsächlich kommen viele Probleme auf. Porsches Team legt sich zwar mächtig ins Zeug und fertigt eine Konstruktionsskizze nach der anderen an, doch schon bald reift eine Erkenntnis: Zwischen Theorie und Wirklichkeit klafft eine große Lücke. Das Auto wird schwerer und vor allem deutlich teurer als ursprünglich geplant. Porsche hatte Hitler zu viel versprochen. 
Eine der größten Schwachstellen ist der Antrieb. Statt des im Exposé beschriebenen  ""luftgekühlten Vierzylinder-Viertaktmotors in waagrechter Gegenläuferanordnung "" experimentiert Porsche nur mit Zweizylinderaggregaten, die selbst seine engsten Mitarbeiter als  ""primitiv und billig "" bezeichnen. Hinzu kommt, dass keiner dieser Motoren einwandfrei funktioniert. 
Auch organisatorisch hatte man sich zu viel vorgenommen. Einen fahrfertigen Versuchswagen kann Porsche aus eigener Kraft gar nicht liefern, denn er besitzt weder Werkstätten noch Maschinen oder Prüfstände. Alles, was er bieten kann, ist eine Garage am Rande seiner Villa auf dem Stuttgarter Killesberg, die eigentlich Sohn Ferry Porsche als Bastelwerkstatt dient. 
Deshalb müssen die großen Autohersteller und Zulieferer Hilfestellung geben. Allen voran Daimler-Benz. Denn dort sitzt ein Mann im Vorstand, dem Hitler blind vertraut: Jakob Werlin. Er ist Österreicher, Parteigenosse, SS-Mitglied, Autoverkäufer und Leiter der Münchener Mercedes-Benz-Niederlassung. Mit ihm hatte Hitler einst die Volkswagen-Idee ausbaldowert und ihm 1933 zu dem Vorstandsposten bei Daimler-Benz verholfen, damit Werlin von dort die Fäden im Sinne des  ""Führers "" ziehen kann. 
So liefert die Stuttgarter Autofirma zunächst die Karosserien für Porsches Prototypen, die er V1, V2 und V3 nennt. Wahrscheinlich gibt Daimler-Benz auch bei der Entwicklung eines vernünftigen Motors die entscheidenden Impulse. Denn einen luftgekühlten 1,2-Liter-Boxermotor mit vier Zylindern, wie ihn Porsches Team binnen weniger Tage plötzlich präsentiert, hatte Daimler-Benz schon 1931 entwickelt, doch dann wieder ad acta gelegt, weil er für die teuren Mercedes-Limousinen nicht taugte. Für den Volkswagen scheint diese Motorentechnik aber goldrichtig zu sein. Flugs wird Ferdinand Porsches Prototyp V3/3 wird mit einem solchen Boxertriebwerk bestückt. 
Viel Zeit zum Experimentieren hätte er ohnehin nicht mehr gehabt. Hitler ist die Querelen zwischen Porsche und der Automobilindustrie leid und will endlich Nägel mit Köpfen machen. Es ist Samstag, der 11. Juli 1936, als der Führer auf dem Obersalzberg bei Berchtesgaden seine Vertrauten Hermann Göring und Fritz Todt sowie Werlin und Porsche um sich versammelt und weitreichende Entscheidungen trifft: Nicht die deutsche Automobilindustrie, sondern der Staat soll den Volkswagen entwickeln und herstellen. 
 
 ""Es kann in Deutschland nur einen Volkswagen geben und nicht zehn "", stellt Hitler klar. Deshalb werde man jetzt eine staatseigene Gesellschaft gründen und mit genügend Kapital ausstatten, um ein neues Werk für jährlich  ""mindestens 300.000 Autos "" zu bauen. Zwischen 80 und 90 Millionen Reichsmark werde man dafür investieren. Und an die Adresse von Ferdinand Porsche und Jakob Werlin richtet Hitler den Auftrag, die nächsten  ""Probewagen "" nur noch an einer Stelle zu bauen: bei Daimler-Benz.  ""Und zwar schleunigst. "" 
Nach der Besichtigung des Prototypen V3/3 mit dem Boxermotor hält Hitler, der unverdrossen an die  ""Genialität "" seines Landmanns Porsche glaubt, den Start einer Vorserienproduktion von 30 Wagen für möglich. Die Kosten für die weitere Entwicklung und Erprobung werde ebenfalls der Staat übernehmen – und wenn es sein muss, werde man der Autoindustrie ihre bisherigen Auslagen kurzerhand erstatten. 
Damit bremst Hitler die Privatwirtschaft in Sachen Volkswagen endgültig aus. Es ist ein Beleg für seine von Größenwahn getriebene Ideologie, die er wenige Monate später in einem Grundsatzpapier manifestiert: Darin spricht er von  ""nationalwirtschaftlichen Aufgaben "", die von der Industrie zu erfüllen seien. Falls nicht, werde  ""der nationalsozialistische Staat aus sich heraus diese Aufgaben zu lösen wissen "", heißt es in Hitlers  ""Denkschrift "" für den zweiten Vierjahresplan der NS-Regierung. 
Es ist der Beginn der Zwangswirtschaft – und das Volkswagenprojekt macht den Anfang. 
Für Ferdinand Porsche bedeutet diese Politik, dass er aus dem Vollen schöpfen kann. Die Kritik an seiner Arbeit verstummt, er darf bei Daimler-Benz jede Unterstützung abrufen, die er benötigt. Mit Hitlers Rückendeckung wird er vom Auftragnehmer zum Auftraggeber. Daimler-Benz, so der Befehl des Führers, soll in Sachen Volkswagen  ""unter der Aufsicht von Porsche-Ingenieuren "" arbeiten. 
Das nächste Ziel ist die sogenannte W30-Vorserie. 30 Stück sollen hergestellt und in einem Großversuch getestet werden. Die Arbeitsplanung: Das Werk in Sindelfingen stellt die Stahlkarosserien her, die Fahrzeugmontage erfolgt im Daimler-Stammwerk Stuttgart-Untertürkheim. Nur ein Exemplar der Vorserie wird noch in der Garage neben der Porsche-Villa zusammengebaut, 28 weitere Limousinen und ein Cabriolet liefert Daimler-Benz. 
Hitlers Freund und Autoberater Jakob Werlin wird zur Schlüsselfigur bei der weiteren Entwicklung des Volkswagens – der übrige Daimler-Vorstand geht vorsichtig auf Distanz.  ""Wir bleiben lediglich und allein ausführendes Organ "", hält Daimler-Benz-Chef Wilhelm Kissel in einem internen Papier fest und betont, dass sein Unternehmen  ""niemals eine Verantwortung für die Konstruktion des Herrn Porsche "" übernehmen werde. Man werde sich  ""strikt an die zeichnerischen Unterlagen "" halten,  ""welche uns die Dr. Porsche GmbH zur Verfügung stellt "". 
Doch das ist nur die offizielle Sprachregelung. Bei Daimler-Benz hat man offenbar große Mühe, die Konstruktionsvorgaben von Ferdinand Porsche und seinem Team zu verwirklichen. Es gibt haufenweise Probleme.  ""Wir haben ein dickes Buch voller Änderungen "", soll Kissel seinem Vorstandskollegen Werlin eines Tages berichtet haben. So waren die Mercedes-Ingenieure wohl gezwungen, die Porsche-Konstruktion zu verbessern, damit am Ende auch wirklich einsatztaugliche Versuchswagen entstehen. 
Diesen Eindruck bestätigt Werlin in seiner unveröffentlichten Autobiografie aus dem Jahre 1964, die im Daimler-Archiv aufbewahrt wird.  ""Daimler-Benz hat die Konstruktion Porsches technisch vervollkommnet und durch den Bau der Versuchswagen die wesentlichen Voraussetzungen für die Entstehung des Volkswagenwerks geschaffen. "" Und Werlin ergänzt:  ""Ohne Daimler-Benz wäre das Volkswagenwerk nicht noch vor dem Krieg gebaut worden. Ohne Daimler-Benz wäre es auch heute nicht vorhanden. "" 
Werlin spricht auch davon, dass  ""großzügig Mercedes-Erfahrung und Mercedes-Patente zur Verfügung gestellt werden mussten "" und dass Daimler-Benz  ""ganz allgemein Vorspanndienste "" für den Volkswagen zu leisten hatte. 
Dass sich Porsches Konstruktion ab Mitte 1936 deutlich verändert, ist unverkennbar. Offenbar um Geld und Gewicht zu sparen, verkürzt man den Radstand gegenüber den V3-Prototypen um 100 auf 2.400 Millimeter und die Karosserie auf 3.850 Millimeter. Noch deutlicher sind die formalen Änderungen an der Karosserie. Ähnelten die bisherigen Prototypen eher einem früheren Modell des tschechischen Autobauers Tatra, so folgt das Design der W30-Vorserie einem Mercedes-Benz aus den Jahren 1933/1934 – dem Typ 150, einer in kleiner Serie speziell für Wettbewerbsfahrten entwickelten zweisitzigen Sportlimousine mit luftgekühltem Heckmotor. 
Die formalen Ähnlichkeiten sind verblüffend: die gewölbte Motorhaube, die schwungvoll gezeichneten Kotflügel, die leicht schräg gestellte Windschutzscheibe, die in die Kotflügel integrierten Rundscheinwerfer und das bogenförmige Dach – das sind Designmerkmale, die die Volkswagen-Vorserie ab 1936 fast unverändert von dem Mercedes übernimmt. 
Hätte der Typ 150 keinen auf der Haube stehenden Stern, würde heute jeder spontan von dem kleinen Mercedes-Benz behaupten, ein VW Käfer fahre vorbei. Er war allerdings schon 1934 unterwegs, als man in Berlin noch über die Papierform des von Adolf Hitler und Ferdinand Porsche geplanten Volkswagens diskutierte. 
Im Frühjahr 1937 ist die Vorserienproduktion der ersten 30 Volkswagen beendet, und Daimler-Benz kann sich aus dem Projekt zurückziehen. Für die Herstellung der Probewagen stellt die Firma 331.843,10 Reichsmark in Rechnung und übergibt die weiteren Arbeiten an die neu gegründete staatseigene Gesellschaft zur Vorbereitung des Deutschen Volkswagens mbH. 
Als Adolf Hitler am 26. Mai 1938 auf einem gräflichen Landgut in der Nähe des Schlosses Wolfsburg im heutigen Niedersachsen den Grundstein des Volkswagenwerks legt, dankt er an erster Stelle nicht etwa Ferdinand Porsche, sondern seinem Vertrauten von Daimler-Benz:  ""Direktor Jakob Werlin ist der Mann der Automobilindustrie, der seit vielen Jahren an meiner Seite stehend meine Gedanken überall zu vertreten und damit zu verwirklichen bemühte. ""
""Auf dem Flughafen London Heathrow ist ein Passagierjet beinahe mit einer Drohne zusammengestoßen. Der Pilot eines Airbus A320 habe kurz vor der Landung eine hubschrauberähnliche Drohne gesehen, teilte die britische Flugsicherheitsbehörde (CAA) mit. Der Vorfall hat sich demnach schon am 22. Juli ereignet. 
Nach Angaben der BBC konnte die Drohne nicht identifiziert werden. Auch ist unklar, wie nah sie dem Passagierflugzeug tatsächlich kam oder wie groß sie war. Die Sunday Times berichtet unter Berufung auf Ausschnitte aus einem amtlichen Report, der Vorfall habe sich in 213 Metern Höhe ereignet. Der vollständige Bericht soll demnach am Freitag veröffentlicht werden. 
Die Flugsicherheitsbehörde ordnete den Vorfall in die Kategorie A ein. Das bedeutet ein ernsthaftes Zusammenstoß-Risiko. Die britische Pilotenvereinigung (Balpa) warnte vor der zunehmenden Zahl von Hobby-Fliegern, die Drohnen aufsteigen ließen. Schon im Mai habe der Pilot einer Propellermaschine auf dem Flughafen von Southend in Westengland eine Drohne gemeldet, die nur etwa 25 Meter von seiner Maschine entfernt geflogen sei. 
Der Generalsekretär der Pilotenvereinigung, Jim McAuslan, sagte, Drohnen könnten einen ähnlichen Zwischenfall auslösen wie Vögel. Er nannte die Landung eines Flugzeugs auf dem Hudson River in Manhattan 2009 als Beispiel. Damals war die Maschine in der Luft in einen Vogelschwarm geraten. Die Tiere wurden in die Triebwerke gezogen, woraufhin das Flugzeug auf dem Fluss notlanden musste. Eine kleine Drohne könne schon alleine deshalb ein Sicherheitsrisiko sein, weil sie den Piloten bei der Landung ablenke, sagte McAuslan. 
Einige Drohnen-Modelle kann man für umgerechnet 38 Euro in Geschäften für Elektronikzubehör kaufen. Gerade vor Weihnachten werden solche Drohnen sehr oft verkauft. Nur wenige Käufer machen nach dem Kauf allerdings ein Training zur Handhabung. Es gebe Regeln, die beachtet werden müssten, sagte McAuslan. Drohnen dürfen demnach maximal bis zu einer Höhe von 120 Metern fliegen und nur etwa 500 Meter von demjenigen entfernt, der sie steuert. Zudem müssen sie einen Abstand von 50 Metern zu Personen und Gebäuden halten. 
In Deutschland sind die Regeln anders. Die Drohnen müssen stets im Sichtbereich der Piloten bleiben. Als Faustregel gilt eine maximale Entfernung von 300 Metern und eine Höhe von 100 Metern für den sicheren Einsatz. Im Umkreis von 1,5 Kilometern von Flughäfen dürfen Drohnen gar nicht fliegen. In manchen Städten ist die Zone sogar noch größer. In Berlin sind beispielsweise Drohnen innerhalb des Stadtgebiets ohne Ausnahmegenehmigung komplett verboten. 
In Frankreich wurden in den vergangenen Wochen immer wieder Drohnen in der Nähe von Atomkraftwerken gesichtet. Gefahr habe nicht bestanden, teilten die Polizei und der Kraftwerkbetreiber EDF mit. Allerdings ist es in Frankreich verboten, Kernkraftwerke in einem Umkreis von fünf Kilometern und einer Höhe unter 1.000 Metern zu überfliegen."	technik
"Vertrauen ist schön. Kontrolle ist notwendig. Will man in der neuen E-Klasse von Mercedes-Benz dem Drive Pilot beim Steuern des Fahrzeugs zuschauen und lässt das Lenkrad einfach los, mahnt auf der Autobahn nach spätestens 30 Sekunden eine optische und nach 30 weiteren Sekunden eine zusätzliche akustische Warnung, wieder selbst die Führung zu übernehmen. Tut der Fahrer das nicht, schaltet die E-Klasse den Warnblinker an und bremst bis zum Stillstand in der eigenen Spur. 
So zwingt der Drive Pilot den Fahrer dazu, mindestens eine Hand am Lenkrad zu lassen. Das Ziel der Hands-off-Detection:  ""Wir verhindern damit einen vorhersehbaren Fehlgebrauch und machen dem Fahrer immer wieder seine Verantwortung deutlich "", sagt Christoph von Hugo. Er leitet den Bereich Aktive Sicherheit bei Mercedes-Benz. 
Der Drive Pilot beeindruckt dennoch: Es gibt subjektiv kein Auto, mit dem man lange Strecken so unbelastet zurücklegen kann wie mit der Mercedes E-Klasse. Komfort und Sicherheit sind der traditionelle Markenkern, und der wird mit dem Drive Pilot konsequent unterstrichen. 
ZEIT ONLINE hat den Drive Pilot über etwa 800 Kilometer getestet. Das interessante ist der Vergleich zu Teslas Autopilot: Obwohl beide Systeme formal nah beieinander sind, grenzen sie sich in der Realität voneinander ab. 
Drive Pilot und Autopilot sind sogenannte teilautomatisierte Fahrhilfen: Sowohl die Längsführung – Gas geben und Bremsen – als auch die Querführung, also das Lenken, können vom Fahrzeug übernommen werden. Das funktioniert am besten auf der Autobahn, wo es keinen Gegenverkehr, keine Ampeln, keine Kreuzungen und keine Fußgänger oder Radfahrer gibt. Beide Hersteller empfehlen die Nutzung allein für diesen Zweck. Auf Überlandstraßen und in der Stadt arbeitet die Elektronik zwar auch; hier sind die Grenzen aber schnell erreicht: Der Fahrer muss eingreifen. 
Wesentliches Element teilautomatisierter Systeme: Der Fahrer muss die Funktion permanent überwachen. Mit dem Smartphone spielen oder tiefenentspannt Dösen ist zwar möglich – es widerspricht aber der Definition dieser Entwicklungsstufe, und es ist potenziell gefährlich. 
In der Mercedes E-Klasse wird der Abstand zum Vordermann durch einen Radar kontrolliert und reguliert. Das ist heute fast selbstverständlich, und es klappt im Alltag hervorragend. Zusätzlich erfasst eine Stereokamera ein Bild der Straße mit Fahrbahnmarkierungen, Parallelstrukturen wie Leitplanken sowie anderen Fahrzeugen, auch Motorrädern. 
 
Auf der Stereokamera basiert auch die Lenkunterstützung. Mercedes macht das so: Der Drive Pilot bewegt das Lenkrad sanft und fast unmerklich in die nötige Richtung. So, als würde die Servounterstützung dort besonders stark sein, wo es hingehen soll. Dabei reicht es aus, mit einer Hand lässig mitzumachen; hier entsteht gewissermaßen eine Kooperation zwischen Technik und Fahrer. 
Erst mit dem nächsten Entwicklungsschritt, der Hochautomatisierung, wird der Reisende sich vorübergehend innerlich abmelden können. Dann übernimmt die Elektronik für eine spezifische Situation, zuerst auf der Autobahn, die Längs- und Querführung, und das System muss nicht permanent überwacht werden. Allerdings wird auch dann das Auto – wenn es erkennt, dass es an Grenzen gerät – die Kontrolle an den Fahrer zurückgeben. Der Fahrer wird dann mit einer ausreichenden Zeitreserve zur Übernahme aufgefordert. 
Mercedes-Benz rechnet ungefähr im Jahr 2020 mit diesem Sprung.  ""Unter anderem müssen eindeutige rechtliche Rahmenbedingungen geschaffen werden "", sagt Entwickler Christoph von Hugo. Die juristische Klarheit verspricht Bundesverkehrsminister Alexander Dobrindt (CSU) bis Ende Juli zumindest im Entwurf. Künftig, so heißt es, soll es erlaubt sein, dass sich  ""der Fahrzeugführer (…) für eine bestimmte Zeit und in bestimmten Situationen (…) vom Verkehrsgeschehen abwenden darf "". 
Eine große Hilfe wird in Zukunft der Kartendienst Here sein, den Daimler zusammen mit BMW und Audi gekauft hat. Das Ziel sind zentimetergenaue und superaktuelle Karten, die zum Beispiel eine neue Baustelle sofort beschreiben und in einer Datencloud aufnehmen. 
Eine ähnliche Form des Fleet Learnings gibt es bei Tesla bereits, denn alle Model S sind permanent online und erfassen ihre Umgebung. Von dieser Schwarmintelligenz berichten viele Besitzer des elektrischen Luxusautos: Wer über Wochen immer wieder die gleiche Stelle abfährt, kann wahrnehmen, wie das Auto Stück für Stück lernt und zum Beispiel vor einer engen Kurve langsamer wird. 
Tesla richtet sich mit dem Autopilot an technikaffine Nutzer. Ja, auch Tesla hat eine adaptive Hands-off-Detection. Sie lässt besonders auf der Autobahn aber lange Strecken zu, bei denen der Wagen faktisch von alleine fährt, wie ZEIT ONLINE erleben konnte. Das ist wirklich faszinierend, und es funktioniert erstaunlich gut. Man muss eben nur wissen, womit man es zu tun hat. Wer sich schlicht auf den Begriff Autopilot verlässt und vermutet, es würde jetzt alles von selbst gehen, geht ein erhebliches Risiko ein – darauf weist Tesla ausdrücklich hin. 
Im Ergebnis zielen der Drive Pilot von Mercedes und der Autopilot von Tesla präzise auf ihre Käufergruppen. Die E-Klasse ist ein überlegenes, höchst komfortables Langstreckenauto, bei dem es schwer fällt, überhaupt einen Unfall zu provozieren. Mercedes hat die für die Masse der Fahrer bessere Lösung, weil das System extrem unauffällig arbeitet und jedes Risiko vermeidet. Das Model S dagegen richtet sich an Elektronik-Nerds, die heute schon das Morgen ausprobieren wollen – und mit jedem Softwareupdate ein Stück mehr davon bekommen."	technik
"Die Produktion eines modernen Videospiels ist eine komplexe Angelegenheit. Nicht selten dauert es drei oder mehr Jahre, bis ein Triple-A-Titel fertiggestellt ist und in den Handel kommt. Sonys The Last Guardian befindet sich bereits seit 2005 in der Entwicklung. 2009 wurde es im Rahmen der damaligen E3-Messe der Weltöffentlichkeit präsentiert, viele Spieler handelten es bereits als geistiges Erbe der Ausnahmewerke Ico und Shadow of the Colossus. 
Doch dann verschwand The Last Guardian von der Bildfläche. Immer wieder kursierten Gerüchte über den Entwicklungsstand des Abenteuergames, sogar eine Einstellung des Projekts stand zur Debatte. The Last Guardian avancierte zum Nessie der Videospielbranche: Es wurde viel darüber berichtet, doch gesehen hat es niemand. 
Zumindest bis zur diesjährigen E3-Pressekonferenz von Sony: Unter tosendem Applaus kündigte Shawn Layden, Präsident von Sony Computer Entertainment America, ein Spiel mit einer  ""poetischen Geschichte von Abenteuer und Freundschaft an "", ehe ein Junge und sein Drachenwesen über die Bildschirme liefen. The Last Guardian lebt und soll 2016 für die Playstation 4 erscheinen. Obwohl man dem Spiel seine lange Produktionszeit ansieht, könnte es für den japanischen Spieledesigner Fumito Ueda, der auch schon bei Ico und Shadow of the Colossus federführend war, sein bislang wichtigstes Werk sein. 
The Last Guardian gab den Takt der Sony-Show vor: Spiele für jeden Geschmack – und gerne mit Mut zu Neuem oder zumindest zu neuen Perspektiven. Verließ sich Microsoft bei seiner Pressekonferenz vor allem auf bekannte Marken wie Gears of War oder Halo, wartete Sony gleich mit mehreren von den Konsumenten geforderten Überraschungen auf. Square Enix verkündete das Remake seines Rollenspielklassikers Final Fantasy VII von 1997 und untermauerte die Neuankündigung mit einem grafisch beeindruckenden Trailer. Final Fantasy VII erscheint zuerst auf der Playstation 4, dürfte aber später auch für andere Systeme kommen. 
Nach einem kurzen Auftritt des US-Studios Devolver Digital, das Indiespiele wie Ronin vorstellte, schlug man den Bogen hin zur Enthüllung von Shenmue 3. Shenmue gilt als Wegbereiter für Open-World-Spiele wie GTA V und setzte um die Jahrtausendwende mit zwei Spielen auf Segas Dreamcast-Konsole Maßstäbe. Ein dritter Teil der Serie war von den Fans lange gewünscht worden, Chefentwickler Yu Suzuki hatte ihn jedoch nie in die Tat umgesetzt. Suzuki nutzte die Pressekonferenz als Werbeplattform für die Kickstarter-Kampagne von Shenmue 3. Mit Erfolg: Bereits wenige Stunden nach der Ankündigung erreichte das Projekt die Zielsumme von zwei Millionen US-Dollar. Shenmue 3 soll 2016 für PC und Playstation 4 erscheinen. 
Das holländische Studio Guerilla Games (Killzone) präsentierte Horizon: Zero Dawn, ein futuristisch anmutendes Actionspiel, in dem Science-Fiction auf ein archaisches Szenario trifft. Media Molecule, die Macher von Little Big Planet und Tearaway, bewiesen hingegen mit dem surrealen Dreams ihre Vorliebe für ungewöhnliche Spielkonzepte. Im Trailer sah man, wie Spieler mithilfe des PS4-Gamepads eigene Traumlandschaften zeichneten und diese zum Leben erweckten. Alex Evans, Co-Gründer des Entwicklerstudios, blieb in seinen Ankündigungen noch sehr vage, versprach aber, dass man Dreams  ""verstehen würde, wenn man sich damit beschäftige "". Weitere Details zu dem vielversprechenden Titelsollen Ende Oktober auf der Paris Games Week folgen. 
 
Bewährtes in neuer Verpackung gab es bei Sony ebenfalls zu sehen. So erscheint am 6. Dezember 2015 ein neuer Teil der Actionspielreihe Hitman als digitaler Download für PC, Xbox One und PS4 und bietet erstmals eine offene Spielwelt, die man in der Rolle des glatzköpfigen Hauptdarstellers Agent 47 erforscht. Entwickler IO Interactive plant, im Folgejahr in Zusammenarbeit mit der Community noch weitere Zusatzinhalte nachzuliefern. 
Außerdem waren exklusive Erweiterungen und Anspielphasen populärer Spiele beliebtes Thema auf der Sony-Veranstaltung. Wer etwa Hitman vorbestellt, darf das Spiel auf der PS4 in einer Betaversion vor allen anderen testen; das kommende Woche erscheinende Batman: Arkham Knight bekommt mit den Scarecrow-Missionen eine PS4-exklusive Vorgeschichte. Und der Egoshooter Call of Duty: Black Ops 3 geht mit einer PS4-Beta bereits im August an den Start. Künftig erscheinen zusätzliche Inhalte wie Kartenpakete zuerst auf der Sony-Plattform, das war zuletzt noch anders. Beim aktuellen Teil Call of Duty: Advanced Warfare erhielten stets zuerst Xbox-One-Spieler Zugriff auf bestimmte Inhalte. Offensichtlich hat sich Sony durchgesetzt. 
Auch Star-Wars-Fans und Figurensammler kommen nicht zu kurz: Warner Bros. und Disney liefern für die aktuelle Version ihres Spielzeughybriden Disney Infinity ein Startpaket aus, in dem neben Helden wie Prinzessin Leia Kopfgeldjäger Boba Fett enthalten sein wird. 
Das auf der E3 2014 von den Kritikern wohlwollend aufgenommene Weltraumspiel No Man's Sky war dieses Jahr nur eine Randnotiz. Sean Murray von Hello Games zeigte einmal mehr die beeindruckende Größe des prozedural berechneten Spieleuniversums, blieb aber einen konkreten Erscheinungstermin ebenso schuldig wie tiefere Einblicke in die Gameplay-Mechanik. 
Hinterließ Microsoft mit einem holografischen Minecraft und Ankündigungen zur Zusammenarbeit mit Oculus Rift und HTC Vive einen bleibenden Eindruck im Virtual-Reality-Sektor, gab sich Sony mit Project Morpheus auf der E3 überraschend zurückhaltend. Die VR-Brille erhält mit dem Actionspiel Rigs immerhin einen interessanten Mehrspielertitel, der im Modus drei gegen drei ablaufen wird. Die Zukunft des E-Sports könnte also durchaus auch im VR-Bereich liegen. 
Die Playstation Vita ist in Sonys Prioritätenliste scheinbar ganz nach hinten gerückt: Lediglich ein kurzer Trailer als Lebenszeichen, mehr war von der tragbaren Spielkonsole auf der E3-Pressekonferenz nicht zu sehen. Den Handheld könnte ein ähnliches Schicksal ereilen wie Microsofts Bewegungssensor Kinect: Ein technisch interessantes Spielzeug als Ergänzung zur eigentlichen Hardware, doch neue Spiele und Konzepte bleiben aus. 
Endgültig tot ist zudem die alte Konsolengeneration in Form der Playstation 3: Dem Oldie wurde auf der Sony-Pressekonferenz keinerlei Beachtung mehr geschenkt. 
Sony schloss seine Pressekonferenz mit seinen großen Spielen. Der 3D-Shooter Star Wars: Battlefront vom schwedischen Entwicklerstudio DICE liefert beeindruckende Bilder aus dem Star-Wars-Universum und greift mit spielbaren Charakteren wie Luke Skywalker oder Darth Vader die Euphorie im Vorfeld der Veröffentlichung des Kinofilms Star Wars – Episode VII: Das Erwachen der Macht auf. Star Wars: Battlefront erscheint am 19. November 2015 für Playstation 4, Xbox One und PC. 
Den Schlusspunkt der Show setzte allerdings Sonys Eigenproduktion Uncharted 4: A Thief's End. In einer knapp achtminütigen Spielsequenz kämpfte sich Protagonist Nathan Drake durch ein Dorf und raste wie Indiana Jones über staubige Pisten. Spielerisch ist Uncharted 4 kaum mehr als ein simpler Deckungsshooter, dafür verbindet Entwickler Naughty Dog die Mischung aus spielbarem Film und Gamepad-Action nahezu perfekt. Uncharted 4: A Thief's End erscheint im 1. Halbjahr 2016 und dürfte das bisher schönste Spiel für die PS4 werden."	technik
"Die EU-Kartellwächter verschärfen ihr Vorgehen gegen Google und werfen dem Internetkonzern erstmals auch unfairen Wettbewerb in seinem Kerngeschäft vor. Unter anderem schränke der Konzern die Möglichkeiten von Unternehmen ein, auf ihren Websites Suchmaschinenwerbung von Googles Wettbewerbern anzuzeigen, teilte die EU-Kommission mit. Zusätzlich weitete die Brüsseler Behörde die Vorwürfe im Wettbewerbsverfahren zur Shopping-Suche aus. 
Es ist der dritte Bereich, in dem die Kommission Google in einem sogenannten Statement of Objections unfairen Wettbewerb vorwirft und Änderungen fordert. In den ersten beiden Verfahren, die auf diese Eskalationsstufe kamen, geht es um Shopping-Angebote und das weltweit dominierende Betriebssystem Android. Zu den Shopping-Diensten hieß es jetzt, Google seien weitere Beweismittel und Daten übermittelt worden, die die Vorwürfe der Kommission untermauerten. 
Auch das Werbegeschäft prüfen die Brüsseler Kartellwächter schon seit Jahren. Google wies die Vorwürfe wettbewerbswidrigen Verhaltens stets zurück.  ""Wir sind davon überzeugt, dass unsere Innovationen und Produktverbesserungen nicht nur die Auswahlmöglichkeiten für europäische Konsumenten verbessert haben, sondern auch dem Wettbewerb förderlich sind "", heißt es in einer Stellungnahme von Google.  ""Wir werden die neuen Punkte der Kommission prüfen und darauf in den nächsten Wochen detailliert antworten. "" 
Rund 90 Prozent der Google-Umsätze von 74,5 Milliarden Dollar kamen im vergangenen Jahr aus dem Geschäft mit Online-Werbung. Dabei bringen Anzeigen im Umfeld der Internet-Suche nach wie vor das meiste Geld ein. Beim neuen Dachkonzern Alphabet steuerten alle anderen Geschäftsbereiche neben Google nur knapp 450 Millionen Dollar Umsatz bei. Die Ermittlungen der EU-Kommission treffen somit das Kerngeschäft des gesamten Konzerns. 
Die Kommission stört sich unter anderem daran, dass Kunden Google eine Mindestzahl von Suchmaschinenanzeigen abnehmen und dafür auf ihren Websites den am besten sichtbaren Platz reservieren müssten. Außerdem dürfe konkurrierende Suchmaschinenwerbung weder über noch neben Googles Suchmaschinen-Anzeigen platziert werden, erklärte die Kommission. Der Konzern hat nun zehn Wochen Zeit für eine Stellungnahme. 
Im April warf die Kommission Google in einem zweiten Verfahren vor, mit der Geschäftspolitik bei Android den Wettbewerb im Smartphone-Markt zu behindern. Sie stört sich unter anderem daran, dass Hersteller von Android-Geräten mit integrierten Diensten des Konzerns zwingend auch die Google-Suche und den Web-Browser Google Chrome vorinstallieren müssten. Außerdem biete der Konzern Herstellern und Mobilfunk-Betreibern finanzielle Anreize dafür, dass sie ausschließlich die Google-Suche auf den Geräten vorinstallieren. 
Bei EU-Wettbewerbsverfahren drohen in letzter Konsequenz Strafen von bis zu zehn Prozent des Jahresumsatzes. Die Kommission startete die erste Wettbewerbsprüfung von Google bereits Anfang 2010. Ähnlich lange könnte sich auch das neue Verfahren hinziehen."	technik
"Die EU-Kommission wusste offenbar schon 2010, dass Autohersteller die Abgaswerte von Dieselfahrzeugen manipulieren – fünf Jahre bevor der VW-Skandal im September 2015 öffentlich wurde. Das berichtet Spiegel Online unter Berufung auf interne Dokumente. Die Schreiben sollen zeigen, dass es sowohl innerhalb der Kommission als auch mit Regierungen der EU-Staaten ein jahrelanges Hin und Her gab. Auch die deutsche Bundesregierung war demnach schon 2012 an Treffen beteiligt, in denen es um Abgasmanipulationen ging. 
Am 8. Oktober 2010 habe ein internes Schreiben festgehalten, es sei wohlbekannt, dass es eine Diskrepanz zwischen den Emissionen von Dieselautos bei der Typenzulassung und im normalen Fahrbetrieb gebe. Es sei auch klar, woran das liege: am  ""verbreiteten Einsatz gewisser Minderungstechnologien in Dieselfahrzeugen "". 
Im Mai 2012 soll ein Kommissionsbeamter per E-Mail die zuständigen Ministerien in mehreren EU-Ländern, darunter auch das deutsche Umweltministerium, über das Treffen einer Arbeitsgruppe informiert haben zu Abgastestverfahren informiert haben. Dabei sei es auch darum gegangen, dass die Autohersteller sich gegen bestimmte Abgastests gewehrt hätten. Und es wird die Vermutung geäußert, das Ziel sei,  ""die Tür offenzulassen "" für die Umgehung von Abgastestzyklen. 
Während die EU-Kommission bis heute behauptet, von illegalen Praktiken nichts gewusst zu haben, soll das Problem schon im Sommer 2012 auf höchster Ebene bekannt geworden sein. EU-Industriekommissar Antonio Tajani sei vom Autozulieferer Schrader Electronics per Brief und in einem persönlichen Treffen über die Softwaremanipulationen der Autohersteller informiert worden. 
Volkswagen hatte im September 2015 zugegeben, weltweit in rund elf Millionen Dieselfahrzeuge unterschiedlicher Marken des Konzerns eine illegale Software eingebaut zu haben. Das Programm reduziert den Ausstoß von schädlichen Stickoxiden bei standardisierten Tests. Die Manipulation war in den USA aufgeflogen. 
Dort steht VW unter hohem Druck: Die kalifornische Umweltbehörde Carb hat den von VW eingereichten Rückrufplan für 3,0-Liter-Dieselmotoren bei größeren Autos als unzureichend und unvollständig abgelehnt. Die Epa, das Umweltamt der US-Regierung, schloss sich der Auffassung an. Ein VW-Sprecher sagte, man sehe die Ankündigung des Carb als einen  ""verfahrensrechtlichen Schritt "" an. Man arbeite weiterhin eng mit Epa und Carb zusammen."	technik
"Als Reaktion auf neue Enthüllungen im NSA-Skandal hat die SPD vom Kanzleramt Aufklärung über die Rolle des BND gefordert.  ""Das Kanzleramt muss jetzt mit höchster Priorität und ohne Ansehen der Person für Aufklärung vor dem Untersuchungsausschuss sorgen "", erklärte SPD-Vize Thorsten Schäfer-Gümbel in Berlin.  ""Ich schließe personelle Konsequenzen ausdrücklich nicht aus "", sagt Generalsekretärin Yasmin Fahimi der Berliner Zeitung. Dem Kanzleramt scheine die Aufsicht über den Bundesnachrichtendienst völlig entglitten zu sein. 
Ähnliche Worte fand Linken-Fraktionschef Gregor Gysi. Er warf dem Kanzleramt massives Versagen vor.  ""Das Kanzleramt ist das Kontrollgremium. Entweder sie haben nichts gewusst, dann funktioniert die Kontrolle nicht "", sagte er im Deutschlandfunk.  ""Oder sie haben es gewusst, dann hätten sie sich an rechtswidrigen Handlungen beteiligt. "" Gysi forderte eine Umstrukturierung des BND, um solche Vorgänge in Zukunft zu verhindern. Zudem müsse geklärt werden, wer zu welchem Zeitpunkt über die Angelegenheit informiert gewesen sei. 
Die Bundesregierung äußerte sich nicht zur Zukunft von BND-Präsident Gerhard Schindler. Regierungssprecher Steffen Seibert sagte, die Bundesregierung stehe weiterhin zur engen Zusammenarbeit mit den US-Geheimdiensten in der Terrorismusbekämpfung. 
Am Donnerstag war bekannt geworden, dass der BND für den US-Geheimdienst gezielt die Kommunikation europäischer Unternehmen und Politiker ausgehorcht haben soll. Betroffen sein sollen etwa der Rüstungskonzern EADS, der Hubschrauberhersteller Eurocopter und französische Behörden. 
Die Bundesregierung forderte vom BND volle Aufklärung. Das Kanzleramt soll erst kürzlich darüber informiert worden sein. Regierungssprecher Steffen Seibert hatte am Donnerstag gesagt, das Bundeskanzleramt habe  ""technische und organisatorische Defizite beim BND identifiziert "" und  ""unverzüglich Weisung erteilt, diese zu beheben "". 
Die Bundesanwaltschaft dementierte, dass sie wegen der neuen Spionage-Affäre Ermittlungen aufgenommen hat. Die Behörde erklärte auf Anfrage, dass sie aufgrund von Medienberichten über Aktivitäten britischer und amerikanischer Nachrichtendienste in Deutschland im Juni 2013  ""einen Prüfvorgang angelegt "" habe. Ein Zusammenhang zu den aktuell gegen den BND erhobenen Vorwürfen bestehe aber nicht. 
Zuvor hatte der Vorsitzende des NSA-Untersuchungsausschusses, Patrick Sensburg (CDU), in der ARD gesagt, die Generalbundesanwaltschaft sei eingeschaltet und ermittle. Der Generalbundesanwalt habe  ""in den letzten Tagen auch Akteneinsicht in unsere Protokolle des Untersuchungsausschusses erbeten "", sagte er.  ""Wir werden ihm das natürlich gewähren, weil wir beide ein gemeinsames Interesse haben, die Sachverhalte aufzuklären. ""
""Mit beiden Händen lässig in den Shorts steht Marcel Hutfilz auf einem elektrischen Einrad und kurvt in coolen Schlangenlinien über das zum Übungsplatz umfunktionierte Gelände des früheren Flughafens Tempelhof in Berlin. Hutfilz reißt zwischendurch auch mal beide Arme hoch oder steht wie ein zum Flug abhebender Flamingo auf nur einem Bein auf dieser kleinen 25 km/h schnellen  ""Rennmaschine "", wie er das Gefährt nennt. 
Die  ""Rennmaschine "" ist das mit zwei Trittbrettern ausgerüstete Monowheel E-400, und Marcel Hutfilz, ehemals Autoverkäufer, ist jetzt als Geschäftsführer der Berliner Firma Scooterhelden Spezialist für kompakte E-Mobilität. Er kennt alle Novitäten und fabriziert im Internet Testberichte über Hoverboards, Elektro-Scooter, E-Roller, Monowheels und andere Fahrzeuge. Das in eine Hartschale montierte Monowheel E-400 mit 1.500 Watt Motorleistung findet er faszinierend:  ""Das ist wirklich eine coole Nummer "", sagt Hutfilz:  ""Sieht toll aus, ist solide mit einer großen Reichweite und leicht zu bedienen. "" 
Das Beschleunigen und Bremsen funktioniert wie beim Segway durch Gewichtsverlagerung: Mit leichtem Neigen nach vorne wird das Tempo erhöht, ein Neigen nach hinten lässt das Monowheel bremsen. Mit sanft ausgeführten Bewegungen nach rechts oder links fährt man Kurven. Anfänger sollten für das Eingewöhnen rund zwei Stunden veranschlagen und für etwa zehn Minuten die Stützräder benutzen – und man sollte unbedingt Schutzkleidung tragen, rät Hutfilz. 
Das handliche Einrad wiegt nur gut zwölf Kilogramm, es kann an einem Griff leicht getragen werden und erreicht mit einer Akkuladung bis zu 35 Kilometer. Abgesehen vom enormen Spaßfaktor, der auch jugendliche Skater anspricht, wäre das Vehikel – trotz des hohen Preises von rund 900 Euro – eigentlich das ideale Gerät für Pendler auf der sogenannten letzten Meile, die im trubeligen Verkehr schnell den Arbeitsplatz erreichen wollen. 
Doch laut der Straßenverkehrsordnung darf man Monowheels ebenso wie E-Boards und ähnliche kleine E-Roller weder auf öffentlichen Straßen noch auf Gehwegen benutzen. Sonst kann das Gerät eingezogen und eine Geldbuße von mindestens 50 Euro erhoben werden. Sogar den Führerschein kann man bei Zuwiderhandlungen verlieren. 
In ihren Testvideos erwähnen die Scooterhelden diese Regelung aber nicht.  ""Ich fahre regelmäßig den Kudamm rauf und runter und kurve durch die Stadt, ohne dass mich jemals ein Polizist behelligt hätte "", sagt Hutfilz.  ""Die gesetzliche Lage ist auch nicht ganz klar, viele Beamte müssen selber erst nachfragen, was Stand der Dinge ist, weil E-Boards, Monowheels und andere nicht eindeutig eingestuft sind. Somit kann mich ein Beamter auch nicht entsprechend ahnden. "" 
Dann holt der ehemalige Autoverkäufer aus und bezeichnet die offizielle deutsche Handhabung beim Umgang mit den neuartigen Flitzern als hinterwäldlerisch:  ""Es kann doch nicht sein, dass man den Kauf von Elektroautos staatlich subventioniert oder Senioren auf 45 km/h schnellen Pedelecs ohne Führerschein fahren lässt, aber E-Board- und Monowheel-Fahrer mit pedantischen Vorschriften bevormunden will. Andere Länder wie Norwegen, Frankreich oder die Schweiz sind da viel weiter und behindern diese kompakte Mobilität nicht. "" 
 
Inzwischen gehören auch diverse kleine, circa sieben Kilogramm schwere Elektroroller wie der Gotway, der SXT-300 E-Roller oder der Revoluzzer zu den Elektromobil-Novitäten. Da sie aber über einen Sitz, Lenker, Rückspiegel und Licht verfügen und nicht selbstbalancierend sind, haben sie keine Probleme mit einer Straßenzulassung. Ganz anders sieht es beim Hoverboard aus: Das rund 300 Euro teure Zweiradbrett – das auch als Mini-Segway bezeichnet wird, weil es im Stehen gefahren wird – wird wie ein Monowheel durch veränderte Körperneigungen beschleunigt und gebremst. 
Michael Zimmermann, der in Hamburg das Hoverboard und auch das 1.000 Euro teure Ninebot-Einrad verkauft, sagt, der Einsatz im Straßenverkehr würde in der Hansestadt von der Polizei ohne Beanstandungen toleriert. Er moniert jedoch, dass diese Form einer Mobilität der Zukunft sich immer noch in einer juristischen Grauzone bewege:  ""Da hinken wir hier immer noch anderen Ländern hinterher! "" 
Für selbstbalancierende Monowheels, Hoverboards und ähnliche Geräte gelten die Bestimmungen der Straßenverkehrszulassungsordnung (StVZO), wenn die bauartbedingte Höchstgeschwindigkeit über 6 km/h liegt – was hier zutrifft. Diese Fahrzeuge erfüllen konstruktionsbedingt keine Zulassungsvorschriften hinsichtlich Sitz, Lenkung, Bremsen und Beleuchtung und dürfen daher nur im abgegrenzten nichtöffentlichen Bereich bewegt werden. 
In Blogs zu diesem Thema wird berichtet, mit einer für diese Geräte gelieferten Smartphone-App könne man das Höchsttempo auf 6 km/h begrenzen und hätte damit alle Probleme gelöst. Das ist jedoch nicht der Fall, denn diese Einstellung kann jederzeit geändert werden und ist nicht konstruktionsbedingt. Offenbar wird die 6-km/h-Regelung aber in den Bundesländern von den Beamten unterschiedlich ausgelegt. 
Zimmermann erklärt die Handhabung des E-Boards und des Ninebot-Monowheels auf seinem Verkehrsübungsplatz für Kinder:  ""Das ist eigentlich wie beim Radfahren. Man braucht etwa eine halbe Stunde, um mit der Gewichtsverlagerung klarzukommen, dann klappt es meistens schon gut. "" Kinder und Eltern können auf dem privaten Übungsplatz die Geräte ausprobieren. Zimmermann merkt kritisch an, dass viele Kinder mit dem sensibel reagierenden Hoverboard Probleme haben:  ""Das Board wühlt sich schnell in kleine Unebenheiten und bleibt dann stecken – es funktioniert nur auf einer absolut glatten Oberfläche optimal. "" 
Unsafe at any speed? Die brisante Sicherheitsfrage, die der amerikanische Verbraucherschützer Ralph Nader vor Jahrzehnten nach Unfällen mit Heckmotor-Autos aufbrachte, stellt sich nun offenbar auch bei einigen der kompakten E-Vehikel. Sie sind fast alle aus China importiert und zum Teil ziemlich salopp montiert: Lose Kabel oder zu dicht aneinander verbaute Akku-Packs haben schon zu extremer Überhitzung beim Aufladen und zu Brandunfällen geführt. 
In Großbritannien wurden von 17.000 importierten Hoverboards mehr als 15.000 wegen zu hoher Sicherheitsrisiken aus dem Verkauf gezogen. Und nach mehreren Bränden weigern sich die meisten US-Fluggesellschaften inzwischen, Hoverboards zu transportieren. Amazon hat in den USA einige dieser Modelle aus dem Sortiment genommen. 
Verheerend verlief auch ein Hoverboard-Test des Computermagazins c't: Mehrere Tester erlebten schwere Stürze, weil das getestete Smart Balance Wheel (360 Euro) aus  ""Sicherheitsgründen "" bei niedrigem Akkuladestand während der Fahrt abrupte Notbremsungen vornahm. Das Blatt hält die Funktion  ""für lebensgefährlich: Alle Tester, bei denen sich das Board während der Fahrt abschaltete, sind mit voller Wucht nach vorne auf den Boden geknallt. "" Ein Kollege, der ohne Schutzausrüstung unterwegs gewesen sei, habe sich bei einem E-Board-Unfall einen komplizierten Ellbogenbruch zugezogen. Bei c't nennt man das Hoverboard seitdem nur noch das  ""Aua-Board "". 
Das Bundesverkehrsministerium will sich jetzt einen kompletten Überblick über den Markt der elektrisch angetriebenen Kleinstfahrzeuge verschaffen. Dazu wurde die Bundesanstalt für Straßenwesen (BASt) beauftragt, wie das Ministerium auf Anfrage von ZEIT ONLINE mitteilte.  ""Seit Januar gilt auf europäischer Ebene die neue Typgenehmigungsverordnung (EU) 168/2013, unter die die meisten selbstbalancierenden Fahrzeuge fallen "", erläuterte das Ministerium. 
Die BASt solle prüfen, ob die Fahrzeuge  ""kategorisiert werden können, um dann unter bestimmten technischen und verhaltensrechtlichen Voraussetzungen im öffentlichen Straßenverkehr bewegt werden zu können "", so das Haus von Verkehrsminister Alexander Dobrindt (CSU). Da müssten dann allerdings auch solche ab Werk mitgelieferten gefährlichen technischen Mängel zur Sprache kommen. Die Untersuchung solle im Laufe des Jahres vorliegen."	technik
"Im Spielerparadies Las Vegas stehlen Autobosse den Glücksrittern die Show: Zur Technikmesse CES, die an diesem Dienstag eröffnet wird, lässt Audi einen A7 ( ""Jack "") aus dem Silicon Valley anreisen. Der Clou: Das Auto fährt automatisch, einen Fahrer gibt es nicht. 
Die Hälfte der 900 Kilometer langen Strecke hatte der Audi, der mit 20 Sensoren und modernster Software vollgestopft wurde, bis Sonntagabend geschafft. Kommt er unfallfrei in Las Vegas an, hat die VW-Tochter demonstriert, dass sie bei einem der wichtigsten Themen der Branche Trendsetter ist: beim automatisierten Fahren, Big Data auf Rädern, das total vernetzte Auto. 
Vorreiter wollen freilich auch andere sein: Daimler präsentierte schon auf der IAA 2013 eine automatisierte S-Klasse, mit der sich Konzernchef Dieter Zetsche auf die Bühne chauffieren ließ – ohne Chauffeur. Zuvor hatte die Limousine die rund 100 Kilometer lange Strecke von Mannheim nach Pforzheim bewältigt, auf der vor 127 Jahren Bertha Benz ins automobilie Zeitalter aufgebrochen war. 
Der schwedische Autobauer Volvo plant ab 2017 in Göteborg einen Großversuch mit 100 autonom fahrenden Autos. Auch BMW, Nissan und andere forschen und experimentieren. 
Matthias Wissmann, Präsident des Autoverbandes VDA, spricht von der  ""interessantesten Revolution rund ums Auto "" und sieht die Deutschen  ""auf dem Fahrersitz "". Doch bis die selbst fahrenden Audis und Mercedes’ regulär auf der Straße rollen, vergehen noch mindestens 15 Jahre, schätzt das Beratungsunternehmen Roland Berger. 
Dann aber, ab 2030, seien ganz neue Geschäftsmodelle möglich: Der weltweite Markt für Komponenten wie Kameras, Sensoren oder Kommunikationssysteme werde  ""ein zusätzliches Umsatzvolumen von 30 bis 40 Milliarden Dollar erreichen "". Weitere Umsätze in Höhe von zehn bis 20 Milliarden Dollar könnten durch hochentwickelte Software und ähnliche Produkte generiert werden. 
Hinter dem Milliardenmarkt stehen riesige Datenmengen, über deren Schutz, Verwertung und Eigentum bereits heute gestritten wird. Denn Google, Apple & Co. haben ebenso wie die Autoindustrie großes Interesse daran,  ""auf dem Fahrersitz "" die Entwicklung zu steuern. So tüftelt Google schon seit langem am selbst fahrenden Auto. Die kleine Kugel auf vier Rädern, die der Internetkonzern zu Testzwecken durch die Gegend rollen lässt, ist in Augen von Experten kein Angriff auf die klassische Autoindustrie. 
 ""Google und Apple wollen vor allem mit ihren mobilen Betriebssystemen Android und iOS in die Fahrzeuge der Hersteller "", sagt Digitalexperte Ralf Kaumanns. Insbesondere Google habe viele Produkte, die auf Geo- und Bewegungsdaten basieren, sei finanziell an Uber und anderen Taxi-Apps beteiligt. 
Datenschützer sehen am vernetzten Auto durchaus gute Seiten. So will die EU Autobauer in die Pflicht nehmen, Neufahrzeuge mit einem E-Call genannten System auszustatten, das bei Auslösen des Airbags einen Notruf absetzt. Gleichzeitig sehen die Fachleute den Gesetzgeber am Zug.  ""Die Automation bei Kraftfahrzeugen schafft ein gewaltiges neues Geschäftsfeld "", sagt der Hamburger Datenschutzbeauftragte Johannes Caspar. Das Spektrum reiche von Versicherern über Werkstätten und Arztnotdiensten bis hin zu Werbewirtschaft und Behörden. Der Datenschutz sei hier aber nicht hinlänglich geregelt. 
Für Versicherer ist das fahrerlose Auto noch  ""Zukunftsmusik "". Eines verspricht der Verband GDV Autohaltern schon heute –  ""auch Autos ohne Fahrer werden künftig eine Haftpflichtversicherung bekommen "", versichert der Verband."	technik
"Dass Smartphones Ortungswanzen sind, wie der Chaos Computer Club sie nennt, dürfte vielen Nutzern längst klar sein. Immerhin ist es nicht unwahrscheinlich, dass ein Gerät, das ständig seinen Aufenthaltsort kennt, sich diesen auch merkt. Trotzdem erzeugt es ein ungutes Gefühl, wenn Entdeckungen wie die folgenden bekannt werden: iPhone und iPad speichern sämtliche aufgezeichneten Ortungsdaten und übertragen sie an den Rechner des Nutzers, sobald der sein Smartphone oder seinen Tabletcomputer mithilfe der Software iTunes aktualisiert und abgleicht. Das berichteten die beiden Programmierer Alasdair Allan und Pete Warden gerade auf der Konferenz Where 2.0 im kalifornischen Santa Clara . 
Die beiden haben ein Programm namens iPhone-Tracker geschrieben , mit dem jeder Apple-Nutzer die über ihn gespeicherten Positionsdaten auslesen und auf einer Karte darstellen kann. 
Die Daten ähneln denen, die in Deutschland im Zusammenhang mit der Vorratsdatenspeicherung für heftige politische Diskussionen sorgen. Denn Mobilfunk-Provider speichern, von wo und wann welches Handy auf das Funknetz zugriff. Sie tun das unter anderem für die Abrechnung der Kosten beim Kunden, meist bis zu 90 Tage. Die EU verlangt sogar mindestens ein halbes Jahr Speicherfrist, damit die Polizei in bestimmten Fällen die Daten für Ermittlungen verwenden kann. Datenschützer halten das für eine anlasslose Totalüberwachung. Um das Problem darzustellen, visualisierte ZEIT ONLINE in einer interaktiven Grafik die Kommunikationsdaten des Grünen-Politikers Malte Spitz. 
 
Was iPhone und iPad aufzeichnen, ist diesen sogenannten Vorratsdaten recht ähnlich. Die Geräte speichern Längen- und Breitengrad basierend auf der Position von Handyfunkzellen.  ""Das ist weniger Präzise als mit GPS, aber verbraucht wahrscheinlich weniger Strom "", schreibt Allan auf seiner Website. Außerdem enthält die Datei zu jeder Position einen Zeitstempel in Sekunden, gerechnet vom 1. Januar 2001 an. 
Wie häufig das Gerät diese Daten ablegt, ist unterschiedlich, wahrscheinlich jedoch geschieht es bei jedem Einloggen in eine neue Funkzelle. Laut Allan und Warden ist diese Funktion erst in dem neuen Betriebssystem iOS 4 enthalten, frühere Versionen speicherten die Daten nicht. Im Zweifel also wird seit etwa einem Jahr aufgezeichnet, wo sich jemand aufhielt. 
Die Daten sind nicht verschlüsselt, sondern liegen offen in einem XML-Format vor, berichteten die beiden Programmierer. Was auch bedeutet, dass jeder, der Zugriff auf das Gerät selbst oder den Computer hat, mit dem es synchronisiert wurde, sie einsehen kann. Beispielsweise Lebensgefährten oder Freunde. Apple dagegen scheint die Daten nicht abzurufen, zumindest sagen Allan und Warden, dass sie keine Belege gefunden hätten, dass die Datei an Apple geschickt wird. 
Warum aber werden diese Informationen überhaupt gespeichert? Eine Erklärung von Apple gibt es dazu bislang nicht. Allan und Warden spekulieren , dass Apple künftig irgendwelche Anwendungen plant, die darauf aufbauen. Sicher jedoch sind sie sich, dass die Speicherung kein Zufall ist – immerhin würde die Datei gezielt bei jeder Synchronisation aktualisiert und auch auf neue Geräte übertragen. 
 
Die beiden waren nach eigenen Angaben zufällig auf die Datei gestoßen, als sie an einem Datenvisualisierungs-Projekt arbeiteten. Zunächst sei ihnen selbst unklar gewesen, dass Smartphone und Rechner Informationen über einen so langen Zeitraum speicherten.  ""Weder Pete noch ich glauben, dass es eine Art Verschwörung gibt, wir sind jedoch beide darüber besorgt, das so detaillierte Ortsinformationen gespeichert werden "", schrieb Allan. 
Wer das verhindern will, muss die Ortungsfunktion seines Gerätes abschalten und auch allen Anwendungen untersagen, sie zu nutzen. Das ist kein Problem, einige Angebote wie Kartendienste oder Foursquare funktionieren dann aber nicht oder nur eingeschränkt. Wer die Datenspeicherung im Grundsatz zulassen, die Informationen aber schützen will, kann das auch. iTunes bietet die Möglichkeit, alle bei der Synchronisation übertragenen Daten zu verschlüsseln. Damit kann die auf dem Rechner hinterlegte Datei nicht mehr von jedem ausgelesen werden. 
Genau diesen Punkt kritisieren Allan und Warden auch. Immerhin seien die von Mobilfunkprovidern erhobenen Ortsdaten vergleichsweise aufwändig geschützt. Die Apple-Ortsdaten jedoch überhaupt nicht. 
Ebenfalls ein Problem ist die Verschwiegenheit des Konzerns. Das Technologie-Blog Engadget wies zwar darauf hin, dass die Ortungsdaten-Aufzeichnung durch Apple-Geräte in Fachkreisen schon seit einiger Zeit bekannt war, und verlinkte auch auf einen entsprechenden Artikel des französischen Autors Paul Courbis von September 2010. 
Apple jedoch weist seine Kunden nicht darauf hin. Zumindest, solange man die sehr klein geschriebenen Geschäftsbedingungen für iPhone und iPad nicht genau gelesen hat. Denn dort heißt es:  ""Indem Sie ortsbasierte Dienste auf Ihrem iPhone verwenden, erklären Sie sich damit einverstanden, dass Apple, seine Partner und Lizenznehmer Ihre Ortungsdaten und -abfragen übertragen, sammeln, verwalten, verarbeiten und verwenden, um Ihnen diese Produkte und Dienste anbieten und sie optimieren zu können. ""
""Die konservativen Medien in den USA sind sauer auf Facebook. Alexander Marlow, Chefredakteur der Nachrichtenseite Breitbart, spricht von Diskriminierung  ""Millionen konservativer Menschen "" auf Facebook und fordert Mark Zuckerberg auf, sich in einem Live-Interview zu rechtfertigen. Lautstark äußern sich auch viele Unterstützer der Republikaner auf Twitter. Sie sehen ihre Vermutungen bestätigt: Facebook filtert Meldungen zu Ereignissen und von bestimmten Quellen bewusst aus und beeinflusst somit die Nutzer. 
Die Debatte angestoßen hatte ein Artikel des US-Onlineportals Gizmodo am Montag. Darin geht es um die in den USA verfügbaren Trending Topics, eine Sammlung von Links zu aktuellen Debatten und Ereignissen, vergleichbar etwa mit Google News. Ein ehemaliger, anonymer Facebook-Mitarbeiter sagte Gizmodo, die Inhalte seien nicht neutral ausgewählt. Unter anderem sollen Mitarbeiter dafür gesorgt haben, dass Nachrichten über konservative Politiker wie Mitt Romney und Rand Paul nicht als Trend auftauchten, obwohl die Facebook-Nutzer darüber diskutierten. 
Bereits vergangene Woche veröffentlichte Gizmodo einen Artikel über die Arbeit der Nachrichtenkuratoren bei Facebook. Demnach sei für die Auswahl der Trending Topics bloß ein kleines Team von Journalisten verantwortlich. Sie bekommen von Facebooks Algorithmen Themen vorgeschlagen, die Nutzer auf der Plattform gerade diskutieren. Anschließend müssen sie entscheiden, ob es wirklich ein aktuelles Thema ist. Falls ja, setzen sie einen Link auf eine ausgewählte Nachrichtenseite und schreiben einen möglichst objektiven Teaser. 
Genau diese Auswahl aber sei nicht neutral abgelaufen, sagt der anonyme Ex-Mitarbeiter. Je nachdem wer gerade Dienst hatte, seien einzelne Themen komplett ausgeschlossen worden. In anderen Fällen wurden keine konservativen Websites verlinkt, sondern gewartet, bis auch mutmaßlich liberalere Medien wie die New York Times über Themen berichteten. Zudem sei es vorgekommen, dass Themen in die Trendliste  ""injiziert "" wurden, obwohl sie auf der Plattform gar nicht groß debattiert wurden.  ""Wir bestimmen, was trendet "", zitiert Gizmodo eine weitere Quelle. 
Facebook hat am Dienstag auf die Vorwürfe reagiert. In einem Blogbeitrag schreibt der verantwortliche Manager Tom Stocky, man nehme Vorwürfe der Befangenheit sehr ernst. Manipulationen seien gemäß interner Richtlinien verboten und sämtliche politische Ausrichtungen sollten auf der Plattform ähnlich behandelt werden. Stocky bestätigt, dass die Vorauswahl ein Algorithmus trifft und die Themen anschließend von einem Team überprüft werden, etwa ob es sich um Falschmeldungen oder Doppelungen handelt. 
Stocky schreibt, es gebe keine Hinweise darauf, dass bestimmte Themen, Personen oder Ereignisse systematisch diskriminiert wurden. Die Aktionen der Mitarbeiter werden zunehmend protokolliert und Verstöße seien ein Kündigungsgrund. Ein dritter früherer Facebook-Mitarbeiter sagte im Gespräch mit dem Guardian, während seiner Zeit hätte er ebenfalls keine Manipulationen mitbekommen. Gegenüber Gizmodo bestätigte zumindest eine zweite Quelle, das Team der Trending Topics hätte eine gewisse Ablehnung gegenüber rechtskonservativen Nachrichten gehabt. 
Dass der Fall so stark diskutiert wird, illustriert den steigenden Einfluss von Facebook auf die Medienbranche. Eine Studie des Pew Research Center kam im vergangenen Jahr zu dem Ergebnis, dass 63 Prozent der Facebook-Nutzer das soziale Netzwerk als Nachrichtenquelle nutzen, in der Altersgruppe der Millennials ist Facebook sogar die erste Nachrichtenquelle. Allein in den USA hat das Netzwerk inzwischen weit mehr als 200 Millionen aktive Nutzer und eine Funktion wie die Trending Topics ist einer von mehreren Wegen, sie mit aktuellen Nachrichten zu versorgen. 
 ""Es gibt längst eine starke Abhängigkeit zwischen sozialen und klassischen Medien "", sagt Cornelius Puschmann vom Hans-Bredow-Institut für Medienforschung im Gespräch mit ZEIT ONLINE. Einerseits werden Artikel von Nachrichtenseiten auf Facebook gepostet und dort diskutiert. Andererseits schicken vielgeteilte Beiträge aber auch wieder mehr Besucher auf die Nachrichtenseiten. Prominent platzierte Links wie in den Trending Topics sind deshalb äußerst begehrt, aber sie werden auch argwöhnisch betrachtet. 
 
Ein oft gehörtes Argument von Kritikern: Facebook sei mittlerweile so einflussreich, dass es mit gezielter Auslassung von Inhalten die Nutzer beeinflussen könne. Wenn etwa in den Trending Topics oder ihren individuellen Newsfeeds keine konservativen Inhalte auftauchen, sei das eine Form der Meinungsmache, wenn nicht sogar Wählermanipulation. Und wenn in den Newsfeeds der Nutzern ohnehin nur Inhalte auftauchen, die ihren Vorlieben entsprechen, sei keine ernsthafte Debatte mehr möglich. 
 ""Das Filtern bestimmter Quellen kann sicherlich den Effekt haben, dass Nutzer auf Inhalte nicht so schnell aufmerksam werden "", sagt Puschmann, betont aber zugleich:  ""Die Leute leben nicht in einer Blase, in der Facebook ihre einzige Informationsquelle ist. "" Eine Studie aus dem vergangenen Jahr kam zu dem Ergebnis, dass sich Nutzer auf Facebook zu großen Teilen zwar mit Gleichgesinnten abgeben. Die vielzitierte Filterbubble aber sei kleiner als gedacht. Die Nutzer werden nämlich trotzdem mit Inhalten konfrontiert, denen sie kritisch gegenüberstehen (Bakshy et al., 2015). 
Abgesehen von möglichen Einflüssen auf die Nutzer geht es in der aktuellen Debatte darum, dass tatsächlich Menschen die Auswahl der Trending Topics treffen. Das ist verständlich: Vor den Gizmodo-Berichten gab es kaum Einblicke in das System und Facebook hält sich traditionell zurück. Das Unternehmen beruft sich stattdessen gerne auf seine automatisierten Analysen. Und genau dort liegt das Problem. 
 ""Es ist ein Irrglaube, dass Algorithmen völlig autonom und neutral arbeiten "", sagt Puschmann,  ""algorithmische Entscheidungen werden in den meisten Fällen durch menschliche Entscheidungen angereichert "". Als Beispiel führt er die Unterscheidung zwischen harten und weichen Nachrichten an: Würde ein Algorithmus eigenständig beliebte Inhalte in sozialen Netzwerken suchen, würde vermutlich jeden Tag Beyoncé oder Justin Bieber trenden. Dass dem nicht so ist, liegt letztlich an Menschen wie den Nachrichtenkuratoren von Facebook. 
Ähnlich schreiben es die Autoren des von Microsoft gesponserten Blogs Social Media Collective: Algorithmen können die unzähligen Beiträge auf Facebook durchforsten und Trends erkennen, aber sie können keine Themen einordnen, aufbereiten und abgleichen. Anders gesagt: Sie sind nicht – jedenfalls noch nicht – in der Lage, journalistisch zu arbeiten. 
Deshalb entscheiden im Fall von Facebook derzeit Hilfskräfte, was letztlich in welcher Art und Weise ein aktuelles Thema ist. Hilfskräfte, die bewusst oder unbewusst falsche Entscheidungen treffen und dies, sollte die Quelle von Gizmodo Recht behalten, auch in einigen Fällen taten. Oder anders gesagt: Hinter den Vorwürfen steckt entgegen der konservativen Meinungen keine politische Verschwörung vonseiten Facebooks und kein Algorithmus, der gezielt manipuliert wurde. Sondern bloß einzelne Menschen, die ihrer politischen Einstellung freien Lauf ließen – und dies offenbar auch ungestört konnten."	technik
"900 Millionen Menschen nutzen regelmäßig Facebooks Messenger. Damit ist er nach WhatsApp der zweitbeliebteste mobile Chatdienst der Welt. Doch während WhatsApp mittlerweile Ende-zu-Ende-Verschlüsselung zwischen zwei oder auch mehreren Nutzern unterstützt, werden die Nachrichten im Messenger bislang nur auf dem Weg von und zu Facebooks Servern verschlüsselt. Das Unternehmen kann die Nachrichten auf seinen eigenen Servern einsehen und an Behörden herausgeben, wenn ein richterlicher Beschluss vorliegt. 
Damit könnte bald Schluss sein – wenn die Nutzer das möchten. Wie der Guardian unter Berufung auf drei interne Quellen berichtet, möchte Facebook auch in seinem Messenger demnächst eine optionale Ende-zu-Ende-Verschlüsselung anbieten. Die Betonung liegt auf optional, denn standardmäßig soll das Feature nicht aktiviert sein. 
Damit würde Facebook dem Prinzip von Googles kommender Chat-App Allo folgen – beziehungsweise es vorwegnehmen. Wie Google auf seiner Entwicklungskonferenz I/O angekündigt hat, wird Allo im Herbst auf den Markt kommen und in der Default-Einstellung nicht Ende-zu-Ende-verschlüsselt sein, damit Googles künstliche Intelligenz (KI) die Nachrichten analysieren und passende Ergänzungen wie zum Beispiel Such- oder Antwortvorschläge machen kann. Wer mehr Wert auf Privatsphäre legt, wird den sogenannten Inkognito-Modus aktivieren und Googles virtuelle Assistentin damit ausschließen können. 
Auch Facebook verknüpft seinen Messenger mittlerweile mit künstlicher Intelligent: Chatbots von Unternehmen sollen sich mit Nutzern unterhalten und sie zum Beispiel beim Shoppen innerhalb des Messengers unterstützen können. 
Das mag in der Praxis bisher mehr schlecht als recht funktionieren, aber neben Google und Facebook hat auch Microsoft bereits das Zeitalter der conversation as an interface ausgerufen, und die verträgt sich eben nicht mit einer voreingestellten Ende-zu-Ende-Verschlüsselung. Die KI hinter den Chatbots und virtuellen Assistenten ist komplex und benötigt eine mächtige Rechner-Infrastruktur. Sie muss deshalb auf den Servern der Unternehmen verbleiben und von dort auf die Nachrichten der Nutzer zugreifen. 
Weil sowohl Facebook als auch Google davon ausgehen, dass ihren Nutzern die virtuellen Assistenten in ihren Messengerdiensten wichtiger sind als eine Verschlüsselung, mit der nur Sender und Empfänger eine Nachricht entziffern können, bleibt diese nur eine Option, die standardmäßig ausgeschaltet ist – auch wenn das die Entwickler mitunter selbst bedauern. Facebook wollte sich auf Nachfrage des Guardian zu den Plänen bislang nicht äußern."	technik
"Mitglieder des Chaos Computer Clubs haben die neue biometrische Sicherheitsfunktion des iPhone 5S gehackt und den eingebauten Fingerabdruckscanner überlistet. Sie hatten dazu einen Fingerabdruck von einer Glasoberfläche abfotografiert und so einen künstlichen Abdruck erzeugt. Damit gelang es ihnen ein iPhone 5s zu entsperren, welches mit dem TouchID geschützt war. 
Das neue iPhone 5s ist mit einem Fingerabdrucksensor ausgestattet, der das Telefon laut Aussagen von Apple sicherer machen soll als bisherige Sensoren. Zweifel an Nutzen und Sicherheit dieser Technik gab es praktisch sofort. Auch Datenschützer warnten davor. 
Nun zeigte der Chaos Computer Club (CCC), dass die Technik alles andere als sicher ist. Der CCC kommentiert:  ""Damit demonstrierten die Hacker wieder einmal, dass biometrische Daten zur Verhinderung eines unberechtigten Zugriffs vollkommen ungeeignet sind. "" 
Um die Sicherheitsfunktion zu überwinden, brauchten die Hacker nur wenige Materialien: einen Computer, einen Laserdrucker, eine Overhead-Folie und weißen Holzleim. Die Entsperrung dokumentierten sie in einem Video. 
Es ist das gleiche Verfahren, mit dem der CCC schon vor vielen Jahren demonstriert hat, dass sich Fingerabdrücke fälschen lassen: Ein echter Fingerabdruck wird dabei mit Sekundenkleber bedampft, wobei sich dessen Linien deutlich abzeichnen. Dann wird dieser Abdruck abfotografiert, das Bild am Computer nachbearbeitet und dann auf die transparente Folie gedruckt. Die wird anschließend mit Holzleim bestrichen. Ist dieser trocken, dient die entstandene dünne Schicht als neuer Fingerabdruck, um den iPhone-Scanner zu überlisten. 
Im Gegensatz zum früheren Verfahren musste CCC-Mitglied Starbug nur die Auflösung des Fotos und des Druckes erhöhen. Außerdem musste er den gefälschten Finger noch anhauchen, damit er feucht wird. Das trickst den Metallring im iPhone-Scanner aus. Der soll eigentlich sicherstellen, dass ein lebender Finger aufgelegt wird und keine Attrappe. 
CCC-Mitglied Dirk Engling twitterte, das Schwierigste dabei sei gewesen, ein iPhone 5S zu besorgen. 
 ""Die Öffentlichkeit sollte nicht länger von der Biometrie-Industrie mit falschen Aussagen an der Nase herumgeführt werden "", sagte Frank Rieger, einer der Sprecher des CCC.  ""Biometrie ist geeignet, um Menschen zu überwachen und zu kontrollieren, nicht um alltägliche Geräte vor dem Zugriff zu sichern. Es ist einfach eine dumme Idee, etwas als alltägliches Sicherheitstoken zu verwenden, was man täglich an schier unendlich vielen Orten hinterlässt. "" 
Hacker in aller Welt hatten für den ersten, der den Scanner des iPhone 5S überwindet, einen Preis ausgelobt. Bislang stand die Seite  ""Ist TouchID gehackt "" auf  ""No "", seit Veröffentlichung des CCC-Videos steht dort  ""Maybe "", vielleicht. Der CCC habe es möglicherweise geschafft, heißt es nun auf der Website. Vor Anerkennung des Hacks soll der Club aber noch ein Video einreichen, auf dem die Herstellung der Abdruck-Attrappe demonstriert wird. Das ist derzeit in Arbeit. 
Viele Länder haben ihre Ausweisdokumente inzwischen mit Fingerabdrücken ausgestattet. In Deutschland werden im Personalausweis seit 2010 optional zwei Fingerabdrücke gespeichert. Laut CCC ist das wirkungslos und erhöht die Sicherheit des Dokuments nicht."	technik
"Biometrie und Sicherheit passen nicht zusammen. Findet jedenfalls Jan Krissler. Der Forscher von der TU Berlin demonstrierte am Samstagabend beim 31. Chaos Communication Congress (31C3), wie haarsträubend einfach Systeme auszuhebeln sind, die biometrische Merkmale zur Authentifizierung verwenden. 
Um einen Fingerabdruck zu fälschen, reicht ihm ein hochauflösendes Foto, zum Beispiel eines von den Händen von Bundesverteidigungsministerin Ursula von der Leyen. Das zu besorgen, ist für einen Fotografen kein größeres Problem. Schon ein Objektiv mit einer Brennweite von zweihundert Millimetern reicht, um geeignete Bilder selbst aus sechs Metern Entfernung zu machen. Der Abdruck kann dann für eine Attrappe genutzt werden. 
Andere Sicherheitssysteme verwenden nicht den Finger als biometrisches Merkmal, sondern die Iris oder das Gesicht. Aber auch die sind mit guten Fotos problemlos zu täuschen. Mitunter reicht es, Fotos mit hoher Auflösung (1.200 dpi) auszudrucken und vor den Irisscanner beziehungsweise die Kamera zu halten. Krissler zeigt das am Beispiel der Software KeyLemon. Die entsperrt einen Rechner auf Wunsch nur nach Gesichtsabgleich, fällt aber auch auf ein Foto herein. 
Der Biometrieforscher weist nebenbei darauf hin, dass ein Angreifer solche Fotos nicht unbedingt selbst machen muss. Von vielen Menschen gibt es geeignete Bilder im Netz. Von Politikern wie Angela Merkel etwa finden sich dort qualitativ hochwertige Wahlplakate – eine exzellente Vorlage. Der Ausdruck muss nur gut genug sein, bei 600 dpi klappt es meist nicht. 
Noch absurder wird es, wenn der Angreifer auf das Smartphone seines Opfers zugreifen kann, genauer gesagt auf die Kamera. Dann braucht das Opfer den Finger nur kurz über das Gerät zu halten. Unter Laborbedingungen hat es Krissler geschafft, mithilfe einer präparierten App ein Bild des Fingers zu machen. Auch das reichte aus, um eine funktionierende Fingerabdruckattrappe herzustellen. Krissler kommt dabei zugute, dass heutige Smartphonekameras zum Teil 13 Megapixel Auflösung und mehr bieten. Sogar die schwächeren Frontkameras werden immer besser, um sie für Selfies und Videotelefonie nutzen zu können. 
Unter Umständen reicht das dann für einen Trick, für den Krissler beim 31C3 großen Applaus bekommt: In Fotos, die mit der Frontkamera gemacht werden, spiegelt sich das Display des Smartphones in der Pupille des Nutzers. Reicht die Auflösung, kann Krissler im Zoom erkennen, wie der Finger des Nutzers die PIN eingibt.  ""Videos sind in der Regel weniger hoch aufgelöst "", sagt er im Gespräch mit ZEIT ONLINE.  ""Deswegen ist es aus Sicht eines Angreifers sinnvoller, mehrere Fotos pro Sekunde zu machen. Oder man löst die Kamera immer dann aus, wenn das Gyroskop im Smartphone die PIN-Eingabe, also das Tippen auf dem Display registriert. "" Noch sei das ein wenig in die Zukunft gedacht, weil die meisten Frontkameras nur mit 1,2 bis zwei Megapixel auflösen. Aber dass es prinzipiell funktioniert, hat Krissel bereits zusammen mit Tobias Fiebig und Ronny Hänsch in einer Forschungsarbeit mit dem Titel Security Impact of High Resolution Smartphone Cameras beschrieben. 
Während des Gesprächs hat er schon die nächsten Ideen:  ""Man kann sich auch Kontaktlinsen bemalen lassen, das ist weniger auffällig, als ein Foto vor eine Kamera zu halten. "" Solche Irislinsen gibt es bereits zu kaufen, sie sind allerdings teuer. Geld dürfte für entschlossene Gegner jedoch kein Hindernis darstellen. Ausprobiert hat Krissler den Trick noch nicht.  ""Aber man kann die Strukturen einer Iris sehr fein nachmalen. Ich bin relativ sicher, dass es funktionieren würde. "" 
Dann fängt Krissler an, laut über holografische Kontaktlinsen nachzudenken:  ""Es gibt Holografie, die auf eine definierte Wellenlänge ausgerichtet ist. Man sieht das Hologramm also nur, wenn man es aktiv mit Licht dieser Wellenlänge beleuchtet. "" Eine entsprechend präparierte Kontaktlinse würde in Weißlicht ganz normal aussehen. Im typischen Infrarotlicht eines Irisscanners dagegen würde das Hologram auf der Linse sichtbar werden und die Iriserkennung austricksen können. 
Jetzt ist der Berliner Forscher in Fahrt:  ""Man kann die Kontaktlinse natürlich auch mit Infrarotfarbe bedrucken, das ist noch viel einfacher! Die ist im normalen Licht nicht zu sehen, aber im Infrarotlicht des Scanners. Warte, das muss ich mir aufschreiben. Ich glaube, das werde ich mal ausprobieren. "" 
 
Nun lassen sich manche Irisscanner nicht ganz so einfach überlisten. Sie setzen ein Lebenszeichen voraus, nicht nur ein unbewegtes Bild. Oft reicht ihnen allerdings ein Blinzeln. Auch das demonstriert Krissler am Beispiel von KeyLemon. Er legt dem Programm wiederum ein Foto vor, schwenkt dann aber einmal kurz mit einem Stift über die Augen. Die sind dadurch kurz bedeckt und KeyLemon interpretiert das als Blinzeln. Es wäre lustig, wenn es nicht so deutlich zeigen würde, in welch traurigem Zustand vermeintliche Sicherheitssysteme mitunter sind. 
Zwar sind einige der Angriffsszenarien nicht ganz alltagstauglich, weil es zum Beispiel neben der biometrischen noch andere Überprüfungen gibt, bevor jemand gewisse militärische oder sonstige Einrichtungen betreten kann. Oder weil jemand auch den Rechner oder das Smartphone eines Opfers in seinen Besitz bringen muss, um dann in Ruhe die Iriserkennung darauf aushebeln zu können. Aber die biometrischen Verfahren für sich genommen lassen sich laut Krissler fast alle austricksen – vor allem, wenn man genug Zeit und Geld hat.  ""Ich glaube nicht, dass man sie sicher machen kann. Allenfalls sicherer "", sagt er. 
Er rät den Herstellern solcher Technik, die Attrappenerkennung zu verbessern oder auf Merkmale auszuweichen, an die Unbefugte nicht so einfach kommen können. Venenmuster oder den Augenhintergrund etwa. Aber Krissler hat schon ein paar Ideen, wie jemand mit der nötigen Entschlossenheit auch an so etwas gelangen könnte."	technik
"Autor unbekannt, Inhalt reißerisch. Das ist die einfache Formel für Kettenbriefe, die sich im Netz verbreiten. Das Prinzip funktioniert immer noch. Selbst Menschen, die es besser wissen sollten, fallen noch darauf herein. Wie auch in diesem Fall: Derzeit warnt ein Text vor vermeintlichen Änderungen in den Facebook-Richtlinien. Die Nutzer sollen den Text kopieren und im eigenen Profil teilen, um ihr geistiges Eigentum vor Facebook zu schützen, besagt er. Dabei geht der Kettenbrief auf mehrere Gesetze, Statuten und Artikel ein, die Facebook daran hindern sollen, Inhalte aus den Nutzer-Profilen zu verwenden. Er lautet so, oder so ähnlich: 
 
Bei genauerer Betrachtung wird schnell klar, dass der Text irreführend und stellenweise völlig falsch ist. Das beginnt schon mit den  ""neuen Facebook Richtlinien "", auf die sich der vermeintliche Widerspruch bezieht. Facebook hat aber seine Richtlinien seit mehr als einem Jahr nicht mehr geändert. Zuletzt war das im Januar 2015 der Fall. 
Anschließend verweist der Text auf die Artikel  ""I. 111, 112 und 113 "" des Strafgesetzbuchs (StGB). Dazu sagt Sebastian Dramburg, Fachanwalt für IT- und Urheberrecht in Berlin:  ""Es gibt keine Artikel im StGB, sondern nur Paragraphen. Dort steht nichts zum geistigen Eigentum. Paragraph 112 StGB gibt es sogar gar nicht. "" Tatsächlich geht es im Paragraphen 111 um die öffentliche Aufforderung zu Straftaten, Paragraph 113 behandelt den Widerstand gegen Vollstreckungsbeamte. 
Der Kettenbrief behauptet weiter, Facebook benötige eine schriftliche Genehmigung, um Daten, Bilder oder Texte kommerziell zu nutzen. Auch das stimmt nicht. Facebook hat in seinen Richtlinien festgeschrieben, wie die Firma die Daten der Nutzer verwendet. Die Nutzer haben den Bedingungen mit einem Klick zugestimmt. Eine schriftliche Genehmigung ist nicht erforderlich. 
Auch besagt der vermeintliche Widerspruch, dass  ""veröffentlichen, vervielfältigen, verbreiten, senden "" (sic) für Facebook nun verboten sei. Sollte das stimmen, hätten die Nutzer das komplette soziale Netzwerk lahmgelegt.  ""Die Plattform würde nicht mehr funktionieren "", sagt Dramburg,  ""denn dort wird der Inhalt angezeigt, also veröffentlicht, gespeichert, also vervielfältigt und auf der Plattform wiedergegeben, also verbreitet. "" 
Ähnlich absurd geht es weiter. Er bezieht sich auf ein vermeintliches Gesetz namens  ""UCC 1-308 1-308 1-103 "". Dabei handelt es sich um den Uniform Commercial Code. Darin sind Empfehlungen von Rechtsexperten für das amerikanische Handelsrecht zusammengefasst, das viele US-Bundestaaten in großen Teilen übernommen haben.  ""Für deutsche Facebook-Nutzer ist das absolut bedeutungslos und scheinbar Objekt wildester Vorstellungen "", sagt Dramburg. 
Auch das verkürzt angegebene  ""Statut von Rom "" hat keine Bedeutung für Facebook-Nutzer. Es handelt sich offiziell um das Römische Statut des Internationalen Strafgerichtshofs. Es legt die Grundlagen für den Internationalen Strafgerichtshof in Den Haag fest. Einen Bezug zum sozialen Netzwerk gibt es nicht. 
Trotzdem haben die Bedingungen von Facebook Grenzen.  ""Viele, vielleicht sogar alle Nutzungsbedingungen könnten rechtswidrig sein, wenn sie nicht deutschem oder europäischem Recht entsprechen "", sagt Dramburg.  ""Facebook darf das, was für den Betrieb der Plattform erforderlich ist. Alles darüber hinaus, etwa die Nutzung von Nutzerfotos für eigene Werbezwecke oder zum Weiterverkauf, ginge nur, wenn der Nutzer diesem ausdrücklich zustimmt. Der Kettenbrief ist jedoch absoluter Humbug. "" 
Derzeit prüft das Bundeskartellamt, ob die Nutzungsbedingungen von Facebook rechtswidrig sind. Um die Facebook-Richtlinien bis dahin zu umgehen, bleibt kritischen Nutzern nichts anderes übrig, als die Plattform zu verlassen. 
"	technik
"Laut Apple ist der Fingerabdruck eines der besten Passwörter der Welt. So bewirbt das Unternehmen seinen Fingerabdrucksensor Touch-ID. Folgerichtig können Apple-Nutzer seit dem iPhone 5S mit dem Finger ihre Geräte ver- und entriegeln. Samsung und HTC haben nachgezogen. Tatsächlich ist der Fingerabdruck einmalig und technisch fast fälschungssicher. Doch im Strafprozess könnte er mancherorts bald seine Schutzfunktion einbüßen. 
Den Anfang könnte der US-Bundesstaat Virginia machen. Dort haben Richter in der vergangenen Woche entschieden, dass Polizisten einen Beschuldigten zum Entsperren seines Smartphones mittels Fingerabdruck zwingen dürfen. Das berichtet das Wall Street Journal. Im konkreten Fall ging es um häusliche Gewalt eines Mannes gegen seine Frau. Die Polizei wollte auf das Smartphone des Beschuldigten zugreifen, um weiteres belastendes Material zu finden. Das Smartphone war jedoch per Fingerabdruck gesichert. Der Beschuldigte weigerte sich, es zu entsperren, woraufhin die Polizei ihn zwang. 
Das Urteil ist insofern bemerkenswert, als im US-amerikanischen Recht, ebenso wie im deutschen, der Nemo-tenetur-Grundsatz gilt. Danach muss sich ein Beschuldigter nicht selbst belasten. Er kann also jedwedes Mitwirken an den Ermittlungen gegen seine Person verweigern. Darauf berief sich auch der Beschuldigte in Virginia. Es half ihm nichts. 
Hierzulande entfaltet das Urteil aus Virginia zwar keine Wirkung. Vollständig geklärt ist die Rechtslage in Deutschland aber nicht. Denn deutsche Gerichte haben über einen derartigen Fall noch nicht entschieden. Sicher ist bislang nur, dass ein Beschuldigter – anders als etwa in England – sein Passwort nicht verraten muss. Das gilt selbst, wenn er schwerster Straftaten verdächtig ist. Der Beschuldigte darf auch nicht dazu gezwungen werden, seinen Computer oder sein Smartphone aktiv selbst zu bedienen. 
All das umfasst aktive Handlungen. Was aber, wenn ein Polizist den Finger des Beschuldigten auf den Sensor drückt? Dann handelt es sich nicht um aktives Tun, sondern um Dulden. Im deutschen Strafprozessrecht könnte das den Unterschied machen. Auch der Strafverteidiger und Blogger Udo Vetter hat darüber bereits spekuliert und die unsichere Rechtslage bemängelt. 
Martin Lemke vom Republikanischen Anwaltsverein sieht für ein Urteil wie in Virginia keinen Raum:  ""Das ginge in Deutschland nicht. Ein Beschuldigter darf nicht Beweismittel gegen sich selbst sein und nicht zum Objekt des Verfahrens werden "", sagt Lemke. Der Beschuldigte hat das Recht, das Strafverfahren selbst zu gestalten, auch indem er eben nicht mitwirkt. 
Der Fall mit dem Fingerabdrucksensor unterscheidet sich auch von der erlaubten Abgabe eines Fingerabdrucks oder einer Speichelprobe. Diese Maßnahmen können zwar mit Zwang durchgesetzt werden und der Beschuldigte muss sie dann dulden. Der Beschuldigte wird aber nicht zum Objekt des Verfahrens gemacht. Denn der Beweis ist in diesen Fällen nicht allein die Probe oder der Fingerabdruck des Beschuldigten, sondern erst der positive Abgleich mit einer gesondert ermittelten Vergleichsprobe. Es ist also ein Beweismittel aus zwei Komponenten. 
Vorstellbar ist aber noch ein anderes Vorgehen: Ist von einem Beschuldigten bereits der Fingerabdruck bekannt, könnte die Polizei diesen Abdruck modellieren und auf das mit einem Fingerabdruck gesicherte Smartphone drücken.  ""Solche Maßnahmen wären wahrscheinlich zulässig "", sagt Lemke. Denn auch, wenn das Smartphone mit einem normalen Passwort geschützt ist, darf die Polizei im Umfeld des Täters ermitteln, etwa Notizen beschlagnahmen, und so das Passwort ermitteln. Ein modellierter Fingerabdruck wäre mit dieser Art von Ermittlung vergleichbar. Der Chaos Computer Club hat bereits vor Jahren gezeigt, wie einfach der Nachbau eines Fingerabdrucks ist, Berliner Sicherheitsforscher haben mit solchen Methoden die Sensoren des iPhone 5S und des Samsung Galaxy S5 ausgetrickst. 
 
Wenn Fingerabdrücke keinen Schutz mehr bieten, wird auch die von Apple und Google eingeführte standardmäßige Datenverschlüsselung auf Smartphones unter Umständen sinnlos. Denn die Daten sind frei zugänglich, sobald das Gerät entsperrt ist. 
Strafverfolgungsbehörden vor allem in den USA verstehen die neue Verschlüsselung in iOS 8 und Android 5.0 als Behinderung ihrer Arbeit. Das Urteil aus Virginia könnte ihnen in die Hände spielen, weil sie einfach den Finger eines Beschuldigten zum Öffnen ergreifen können, wenn sie sein Smartphone nicht mit anderen Mitteln entsperrt bekommen. 
Zumindest das iPhone muss aber nach dem Neustart oder nach mehr als 48 Stunden Inaktivität zunächst per PIN-oder Passworteingabe entsperrt werden. Mit dem Fingerabdruck geht es erst danach. Beschuldigte, die es schaffen, ihr iPhone rechtzeitig auszuschalten, können von der Verschlüsselung profitieren – vorausgesetzt, sie haben zuvor ein hinreichend starkes Passwort zum Entsperren des Geräts gewählt. 
Auf das Urteil aus Virginia wird aller Wahrscheinlichkeit nach eine Berufung folgen. Bestätigen es die nächsten Instanzen, könnte es in den USA zu geltendem Recht werden. 
Update: Nach Gesprächen mit weiteren Rechtswissenschaftlern hat sich das Bild präzisiert. Die Mehrheit der Experten hält es für zulässig, wenn Polizisten hierzulande das Handy eines Beschuldigten entsperren, indem sie den Finger des Beschuldigten auf den Sensor drücken. Der Beschuldigte muss dies nur erdulden, folglich läge keine 'aktive Selbstbelastung' vor."	technik
"Auf den ersten Blick sieht das Urteil des Europäischen Gerichtshofs (EuGH) nach einem Sieg für die Gegner der Vorratsdatenspeicherung aus. Der EuGH hält die EU-Richtlinie 2006/24/EG nämlich für unzulässig. Sie gilt damit nicht mehr. 
Auf den zweiten Blick könnte allerdings der Eindruck entstehen, es handele sich nur um einen Teilerfolg der Vorratsdaten-Gegner. Denn das Gericht hält die Datensammlung an sich für durchaus legitim. Die nächste Richtlinie muss eben nur strenger und deutlicher formuliert werden. 
Der dritte Blick wiederum belegt: Das Urteil ist ein Triumph der Vorratsdaten-Gegner. Denn das EuGH-Urteil stellt das Prinzip der verdachtsunabhängigen Datensammlung so fundamental infrage, dass auch eine neue Richtlinie es nicht retten könnte. Das bisherige Konzept der Vorratsdatenspeicherung ist passé. 
Zwei Grundrechte sind von der bisherigen Richtlinie betroffen: Die Achtung des Privatlebens und der Schutz personenbezogener Daten nach Artikel 7 und 8 der EU-Charta. Deren Einschränkung sei  ""auf das absolut Notwendige "" zu beschränken, urteilt der EuGH. 
Was das absolut Notwendige ist, sagen die Richter nicht – aber es ist schlicht nicht vorstellbar, dass damit die Speicherung der Metadaten von 500 Millionen unverdächtiger Bürgern gemeint ist. Zumal die Richter in diesem Zusammenhang  ""objektive Kriterien "" für die Festlegung der maximalen Speicherdauer verlangen – wie auch immer die aussehen sollen. 
Eine neue Richtlinie müsste garantieren, dass Daten nicht missbraucht werden oder sich Unbefugte Zugang zu diesen verschaffen können. Angesichts der Fähigkeiten und der Skrupellosigkeit, mit der sich die NSA und ihre engsten Partner auch auf illegalem Weg Zugriff auf Daten verschaffen, sind solche Garantien jedoch kaum realistisch. 
Der EuGH kritisiert auch eine fehlende Differenzierung in der Richtlinie 2006/24/EG, sowohl bei den Kommunikationskanälen – also Telefon, Mobilfunk, Internetdienste – als auch bei den Menschen, deren Daten gespeichert werden. Die Richter legen nahe, dass es Ausnahmen von der Datenspeicherung geben müsse, und zwar für völlig unverdächtige Menschen und für solche, deren Kommunikation dem Berufsgeheimnis unterliegt – Ärzte, Anwälte und Journalisten etwa. 
Ganz grundsätzlich wird der EuGH in Abschnitt 59 seines Urteils.  ""Insbesondere beschränkt sie [die Richtlinie 2006/24/EG – Anm. der Redaktion] die Vorratsspeicherung weder auf die Daten eines bestimmten Zeitraums und/oder eines bestimmten geografischen Gebiets und/oder eines bestimmten Personenkreises, der in irgendeiner Weise in eine schwere Straftat verwickelt sein könnte … "" 
Eine Datenspeicherung aber, die solche Einschränkungen berücksichtigt, kann keine anlasslose, monate- oder gar jahrelange Speicherung sein. Allenfalls das von der ehemaligen Bundesjustizministerin Sabine Leutheusser-Schnarrenberger (FDP) vorgeschlagene Quick-Freeze-Verfahren würde solche Anforderungen erfüllen. Dabei werden Daten erst im Verdachtsfall nachträglich eingefroren und auf richterlichen Beschluss an die Strafverfolger gegeben. 
Sicherheitsbehörden und Regierungen werden aber natürlich weiterhin nach Metadaten verlangen, die sie nachträglich durchsuchen können. Notfalls eben auch ohne EU-Richtlinie. Das zeigen erste Reaktionen wie die des Unionsabgeordneten Marco Wanderwitz:  ""Das heutige Urteil zur VDS ist wie ein Feiertag für das organisierte Verbrechen "", schrieb er auf Twitter. Für Menschen wie Wanderwitz besteht die EU weiterhin aus 500 Millionen Verdächtigen. Und auch die Signale aus der Bundesregierung lassen wenig Zweifel aufkommen, dass es trotz des heutigen Gerichtsurteils irgendwann eine wie auch immer geartete Speicherung von Verbindungsdaten geben wird. 
Aber die Minister wissen nun, was alles ins Gesetz muss und was alles nicht ins Gesetz darf, wenn es der Prüfung durch die höchsten Gerichte standhalten soll. Der EuGH hat heute nicht nur eine Hürde hochgelegt. Er hat einen Hindernisparcours aufgebaut. Und an dessen Ende steht hoffentlich etwas, das nicht 500 Millionen Menschen zu Verdächtigen macht."	technik
"Unter dem Namen Giulietta legte Alfa Romeo schon in den 1950er und 1970er Jahren ein Auto auf – damals in der Mittelklasse. Seit 2010 gibt es nun wieder ein Alfa-Modell mit der Traditionsbezeichnung. Es ist heute allerdings in der Kompaktklasse angesiedelt, also ein Konkurrent zum Bestseller VW Golf. Und zwar ein schmucker Konkurrent. Die Giulietta zählt zu den schönsten Modellen des Segments – die Fans müssen ihr aber verzeihen, dass sie nicht gerade das alltagspraktischste Auto der Golf-Klasse ist. 
Zwischen Golf, Focus und Astra ist die Giulietta die individualistische Alternative mit südeuropäischem Flair. Die hinteren Türgriffe sind in den Seitenscheiben integriert, sodass der Wagen auf den ersten Blick wie ein Zweitürer wirkt. Die Kehrseite der schönen Form ist ein geringes Platzangebot. Vor allem im Fond geht es eng zu. Auch der Kofferraum ist mit 350 Litern Volumen eher klein – und das, obwohl das Fahrzeug mit seinen 4,35 Metern Länge den Golf VI um 15 Zentimeter überragt. 
Auch beim Fahren unterstreicht die Giulietta den Anspruch ihrer sportlichen Optik: Das Fahrwerk ist straff, die Lenkung direkt. Dennoch ist der Wagen ausreichend komfortabel, und die Sitze sind auch auf langen Fahrten bequem. Die Verarbeitung im Innenraum stellt den Nutzer zufrieden. Schon in der Basisversion sind neben sechs Airbags auch eine manuelle Klimaanlage und diverse elektronische Helfer serienmäßig. Beim Crashtest von EuroNCAP erreichte die Giulietta 2010 die Höchstpunktzahl von fünf Sternen. 
Die Giulietta ist das erste Modell von Alfa Romeo, das ausschließlich mit Turbomotoren angeboten wird. Zur Wahl stehen vier Benziner zwischen 105 und 235 PS sowie drei Dieselvarianten mit 105, 140 und 170 PS. Ein Start-Stopp-System ist bei allen Modellen serienmäßig, bis auf den stärksten Turbobenziner im Quadrifoglio Verde. Dieser ist die sportlichste Version der Giulietta, mit tiefergelegtem Sportfahrwerk sowie Multifunktionstasten am Lenkrad. 
In puncto Langzeitqualität schlägt sich der kompakte Alfa ganz ordentlich. Die Giulietta ist lange nicht so anfällig wie der Vorgänger, der Alfa Romeo 147. In einigen Fällen machte das Doppelkupplungsgetriebe mit Schaltproblemen auf sich aufmerksam, und manche Giulietta-Fahrer klagen über vibrierende Außenspiegel, Lackplatzer oder Probleme bei der Bluetooth-Kopplung. 
In den vier Jahren, in denen die Alfa Romeo Giulietta nun auf dem Markt ist, zeigt sie bisher keine ausgeprägten Mängelschwerpunkte. In der Hauptuntersuchung werden Federn und Dämpfer nur minimal öfter beanstandet als der Gesamtdurchschnitt. Besonders positiv sind den Prüfern dagegen die Bremsen aufgefallen. Mängelfrei ist oft auch die Abgasanlage. 
Alles in allem verlangt ein ordentliches Exemplar der Giulietta von seinem Besitzer also nicht allzu viel Leidensfähigkeit. Das kostet ihn dann mindestens 10.000 Euro, denn mit diesen Preisen startet das Modell zurzeit auf dem Gebrauchtwagenmarkt."	technik
"Bei der Umsetzung der Vorratsdatenspeicherung in Europa gibt es schwerwiegende technische und rechtliche Probleme. Nachweise dafür, dass sie unerlässlich ist, sind offenbar schwer zu finden. Dennoch wünschen sich verschiedene Interessengruppen eine Ausweitung auf weitere Straftatbestände wie Urheberrechtsverletzungen und Hacking. Das ist das beunruhigende Ergebnis eines Zwischenberichts der EU-Kommission , der nun von der Bürgerrechtsorganisation Quintessenz geleakt wurde. 
Der Bericht gibt die Ergebnisse von Gesprächen der Kommission mit den Regierungen der EU-Mitgliedsstaaten, Strafverfolgungs- und Justizbehörden, Datenschützern, der Industrie sowie Vertretern der Zivilgesellschaft wieder. Eine Arbeitsgruppe der EU-Kommission soll diese Ergebnisse berücksichtigen, wenn sie in den kommenden Monaten ihre Vorschläge zur Reform der Vorratsdatenspeicherungsrichtlinie erarbeitet. 
Die Kommission hält damit an ihrer Überzeugung fest, dass die mindestens sechsmonatige, verdachtsunabhängige Speicherung von Telekommunikationsdaten aller EU-Bürger für die Aufklärung von Straftaten unerlässlich sei. Für Kritiker hingegen ist der Bericht ein weiterer Beleg dafür, dass die Vorratsdatenspeicherung komplett abgeschafft werden muss. 
Malte Spitz, Mitglied im Bundesvorstand der Grünen , hält den Bericht für  ""ein Dokument des Scheiterns "".  ""Die Kommission hält in ihrem Bericht ja selber fest, dass die Notwendigkeit dieser Speicherorgie nicht erwiesen ist, sondern immer nur mit Zurufen der entsprechenden Behörden und Ermittlungsstellen begründet wird. Eine wissenschaftliche Evidenz, ob die Vorratsdatenspeicherung sinnvoll, geschweige denn hilfreich ist, gibt es schlicht nicht. "" 
WAS VORRATSDATEN ÜBER UNS VERRATEN: 
 
Tatsächlich beginnt der Bericht mit dem Eingeständnis der EU-Kommission, dass Strafverfolgungs- und Justizbehörden vor allem  ""starke Meinungen "" zur Wichtigkeit der Daten geäußert hätten. Lediglich elf der 27 Mitgliedsstaaten hätten aber  ""starke qualitative Beweise "" übermittelt, die den Wert gespeicherter Kommunikationsdaten im Kampf gegen bestimmte Formen des Terrorismus, schwere Verbrechen sowie Straftaten, die mithilfe des Internets oder Telefons verübt werden, belegten. Letztere werden in der entsprechenden EU-Richtlinie 2006/24/EG allerdings nicht erwähnt. 
Auf EU- und nationaler Ebene herrsche noch immer der Eindruck vor,  ""dass es wenig Beweise für den Wert der Vorratsdatenspeicherung in Bezug auf die öffentliche Sicherheit und die Strafjustiz gibt "", heißt es in dem Bericht weiter. Auch sei nicht klar, welche Alternativen in Betracht gezogen worden seien. Zudem seien die in Artikel 10 der jetzigen Richtlinie beschriebenen Statistiken zur Verwendung von Vorratsdaten nicht geeignet, um die Notwendigkeit und Effektivität der Datenspeicherung zu evaluieren. 
Der Bericht nimmt also wichtige Punkte aus dem Evaluationsbericht vom April 2011 erneut auf. Schon damals wurde deutlich, dass die Datenbasis zu dünn war, um die Dauerüberwachung der EU-Bürger zu rechtfertigen. 
Aus dem nun veröffentlichten Bericht geht trotz solcher schwerwiegenden Bedenken hervor, dass manchen die derzeitige Ausgestaltung der Vorratsdatenspeicherung noch nicht weit genug geht. So steht auf Seite 4 des Berichts, dass Hacking zwar keine schwere Straftat sei, aber nur mithilfe von Vorratsdaten aufgeklärt werden könne. Außerdem gebe es  ""einige Rufe nach Ausweitung auf Urheberrechtsverletzungen wie dem illegalen Herunterladen von Dateien "". 
 
Strafverfolgungsbehörden hätten zudem den Wunsch geäußert, weitere Kommunikationskanäle in die Richtlinie aufzunehmen. So seien etwa Instant Messenger und Chatfunktionen vergleichbar mit E-Mails, aber noch nicht in der Richtlinie berücksichtigt. 
Datenschützer beklagen laut dem Bericht, dass es keinerlei Vorgaben zur Meldung von Datenmissbrauch oder -diebstahl gebe. Bürger, deren Daten abgerufen wurden, erfahren davon meistens nichts. Außerdem sei die Trennung von Daten, die sowieso aus betrieblichen Gründen von den Providern gespeichert werden, und denen, die auf Grundlage der EU-Richtlinie gespeichert werden müssen, unscharf. Übersetzt bedeutet das: Möglicherweise bräuchte es die Richtlinie gar nicht, weil die Telekommunikationsanbieter sowieso genug Daten vorhalten. 
Dennoch wird die Vorratsdatenspeicherung von der EU-Kommission nicht infrage gestellt. Vielmehr soll die zuständige Arbeitsgruppe nun überlegen, wie sie den Wert der Datenspeicherung öffentlich am besten demonstrieren kann. 
Wer mit wem, wann, wie lange, von wo aus und womit: Das ungefähr sind die Informationen, die anhand der Vorratsdatenspeicherung erfasst werden sollen. 
Das Gesetz, das das Bundesverfassungsgericht am 2. März 2010 für verfassungswidrig erklärte, war 2008 in Kraft getreten. Die Basis bildet eine EU EU-Richtlinie von 2006. Sie verpflichtete alle Telekommunikationsanbieter, die mehr als 10.000 Kunden haben, die sogenannten Verbindungsdaten für mindestens sechs Monate und maximal zwei Jahre zu speichern. 
Dies bedeutet: Die gesamte Kommunikation und auch alle Kommunikationsversuche via Telefon, SMS, E-Mail oder Internet werden erfasst und sind ein halbes Jahr rückwirkend noch nachvollziehbar – nicht ihr Inhalt, aber sämtliche Metadaten, die über Art und Umfang des Kontaktes etwas aussagen. 
Diese Daten sollen, so die Idee des Gesetzgebers, Strafverfolgern zur Verfügung stehen und ihnen vor allem bei der Suche nach Terroristen helfen. Allerdings lassen Schätzungen der Kommunikationsanbieter den Schluss zu, dass sie vor allem dazu dienen, leichtere Vergehen wie illegales Datentauschen, Betrug oder Beleidigungen zu verfolgen. 
Unabhängig davon ist der Hauptkritikpunkt, dass mit der Vorratsdatenspeicherung jeder Bürger potenziell verdächtig ist und überwacht wird und dass die Datenspeicherung so dazu beiträgt, die Unschuldsvermutung abzuschaffen. 
Außerdem gibt es Studien, die zeigen, dass sich anhand von solchen Verbindungsdaten detaillierte Aussagen über das Verhalten der Beobachteten machen lassen und dass die Daten mindestens genauso aufschlussreich sind wie ein Abhören der Inhalte der Kommunikation. 
Das Bundesverfassungsgericht hatte zwar das deutsche Gesetz dazu kassiert, nichtsdestotrotz schreibt die zugrunde liegende EU-Richtlinie von 2006 vor, Telefon- und Internetdaten zu Fahndungszwecken zu speichern. Die Bundesregierung, weder die schwarz-gelbe noch die große Koalition, hat sich bislang auf ein neues Gesetz einigen können. Man wollte die Entscheidung des Europäischen Gerichtshof im April 2014 abwarten. 
Am 8. April 2014 kippte der Europäische Gerichtshof die EU-Richtlinie von 2006. Die Vorratsdatenspeicherung stelle einen  ""Eingriff von großem Ausmaß und besonderer Schwere in die Grundrechte auf Achtung des Privatlebens "" dar, hieß es in dem Urteil. Die Speicherung von Kommunikationsdaten ohne Verdacht auf Straftaten sei nicht mit EU-Recht vereinbar. Bürgerrechtler aus Irland und Österreich hatten zuvor geklagt. 
Im Juli 2012 will die Kommission einen Reformvorschlag für die Vorratsdatenspeicherung vorlegen. Malte Spitz fordert dagegen ein Umdenken:  ""Statt die Nicht-Umsetzung der Richtlinie zu verfolgen, muss sie klar und deutlich für ein Ende der Vorratsdatenspeicherung in Europa eintreten. "" 
Spitz fordert die Bundesregierung auf, sich für die Abschaffung der Richtlinie einzusetzen:  ""Statt Formelkompromisse vorzubereiten, die einzig und allein dem Koalitionsfrieden dienen, sollte sie in Europa die Speerspitze für ein Ende des Speicherwahnsinns und dieses grenzenlosen Grundrechtseingriffes sein. "" 
Ähnlich klingt das auch bei Frank Herrmann vom AK Vorrat:  ""Der Kommission gelingt es auch sechs Jahre nach Einführung der Richtlinie zur Vorratsdatenspeicherung nicht, deren Notwendigkeit zu belegen. Stattdessen soll nun nach beliebigen Beispielen für die Vorteile von Vorratsdaten gesucht werden. Das Eingeständnis des Scheiterns der Richtlinie wäre jetzt ein mutiger und richtiger Schritt. "" 
Union und FDP sind sich weiterhin nicht darüber einig, wie sie die Richtlinie in Deutschland umsetzen wollen. Justizministerin Sabine Leutheusser-Schnarrenberger von der FDP plädiert für eine nur zweiwöchige Speicherfrist – aber nur im Falle eines berechtigten Verdachts. Das unionsgeführte Innenministerium dagegen sieht eine  ""Schutzlücke "" in Deutschland und verweist auf Fallbeispiele , die zeigen sollen, dass die Datenspeicherung nötig sei. Den geleakten Bericht der EU-Kommission will man im Ministerium derzeit nicht kommentieren. Ein Sprecher verweist aber darauf, dass die Frist für die Umsetzung der Richtlinie in nationales Recht abgelaufen und Deutschland verpflichtet sei,  ""eine Neuregelung für die Wiedereinführung von Mindestspeicherfristen zu treffen ""."	technik
"Auch ohne Vorratsdatenspeicherung bekommt die Polizei fast immer die Telefondaten, die sie benötigt. Das belegt eine Studie des Bundeskriminalamtes (BKA). Nur bei Internetverbindungsdaten wie IP-Adressen haben die Fahnder Probleme: Diese Daten werden oft gar nicht oder für ihre Zwecke nicht lange genug gespeichert. 
Diese Ergebnisse hat das Bundesinnenministerium am 27. Januar veröffentlicht. Am selben Tag veröffentlichten Wissenschaftler des Max-Planck-Instituts für ausländisches und internationales Strafrecht eine Untersuchung zur Vorratsdatenspeicherung , die sie im Auftrag des Bundesjustizministeriums erstellt hatten. Das Ergebnis ihrer 200 Seiten starken Studie : Der Wegfall der Vorratsdatenspeicherung kann nicht als Ursache für Veränderungen bei der Aufklärungsquote gelten. Insgesamt  ""ergeben sich keine belastbaren Hinweise darauf, dass die Schutzmöglichkeiten durch den Wegfall der Vorratsdatenspeicherung reduziert worden wären "". Die Datenbasis, auf die sie zurückgreifen konnten, sei allerdings unsicher und lückenhaft, räumten die Strafrechtsforscher ein. Ihre Ergebnisse seien deshalb lediglich eine  ""Momentaufnahme "". 
Die BKA-Studie vom selben Tag ist bislang kaum beachtet worden. Sie besteht aus einer statistischen Datenerhebung von Auskunftsersuchen des BKA an die Telekommunikationsanbieter im Zeitraum zwischen März 2010 und April 2011. Im März 2010 hatte das Bundesverfassungsgericht die Vorratsdatenspeicherung in der damals bestehenden Form für teilweise verfassungswidrig erklärt und damit gestoppt. 
Die Daten aus der Zeit bis April 2011  ""bekräftigen "" nach Darstellung des Innenministeriums  ""die Notwendigkeit der Speicherung von Telekommunikationsdaten für die Strafverfolgung und Gefahrenabwehr. "" Denn die dazu benötigten Verkehrsdaten seien  ""bei den Providern oftmals bereits gelöscht, wenn bei den zuständigen Behörden entsprechende Ermittlungen aufgenommen werden "". Etwa 85 Prozent der Auskunftsersuche aus dem untersuchten Zeitraum wurden von den Providern nicht beantwortet, da die entsprechenden Daten nicht oder nicht mehr vorhanden waren. Das Fazit des Innenministeriums:  ""Hinsichtlich der Beauskunftung von IP-Adressen hat dies im Ergebnis dazu geführt, dass von diesem Ermittlungsansatz wegen Aussichtslosigkeit kaum noch Gebrauch gemacht wird. "" 
 
Ein Blick auf die Details zeigt jedoch, dass diese Aussagen differenzierter betrachtet werden müssen: Erfasst wurden 5.082 Auskunftsersuche. Darunter waren zum Einen Erhebungen von  ""retrograden Verkehrsdaten  "", also etwa ein- und ausgehende Telefonnummern, Standortdaten sowie Beginn und Ende der Verbindung. 
Den mit Abstand größten Anteil aber machten Versuche aus, die hinter einer IP-Adresse stehenden Kundendaten zu ermitteln. 90,2 Prozent der Fälle bezogen sich auf solche Auskunftsersuche. Nur 9,2 Prozent bezogen sich auf retrograde Verkehrsdaten. 
In absoluten Zahlen: Bei den 5.082 Auskunftsersuchen ging es in 4.584 Fällen um IP-Adressen und nur in 467 Fällen um Telefonverbindungsdaten. Von diesen 467 wiederum wurden 380 von den Providern beantwortet. Größtenteils unbeantwortet blieben nur die Anfragen nach Daten zu IP-Adressen, nämlich in 4.195 Fällen. Von allen unbeantworteten Anfragen des BKA bezogen sich also nur etwas mehr als zwei Prozent auf Telefondaten, die Behörde bekommt diese demnach fast immer. Die Telekom etwa kopiert alle Verkehrsdaten ihrer Kunden für 30 Tage in eine Datenbank, die nur für polizeiliche Anfragen gepflegt wird. 
 
44,6 Prozent der BKA-Ermittlungen bezogen sich auf Betrugsdelikte, knapp 38 Prozent auf Kinderpornografie. In einem Prozent der Fälle ging es um Mord und Totschlag, in 2,2 Prozent der Fälle um Terrorismus. 
Nach Ansicht des BKA wäre in knapp sechs Prozent der Fälle, in denen Provider keine Auskunft geben konnten, eine Speicherung von Daten für mindestens einen Monat nötig gewesen. In 33 Prozent der Fälle wären mindestens zwei bis fünf Monate erforderlich gewesen, in 61 Prozent der Fälle hätten die Daten mindestens ein halbes Jahr vorgehalten werden müssen. 
Wer mit wem, wann, wie lange, von wo aus und womit: Das ungefähr sind die Informationen, die anhand der Vorratsdatenspeicherung erfasst werden sollen. 
Das Gesetz, das das Bundesverfassungsgericht am 2. März 2010 für verfassungswidrig erklärte, war 2008 in Kraft getreten. Die Basis bildet eine EU EU-Richtlinie von 2006. Sie verpflichtete alle Telekommunikationsanbieter, die mehr als 10.000 Kunden haben, die sogenannten Verbindungsdaten für mindestens sechs Monate und maximal zwei Jahre zu speichern. 
Dies bedeutet: Die gesamte Kommunikation und auch alle Kommunikationsversuche via Telefon, SMS, E-Mail oder Internet werden erfasst und sind ein halbes Jahr rückwirkend noch nachvollziehbar – nicht ihr Inhalt, aber sämtliche Metadaten, die über Art und Umfang des Kontaktes etwas aussagen. 
Diese Daten sollen, so die Idee des Gesetzgebers, Strafverfolgern zur Verfügung stehen und ihnen vor allem bei der Suche nach Terroristen helfen. Allerdings lassen Schätzungen der Kommunikationsanbieter den Schluss zu, dass sie vor allem dazu dienen, leichtere Vergehen wie illegales Datentauschen, Betrug oder Beleidigungen zu verfolgen. 
Unabhängig davon ist der Hauptkritikpunkt, dass mit der Vorratsdatenspeicherung jeder Bürger potenziell verdächtig ist und überwacht wird und dass die Datenspeicherung so dazu beiträgt, die Unschuldsvermutung abzuschaffen. 
Außerdem gibt es Studien, die zeigen, dass sich anhand von solchen Verbindungsdaten detaillierte Aussagen über das Verhalten der Beobachteten machen lassen und dass die Daten mindestens genauso aufschlussreich sind wie ein Abhören der Inhalte der Kommunikation. 
Das Bundesverfassungsgericht hatte zwar das deutsche Gesetz dazu kassiert, nichtsdestotrotz schreibt die zugrunde liegende EU-Richtlinie von 2006 vor, Telefon- und Internetdaten zu Fahndungszwecken zu speichern. Die Bundesregierung, weder die schwarz-gelbe noch die große Koalition, hat sich bislang auf ein neues Gesetz einigen können. Man wollte die Entscheidung des Europäischen Gerichtshof im April 2014 abwarten. 
Am 8. April 2014 kippte der Europäische Gerichtshof die EU-Richtlinie von 2006. Die Vorratsdatenspeicherung stelle einen  ""Eingriff von großem Ausmaß und besonderer Schwere in die Grundrechte auf Achtung des Privatlebens "" dar, hieß es in dem Urteil. Die Speicherung von Kommunikationsdaten ohne Verdacht auf Straftaten sei nicht mit EU-Recht vereinbar. Bürgerrechtler aus Irland und Österreich hatten zuvor geklagt. 
Zusamengefasst legen die BKA-Zahlen nahe, dass die Behörde eine Vorratsdatenspeicherung höchstens für Internetverbindungsdaten braucht. Informationen zu Telefonverbindungen bekommt sie auch ohne eine Wiedereinführung der Vorratsdatenspeicherung relativ problemlos. 
Malte Spitz, Vorstandsmitglied der Grünen , lehnt die Dauerspeicherung solcher Daten grundsätzlich ab:  ""Die interne Auswertung des Bundeskriminalamts, die teils mit aufgewärmten Zahlen aus der Vergangenheit operiert, zeigt einmal mehr, dass das Bestreben nach der Wiedereinführung einer sechsmonatigen Vorratsdatenspeicherung falsch, überzogen und nicht zu rechtfertigen ist. "" Die verdachtsunabhängige Protokollierung von Verkehrsdaten bei Telefongesprächen sei  ""völlig unbegründet. "" 
Sprachliches Detail am Rande: Die BKA-Erhebung sollte, so steht es im Dokument auf Seite drei,  ""quantitativ belegen, dass und in welchem Umfang polizeifachlicher Bedarf an der Auskunft über längerfristig gespeicherte Verkehrsdaten besteht "". Das gewünschte Ergebnis stand also vorher fest, sonst hätte es nicht  ""dass "" heißen müssen, sondern  ""ob. ""
""Das US-Zentralkommando ist Ziel eines proislamistischen Hackerangriffs geworden. Hinter der Attacke steckt möglicherweise die Terrormiliz  ""Islamischer Staat "" (IS) oder eine Gruppe, die den Extremisten nahesteht. Die Hacker hatten sich am Montag Zugang zum Twitter- sowie zum YouTube-Konto des US-Militärkommandos Centcom, das den Einsatz gegen den IS im Irak und in Syrien steuert, verschafft und interne Dokumente veröffentlicht. Ein Vertreter des US-Verteidigungsministeriums bestätigte, dass die Auftritte bei Twitter und YouTube gehackt worden seien. Entsprechende Maßnahmen würden getroffen, um auf den Angriff zu reagieren. Das Central Command ist unter anderem für den Nahen Osten zuständig. 
Der Twitter-Account des Central Command zeigte das Bild eines vermummten Dschihadisten und den Schriftzug  ""i love you isis "". Auf dem schwarzen Hintergrund waren die Worte  ""Cyber Kalifat "" zu lesen. Auch eine Namensliste von Generälen mit ihren Privatanschriften soll veröffentlicht worden sein.  ""Der IS ist schon hier, wir sind in euren Computern, in jedem Militärstützpunkt "", stand zwischenzeitlich auf dem Twitter-Konto des Centcom. Inzwischen wurde der Account gesperrt. 
Auf dem Konto von Centcom bei YouTube wurden außerdem zwei Propaganda-Videos des IS hochgeladen. Laut US-Medienberichten wurden über das Twitter-Konto auch mehrere Nachrichten abgesetzt.  ""Amerikanische Soldaten, wir kommen, seht euch vor "", soll demnach der erste Tweet gelautet haben. Auch Präsentationssoftware mit Bezug auf Nordkorea und China soll veröffentlicht worden sein. 
Das Ausmaß der Cyberattacke war zunächst unklar. Geheimdokumente des US-Militärs seien wohl nicht an die Öffentlichkeit gelangt, hieß es im Pentagon."	technik
"Die Besucher von Konferenzen und Messen wie der CES in Las Vegas kennen das: An jeder Ecke locken Gelegenheiten für Bilder und Videos, das Programm gibt es per Smartphone-App, Verabredungen werden per Messenger getroffen und zwischendurch wird geguckt, was auf Twitter und Facebook los ist. All das frisst nicht nur Zeit, sondern auch Strom. Für viele Besucher gehört der regelmäßige Gang zur Steckdose deshalb ebenso zum Messe-Alltag wie der Einblick in die neusten Trends und Techniken. 
Die Akkus in den modernen Gadgets sind ein Flaschenhals, da sind sich die Nutzer und Experten einig. Immer leistungsfähigere Geräte sollen immer dünner und leichter gebaut sein und gleichzeitig bei steigender Nutzung eine immer längere Akkulaufzeit liefern. Die Technik kann mit diesen Anforderungen nur schwer mithalten. Ein modernes Smartphone im alltäglichen Betrieb schafft es selten länger als einen Tag ohne Aufladen, während die Dumbphones von früher locker auf zwei bis drei Tage kamen. Heute ist es nicht mehr die Telefonie, sondern Funktionen wie die mobile Datennutzung, WLAN, GPS oder Games, die den Akku leersaugen. 
Daran wird sich in den nächsten Jahren wenig ändern. Die Lithium-Ionen-Akkus, die in den meisten Mobilgeräten verbaut werden, sind zu bewährt, zu günstig in der Herstellung und zu sicher, als dass sie bald abgelöst werden könnten. An neuen Techniken wie etwa Lithium-Schwefel- oder Lithium-Luft-Batterien wird zwar geforscht, eine serienreife Anwendung in Smartphones oder Tablets aber ist noch lange nicht in Sicht; viele Unternehmen konzentrieren sich zurzeit vor allem auf das Gebiet der Elektromobilität. 
Doch es gibt Versuche, die Abhängigkeit von der Steckdose zu verringern oder wenigstens den Ladevorgang erheblich zu beschleunigen. 
Auf Kickstarter macht in diesen Tagen ein Projekt aus Dresden auf sich aufmerksam. Kraftwerk heißt die mobile Brennstoffzelle, die kleine Geräte über USB wieder aufladen soll. Hinter dem Projekt steckt das Unternehmen eZelleron, das seit 2008 an emissionsarmen Energiequellen arbeitet. 
Kraftwerk ist eine mobile Stromversorgung für elektronische Geräte. Doch anders als gewöhnliche Akkupacks, die bloß besonders leistungsfähige Lithium-Ionen-Batterien sind, ist Kraftwerk eine Brennstoffzelle in Miniaturform. Sie wird mit gewöhnlichem Feuerzeug- oder Camping-Gas gefüllt, was bloß einige Sekunden in Anspruch nimmt. Über USB werden anschließend die Geräte angeschlossen. Der Ladevorgang soll in etwa so lange dauern wie von einem PC aus. 
Der Vorteil der Brennstoffzelle: Sie ist völlig unabhängig vom Stromnetz und liefert Strom für mehrere Wochen. Selbst in Situationen, in denen keine Steckdose verfügbar ist, lassen sich die Geräte aufladen. Das ist attraktiv für Forscher, Dokumentarfilmer oder Outdoor-Touristen in entlegenen Gebieten. Kraftwerk ist mit rund 200 Gramm zudem etwas leichter als gewöhnliche Akkupacks. Nach den Angaben auf Kickstarter soll Kraftwerk mindestens zwei Jahre lang funktionieren. So lange ist auch die Garantiezeit des Herstellers. Das Produkt scheint anzukommen: Mehr als die Hälfte der angepeilten 500.000 US-Dollar konnten die Entwickler bereits sammeln. 
Während Akkupacks und Brennstoffzellen den Smartphone-Nutzern mehr Unabhängigkeit geben, suchen andere Unternehmen nach einem Weg, den Ladevorgang einfacher und schneller zu gestalten. Auf der CES in Las Vegas präsentierte das israelische Start-up StoreDot neue Prototypen ihrer Akku-Technik. In weniger als zwei Minuten konnte der Akku eines Samsung Galaxy S5 vollständig geladen werden. 
StoreDot hat das Prinzip bereits im vergangenen Jahr vorgestellt. Es basiert nicht auf Lithium-Ionen, sondern auf Nanokristallen (nano dots), die aus organischen Peptiden mit bestimmten elektrochemischen Eigenschaften gewonnen werden. Dieses Verfahren ermöglicht einen schnellen Ladevorgang, allerdings auf Kosten der Leistungsfähigkeit: Die aktuellen Akkus von StoreDot halten nur etwa fünf Stunden und sind zudem etwas größer und teurer als die Standard-Akkus in gängigen Smartphones. Nach Angaben der Entwickler soll die Technik bis zum geplanten Verkaufsstart 2016 noch optimiert werden, doch ob die großen Smartphone-Hersteller sie tatsächlich adaptieren, ist fraglich. 
Attraktiv wäre sie. Schließlich ist es lästig, dass leere Smartphones im Schnitt zwei Stunden benötigen, um wieder vollständig aufgeladen zu sein. Ein etwas schwächerer Akku, der dafür in nur zwei Minuten geladen ist, ist für viele Nutzer möglicherweise interessanter. 
 
Bei den klassischen Lithium-Ionen-Akkus sind schnellere Ladezeiten aufgrund der physikalischen Eigenschaften schwieriger zu erreichen. Doch es gibt Fortschritte. Der Chipentwickler Qualcomm, dessen Prozessoren in vielen Smartphones zu finden sind, hat vergangenen Sommer die zweite Generation der Quick Charge Technik präsentiert. Sie verspricht bis zu 75 Prozent schnellere Ladezeiten, jedenfalls unter Laborbedingungen. In der Praxis scheint eine Ladung von etwa 60 Prozent in 30 Minuten realistisch. 
Der Trick liegt nicht in der Batterie, sondern in speziellen Ladegeräten. Sie haben eine höhere Stromaufnahme und können die Akkus deshalb schneller laden. Der Nachteil: Quick Charge unterstützen bis jetzt nur ein knappes Dutzend Geräte mit Qualcomms neusten Snapdragon-Prozessoren, darunter befinden sich aber einige Flaggschiffe wie das HTC One M8, das Samsung Galaxy Note 4 und das Nexus 6. 
Zusätzliche Akkupacks sind eine Sache. Neue Ladegeräte oder Techniken eine andere. Aber wieso gibt es eigentlich so wenige Lösungen, die den Akku entlasten? Eine dieser Lösungen heißt Wysips (What You See Is Photovoltaic Surface) und kommt vom französischen Unternehmen SunPartner. Bei Wysips handelt es sich um ein dünne Schicht Kristallglas, die nicht nur Daten empfangen kann (Li-Fi), sondern gleichzeitig auch Licht in Strom verwandelt und somit den Akku entlastet. 
Die Mini-Solarpanels reichen zwar nicht aus, um einen Akku komplett zu laden. Über den Tag soll die Technik allerdings bis zu 15 Prozent mehr Laufzeit ermöglichen, sagen die Forscher. Tatsächlich gibt es Wysips bereits in einem Handy: Das Meridiist Infinite des Schweizer Uhrenherstellers Tag Heuer verwendet die Technik. Es kostet allerdings auch 9.000 Euro und ist ansonsten kaum zu gebrauchen. 
Neben Licht könnte auch Körperwärme die Geräte der Zukunft betreiben. Das glaubt jedenfalls eine Forschungsgruppe aus Südkorea. Sie hat einen thermoelektrischen Generator entwickelt, der besonders klein und biegsam ist. Er ist in der Lage, aus Körperwärme eine kleine Menge Elektrizität zu generieren. Die Technik böte sich vor allem für Wearables wie Smartwatches an, die permanent mit der Haut in Berührung stehen und weniger Energie benötigen als ein Smartphone. 
Es geht noch abenteuerlicher. Das Bristol Robotics Laboratory hat einen Akku entwickelt, der sich mit Urin aufladen lässt. Mikroorganismen zersetzen die Bestandteile des Urins und erzeugen damit genug Strom, um ein (älteres) Handy betreiben zu können. 
Entwicklungen wie diese zeigen, welche Auswüchse die Suche nach besseren Akku-Techniken hervorbringt. Dennoch dürften Produkte wie das Kraftwerk, Nano Dots oder Solar-Displays in Zukunft weit entfernt von der Massentauglichkeit sein. Auch in den kommenden Jahren wird der Gang zur Steckdose deshalb für Smartphone-Besitzer zum Alltag gehören. Die digitale Mobilität, sie hängt am Ende eben doch am Kabel."	technik
"Nach dem Attentat auf die Redaktion des französischen Satiremagazins Charlie Hebdo hat der britische Premierminister David Cameron neue Gesetzesmaßnahmen zum Schutz vor Terroristen in Aussicht gestellt. Diesen dürfe  ""kein Raum zur sicheren Kommunikation "" geboten werden, sagte Cameron am Montag in einer Rede in Nottingham. 
Konkret bezieht sich Cameron auf die verschlüsselten Nachrichten in E-Mails oder Messengern.  ""Wollen wir in unserem Land wirklich eine Kommunikation zulassen, die wir im Extremfall nicht mitlesen können? "", fragte Cameron. In einer modernen und liberalen Demokratie sei es gerechtfertigt, dass die Behörden nach einem richterlichen Beschluss Zugriff auf die Inhalte haben. Sei dies nicht gewährleistet, müssten verschlüsselte Chat-Protokolle verboten werden. 
Mit diesen Aussagen richtet sich Cameron gezielt an Dienste wie WhatsApp oder Apples iMessage, die Nachrichten zwischen zwei Nutzern inzwischen verschlüsselt anbieten. Mit der Ende-zu-Ende-Verschlüsselung ist es nicht möglich, die Nachrichten auf dem Verkehrsweg abzufangen. Nach den Enthüllungen von Edward Snowden sind immer mehr große Konzerne darum bemüht, ihren Nutzern eine möglichst sichere Kommunikation zu gewährleisten. Genau das möchte Cameron offenbar verhindern – notfalls per Gesetz. 
Ein möglicher Entwurf liegt bereits vor. Die Draft Communications Data Bill, unter Kritikern auch  ""Schnüffler-Charta "" genannt, ist ein britisches Gesetz, das unter anderem eine zwölfmonatige Vorratsdatenspeicherung vorsieht. Zudem legitimiert es technische Maßnahmen wie die Deep Packet Inspection, mit der Datenpakete auf ihre Inhalte hin untersucht werden können. Im vergangenen Jahr blockierten die Liberaldemokraten um den stellvertretenden Premierminister Nick Clegg das Gesetz noch. 
Am 7. Mai stehen allerdings Neuwahlen in Großbritannien an. Sollten die Konservativen um David Cameron siegen, könnte das Überwachungsgesetz wieder auf der Agenda landen. Ohnehin wurden die Überwachungsmaßnahmen in den vergangenen zwölf Monaten verschärft. Vergangenen Juli setzte die Regierung ein umstrittenes Notstandsgesetz in Kraft, das bereits die Vorratsdatenspeicherung auf Umwegen einführte und dem britischen Geheimdienst GCHQ mehr Befugnisse erteilte. 
David Cameron äußerte sich in der Vergangenheit wiederholt besorgt über moderne Kommunikationskanäle. Nach den Unruhen in London im Jahr 2011 rief er dazu auf, soziale Netzwerke wie Twitter und Facebook schärfer zu kontrollieren und mögliche Straftäter stärker zu verfolgen. 
Auch in Deutschland werden nach den Anschlägen von Paris neue Maßnahmen zur Verhinderung von Anschlägen diskutiert. Neben der stärkeren Überwachung mutmaßlicher Dschihadisten steht auch hier die Vorratsdatenspeicherung im Mittelpunkt. Die möchte die Union ohnehin gerne einführen. SPD-Justizminister Heiko Maas lehnte das dieser Tage ab. Der Fall in Frankreich hätte gezeigt, dass sie nicht geeignet sei, Anschläge zu verhindern. 
Ebenso unwahrscheinlich ist, dass ein von David Cameron gewünschtes Verbot verschlüsselter Nachrichten den gewünschten Erfolg in der Terrorbekämpfung hat. Die Idee sei verrückt, sagen Sicherheitsexperten dem Guardian. Hintertüren für Behörden pauschal einzuführen sei undenkbar, da niemals sämtliche Unternehmen einstimmen würden. Ein komplettes Verbot würde zudem die Onlinesicherheit der britischen Bürger einschränken.  ""Cameron versteht die Technik nicht, über die er spricht "", schreibt der Autor Cory Doctorow in seinem Blog Boingboing und warnt vor möglichen Folgen für die Software- und IT-Industrie. 
Außerdem könnte es die ohnehin schon weitreichenden Überwachungsprogramme erleichtern und weiter legitimieren. Das hätte fast schon Tradition: Bereits im Jahr 2007, damals noch unter Premierminister Tony Blair, schrieb ZEIT-Autor Reiner Luyken:  ""Das Mutterland der Demokratie verwandelt sich in den rabiatesten Überwachungsstaat der westlichen Welt. ""
""Wer einmal versucht hat, eine Haustür mit einer Kreditkarte zu öffnen, hat vermutlich festgestellt: Kreditkarten zu ersetzen, kann unangenehm lange dauern und die meisten Haustürschlösser sind gut gesichert. Sperrt man sich aus, ist die Ultima Ratio deshalb meist der Schlüsseldienst. Der ruckelt fünf Minuten am Schloss und ist drin. An Sonn- und Feiertagen kostet die Ruckelei mehrere Hundert Euro. Schlüsseldienst-Apps kosten zwar weniger, bergen aber eine Gefahr: Sie könnten interessant für Kriminelle sein. 
Das Unternehmen KeyMe aus San Francisco arbeitet daran, den Schlüsseldienst zu ersetzen. Um in eine Wohnung zu kommen, braucht man nur noch ein Foto vom Schlüssel. Nutzer erstellen dieses Foto einmal und speichern es in der App von KeyMe ab. Haben sie sich ausgesperrt, gehen sie mit dem eingescannten Foto zum KeyMe-Kiosk, einem Automaten oder zum Schlüsselladen selbst. Eine halbe Minute braucht der KeyMe-Kiosk, um aus dem gespeicherten Foto einen Kunststoffschlüssel zu drucken. Das kostet beim Schlüsselmacher zehn US-Dollar, beim KeyMe-Kiosk 20. 
Der Markt für KeyMe ist groß. Nach Angaben der Firma sperren sich allein in den USA pro Jahr 90 Millionen Menschen aus. Das ist fast jeder dritte US-Bürger. Bislang beschränkt sich KeyMe, genau wie Konkurrent KeyDuplicated, ausschließlich auf die USA. Rechtliche Probleme gab es mit den Schlüsselkopierern bislang nicht.  ""Es ist uns kein Fall bekannt, in dem Diebe einen Schlüssel nachgemacht haben "", teilt Michael Harbolt mit, der Marketingchef von KeyMe. 
Dass Missbrauch aber doch möglich ist, zeigte Andy Greenberg, Redakteur von Wired. Er machte ein Foto vom Schlüssel eines Freundes. Eine Stunde später stand er in dessen Wohnzimmer. 
Greenberg ist geglückt, was die KeyMe-Gründer vorgeben, um jeden Preis verhindern zu wollen: der unbefugte Gebrauch der App. Zahlreiche Hürden haben sich die Entwickler einfallen lassen. Das Foto des Schlüssels soll auf weißem Untergrund, von beiden Seiten und im Abstand von zehn Zentimetern aufgenommen werden. Wer den Schlüssel nachmachen will, soll außerdem per Kreditkarte zahlen und seine Adresse hinterlassen. Sogar die Verwendung von Überwachungskameras hat KeyMe angedacht. 
Diebe werden sich von derlei Hürden kaum abhalten lassen. Dass es ohne Extras geht, beweist Greenberg, der einfach ein normales Foto von dem fremden Schlüssel machte. KeyMe ist damit die erste kommerzielle Anwendung, die das Schlüsselmachen jedem ermöglicht. Eine Schlüsselfabrik für die Hosentasche. 
Auf die Probleme der normalen Metallschlüssel hatten CCC-Mitglieder aus München bereits lange vor KeyMe hingewiesen. Schon auf dem 29. Chaos Communication Congress vor zwei Jahren (29C3) zeigten sie, wie sie Schlüssel kopieren. Darunter waren auch Nachbauten einiger Handschellenschlüssel. Die dabei eingesetzte Technik ähnelt der von KeyMe: Auf Grundlage eines Fotos wird ein 3D-Modell des Schlüssels erstellt. Ein 3D-Drucker druckt anhand dieses Modells den neuen Schlüssel aus. 3D-Drucker sind bei der Herstellung also die Brücke zwischen dem digitalen Modell und dem materiellen Schlüssel. Da es mittlerweile brauchbare 3D-Drucker für ein paar Hundert Euro gibt, kann jeder zum Schlüsselhersteller werden, auch ohne KeyMe. 
 
Der Metallschlüssel wird in Zukunft also leicht kopierbar sein. Eigentlich war er das aber schon immer: Denn der Schlüsselladen um die Ecke braucht auch nur zehn Euro und weniger als fünf Minuten, um einen vorgelegten Schlüssel zu kopieren. Der entscheidende Unterschied ist ein anderer: Der Personenkreis der Schlüsselnachahmer erweitert sich massiv. Bislang konnte man nur mit dem Schlüssel in der Hand zum Schlüsseldienst gehen. Mit KeyMe reicht schon heute ein Handyfoto aus, Zeugen gibt es im Zweifel nicht. 
Es dürfte nicht lange dauern, bis auch schlechte oder verzerrte Fotos als Grundlage für ein Modell ausreichen. Behörden werden sich darauf einstellen müssen, ihre bisherigen Schließsysteme besser zu sichern. Am offensichtlichsten tritt das bei den Handschellenschlüsseln zutage, wie der Vortrag auf dem 29C3 zeigte. Nachgemachte Schlüssel sind aber nicht nur im Haus und im Gefängnis ein Problem: Überall werden Menschen bald Schlüssel kopieren können. Sei es im Schwimmbad bei den Spinden oder im Büro bei den verschließbaren Trolleys. Ein Foto genügt und das Schloss ist passé. Und ständig Schlösser auszutauschen, wird teuer. 
Statt Metall werden daher künftig elektronische Transponder die Aufgabe des Schlüssels übernehmen. Darauf stellen sich namhafte Schlüsselhersteller wie Assa Abloy seit Langem ein, die dazu vor allem RFID-Chips einsetzen. Bislang werden diese Systeme vor allem in Unternehmen gebraucht. KeyMe lässt das unberührt. Das Unternehmen hat angekündigt, bald auch Autoschlüssel zu kopieren. Marketingchef Harbolt sagt zudem, dass KeyMe nach Europa expandieren wolle. Die Konkurrenz der elektronischen Schlösser sieht er nicht als Problem an. 
Das größte Problem für KeyMe werden aber nicht die Gesetze und Märkte anderer Länder sein, sondern das Geschäftsmodell. Denn mit KeyMe und dem dahinterstehenden Verfahren wird der Metallschlüssel unsicher. Ein unsicheres Schloss aber ist sinnlos. Mit anderen Worten: Je erfolgreicher KeyMe läuft, desto mehr schafft sich das Unternehmen selbst ab."	technik
"Wie so oft in der Geschichte war es eine Krise, die als Katalysator für geniale Kreativität wirkte. Vor einem halben Jahrhundert suchte der damalige Renault-Chef Pierre Dreyfus dringend nach einem neuen Fahrzeugkonzept. Damit sollte das französische Staatsunternehmen wieder profitabel werden und mit neuen Modellen in die prestigeträchtigen Fahrzeugklassen zurückkehren. Schließlich mangelte es Renault seit dem 1960 eingestellten Frégate an Mittelklasse-Modellen. Außerdem brachte die Beschränkung auf nur zwei kleine Volumen-Baureihen sowohl Image-Verlust als auch eine klamme Kassenlage. 
Dieser Situation wollte auch der französische Staat unter Präsident Charles de Gaulle nicht länger zuschauen, denn der damals größte Autohersteller des Landes – und zu der Zeit viertgrößte europäische – galt als Wirtschaftslokomotive. Als Frankreich 1963 auch noch die Schutzzölle auf Autoimporte lockern musste, ging die Renault-Produktion um fast ein Viertel zurück. Was folgte, hat bei unseren westlichen Nachbarn Tradition: Revolution. In diesem Fall verkörpert durch den Renault 16. Als weltweit erstes Massenmodell in der Mittelklasse vereinte er Vorderradantrieb, Fließheck, variablen Innenraum und fünf Türen, und so brachte er Renault wieder nach vorn. 
Der Renault 16, vom erst 31-jährigen Nachwuchsdesigner Gaston Juchet gezeichnet, war seiner Zeit ein halbes Jahrzehnt voraus. Erst dann folgte aus England der gleichermaßen konzipierte Austin Maxi; weitere fünf Jahre sollte es dauern, bis mit dem Volkswagen Passat das dritte fünftürige frontgetriebene Modell dieses Formats präsentiert wurde. Fester Bestandteil bei fast allen Mittelklasse-Baureihen wurden die Schräghecks jedoch erst Mitte der 1980er Jahre – da fuhren bei Renault bereits die Enkel des R16 Verkaufserfolge ein. 
Das führte dazu, dass der Renault 16 von Beginn an skurril wirkte und die Blicke auf sich zog. Das erlebte Renault-Chef Dreyfus etwa bei seinem Antrittsbesuch bei Präsident de Gaulle am Jahresanfang 1965. Auf dem Parkplatz des Élysée-Palastes sah der 4,23 Meter kurze R16 mit seinem Fließheck zwischen den repräsentativen Stufenhecklimousinen aus wie ein Paradiesvogel – selbst im Vergleich mit dem avantgardistischen Citroen DS. Ein Anblick, an den sich die europäischen Autofahrer schon bald gewöhnen sollten, denn wo immer ein Renault 16 auftauchte, er war der Nonkonformist unter den Normalo-Mittelklassemodellen. 
Wer den Renault 16 wählte, setzte auf eine große Heckklappe und schräge Linien statt steifer Stufe oder statt ausladendem American Style wie etwa bei Ford und Opel. Zu den merkwürdigen Seiten des Modells zählte das Kuriosum, dass – wie schon beim Renault 4 – der Radstand links und rechts unterschiedlich ausfiel, aufgrund hintereinander platzierter Drehstäbe an der Hinterachse. Links betrug der Radstand 2,72 Meter, rechts 2,65 Meter. So viel Raum boten Mitte der 1960er Jahre nicht einmal Oberklasse-Limousinen, wie die begeisterte Motorpresse feststellte. 
Größter Clou des Renault aber war die variable Bestuhlung. Die Sitze ließen sich um bis zu 15 Zentimeter verschieben, umklappen oder herausnehmen. Vergleichbares hatte die Autowelt noch nicht gesehen. Der Renault wies damit die Richtung, wie die Raumnutzung in Autos künftig auszusehen hatte, und tatsächlich orientierten sich sogar Kombis fortan daran. 
Der R 16, der sich fast 1,9 Millionen Mal verkaufte, war eben eine Familienlimousine  ""für die Zukunft "" (Renault-Werbung), wie sie damals nur Franzosen realisieren konnten. Nicht einmal die Italiener mit ihrem feinen Gespür für Formen und Familienfreundlichkeit waren so früh für ein fünftüriges Fließheck bereit, obwohl der kleine Autobianchi Primula bereits Vorarbeit geleistet hatte. Der geniale Autokonstrukteur Alec Issigonis hatte mit dem Mini von 1959 der Autowelt zwar Frontantrieb und raumsparende Bauweise beschert, aber der erste großformatige Fünftürer kam eben aus Frankreich. Erst danach bereiteten die Briten für 1969 den fünftürigen Austin Maxi vor. 
 
In Deutschland näherte sich der Autobauer Glas – bis heute vor allem als Erfinder des Goggomobils bekannt – dem Kombiheck-Design 1966 mit seinem sportiven Lifestylemodell Glas CL; BMW wagte den ersten Versuch mit dem o2 Touring. Beide Modelle waren allerdings nur Dreitürer mit konventionellem Hinterradantrieb. Erst der VW Passat brachte 1974 in Deutschland den Durchbruch des schrägen Hecks. 
Zu dieser Zeit führte der Renault 16 seine globale Erfolgsstory bereits zum Zenit. Der neue, besonders luxuriös ausgestattete Renault 16 TX des Modelljahres 1974 mit einer Leistung von 68 kW (93 PS) konnte es sogar mit Modellen gehobener Klassen aufnehmen. 
Nach anfänglicher Skepsis, vor allem bei weiblichen Autokäufern, hatte sich das revolutionäre Raumkonzept des großen Renault um 1970 herum weltweit durchgesetzt. Sogar in Nordamerika. Dort fühlten sich Ford, Chevrolet und AMC durch den Renault 16 inspiriert, mit eigenen, allerdings nur dreitürigen Hatchbacks zu antworten. So wollten sie die Flut europäischer und japanischer Importe bekämpfen. 
Auf der IAA 1973 in Frankfurt erklärten GM-Manager ausführlich die Vorzüge des variablen Chevrolet Vega (mit Fließheck, aber Hinterradantrieb) im Vergleich zu den europäischen Modellen, allen voran dem R16. Die Realität sah aber anders aus. Nicht einmal der VW Passat verfügte über so vielfältig verschiebbare Sitze wie der damals bereits acht Jahre alte Renault 16. 
Dessen Schrägheckkonzept wurde im Frühjahr 1975 auf den neuen Renault 30 übertragen, das erste französische Sechszylinder-Modell mit Fließheck. Auch als ein Jahr später mit dem Renault 20 eine preiswertere Vierzylinder-Variante des Fünftürers angeboten wurde, blieb der Renault 16 weiter im Angebot. So fand dessen Karosserieform klassenübergreifend Fans. Etwa durch den Rover SD1, der das Konzept der fünftürigen Schräghecklimousine ab 1976 in die Achtzylinder-Klasse einführte. Oder die Saab 99 Combi Coupés, die 1973 als Dreitürer und ab 1976 als Fünftürer Kultstatus gewannen. 
Selbst die erste koreanische Eigenentwicklung, der Hyundai Pony von 1974, wurde durch die später ergänzte Schrägheckvariante zum Exporterfolg. Zum weltweiten Standard wurde die fünftürige Karosserieform in der Mittelklasse aber erst Mitte der 1980er Jahre. Da hatte sich der Pionier R16 bereits still verabschiedet. Und seinen Ehrenplatz unter den innovativsten Automobilen der Fahrzeuggeschichte eingenommen."	technik
"Volvo bietet die Mittelklasse-Limousine S60 mit einem der neuen Downsizing-Dieselmotoren an. Das Triebwerk erzeugt aus zwei Litern Hubraum 133 kW (181 PS) und soll nebenbei mit rund vier Litern Diesel im Schnitt auskommen. Genau genommen 4,1 Liter pro 100 Kilometer. Das ist für eine Limousine, die mit 3er BMW und Mercedes C-Klasse konkurriert, kein schlechter Wert, zumal sie mit einem Drehmoment von 400 Newtonmetern einhergehen. 
Doch wie nah an der Realität ist diese Verbrauchsangabe? Im richtigen Leben werden die Abweichungen schnell recht groß, wie unser Praxistest gezeigt hat. 
Bei lausigem Wetter und mit einem sehr leichten Gasfuß, der die Tachoanzeige kaum über 100 km/h hochtrieb, kamen wir bereits auf Verbrauchswerte von 5,0 bis 5,5 Liter im Bordcomputer. Bewegt man die Limousine auf der Autobahn etwas schneller, aber noch immer im Rahmen der Richtgeschwindigkeit von 130 km/h, hat man schon eine sechs vor dem Komma – aber noch keinerlei Begründung, warum man nun ein Auto mit 181 PS geordert hat. 
Als wir die Motorstärke zumindest ab und an zu kürzeren Sprints nutzten, benötigte der S60 im Schnitt 7,5 Liter je 100 Kilometer. Das ist nun doch ein gewaltiges Stück vom Laborwert entfernt. Allerdings sind der Volvo und sein Motor diesbezüglich kein Einzelfall. Die meisten verkleinerten Motoren sind vor allem auf dem Prüfstand Knauser. 
Widmen wir uns lieber den erfreulichen Aspekten. Das Design des S60 – nordisch unterkühlt, aber elegant-sportiv – wirkt edel, und das will die 4,64 Meter lange Limousine auch sein. Die Verarbeitung ist top; alles ist so solide, wie man es von einem Volvo erwartet. Auch die Achtgang-Automatik schaltet unmerklich. Das Fahrwerk, zwischen kommod und sportlich eingestellt, dürfte mit den Ansprüchen der allermeisten Nutzer harmonieren. Ein Raumwunder ist der S60 allerdings nicht, vor allem im Fond wird es schnell eng. 
Volvo schreibt bekanntlich die Sicherheit besonders groß. Darum bietet der Hersteller im S60, der der etwas kompakteren Businessklasse zuzuordnen ist, fast alles, was an Assistenzsystemen zu haben ist. Allerdings tun sich Spurhaltesysteme im Winter schwerer, weil Markierungen gerne von Schnee bedeckt sind. Nicht wetterabhängig ist indes eine Unart der Volvo-Verkehrsschilderkennung. Deutsche Ortsschilder sind ihr offensichtlich unbekannt, und so zeigt sie nach dem Passieren eines solchen weiterhin beharrlich 70 km/h oder gar freie Fahrt an, um dann schnurstracks in den Fußgängerzonenmodus zu wechseln, wenn das entsprechende Schild auftaucht. 
 
Unpraktisch ist auch, dass eine Anzeige für das Licht fehlt. Natürlich brennt das Tagfahrlicht, und die Sensorik schaltet das Abblendlicht zuverlässig ein. Aber an hellen, gleichwohl nebligen Tagen hätte man gerne eine Kontrolle, ob man denn nun für den Restverkehr sichtbar ist oder nicht. So müsste man auf den Drehschalter für das Licht links unten am Armaturenbrett schauen – doch der ist aus keiner normalen Fahrposition heraus zu sehen. 
Dabei wäre im Display noch Platz für eine Lichtanzeige gewesen. Das Display wurde übrigens mit der jüngsten Überarbeitung schicker, aber unpraktischer. Relativ viel Platz nehmen Tankuhr und Ganganzeige ein, der Drehzahlmesser ist dafür nur mehr ein schmales Band, das sich den Raum noch mit zwei Bordcomputerwerten teilen muss. Außer man schaltet in die sogenannte Dynamik-Optik. Dann spielt der Drehzahlmesser als zentrale Anzeige in der Mitte der Tafel mehr Sportlichkeit vor, als der durchschnittliche Volvo-Fahrer je abrufen möchte. 
Nicht alles, was schön aussieht, ist auch praktisch – und an manchen Stellen im Auto ist praktisch einfach besser. Das gilt auch für das Infotainmentsystem: Dessen Bedienung über eine Kombination aus Dreh-Drück-Stellern und Direktwahltasten wirkt zwar gekonnt, ist im Detail aber zu umständlich. Warum man zum Beispiel das bestens integrierte Telefon nicht am Lenkrad mit einer einfachen Taste auflegen kann, haben wir nicht verstanden. 
Ebenso wenig, warum man den Kofferraum zwar mit der Funkfernbedienung entriegeln kann, er sich dann aber nicht zumindest so entrastet, dass man die Klappe an beliebiger und hoffentlich sauberer Stelle öffnen kann. Stattdessen lautet die Abfolge: Per Funk entriegeln, dann die Taste betätigen, die den Kofferraumdeckel öffnet, anschließend Hände waschen – zumindest im Winter. Dass die Rückfahrkamera mangels Spritzschutz nur sehr verschwommene Bilder liefert: geschenkt. Das ist auch bei anderen Premiummarken unnötigerweise oft der Fall. 
Sicher, für sich genommen sind das alles nur Kleinigkeiten, die das Bild des S60 in der Premium-Mittelklasse trüben. Doch bei einem gut ausgestatteten Testwagen zum Preis von rund 55.000 Euro erwartet man derlei nicht. Das Basismodell mit dem neuen D4-Motor gibt es ab 36.000 Euro. Das macht die unfeinen Details allerdings nicht besser. Schön ist der S60 trotzdem. 
Technische Daten 
Motorbauart: 2,0-Liter-Dieselmotor; Getriebe: Achtstufen-Automatik Leistung: 133 kW (181 PS) Beschleunigung (0-100 km/h): 7,4 s Höchstgeschwindigkeit: 230 km/h CO2-Emission: 107 g/km; Normverbrauch: 4,1 Liter je 100 km Abgasnorm: Euro 6 Basispreis: 36.000 Euro 
"	technik
"Die E-Mail ist wie ein verbrannter Toast: Man kann die verbrannten Stellen noch so schön mit Ketchup verzieren, das Ding bleibt ungenießbar. Was haben sich Hacker, Unternehmen und Behörden in den vergangenen Jahren nicht alles einfallen lassen, um die E-Mail attraktiver zu machen: Nutzer können E-Mails verschlüsseln, Einmal-Adressen verwenden, auf die beinahe sichere De-Mail bauen oder sich selbstzerstörende Nachrichten schicken. 
Es sind gut gemeinte Bemühungen, aber am Ende doch nur Nebelkerzen, die vom eigentlichen Problem ablenken. Denn selbst wer seine E-Mails gut verschlüsselt oder später zerstört, mit der Mail werden immer auch Metadaten versendet. Und die lassen sich bislang auch mit der besten Verschlüsselung nicht verbergen. Metadaten aber geben Informationen über Absender, Empfänger, Betreff und Zeit der versendeten E-Mail preis und in der Summe ganze Beziehungsnetzwerke. Testweise lässt sich das mit dem Tool Immersion vom MIT nachvollziehen (siehe Grafik). Geheimdienste wie die NSA sammeln solche Daten deshalb in gewaltigen Mengen. 
Ladar Levison will das ändern. Er entwickelt zusammen mit Phil Zimmermann, dem Vater der E-Mail-Verschlüsselung Pretty Good Privacy (PGP), sowie Jon Callasund Mike Janke die Dark Mail. Alle Entwickler haben langjährige Erfahrung mit E-Mail-Verschlüsselung und deren Grenzen. Levison gründete 2004 die Firma Lavabit. Zu deren Kunden gehörte auch Edward Snowden, bis Levison im Juli 2013 Besuch vom FBI bekam und Lavabit kurz darauf einstellte. 
Dark Mail wird nicht einfach nur ein neuer Dienst, der auf der alten E-Mail aufbaut. Kein Ketchup auf verbranntem Toast. Es wird ein Paket völlig neuer Protokolle. Ein neues Ökosystem, in dem sich dann E-Mail-Dienste ansiedeln können. Dieses Ökosystem heißt Dark Internet Mail Environment (DIME). Die verwendeten Protokolle sind für den Transport DMTP (Dark Mail Transfer Protocol) und für die Verschlüsselung DMAP (Dark Mail Access Protocol). 
Die Idee hinter Dark Mail ist folgende: Die E-Mail wird mehrfach verschlüsselt. Der Nutzer kann dabei zwischen den drei Sicherheitsstufen vertrauensvoll, vorsichtig und paranoid wählen. Auf den Stufen vorsichtig und paranoid wird die Mail bereits auf dem Rechner des Nutzers verschlüsselt und ist damit Ende-zu-Ende-verschlüsselt. Die Verschlüsselungsschichten legen sich wie Briefumschläge um die eigentliche Nachricht. Und jede beteiligte Stelle kann nur auf die Information zugreifen, die sie unbedingt braucht, um die Nachricht weiterleiten zu können. 
Die verschlüsselte E-Mail geht zunächst ihren Weg vom Rechner des Absenders zum Mailserver seines Anbieters, also zum Beispiel GMail oder Posteo. Der Mailserver kann nur die erste Schicht entschlüsseln. Er weiß damit, von wem die Nachricht kam und an welchen Server er sie schicken soll. An welchen Nutzer, weiß er dagegen nicht. 
Die nächste Stelle kann ein weiterer Server sein oder bereits der Mailserver des Empfängers. Dieser entschlüsselt wieder eine Schicht und weiß nur, dass die Mail vom Mailserver des Absenders kommt und an eine nächste Stelle weitergeleitet werden muss – aber nicht, welcher Nutzername zu der Nachricht gehört. Schon das zweite Glied dieser Übertragungskette weiß also nicht mehr, von welchem Absender die E-Mail kommt. 
Das Prinzip ähnelt dem des Tor-Netzwerks. Auch dort kennen Server jeweils nur die Stelle vor und die Stelle nach ihnen. Der eigentliche Nutzer bleibt damit im Optimalfall anonym. Bleibt noch das Problem, wie die einzelnen Schichten von den Servern entschlüsselt werden können. Dark Mail soll dafür ein eigenes Schlüsselsystem mit sich bringen. Jede Verschlüsselungsschicht wird, wie bei PGP, asymmetrisch verschlüsselt. Die Schicht wird also durch einen öffentlichen Schlüssel des jeweiligen Servers verschlüsselt und kann nur durch den geheimen Schlüssel geöffnet werden. Jede Organisation, die an Dark Mail teilnimmt, wird ein solches Schlüsselpaar besitzen. Den erstem Konzept zufolge könnte das Verteilen der Schlüssel etwa über das Domain Name System (DNS) erfolgen. Das Ganze funktioniert also ähnlich wie ein Telefonbuch. 
 
Dark Mail verfolgt neben der Anonymität der Nutzer noch ein weiteres Ziel: Verschlüsseln soll einfacher werden. So einfach, dass Nutzer ihre Nachrichten schon auf ihrem Rechner verschlüsseln. So entsteht eine Ende-zu-Ende-Verschlüsselung. Bei Dark Mail wird das auf den Stufen vorsichtig und paranoid automatisch passieren. E-Mail-Anbieter bekommen den Inhalt der E-Mails also nie zu Gesicht, sogenannter Zero-Knowledge-Ansatz. Nutzer müssen sich wiederum nicht auf Absichtserklärungen ihrer E-Mail-Anbieter verlassen. 
Selbst wenn Angreifer von außen auf die E-Mail-Server zugreifen können, finden sie dort nur verschlüsselte E-Mails. Für das Entwickler-Team von Dark Mail ist das Einbinden einfacher Verschlüsselung wohl auch eine Lehre aus der Lavabit-Geschichte. Denn die Behörden hatten von Lavabit am Ende gefordert, die geheimen Schlüssel der Nutzer herauszurücken. Dies wäre Lavabit auch möglich gewesen. Hätte Levison damals nachgegeben, sämtliche E-Mails der Lavabit-Nutzer wären in die Hand der US-Behörden gelangt. 
Die Entwickler von Dark Mail sind mit ihren Ideen aber nicht allein. Vor allem die Programme Bleep und Ricochet machen beim Wettlauf um den sichersten Nachrichten-Standard im Netz mit. Beide Projekte wollen dezentrale Netzwerke nutzen, um Metadaten zu verschleiern. Bleep setzt dabei zumindest langfristig auf das bekannte BitTorrent-Netzwerk. Ricochet bedient sich des Tor-Netzwerkes. Bleep kann derzeit für Android, Mac und Windows getestet werden. Das Programm ist aber noch nicht fertig. 
Auch wann Dark Mail fertig wird, ist nicht klar. Auf dem Chaos Communication Congress (31C3) sprach Levison von viel Arbeit, die noch vor dem Team liege. Derzeit sei Dark Mail noch nicht einmal in der Testphase. Erste Bibliotheken können Entwickler jedoch auf GitHub einsehen. 
Da die Entwickler nicht nur einen Dienst, sondern einen Standard etablieren wollen, könnte es sich außerdem Monate bis Jahre hinziehen, bis sich dieser durchsetzt. Die Anerkennung als Internetstandard wird vom Internet Architecture Board vergeben. Für einen neuen Standard muss das Gremium einen breiten Konsens in der Öffentlichkeit feststellen. Entscheidend wird also sein, wie viele Mail-Anbieter, neben ihren bisherigen Standards wie SMTP, auch das neue Ökosystem Dark Mail anbieten. Am Ende müssen nicht zuletzt die Nutzer den nötigen Druck ausüben."	technik
"Stärker, schneller, größer – das ist seit Jahren der Trend bei der Entwicklung neuer Automodelle. Karosserien werden länger und breiter. Doch dieser Wachstumsprozess bringt nicht für jeden Fahrertyp auch ein Plus an Komfort und Sicherheit. Im Gegenteil: Unfallforscher kritisieren die Ergonomie vieler Personenwagen. Die wichtigsten Bedienelemente seien oft so angeordnet, dass sie für Menschen mit kleiner Statur kaum erreichbar sind. 
Vor allem Frauen gelten als benachteiligt. Um Gas-, Brems- und Kupplungspedal betätigen zu können, müssten viele den Sitz weit nach vorne schieben. Sie hätten damit eine Sitzposition, die nicht nur unbequem sei, sondern auch gefährlich. Durch den geringen Abstand zur Instrumententafel drohen ihnen beim Unfall schwere Verletzungen an Füßen, Knien, Oberschenkeln und Becken. 
 ""Zum Schutz kleiner Autofahrerinnen und Autofahrer sollte die Ergonomie im Auto deutlich verbessert werden "", sagt Siegfried Brockmann, Leiter der Unfallforschung der deutschen Versicherungen (UDV). Die Pedale und Lenkräder müssten verstellbar sein, außerdem brauche man spezielle Knie-Airbags als Aufpralldämpfer, fordert der renommierte Berliner Experte. 
Dabei stützt sich Brockmann auf eine Unfallstudie seines Instituts, bei der Wissenschaftler in Baden-Württemberg und Bayern mehr als 130 schwere Verkehrsunfälle analysiert haben. Sie stellten einen erschreckenden Zusammenhang zwischen Körpergröße und Verletzungsschwere fest.  ""Auffallend ist, dass die Hälfte der lebensbedrohlich verletzten Fahrer Frauen waren "", resümieren die Unfallforscher in ihrem Bericht – was angesichts der geringeren Verkehrsbeteiligung weiblicher Autofahrer ein deutlicher Hinweis für das Manko bei der ergonomischen Konzeption und der Sicherheitsausstattung der Autos ist. Die Experten vermuten, dass  ""die geringere Körpergröße mit einer dichteren Sitzposition vor dem Lenkrad und der Instrumententafel die Gefahr für Verletzungen erhöht. "" Das Fazit:  ""Fahrzeuginsassen, insbesondere kleine Fahrer, sollten beim Frontalaufprall besser gegen Verletzungen der unteren Extremitäten geschützt werden. "" 
Wie leicht sich der Insassenschutz optimieren lässt, dokumentiert die UDV mit dem Crashtest eines Kleinwagens mit frauengerechten Pedalen. Sie waren so in Längsrichtung einstellbar, dass der Sitz in einer mittleren Position bleiben konnte. Das Ergebnis: Dank des größeren Abstands zur Instrumententafel waren die am Dummy gemessenen Belastungswerte an den Oberschenkeln fast um das Fünffache geringer als beim Crashtest des gleichen Autotyps mit herkömmlicher Pedalanlage. 
Auch der ADAC fand heraus, dass kleine – sprich: vor allem weibliche – Autofahrer ein deutlich höheres Verletzungsrisiko haben. Dies gilt laut der Unfallforschung des Automobilclubs aber nicht nur für Füße, Oberschenkel und Becken, sondern ebenso für den Brustbereich.  ""Das Risiko schwerer oder lebensbedrohlicher Brustverletzungen ist für Frauen 30 Prozent höher als für Männer "", sagt ADAC-Experte Volker Sandner. Die Karosserien seien immer steifer und die Rückhaltesysteme immer aggressiver geworden. Dabei erreichen die Kräfte, mit denen Gurt und Gurtstraffer zupacken, offenbar die Grenzen der anatomischen Belastbarkeit. Frauen und Jugendliche, aber auch Senioren gelten laut ADAC als  ""Risikogruppen "", weil ihr Gewebe und ihr Knochenbau weitaus weniger belastbar ist als bei Männern mit normaler Statur. 
 
Die Folge: Vom Fortschritt auf dem Gebiet des Insassenschutzes hätten Frauen in den letzten Jahren nur wenig profitiert, so der Automobilclub. Die Auswertung von über 6.000 Verkehrsunfällen zeigt laut ADAC, dass die Zahl schwerer Brustverletzungen bei männlichen Pkw-Insassen seit Mitte der 1990er Jahre deutlich zurückging, während sie bei Frauen aber nahezu konstant blieb. Der Club fordert, bei der Sicherheitskonzeption von Autoinnenräumen nicht nur den sogenannten Auslegungsfall zu berücksichtigen. Die Fahrzeughersteller müssten eine  ""größere Abdeckung der Bevölkerung und eine verbesserte Auslegung der Rückhaltesysteme für ältere, kleine, leichte und große Insassen gewährleisten "", urteilen die ADAC-Unfallforscher. 
 ""Auslegungsfall "" – hinter diesem Wortungetüm verbirgt sich der Idealtyp der Sicherheitsentwickler. Er heißt Hybrid III 5th, hat männliche Standardmaße und sitzt bei den meisten Pkw-Crashtests als Dummy hinter dem Lenkrad. Mit 1,75 Metern Körperlänge und rund 78 Kilogramm Gewicht repräsentiert die Testpuppe den Durchschnittsmann Mitteleuropas – auf ihn werden seit Jahren Sitze, Gurte und Airbags aller Neuwagen abgestimmt. Weibliche Maße fanden bisher keine Berücksichtigung. Das erklärt die Sicherheitsdefizite in vielen Automodellen. 
Nur langsam setzt sich bei der Pkw-Entwicklung und -Erprobung eine neue Denkweise durch. Erst im vergangenen Jahr hat die Prüforganisation Euro NCAP ihre Prozedur geändert. Sie berücksichtigt bei der Sternebewertung von Neuwagen jetzt auch die Resultate eines zusätzlichen Frontaufpralls mit 50 km/h, bei dem zwei nur rund 1,50 Meter große Dummys an Bord der Testwagen sitzen: einer als Fahrer und ein zweiter Mitfahrer auf dem Rücksitz. 
 ""Bei diesen Tests gelten strikte Grenzwerte für die Verzögerungskräfte, die auf den Brustkorb wirken und eine starke Brustkorbverformung verursachen "", erklärt Euro NCAP die neue, frauenfreundlichere Prüfmethode. So will man die Autohersteller zwingen, ihre Rückhaltesysteme neu zu entwickeln und künftig auch auf kleinere Menschen abzustimmen. 
Die gesetzlich vorgeschriebenen Sicherheitstests, die für die Typzulassung neuer Automodelle maßgebend sind, sind allerdings noch nicht soweit. Seit Jahren schon berät eine Arbeitsgruppe der zuständigen UNECE (United Nations Economic Commission for Europe) über eine neue Testvorschrift, die Frauen und älteren Menschen einen besseren Insassenschutz bieten soll. Bisher existiert diese Neureglung aber nur als Vorschlag. Geplant ist ein Frontalaufprall, bei dem erstmals auch ein Dummy mit weiblichen Maßen in den Testwagen sitzen soll – auf dem Beifahrersitz. 
Hinweis: Wir haben zu der Information der Unfallforscher, dass jeder zweite lebensbedrohlich verletzte Fahrer eine Frau war, einen einordnenden Satz ergänzt./mbr"	technik
"Im Anfang war die Faust, und die Faust griff das Monster, und das Monster war tot. So beginnt das neue Doom, 23 Jahre nach dem ersten Teil, seine Erzählung von Höllenkreaturen, bombastischen Waffen und exzessiver Gewalt. Damals erschütterte der Shooter die Videospielszene. 3D-Grafik in Egoperspektive und jede Menge Blut waren plötzlich die Form, in die Egoshooter gegossen werden mussten. In Deutschland wurde das Spiel ein halbes Jahr nach der Veröffentlichung indiziert und erst 2011 wieder von der Liste gestrichen. Die Gewaltdarstellungen waren für die BPjM (Bundesprüfstelle für jugendgefährdende Medien) zu problematisch, das Spielen habe zur Verrohung von Minderjähren führen können und wurde somit verboten. 
 ""Killerspiele "" werden solche Titel in Deutschland genannt. Das ZDF hat gerade erst eine gelungene Dokumentation zu diesem Thema gedreht. Sie greift den Diskurs hierzulande auf, der Games eigentlich nur unter einem Aspekt betrachtete: Wie gefährlich sind diese wirklich? Die Frage, ob vermeintlich übertriebene Gewalt in einem Videospiel nötig ist – oder gar schädigend – könnte daher auch das neue Doom treffen. Dabei ist die interessantere Frage, welche Funktion diese radikale Gewalt in solchen Spielen hat. Und ob dieses Schema auch 23 Jahre später noch funktioniert. 
Der Soziologe Norbert Elias hat 1939 in seinem Werk Über den Prozeß der Zivilisation die Genese der heutigen,  ""zivilisierten "" Gesellschaften beschrieben. Ein Hauptaugenmerk legte er dabei auf den Rückgang von alltäglicher Gewalt. Je weiter sich unsere Gesellschaft differenziert habe, desto mehr habe jeder Einzelne sein Verhalten mit dem anderer Menschen abstimmen müssen. Impulsive Affekte, so auch die Gewalt, zu dämpfen, sei ein entscheidender Antrieb dieses Zivilisationsprozesses gewesen. So seien Räume entstanden, in denen der plötzliche  ""schockartige Einbruch "" von zügelloser Gewalt weitestgehend ausgeschlossen werden konnte. Die Einübung solcher gesellschaftlichen Verhaltensregeln sei auch durch die Kunst passiert. So sei etwa der mittelalterliche Minnesang ein Zeugnis davon, wie am Hof das Ansingen einer Frau aus der Ferne einem impulsiven Nachgeben eines Sexualtriebs vorgezogen wurde. 
Doom kehrt dieses Verhältnis um. In einer Gesellschaft, die den Verzicht von alltäglicher, zwischenmenschlicher Gewalt immer wieder ritualisiert einübt, bietet das Spiel einen künstlichen Freiraum, in dem alle Gesetze, alle vermeintliche Stabilität des Systems aufgebrochen, ja niedergeschossen werden. 
Denn sicher ist: Gewalt fasziniert uns. Egal, ob im Videospiel, im Film, in einem Roman oder in mittelalterlichen Heiligenviten. Der Einbruch von purer Gewalt, der Verlust jeglicher Kontrolle, ist eine Angst, mit der sich die Menschheit seit jeher beschäftigt und die sie zu erklären versucht. 
Zu erklären versucht das neue Doom so gut wie gar nichts. Die Story ist vollkommen belanglos – und das muss sie auch sein. Wir sind auf dem Mars, die Hölle ist losgebrochen, im wahrsten Sinne des Wortes. Ein Wesen mit dämonischer Stimme erklärt uns, dass unsere Feindin eine Frau Olivia Pierce ist, die ein wenig aussieht wie Tilda Swinton. Die eigentliche Geschichte jedoch wird durch die vielen Waffen erzählt, die uns zur Verfügung stehen. Von der Shotgun über den Raketenwerfer bis zu einer Kettensäge. Jede Waffe hat ihren eigenen Charakter, einen ganz spezifischen Sound, man meint sie beinahe in den Händen zu spüren, so plastisch sind sie. 
Die schrammelige Gitarrenmusik ist immer noch treibend wie vor 23 Jahren, die Kreaturen sind phantasievoll gestaltet und ihre Schreie schauriger denn je. Die Steuerung ist auf den Punkt, die Missionen sind gerade blöd genug, um sie schulterzuckend zu erledigen. Und wenn hin und wieder jemand darüber schwadroniert, dass die Menschheit die Hölle braucht, um die Energiekrise zu besiegen, dann schmunzelt man kurz und legt das Plasmagewehr an. 
 
Im Videospiel kommt es wie in allen Medien auf den Kontext an, in dem die Gewalt steht. Kürzlich erschien etwa der Titel The Divison, herausgegeben von Ubisoft. In diesem betreten die Spieler ein dystopisches New York City – ein Virus hat zugeschlagen und alle staatlichen und humanitären Strukturen zerstört. Die Aufgabe der Spieler besteht darin, Menschen zu töten, die vom Spiel ohne weitere Gründe als Verbrecher markiert wurden – weil sie Aufständische seien. Jüngst sorgte auch der Trailer zu Battlefield 1 für Aufregung. Hier werden die Spieler in den Ersten Weltkrieg geschickt. Während der Trailer Abrufrekorde bricht, gibt es auch kritische Stimmen: Sollte der Erste Weltkrieg wirklich als Schablone für ein Spielerlebnis dienen, das diesen auf Heldentum und Action reduziert? 
Augenscheinlich an diesen Beispielen ist, dass sie jeweils in einer für uns denkbaren oder einer geschichtlich nachvollziehbaren Welt spielen. Der Einsatz von Gewalt wirkt somit also realistisch. Wie die Gewalt wirkt, hängt ganz davon ab, wie das Spiel sie funktionalisiert und ob den Spielern Momente der Reflexion gegeben werden. Gewalt kann also problematisch sein, wenn sie als dem System inhärent, als nicht hinterfragbar, dargestellt wird. 
Doom funktioniert anders. Das Spiel schafft einen künstlerischen Raum der mehr an Hieronymus Boschs Das jüngste Gericht erinnert, als an unsere Realität. In diesem Raum existieren keine staatliche Gewalt, kein gesellschaftliches Gefüge, kein Diskriminieren oder Stigmatisieren. Es herrscht nur Gewalt. Und diese wirkt in ihrer Übertreibung, in ihrer blutigen und knochensplitternden Kuriosität schlichtweg kunstvoll. An Norbert Elias anschließend könnte man also die These aufstellen, dass Titel wie Doom mit einer Gewalt spielen, die – zu Recht – in unserer Gesellschaft nicht ausgeübt wird. 
Somit können diese Medien, die uns immer wieder eine Anderswelt vor Augen führen als Stabilisatoren der Zivilisation gesehen werden. Ein Durchspielen dessen, was in einem langen Prozess eingeübt, internalisiert und somit unterdrückt wurde – der Ausbruch ungezügelter und umgreifender Gewalt. In diesem Punkt unterscheidet sich das spritzende Blut von Doom dann gar nicht mehr von dem in einer Serie wie The Walking Dead oder Game of Thrones. Und auch wenn die Spielereihe nach 23 Jahren freilich keinen so großen Eindruck mehr hinterlassen kann – Doom macht, Pardon, höllisch viel Spaß."	technik
"Manche Namen stehen stellvertretend für Computerspiele. Sid Meier, der Schöpfer der Civilization-Reihe, ist etwa ein Synonym für komplexe Strategiespiele vor historischem Hintergrund. Hideo Kojima ist spätestens seit Metal Gear Solid gleichbedeutend mit Schleichen als Spielmechanik sowie ausuferndem Hollywood-Bombast. Und ein Peter Molyneux liefert nicht nur in seinen Götter-Simulationen zuverlässig Größenwahn oder die ewige Frage nach Gut und Böse. 
Diese Spieleentwickler haben eine eigene Handschrift entwickelt. Doch obwohl ihre Spiele so stark von ihnen geprägt sind, lassen diese kaum Rückschlüsse auf die Persönlichkeit ihrer Macher zu. Es gehört zu den ungeschriebenen Regeln guten Game-Designs, dass sich die Designer nicht in den Vordergrund schieben. 
Der Texaner Davey Wreden bricht mit dieser Regel. Sein jüngstes Projekt, das PC-Spiel The Beginner’s Guide (8,99 Euro auf Steam) ist eines der ungewöhnlichsten und gleichzeitig interessantesten Titel des Jahres – ein spielerischer Essay, der gekonnt die Grenzen zwischen Spielfigur, Entwickler und Spielenden in Frage stellt. 
Bereits Wredens erstes Werk The Stanley Parable überzeugte im Jahr 2013 mit einem humorvollem Gleichnis auf Games als Kontrollsystem. Immer wieder konfrontierte das Spiel seine Spieler mit der Sinnlosigkeit ihrer Entscheidungen, der Eingeschränktheit ihrer Handlungsmöglichkeiten sowie der Allmacht des Entwicklers. Die Erzählstimme des Spiels war gewissermaßen ein übermächtiger Gott, der keine Flucht aus dem geregelten Spielverlauf duldete. 
In The Beginner’s Guide richtet Wreden den Blick nun zurück auf diesen göttlichen Schöpfer, in Gestalt der ebenso anonymen wie fiktiven Figur Coda. Er habe sie (oder ihn; das Geschlecht bleibt unklar) vor einigen Jahren kennengelernt, erzählt Wreden zu Beginn des Spiels aus dem Off. Nun möchte er den Spielenden das unveröffentlichte Werk von Coda eröffnen. Als Kurator und Museumsführer zugleich, begleitet er die Spieler in den nächsten knapp eineinhalb Stunden durch eine Reihe kurzer, experimenteller Spielereien, die zwischen den Jahren 2008 und 2011 entstanden sein sollen. 
Codas Werk beginnt simpel; das erste Spiel entpuppt sich als kaum veränderte Bearbeitung eines Levels aus Counter-Strike. In typischer Ego-Shooter-Manier lässt sich die 3D-Architektur frei durchlaufen. Nur hier und dort versperren nun Graswände den Durchgang, schweben Holzkisten unmöglich in der Luft. Ihm gefalle, offenbart Wreden ungefragt den Spielern, wie sich Codas Kreativität in eben diesen vielen kleinen Fehlern und Schrulligkeiten zeigt. 
Es sind vor allem Fehler, Glitches und enervierende Spielmechaniken, mit denen The Beginner’s Guide seine Figur Coda charakterisiert. Wenn Sid Meier, Hideo Kojima oder Peter Molyneux oft nur als bloße Markennamen erscheinen, hängt das auch damit zusammen, dass sich ihre Spiele kaum eine ergonomische Blöße geben. Dabei zeigt sich der Schöpfer am deutlichsten dort, wo seine Schöpfung nicht mehr reibungslos funktioniert. 
Codas Experimente sind dagegen geprägt durch Mechaniken, von denen jede Einführung in Game-Design abraten würde. Irritation statt Immersion scheint die Devise zu sein. Eines der Spiele kann etwa nur rückwärts durchschritten werden, bei einem anderen verlangsamen sich die Bewegungen der Spielenden fast bis zum Stillstand. Manche Spiele wären gar nicht zu beenden – würde nicht Wreden aus dem Off regelmäßig Abhilfe schaffen. 
 
Man sollte die Spiele als das sehen, was sie sind und nicht als das, was sie nicht sind, kommentiert Davey Wreden sehr früh in The Beginner’s Guide. Und doch ist gerade er es, der immer wieder in Codas Werk eingreift. Er spannt Brücken über unsichtbare Labyrinthe, kürzt stundenlange Wartezeiten auf wenige Sekunden ab oder blendet Wände aus, um den Blick auf die sonst versteckt gebliebene, monumentale Level-Architektur freizugeben. Der Kurator wird somit zum Co-Entwickler. 
Je unspielbarer und abweisender die Spielwelten mit den Jahren werden, desto tiefer greift Wreden in das Werk ein und bestimmt dessen Interpretation. Er wird nicht müde, den Spielenden von seiner wachsenden Sorge zu berichten, dass Coda langsam in den kreativen Burnout abgleite. Denn er habe selbst schon mal in einer so schwierigen Phase gesteckt, gesteht der Off-Souffleur. The Beginner’s Guide ist damit auch ein Kommentar über das angespannte Verhältnis zwischen Autoren, vertreten durch Coda, und den Spielenden, vertreten durch Wreden. 
In dieser Verschachtelung der Realitätsebenen erinnert das Computerspiel an den Film Adaption von Regisseur Spike Jonze. Sowohl in der Realität als auch in der Fiktion, scheitert der für Being John Malkovich gefeierte Drehbuchautor Charlie Kaufman an der Adaptierung des Sachbuchs The Orchid Thief der Autorin Susan Orlean. Aus dem geplanten Spielfilm wird so eine Meta-Geschichte über einen Autor, der aus seiner kreativen Blockade ein endlos selbstreferentielles Drehbuch macht. 
Nach dem großen Erfolg von The Stanley Parable sah sich der echte Davey Wreden tatsächlich am Rande einer Depression. Auf seinem Blog beschreibt er, wie ihn Hunderte Menschen kontaktierten, um ihm mitzuteilen, wie wichtig sein Spiel für sie sei und wie es ihre Welt verändert habe. Überfordert davon, dass so viele Menschen ihr Innenleben auf The Stanley Parable und seinen Entwickler projizieren, zog sich Wreden zeitweise aus der Öffentlichkeit zurück. Wenig später begann er mit der Arbeit an The Beginner’s Guide. Und wer genau hinschaut, findet in einem Bücherregal des Spiels sogar eine Ausgabe von The Orchid Thief. 
The Beginner’s Guide zeigt, wie Spielregeln und Level-Architekturen genutzt werden können, um Figuren zu charakterisieren und die Persönlichkeit ihrer Schöpfer sichtbar werden zu lassen. Aber es ist auch eine Parabel darauf, wie Deutungsmacht zum zerstörerischen Selbstzweck wird. Wo sich Kreative in ihrem Werk persönlich offenbaren, machen sie sich verletzbar. Der fiktive Wreden des Spiels macht mit Coda, was seine Fans und Kritiker mit dem realen Wreden gemacht haben: Der Schöpfer verliert die Deutungshoheit über seine Schöpfung. 
Aber The Beginner’s Guide weist auch auf eine mögliche Lösung des Dilemmas hin: Zwischen den Rätseln und surrealen Räumen sind immer wieder Orte der Ruhe, Routine und Selbstgenügsamkeit verteilt. Unaufgeräumte Wohnungen, die nur ein wenig in Ordnung gebracht werden müssen, neblige Dunkelkammern in der es nichts zu tun gibt, oder sogar Momente der sinnlichen Transzendenz. In ihnen verstummt schließlich auch die Flüstermaschine Davey Wreden. Manchmal ist eine schöne Erfahrung nur eine schöne Erfahrung. Keine Einführung ist notwendig. Vielleicht sollte man The Beginner’s Guide also einfach als das sehen, was es ist und nicht als das, was es nicht ist."	technik
"Der Ausdruck  ""Killerspiele "" findet sich in der Pressedatenbank erstmals unter dem Datum 20.12.1993. In einem Spiegel-Artikel mit dem Titel Zwitter und Zombies geht es unter anderem um das Kampfspiel Mortal Kombat und die damalige Debatte über gewaltverherrlichende Spiele. Im Text findet sich eine überraschende Passage:  ""'Je differenzierter die Studien, desto geringer sind die tatsächlich nachgewiesenen Effekte', sagt der Freiburger Soziologe Klaus Neumann-Braun. Denkbar scheint allenfalls, dass Killerspiele dann Einfluss auf Kinder haben, wenn die auch wirkliche Gewalt erleben. "" 
Experten waren also bereits vor 23 Jahren der Ansicht, dass Videospiele Jugendliche nicht automatisch zu Gewalttätern machen. Nicht mal, wenn darin Menschen auf andere Menschen schießen, sich gegenseitig vermöbeln und hin und wieder etwas Pixelblut spritzt. Die Debatte war aber nicht damit beendet, im Gegenteil. Spätestens als der damalige bayerische Innenminister Günther Beckstein (CSU) um die Jahrtausendwende ein rigoroses Verbot  ""sogenannter Killerspiele "" forderte, wurde sie ein Politikum. 
In der dreiteiligen Dokumentation Killerspiele nimmt sich ZDFinfo der  ""Killerspiel-Debatte "" an. Der Journalist Christian Schiffer, der auch das Games-Magazin WASD herausgibt, begibt sich zunächst auf Spurensuche: Wie genau begann der Streit um die mutmaßlichen  ""Killerspiele "" und wie kam überhaupt die Gewalt in die Spiele? Der erste Teil der Dokumentation läuft am morgigen Samstag um 23:15 Uhr auf ZDFinfo, in der Mediathek ist die erste Folge schon verfügbar. Die Ausstrahlungstermine für die beiden weiteren Teile stehen noch nicht fest, sie sollen sich inhaltlich mit der jüngeren Vergangenheit beschäftigen. 
Die erste Folge steigt dagegen tief in die Historie ein, genauer gesagt im Jahr 1976. Vier Jahre nachdem mit Pong das erste, weltweit populäre Videospiel auf den Markt kam, hatte die Games-Branche nämlich ihre erste Gewaltdebatte. Im Automatenspiel Death Race mussten die Spieler mit einem Auto Strichmännchen überfahren. Untote  ""Gremlins "" seien das gewesen, sagten die Entwickler. Angesichts der monochromen Pixelgrafik aber konnte man sie schon mit Menschen verwechseln – wenn man denn wollte. Der TV-Sender CBS widmete dem Spiel einen Beitrag und auch die Verbraucherschützer des National Safety Council schalteten sich ein. Ihr Urteil: Death Race sei  ""krank, krank, krank "". 
Der Fall von Death Race soll zeigen, wie lange es bereits Debatten um Gewaltdarstellungen in Games gibt. Sie begleiten die Entwicklung der Spielebranche praktisch seit ihren Anfängen und sind für einige grundlegende Entscheidungen verantwortlich. 1984 etwa landete in Deutschland mit River Raid das erste Videospiel auf dem Index, weil die Spieler mit einem Flugzeug gegnerische Schiffe versenken mussten – es durfte daraufhin nicht mehr verkauft oder öffentlich angeboten werden. Ein Jahr später sorgte ein neues Jugendschutz-Gesetz dafür, dass sämtliche Automatenspiele in den Hinterzimmern verschwanden, ungeachtet davon, ob sie in irgendeiner Form gewalttätig waren oder nicht. 
Die neunziger Jahre entfachten die Diskussion erneut. Spiele wie Wolfenstein 3D und Doom versetzten die Spieler in die Ich-Perspektive und sorgten für ein komplett neues Spieleerlebnis, ein realistischeres und damit ungleich gefährlicheres, jedenfalls in den Augen der Jugendschützer. In Deutschland wurde Doom 1994 indiziert, seinem Erfolg tat es keinen Abbruch: Es begann die Ära der Ego-Shooter – und mit ihr die bis heute andauernde  ""Killerspiel-Debatte "". 
 
In seinen schwächeren Momenten läuft Christian Schiffers Dokumentation Gefahr, zu sehr in die oftmals reißerische Rhetorik der Debatte einzusteigen. Etwa, wenn die Autoren schon von Beginn an den Begriff  ""Killerspiele "" nicht genug differenzieren, sondern ihn sich selbst aneignen, um einzelne Spiele zu beschreiben. 
An den besten Stellen liefert der erste Teil von Killerspiele aber zumindest zaghafte Antworten auf die Frage, weshalb Gewalt und Computerspiele schon immer zur Debatte standen:  ""Das Gewaltempfinden ist dem historischen Kontext geschuldet "", sagt etwa Andreas Lange, der in Berlin das Computerspielmuseum leitet an einer Stelle. Dass in den achtziger Jahren heute komplett harmlos anmutende Spiele wie River Raid für Aufregung sorgten, war demnach auch der politischen Situation geschuldet: Die Bedrohung des Kalten Krieges sollte nicht noch durch Videospiele verstärkt werden. 
An anderer Stelle fragt der Spielejournalist Boris Schneider-Johne, ob der Erfolg von Doom am Ende mit der übertriebenen Gewaltdarstellung zusammenhing, oder nicht doch vielmehr mit dem neuen, innovativen Spielerlebnis. Der Erfolg des Spiels habe anderen Entwicklern möglicherweise den Eindruck vermittelt, dass es sich nur so gut verkauft, weil es so gewalttätig ist. Möglicherweise hätte  ""ein weniger brutales Doom "" auch funktioniert und die Entwicklung der Branche in eine andere Richtung gelenkt, sagt Schneider-Johne. 
Gleichzeitig weist er daraufhin, dass die Indizierung von Games durch die Bundesprüfstelle für jugendgefährdende Medien (BPjM) deren Erfolg kaum verhindert hat: Ähnlich wie einst der Hüftschwung von Elvis ein Statement gegen das Establishment war, sei die Indizierung eine Adelung für die Entwickler gewesen. Auch wenn sie unweigerlich mit Umsatzbußen einherging: Kaum war der Verkauf eines Spiels verboten, florierten illegale Kopien auf Disketten und gebrannten CDs. Selbst die BPjM zog für ihre Prüfungen bisweilen illegale Kopien heran: Wie Killerspiele in einer Szene zeigt, steht in einigen offiziellen Dokumenten sogar der Name der für die Kopie verantwortlichen Cracker als Urheber. 
Überhaupt kommen in der Dokumentation auch die Jugendschützer zu Wort, allen voran die Vorsitzende der BPjM, Elke Monssen-Engberding, die seit 1991 im Amt ist und bis heute den Standpunkt vertritt, gewalttätige Games würden der Entwicklung von Jugendlichen schaden. Zwischen Archivaufnahmen aus den Büros der BPjM aus den neunziger Jahren und heute liegen Welten – an den Argumenten hat sich wenig geändert, auch wenn Games wie River Raid und Doom mittlerweile offiziell begnadigt wurden. Dass die Prüfer aber weiterhin ganz genau auf jeden neuen Shooter gucken, zeigte unter anderem der Fall von Dying Light im vergangenen Jahr. 
Die  ""Killerspiel-Debatte "" in der Öffentlichkeit wird heute gemäßigter geführt, Hardliner wie Günther Beckstein oder Wolfgang Bosbach (CDU) sind mit ihren Verbotsforderungen eher die Ausnahme. Auf Nachfragen von Spiegel Online im Herbst umschifften Vertreter aus Politik und Wirtschaft den Begriff  ""Killerspiel "" bewusst. Es hat also offensichtlich ein Umdenken stattgefunden, aber wie die ZDF-Dokumentation zeigt, lässt sich die Diskussion über Gewalt und Computerspiele auch nach 40 Jahren nicht gänzlich beenden. Aber vielleicht etwas entschärfen."	technik
"Es ist eine Frage, die Game-Designer seit Jahrzehnten umtreibt: Wie offen gestalte ich die Welt meines Spiels? Open-World-Games wie Assassin's Creed oder GTA bieten den Vorteil, dass Spieler riesige Gebiete frei erkunden und ganz ohne Zeitdruck ihre eigenen Abenteuer erleben können. Der Nachteil: Die Haupthandlung gerät häufig aus dem Blick und wirkt im schlimmsten Fall wie ein Fremdkörper. 
Die Uncharted-Reihe war stets anders aufgebaut, mit schlauchartigen Leveln, in denen sich das Geschehen wuchtig inszenieren und perfekt durchtakten ließ, fast wie in einem Hollywood-Blockbuster. Als Schatzsucher Nathan Drake hatten Spieler zwar kaum Einfluss auf die Handlung. Sie erlebten aber eine Indiana-Jones-reife Achterbahnfahrt aus knalliger Action, exotischen Schauplätzen, coolen Sprüchen und homöopathisch verabreichten Rätseln. Die Linearität von Uncharted sorgte für Kritik, wurde aber als notwendiges Übel akzeptiert. Inszenierung ging vor Handlungsfreiheit. 
Am kommenden Dienstag nun erscheint Uncharted 4: A Thief's End (74,99 Euro, USK 16) für die Playstation 4. Entwickler Naughty Dog wird keine weiteren Uncharted-Spiele produzieren, das haben Studioverantwortliche in mehreren Interviews versichert. Zum krönenden Abschluss verlässt sich Naughty Dog zwar weitgehend auf das Rezept, das Uncharted zu einer der erfolgreichsten Spieleserien der Welt machte. Doch die Kalifornier, als Perfektionisten bekannt, reagieren auch auf den Vorwurf enger Level-Grenzen: Unter dem leicht verschwurbelten Begriff wide linear entschlauchen sie das Spiel sozusagen. 
Was das bedeutet, merkt man allerdings erst nach ein paar Spielstunden so richtig. Zu diesem Zeitpunkt ist Drake bereits wieder aus seinem selbstgewählten Abenteuer-Ruhestand entflohen, den er an der Seite seiner Frau Elena als Angestellter einer Bergungsfirma verbrachte. Reaktiviert hat ihn sein älterer Bruder Sam, der erstmals in Uncharted auftritt: Sam hat die letzten 15 Jahre im Gefängnis verbracht und ist nun dem Schatz des legendären Piraten John Avery auf der Spur. Auch Nathans grauhaariger Mentor Victor  ""Sully "" Sullivan, bekannt aus allen drei Uncharted-Hauptspielen, ist mit von der Partie. 
Wide linear kommt erstmals zum Tragen, als sich das Trio in Italien trifft. In einer prachtvollen Burg an der Amalfi-Küste versammelt sich die glitzernde Halbwelt zu einer Versteigerung. Dort soll auch ein wertvolles Artefakt unter den Hammer kommen, von dem sich Nathan, Sam und Sully Hinweise auf den Piratenschatz erhoffen. Allerdings sind auch noch andere Personen hinter dem Artefakt her, und so beginnt in der Burg ein nervenaufreibender Wettlauf. Drake übernimmt wie üblich den Part des Schleich- und Kletterkünstlers, der über Fassaden und Dächer in schwer bewachte Zonen vordringt. 
In früheren Uncharted-Spielen wäre das völlig linear abgelaufen. Doch diesmal erhält Drake als Anhaltspunkt nur einen Funkmast am höchsten Punkt der Burg. Wie er dorthin gelangt, ist Sache der Spieler. Wobei man hier keine ausufernden Palastanlagen wie in Assassin's Creed erwarten sollte. Ja, in Uncharted 4 gibt es Umwege, Abkürzungen und Sackgassen. So richtig verirren kann man sich aber nicht. Das liegt zum einen an den optischen Hinweisen, die Uncharted wenig subtil, aber dafür äußerst effizient einsetzt: Rampen deuten Sprünge an, Lichter weisen den Weg, die Haltegriffe der Kletterpassagen sind farblich markiert. Zum anderen liegt es daran, dass Drake immer wieder durch Engpässe hindurchmuss. Das kann ein Maschinenraum sein. Oder auch ein vereinbarter Treffpunkt. 
Wide linear ist also eine Kette von kleinen, frei erkundbaren Arealen. Wie gut Uncharted 4 eine offene Spielwelt simuliert, zeigt sich auch bei einer Jeep-Tour auf Madagaskar. Nathan, Sam und Sully fahren auf einen Vulkan zu, der sich majestätisch in der Ferne erhebt. Hin und wieder steigt Nathan aus, durchsucht die Umgebung und findet dabei auch kleinere Schätze. Die erwähnten Engpässe gibt es aber auch hier: Zum Beispiel, wenn der Jeep mittels Seilwinde eine rutschige Böschung hinaufgezogen wird oder wenn ein ausgetrocknetes Flussbett der einzige Weg durch steile Felsformationen ist. Uncharted 4 simuliert Bewegungsfreiheit nur. Aber es simuliert sie sehr gekonnt. 
 
Überhaupt ist das Spiel ein Meisterwerk des Welten-Designs. Die Suche nach dem Piratenschatz führt Drake und Kollegen rund um den Globus, ein Schauplatz ist spektakulärer als der andere – ob nun winterkarge schottische Highlands, ein bunter afrikanischer Markt oder luftige Felsplateaus an einer dschungelbewachsenen Steilküste. Diese Schauplätze sind bis ins kleinste Detail gestaltet und bieten immer wieder unvergessliche Ausblicke. Papageienschwärme etwa, die vor tropischen Gewitterwolken über eine Bucht fliegen. Oder eine halb verfallene Villa, deren Teppich von Moos überwuchert wird. 
Grafisch gehört Uncharted 4 zum Besten, was die aktuelle Konsolengeneration zu bieten hat. Es lohnt sich, die mindestens 15 Stunden Spielzeit nicht im Akkord zu absolvieren, sondern hier und da einfach Rast zu machen und die Aussicht zu genießen. 
Neue oder aufgefrischte Spielmechaniken lassen sich indes an einer Hand abzählen. Nathan besitzt jetzt ein hakenbewehrtes Seil, er kann sich damit über Abgründe schwingen oder Gegenstände heranziehen, um sie als Kletterhilfe zu benutzen. Das alles macht Spaß, ist aber schon aus Assassin's Creed, Far Cry, Just Cause und anderen Spielen bekannt. Zudem können Spieler nun wählen, ob sie die obligatorischen Gegner-Horden frontal unter Beschuss nehmen oder ob sie die Patrouillen umschleichen und aus dem Hinterhalt angreifen. Die Rätsel, die Uncharted 4 hin und wieder einstreut, sind ähnlich anspruchslos wie die in den Vorgängern: Mit etwas Beobachtungsgabe lassen sie sich in wenigen Minuten lösen und unterbrechen so auch nicht allzu lang den Flow, der für die Uncharted-Abenteuer so wichtig ist. Zäh wirken dagegen die ausufernden Kletterpassagen, die weder besonders anspruchsvoll noch spannend sind. 
Was Naughty Dog hervorragend gelingt, ist das Zusammenspiel der Charaktere. Die Zwischensequenzen bilden dank Motion Capture auch feine gestische und mimische Nuancen ab: So werden die Gefühle der Figuren sehr viel deutlicher als in den ersten drei Spielen. Zum Glück verzichten die Entwickler auf allzu grobe Klischees. Nate ist zwar immer noch der Draufgänger, der selbst dann lockere Sprüche klopft, wenn er gerade an einer Felskante über einer hundert Meter tiefen Schlucht hängt. Und Sully ist der alte Charmeur, den wir aus Teil 1 bis 3 kennen. Nates Verhältnis zu Elena und zu seinem Bruder Sam setzt aber gekonnt auf Zwischentöne, die ein Action-Spektakel sonst selten bietet. Hier hat sich Naughty Dog sehr deutlich an The Last of Us orientiert, mit dem das Studio 2013 erfolgreich war. Wenig glaubwürdig ist allerdings, dass Nate als netter Allerweltsabenteurer seine Gegner gleich hundertfach tötet, ohne psychische Spuren davonzutragen. 
Der Plot von Uncharted 4 ist nicht sonderlich originell: Der Haudegen, der für einen letzten Coup ins Geschäft zurückkehrt, taucht auf ähnliche Weise in etlichen Filmen auf. Dennoch erzählt Naughty Dog die Geschichte widerstreitender Loyalitäten und wachsender Rivalitäten so gut, dass die Spannung bis zum großen Finale ständig wächst. Auch wenn sich das Spiel allzu sehr auf bestehende Mechaniken verlässt, ist Uncharted 4 ein würdiger Abschluss der Serie."	technik
"Für eine erste Ehrung kommt Spec Ops: The Line , das neue Computerspiel von Yager Development aus Berlin , knapp zwei Monate zu spät. Ende April wurde der deutsche Computerspielpreis für das beste Game 2012 an einen Egoshooter vergeben. Die Entscheidung sollte weniger die kulturelle Bedeutung des Spiels hervorheben, als vielmehr ein Zeichen gegen politisch motivierte Bevormundung setzen. Computerspiele sind schließlich auch Erwachsenenunterhaltung. Und die darf durchaus auch mal heftiger ausfallen. 
Doch weil der Preisträger ein  ""Killerspiel "" Made in Germany war, führte das in der Politik sofort zu Debatten um den Preis selbst . Dabei war nicht das Spiel das Problem. Das auserkorene Crysis 2 ist unter rein technischen Gesichtspunkten durchaus preiswürdig. Das eigentliche Problem ist, dass es ihm hierzulande an Konkurrenz mangelt. In Deutschland werden große Actiongames praktisch nicht entwickelt. Bis jetzt. 
Auch wenn die bloße Spielmechanik oder der Mehrspielermodus von Spec Ops: The Line nicht absolutes Topniveau erreichen, so glänzt der Shooter doch in einem Bereich, in dem viele andere massiv schwächeln: bei der Erzählweise. Spec Ops thematisiert Dinge, die bei anderen Genrevertretern zugunsten der leichteren Konsumierbarkeit gern ausgeblendet werden. 
Referenzobjekt  ""Apokalypse Now "" 
Zugegeben, bereits eine nicht vollkommen unkritische Haltung zu Imperialismus und Krieg und bereits ein vages Hinterfragen des Gut-Böse-Schemas gilt bei Shootern als Alleinstellungsmerkmal. Und Spec Ops: The Line ist weiß Gott auch nicht subtil. Doch gelingt es ihm, den Spieler mit seinen Handlungen und mit den Folgen des Krieges zu konfrontieren. 
Als Referenzobjekt diente den Machern deswegen auch kein Konkurrenzprodukt, sondern Francis Ford Coppolas Vietnam-Kriegs-Film Apocalypse Now . Ebenso wie dieser handelt auch Spec Ops: The Line von einer Reise in das Herz der Finsternis des Krieges und von der Frage nach dem Sinn des eigenen Tuns. 
Captain Martin Walker soll mit zwei Kameraden in einem durch heftige Sandstürme verwüsteten Dubai nach Überlebenden suchen. Als Anhaltspunkt dient ein aufgefangenes Funksignal des einst mit Walker in Afghanistan dienenden Colonel Konrad. Dessen 33. Kompanie, die ursprünglich bei der Evakuierung der Stadt helfen sollte, hält in der prachtvollen Ruine die Stellung und ist in einen unübersichtlichen Kleinkrieg mit mindestens einer weiteren Partei verstrickt. 
Die drei Soldaten glauben anfangs, bei einem notwendigen Einsatz zur Evakuierung zu helfen, entdecken jedoch nach und nach, dass es um ein Massaker an der Zivilbevölkerung geht. 
 
Das bringt die Protagonisten nicht nur gegen den Urheber der Gewalt auf, sondern auch gegeneinander. Plötzlich meldet sich das Gewissen der virtuellen Kameraden und erhebt sie über den Status der stumpf schießenden Erfüllungsgehilfen des Spielers. Und nachdem dieser zu Beginn noch munter die sich ihm in den Weg stellenden Gegner liquidiert, fragt auch er sich bald, auf wen und warum er da eigentlich schießt. Ja, das schafft das Spiel tatsächlich: Der Spieler hinterfragt sich. 
Eine Szene, die besonders anschaulich verdeutlicht, was Spec Ops: The Line anders macht als die Konkurrenz, ist ein Angriff mit Phosphorgranaten auf eine kleine Armee gegnerischer Truppen. Die Attacke lenkt der Spieler in Person von Captain Walker am Laptop, seine angespannten Gesichtszüge spiegeln sich auf dem Monitor; helle Punkte werden von dem noch helleren Aufflackern des Granatfeuers verschlungen. 
Aber die danach folgende Szene, das Schlachtfeld, durch das uns die Kamera zwingt, gehört sicherlich zu den grausamsten und gleichzeitig eindrücklichsten Erlebnissen, die es in Computerspielen bislang zu sehen gab. Weil die Sprintfunktion an dieser Stelle des Spiels deaktiviert ist, wird der Soldat Walker langsamen Schrittes über ein mit verbrannten Gegnern übersätes Schlachtfeld geführt. Dort sucht man förmlich nach erbärmlich jammernden Gestalten, die noch erlöst werden können. Man will gutmachen, was nicht mehr gutzumachen ist. Die Botschaft kommt an. 
 ""Der Spieler soll sich schlecht fühlen "" 
 ""Der Spieler soll sich schlecht fühlen "", sagt Jörg Friedrich, leitender Level-Designer bei Yager. Es ist ein Satz mit absolutem Seltenheitswert in einer Industrie, die sich den unbeschwerten Spielfluss auf die Fahnen geschrieben hat und bei der es normalerweise darum geht, immer neue Reihen von Gegnern so schnell und effizient wie möglich umzulegen. 
 ""Teamintern hat das einen handfesten Streit verursacht "", sagt Friedrich. Streitpunkt war der Kontrollverlust des Spielers, der in manchen Abschnitten zum Hinsehen gezwungen wird, in anderen schnell handeln muss, obwohl die Lage moralisch absolut unübersichtlich ist.  ""Wird dem Spieler die Kontrolle aus der Hand genommen, und er soll etwas machen, wobei er sich nicht wohlfühlt, ist das eigentlich ein Anzeichen für schlechtes Gamedesign. "" Bei Yager ist es ein bewusstes Stilmittel. 
Spec Ops: The Line sieht nicht ganz so gut aus wie andere Shooter mit Kriegshintergrund wie beispielsweise Battlefield . Trotzdem hat das Spiel seine Schauwerte. Die in gigantischen Dünen versandeten Hochhäuser, die plötzlich über die Spielfiguren hereinbrechenden Sandstürme und die absurden, weil in einer Stadt ohne Wasser wertlosen Insignien der Dekadenz machen Dubai zu einem abwechslungsreichen Dystopia – das man gerne freier erkunden würde, als das Spiel es zulässt. 
Und es ist auch kein Egoshooter, sondern zeigt die Spielfiguren aus der Perspektive der dritten Person. Die Kamera schaut der Hauptfigur, die nach 48 Stunden Dubai reichlich mitgenommen aussieht, stets aus einigen Metern Höhe von hinten über die Schulter. Jörg Friedrich sagt, man habe keinen Egoshooter machen wollen. Denn da sei stets die Waffe der Protagonist."	technik
"Der Internetkonzern Yahoo verkauft sein Kerngeschäft an das US-Telekomunternehmen Verizon, das dafür 4,8 Milliarden Dollar zahlt. Nach Angaben von Verizon-Chef Lowell McAdam werde Yahoo mit der Konzerntochter AOL zusammengelegt. McAdam verspricht sich so ein  ""führendes weltweites Unternehmen für mobile Medien "", das vor allem im Bereich der Onlinewerbung mit Google und Facebook konkurrieren kann. 
Der Verkauf markiert das Ende der Unabhängigkeit für eine der bekanntesten Firmen in der Internetwirtschaft. Yahoo war vor mehr als 20 Jahren als Verzeichnis von Webseiten gegründet worden. Im Jahr 2000 hatte es einen Börsenwert von 100 Milliarden Dollar. 
Das Unternehmen steckt jedoch inzwischen seit Jahren in der Krise. Das Problem des einstigen Web-Pioniers: Zwar nutzen Hunderte Millionen Menschen Dienste des Konzerns wie etwa E-Mail. Yahoo tat sich jedoch schon seit Jahren sehr schwer, diese Nutzung in Einnahmen umzumünzen. Das Geld wird vor allem mit Onlinewerbung gemacht – ein Geschäft, in dem Yahoo zunehmend von Google und Facebook abgehängt wurde. 
Mehrere Chefs versuchten erfolglos, Yahoo wieder auf die Erfolgsspur zu bringen. Zuletzt hatte die ehemalige Google-Managerin Marissa Mayer vier Jahre Zeit dazu. Sie setzte auf einen Ausbau des Medienangebots, investierte massiv in die Websuche und kaufte die Blogplattform Tumblr für rund eine Milliarde Dollar, um so auch wieder junge Nutzer anzulocken. Doch auch damit feierte sie keine Erfolge, die Einnahmen von Yahoo fielen allein im vergangenen Quartal um rund ein Fünftel. 
Im Dezember entschied das Unternehmen unter dem Druck der Aktionäre, sein schwächelndes Kerngeschäft mit Suchmaschine und Onlinewerbung auszulagern und einen Investor dafür zu suchen. Verizon galt bereits seit Längerem als Favorit. Daneben besitzt Yahoo noch einen milliardenschweren Anteil an der chinesischen Onlineplattform Alibaba. Zusammen mit Yahoo Japan sollen diese beiden Unternehmen in der alten Yahoo-Gesellschaft verbleiben."	technik
"Ein Auto parkt einen privaten Stellplatz zu. Darf der Eigentümer dieses Grundstücks sofort den Abschleppdienst rufen oder muss er zuerst die Polizei verständigen? 
Ein Fahrer parkt seinen Wagen nur kurz auf einem privaten Parkplatz. Als er zurückkommt, ist das abgestellte Auto verschwunden. Manche kennen und viele fürchten diese Situation. Es soll Städte geben, in denen Abschleppdienste dieses Geschäftsmodell etabliert haben. Das legt zumindest eine Episode aus dem argentinischen Film Wild Tales nahe. Der höfliche Familienvater, gespielt von Ricardo Darín, verliert schließlich die Geduld, weil er seinen Wagen immer wieder gegen viel Geld auf entlegenen Parkplätzen abholen muss. 
Auch hierzulande gilt: Ein Kraftfahrzeug, das unbefugt auf einem privaten Grundstück abgestellt wurde, darf abgeschleppt werden, wie der Bundesgerichtshof (BGH) in einem Urteil aus dem Jahr 2009 (V ZR 144/08) festgestellt hat. Dem Eigentümer des zugeparkten Stellplatzes steht es aber frei, zunächst die Polizei um Hilfe zu bitten. 
Ob überfahrene rote Ampeln, Unfälle oder Streit beim Gebrauchtwagenkauf: Rund um den Straßenverkehr gibt es viele knifflige Rechtsfragen. Eine davon beantworten Fachanwälte für Verkehrsrecht jede Woche donnerstags hier in unserer Serie  ""Gesetz der Straße "". 
Schreiben Sie uns Ihre Frage (und geben Sie dabei bitte Ihren Namen und Ihren Wohnort an). Wir wählen jede Woche eine Frage aus und beantworten sie hier. 
Bitte beachten Sie: ZEIT ONLINE, die Autorin und die beteiligten Fachanwälte übernehmen keinerlei Gewähr für die Aktualität, Korrektheit, Vollständigkeit oder Qualität der bereitgestellten Antworten und Informationen sowie der Rechtsprechung. Haftungsansprüche gegen ZEIT ONLINE und den Autor, welche sich auf Schäden materieller oder ideeller Art beziehen, die durch die Nutzung oder Nichtnutzung der dargebotenen Informationen bzw. durch die Nutzung fehlerhafter und unvollständiger Informationen verursacht wurden, sind grundsätzlich ausgeschlossen. 
 ""Der Polizei obliegt zwar der Schutz privater Rechte, doch in diesem Fall wird sie nur die Identität des Parkenden feststellen "", erklärt Verkehrsjurist Florian Schmidtke. Es kann aber durchaus sein, dass die Beamten den Fahrzeughalter anrufen und bitten, seinen Wagen so schnell wie möglich umzuparken. Auf diese Weise entstehen dem Falschparker keine Kosten, und er muss das abgeschleppte Auto nicht gegen eine Gebühr auf einem meist in einem Industriegebiet gelegenen Parkplatz abholen. Er hat aber keinen Anspruch darauf, dass der Stellplatzbesitzer zunächst den Weg über die Polizei geht. 
Auch wenn sich der Grundstücksbesitzer über den Falschparker ärgert, muss er ihm sagen, welchen Abschleppdienst er beauftragt hat und wo der Fahrzeughalter sein Auto wiederfindet. Allerdings rückt der Abschleppdienst den Wagen erst heraus, wenn der Fahrzeughalter die Gebühr bezahlt hat – auch das hat der BGH für rechtens erklärt. Mitunter kommt es vor, dass die veranschlagten Kosten zu hoch sind. Deshalb empfiehlt es sich, diese nur unter Vorbehalt zu zahlen und sich eventuell mit einem Verkehrsrechtsexperten zu beraten. 
Wenn jemand sein Auto beispielsweise nach Geschäftsschluss auf einem leeren Supermarktparkplatz abgestellt hat und es von dort abgeschleppt wird, stellt er sich vermutlich die Frage, ob das verhältnismäßig ist. Grundsätzlich hat der Grundstücksbesitzer zwar das Recht dazu, doch hier lohnt es sich, genauer nachzufragen, ob der Parkplatzbesitzer die Abschleppfirma beauftragt hat oder ob diese eigenständig solche Parkplätze überwacht. 
Parkt ein Autofahrer dagegen im Halteverbot vor einer Baustelle und vergisst, sein Fahrzeug morgens vor Arbeitsbeginn umzuparken, behindert er damit ziemlich sicher Zufahrtswege und die Bauarbeiten. In diesem Fall besteht durchaus eine große Dringlichkeit, den Wagen möglichst schnell zu entfernen."	technik
"Die ehemalige Google-Tochter Niantic hat einen Hit gelandet. Einen, von dem manche glauben, dass er das bisherige Nischengenre der Augmented-Reality-Games mit einem Schlag im Mainstream verankert und damit gesellschaftliche Konventionen herausfordert. Denn das neue  ""Ich sehe was, das du nicht siehst "" bedeutet auch  ""Ich benehme mich sonderbar und du weißt nicht, warum "". Der Name des Spiels: Pokémon Go. 
Ja, Pokémon. Wer glaubte, Figuren wie Pikachu seien schon in den neunziger Jahren mehr Plage als Phänomen gewesen, wird jetzt mit ganz neuen Dimensionen konfrontiert. 
Um das Ausmaß der Angelegenheit zu verdeutlichen: Offiziell gibt es das Spiel bisher nur in den USA, Australien und Neuseeland. Zwei Tage nach dem Start war die App angeblich auf mehr als fünf Prozent aller Android-Geräte in den USA installiert. Die iOS-Version hat dort innerhalb von 24 Stunden Platz eins der iTunes-Charts übernommen, jedenfalls in der Kategorie Free Apps. Was die Nutzungsintensität angeht, bewegt sich Pokémon Go schon jetzt in derselben Liga wie Twitter, Snapchat, WhatsApp, Instagram und der Facebook Messenger. 
Der Börsenkurs von Nintendo, das Anteile an Niantic und der Pokémon Company hält, ist infolgedessen um 20 Prozent gestiegen. Das Unternehmen ist plötzlich 7,5 Milliarden Dollar mehr wert als noch in der vergangenen Woche. 
Auf Twitter sind Bilder zu finden, wie sich an öffentlichen Orten wie dem New Yorker Central Park regelrechte Scharen von Spielern versammeln, um die virtuellen Comicfiguren zu suchen und gegeneinander kämpfen zu lassen. 
Die Spielserver hielten dem Ansturm zunächst nicht stand, weshalb die Veröffentlichung der App in Europa und Asien verschoben werden musste. Noch im Laufe dieser Woche soll es aber soweit sein. Dann dürfte der Wahnsinn hier seine Fortsetzung finden. Dass viele nicht so lange warten wollten, zeigt der sprunghaft angestiegene Traffic auf der Seite apkmirror.com, wo man sich die Installationsdatei für Pokémon Go herunterladen kann. 
Auf diesem Weg ist das Spiel auch schon in Deutschland gelandet.  ""Ich habe das Spiel gestern zum ersten Mal gespielt. In Aachen bin ich gleich auf Dutzende anderer Spieler gestoßen "", berichtet zum Beispiel Regina aus Köln im Gespräch mit ZEIT ONLINE. Die 26-Jährige war schon in ihren Kindertagen Pokémon-Fan und hatte sich lange auf die Neuerscheinung gefreut. 
Auch Sarah aus Brühl erinnert das Spiel an alte Zeiten.  ""Ich habe früher schon als Teenie gern Pokémon gespielt "", sagt die 34-Jährige. ""Als ich von Pokémon Go gehört habe, keimte die alte Begeisterung wieder auf "". Genau der Effekt, den sich die Betreiber erhofft haben dürften. Zwar sei es etwas ungewohnt, wenn sie mit den gleichen Figuren spiele wie die zwölfjährigen Nachbarskinder, sagt Sarah, aber das mache den Spielspaß aus.  ""Wer sich selbst zu ernst nimmt, wird dieses Spiel sicherlich nicht spielen. "" 
Ernst nehmen kann man Pokémon Go aber durchaus, wenn es um das Genre, die Technik und deren Auswirkungen, um Datenschutz und das Geschäftsmodell geht. 
Regina und Sarah* sind keine Augmented-Reality-Neulinge. Beide hatten vorher schon Ingress gespielt, das Vorläuferspiel von Niantic, das zwar auf eine ähnliche Spielmechanik setzt, aber eher düster als verspielt erscheint. Inzwischen treffen sich zu sogenannten Anomaly-Veranstaltungen wie kürzlich in Wien mehrere Tausend Ingress-Spieler, über ein Nischendasein ist die Firma damit aber bisher nicht hinausgekommen. 
Durch die Partnerschaft mit Nintendo und dadurch der Pokémon Company ändert sich das nun. Der japanische Spiele-Publisher vermarktet die Fantasiewesen seit 1996, als Gameboy-Spiel, als Sammelkarten, als Fernsehserie und als Stoff für Kinofilme. So fand die Franchise Millionen treuer Fans, von denen sich nun viele auf Pokémon Go stürzen. 
Das Grundprinzip ist bei allen Pokémon-Ausgaben gleich: Menschen fangen wilde Pokémon-Figuren, trainieren sie und lassen sie schließlich gegeneinander antreten. Das Besondere an Pokémon Go: Die reale Welt ist das Spielfeld. 
Augmented Reality – die angereicherte Realität – ist sozusagen die Schwester der Virtual-Reality-Anwendungen, die derzeit für Furore sorgen. Für Pokémon Go muss sich der Spieler keine sperrige 3D-Brille wie die Oculus Rift anlegen. Stattdessen dient das Smartphonedisplay als eine Art Schlüsselloch in die Spielwelt. So sieht der Spieler sich selbst auf einer Landkarte herumspazieren, die auf Google Maps basiert. An Orten, in denen sich in der realen Welt eine Kirche, ein Denkmal oder ein besonders originelles Graffiti befindet, befinden sich in der Spielewelt sogenannte PokéStops, an denen die Spieler wertvolle Gegenstände sammeln können. Oder Arenen, in denen die Pokémon gegeneinander kämpfen. 
Um ein wildes Pokémon zu fangen, muss der Spieler sein Smartphone in die Richtung der Fantasiefigur halten und es mit gezielten Ballwürfen treffen. Dazu schaltet Pokémon Go die Kamera hinzu. In das normale Umgebungsbild blendet die App dann die Pokémon ein. So plötzlich auf dem Rasen neben der Straße oder auf dem Tresen einer Starbucks-Filiale erscheint eine Comic-Ente, die es zu fangen gilt. Hat dies geklappt, kann der Spieler das Pokémon trainieren und schließlich gegen die Pokémon anderer Spieler antreten lassen. 
Um reale Welt und Spielewelt zu vereinen, nutzt Pokémon Go die Sensoren, die in jedes Smartphone eingebaut sind. Die Position des Spielers stellt die App über GPS und die Mobilfunkortung fest. Die Lagesensoren sorgen dafür, dass die Figur beim Schwenken des Smartphones scheinbar an der gleichen Stelle bleibt. Das klappt zwar nicht perfekt, aber gut genug, um den Eindruck einer virtuellen Welt zu schaffen. 
 
Dazu muss der Spieler aber seine Bewegungsdaten an die Server von Niantic übermitteln.  ""Wenn ich das verhindern könnte, würde ich das tun "", sagt Regina. Doch auch ohne Pokémon Go habe Google viele Standortdaten von ihr, sagt sie, zum Beispiel über Google Maps. Deshalb hat sich Regina damit arrangiert, dass Niantic haufenweise Metadaten erhebt, mit denen sich ihr Tagesverlauf und ihre Bewegungen sehr genau nachzeichnen ließen. So dürfte es den meisten Spielern gehen – falls sie die Datenschutzrichtlinien von Niantic überhaupt gelesen haben. Pokémon Go ist deshalb für manche ein Zeichen dafür, wie wenig gefragt Datensparsamkeit plötzlich ist, wenn nur die Belohnung stimmt. 
Die plötzliche Popularität des Spiels hat ungeahnte Folgen: So berichtet ein reddit-Nutzer, wie er im Park mit zwei anderen Spielern um drei Uhr morgens von der Polizei überrascht worden sei – und die Beamten nur mit Mühe überzeugen konnte, dass hier keine Drogengeschäfte stattfanden. Am Ende war der Polizist so neugierig, dass er das Spiel selbst installierte. Bessere Werbung für ihre Parallelgesellschaft können sich Niantic und Nintendo nicht erhoffen. 
Andere Nutzer hatten weniger friedliche Erlebnisse: In Missouri haben vier Räuber die App benutzt, um arglose Spieler anzulocken und auszurauben. Andere spielen das Spiel, während sie am Steuer ihres Autos sitzen. In solchen Momenten zeigt sich, dass Augmented Reality etwas ist, das viele Menschen noch nicht gelernt haben. 
Gleichzeitig ist für Nicht-Eingeweihte in solchen Momenten nicht erkennbar, was die Menschen da mit ihren Smartphones machen, was zu Irritationen führen kann. Und das ist vorsichtig ausgedrückt. Ein Mann namens Omari Akil warnt auf Medium schon, dass merkwürdiges Verhalten in den USA lebensgefährlich sein könnte, insbesondere für Schwarze. Es fällt schwer, das angesichts der jüngsten Vorfälle für reinen Alarmismus zu halten. Je beliebter Augmented-Reality-Anwendungen werden, desto mehr müssen sich Nutzer wie Nicht-Nutzer damit befassen, in welchen Situationen man einander begegnen könnte und wie man miteinander umgehen sollte. 
Ob sich die derzeitige Begeisterung in ein dauerhaftes Geschäft verwandelt, ist noch nicht abzusehen. So soll sich die kostenlose App über den Kauf von Fanartikeln und virtueller Münzen finanzieren, mit denen sich Spieler Vorteile verschaffen können. Aber das könnte viele abschrecken:  ""Ich glaube nicht, dass ich Pokémon Go lange spielen werde "", sagt zum Beispiel Sarah aus Brühl. Sollte Nintendo den zahlenden Spielern zu viele Vorteile gewähren, ginge für sie der Spielspaß verloren.  ""Dann bin ich raus. "" Bis es soweit ist, will sie sich weiter auf die Suche nach Figuren wie Enton, Raupy und Bisasam begeben. Egal, ob das für Außenstehende vielleicht seltsam aussieht. 
* Ihre Nachnamen möchten beide nicht veröffentlichen."	technik
"Kein namhafter Hersteller kann es sich leisten, ohne ein Fahrzeug in diesem Segment auf dem Genfer Autosalon zu erscheinen: Das Pseudo-SUV ist die neue Norm des Autos. Entweder mit einem der Modelle auf dem Messestand, die bereits erfolgreich den Markt bedienen. Oder mit einer Neuvorstellung. Oder mit der Ankündigung eines in Kürze kommenden SUVs. Der Grund für die Aktivität ist simpel: Der Kunde will diese Autos, die eigentlich nicht gerade geländetauglich sind. Fragt man die Besitzer, warum sie ausgerechnet zu diesem Fahrzeugtyp gegriffen haben, erhält man meistens zwei Antworten: Der ist so praktisch. Und ich fühle mich sicher darin. 
Mit klassischen Geländewagen wie einem Mercedes G-Modell oder einem Toyota Landcruiser haben diese Autos bestenfalls Designelemente gemeinsam. Und auch die Schnittmenge mit den US-amerikanischen sports utility vehicles – daher das Kürzel SUV – ist gering. Allradantrieb? Überflüssig. Geländegetriebe? Braucht keiner. Pure Größe? Nicht für die breite Masse. Bei gelassener Betrachtung sind viele Modelle lediglich hochgelegte Kombis oder Vans in robustem oder bisweilen martialischem Gewand. 
So stellt Volkswagen mit dem T-Cross Breeze nur scheinbar die Studie eines Cabrios vor – die Serienversion wird ein kleines SUV. In Wolfsburg kennt man das immense Potenzial dieser Gattung und will, so sagt es Vorstandvorsitzender Herbert Diess zur Premiere in Genf,  ""unser Angebot an SUVs kräftig ausbauen und künftig in jedem Kernsegment ein SUV anbieten. "" 
Volkswagen hat mit dem Tiguan, der inzwischen in der zweiten Generation beim Händler steht, eins der meistverkauften Autos der Republik überhaupt im Portfolio. Der Anteil der privaten Käufer lag im Januar laut Kraftfahrtbundesamt bei knapp 50 Prozent – das ist viel im Vergleich zum Durchschnitt von gut 30 Prozent, aber exemplarisch: Privatkunden sind überrepräsentiert. Diesen Trend will Volkswagen mit dem T-Cross nach unten fortsetzen, also in ein Preissegment, das für besonders viele Kunden als bezahlbar gilt. 
Das Rezept bleibt simpel: Neue Optik mit bekanntem technischen Kern. Wahrscheinlich wird der T-Cross ab 2017 erhältlich sein. Und vielleicht stellt ihn Volkswagen jetzt als Appetithappen der Öffentlichkeit vor, weil der Kunde sonst woanders kauft. 
Bei Kia oder Toyota zum Beispiel. Der Niro von Kia und der C-HR von Toyota bieten neben der obligatorischen Gestaltung Benzinhybridantriebe. Beiden Konzepten ist gemeinsam, dass sie vereinfacht gesagt als Bremsenergieverwertungsmaschinen funktionieren. Beim Verzögern arbeitet ein Elektromotor als Generator, der Strom erzeugt und diesen in einer Pufferbatterie speichert. Beim Beschleunigen wird diese elektrische Energie genutzt, um den Verbrennungsmotor entweder zu unterstützen oder kurzfristig komplett überflüssig zu machen. 
Toyota hat über acht Millionen Hybridautos weltweit verkauft. Die Technik erweist sich als zutiefst zuverlässig und tatsächlich sparsam. Das allein reicht aber nicht. Das Drumherum muss ebenfalls stimmen. Und hier hatten die Japaner bisher eine Lücke; es fehlte das Hybrid-SUV. Das gibt es seit einigen Monaten in Gestalt des RAV4 Hybrid und nun, viel wichtiger für große Stückzahlen, mit dem C-HR. Ohne SUV oder SUV-Artigem läuft es nicht in Deutschland. 
Und Kia? Die SUVs der koreanischen Marke kommen gut an. Der Topseller Sportage kann ein Plus bei den Verkaufszahlen von 46 Prozent gegenüber dem Vorjahr verbuchen. Eine Klasse darunter bringt Kia also nun den Niro. Legt man das Zentimetermaß an, zeigt sich, dass er eigentlich ein klassischer Kompaktwagen ist: Mit 4,36 Meter Länge bei 1,80 Meter Breite und 1,54 Meter Höhe ähnelt er einem Volkswagen Golf Sportsvan. Nur, dass der Niro stämmiger wirkt und, ach ja, der VW setzt auf TDI und TSI statt auf Hybrid, um den Verbrauch zu senken. 
Der Kraftstoffkonsum hat ohnehin keine Priorität. Man kann es drehen und wenden wie man will, die SUVs verbrennen mehr Sprit als die mechanisch weitgehend identischen Geschwister. Das liegt an der gewachsenen Höhe, die in einer größeren Stirnfläche und damit in einer schlechteren Aerodynamik mündet. Selbst, wenn es nur zehn oder 20 Prozent mehr als bei den Standardmodellen sind, könnten die Hersteller hier ein Problem bekommen: Der Boom verdirbt die Bilanz bei den gesetzlich limitierten Flottenemissionen. 
Prognosen gehen davon aus, dass der Marktanteil der SUVs von rund elf Prozent (2010) über zurzeit gut 22 Prozent auf 33 Prozent im Jahr 2020 ansteigen könnte. Auf dem Genfer Autosalon tun die Hersteller alles, um diesen Zahlen gerecht zu werden. Es ist, als würden sich Produzenten und Kunden gegenseitig hochschaukeln. Die Käufer fordern das SUV, die Firmen bauen es und zeigen immer neue Modelle, um das Habenwollengefühl zu unterstützen. Denn am Ende geht es für Nissan mit dem Qashqai, für Audi mit dem Q2 und alle anderen darum, Geld zu verdienen."	technik
"Das DFB-Pokalspiel zwischen Chemnitz und Mainz am vergangenen Freitag war ein Spektakel: Mit 5:5 ging es ins Elfmeterschießen und das nur, weil der Mainzer Spieler Johannes Geis aus 50 Metern Torentfernung in der letzten Minute den Ausgleich erzielte. Wer in den sozialen Netzwerken unterwegs war, konnte das Tor auch ohne Sky-Abo bestaunen. Abgefilmt aus dem Fernsehen, gab es den Treffer wenig später auf der Kurzvideoplattform Vine zu sehen. Beeindruckend ist die Bildqualität nicht gerade, aber gut genug für die meisten Fans, um mitreden zu können. 
Aktionen wie diese gefallen den Verantwortlichen der europäischen Fußballligen und Verbände gar nicht. Zum Start der neuen Premier-League-Saison kündigte der englische Veranstalter am Wochenende an, künftig stärker gegen Vines, animierte Gifs und YouTube-Videos von Spielszenen vorgehen zu wollen. Wie ein Liga-Sprecher der BBC mitteilte, arbeite man bereits an Gif-Crawlern, Vine-Crawlern und gemeinsam mit Twitter an einer Lösung, um das eigene  ""geistige Eigentum "" zu schützen. Auch die deutsche Bundesliga hatte im April zwei Jugendliche davor gewarnt, Spielszenen auf Twitter zu verbreiten und sprach von  ""digitaler Piraterie "". 
Natürlich geht es dabei ums Geld. Keine andere Fußballliga kassiert so viel für die TV-Übertragungen wie die Premier League. 3,7 Milliarden Euro gaben die Sender Sky und BT Sport vergangenes Jahr für die kommenden drei Spielzeiten aus. Hinzu kommen die Einnahmen aus dem Ausland. In der Bundesliga sind die aktuellen Übertragunsgrechte insgesamt mit 2,5 Milliarden über fünf Spielzeiten dotiert. 
Wer solche Summen zahlt, möchte nicht, dass im Internet die gleichen Bewegtbilder verfügbar sind, im schlimmsten Fall noch kostenlos. In der Vergangenheit stritten die Pay-TV-Sender sowohl mit Anbietern von digitalen Angeboten als auch den Öffentlich-Rechtlichen, wer zu welchem Zeitpunkt was zeigen darf. Die Fußballfans im Netz scheren sich freilich nicht um Verträge und stellen fleißig die besten Tore und kuriosesten Szenen online. 
Spätestens mit der Weltmeisterschaft in Brasilien ist so ein Trend im Netz entstanden. War in den vergangenen Jahren YouTube die erste Anlaufstelle für Fußballszenen, ist inzwischen Vine immer beliebter. Nicht nur geht der Upload schneller als bei YouTube, die sechs Sekunden, die Vine maximal erlaubt, reichen meist aus, um die Treffer zu zeigen. Durch die direkte Anbindung an Twitter erreichen die Clips meist innerhalb weniger Minuten ein großes Publikum. 
Wie das geht, zeigen beliebte Accounts wie Football Vines und Footy Humour. Sie haben jeweils über 500.000 Follower. Jeder einzelne Beitrag wird bis zu 3.000 Mal retweetet – eine Reichweite, von denen selbst die offiziellen Accounts vieler Vereine und Verbände nur träumen können. Dass die Zweit-Aufnahmen meist wackelig sind und flimmern, scheint niemanden zu stören. 
Die einzelnen Vines stammen zumeist aus unterschiedlichen Quellen. Der Betreiber eines beliebten Accounts sagte dem Guardian, er und sein Team würden keine eigenen Clips hochladen. Die Aktualität der Inhalte verrät allerdings, dass sie mittlerweile ein ziemlich gutes Netzwerk von Menschen aufgebaut haben müssen, die innerhalb weniger Minuten die interessantesten Szenen aktueller Fußballspiele online stellen. Für die Fußballfans, die das Spiel nicht selbst gucken können, ist das ein Service, den sonst nur kostenpflichtige Apps bieten – und selbst dort dauert es oft länger, bis die Tore als Video erscheinen. 
Auf die Netzkultur hat die Verbreitung auf Twitter ebenfalls Einfluss: Dass Szenen wie die Beißattacke von Luis Suárez oder der Flugkopfball von Robin van Persie während der WM so schnell zum Internetphänomen wurden, ist letztlich auch der schnellen Verbreitung der jeweiligen Spielszene auf Twitter zu verdanken. 
 
Andere Angebote, wie die US-Website Fusion Soccer, binden in ihre Liveblogs inzwischen animierte Gifs aus den aktuellen Fernsehbildern ein – auch das ist nicht erlaubt, sagen die Sender und Sportverbände. 
 ""Bei der Liveberichterstattung ist es umstritten, ob es einen urheberrechtlichen Schutz gibt "", sagt der auf Sport- und Urheberrecht spezialisierte Anwalt Dieter Frey im Gespräch mit ZEIT ONLINE,  ""denn das Livebild bildet zunächst nur die Wirklichkeit ab "". Um urheberrechtlich geschützt zu sein, bedarf es einer gewissen Kreativität, die über das Livebild hinausgeht. Das können etwa die Schnittfolge und der eingefügte Kommentar sein.  ""Viele Ligen reichern die Liveberichterstattung zudem urheberrechtlich mit Logos oder Jingles an "", sagt Frey. Denn diese dürfen nicht einfach durch Unbefugte verbreitet werden. 
Wäre ein animiertes Gif ohne Ton, Schnitt und Logo also theoretisch erlaubt? Nicht ganz. Denn auch jenseits des Urheberrechts genießen Aufnahmen von Sportveranstaltungen den sogenannten Laufbildschutz – und das auch in Ausschnitten, also wenn es sich nur um eine einzelne Torszene handelt. Das Argument der mangelnden Schöpfungshöhe, wenn man nur eine Torszene daraus zeigt, greift hier nicht. 
Anders sieht es mit Aufnahmen aus, die direkt aus dem Stadion stammen. Auf den erwähnten Vine-Accounts finden sich nämlich nicht bloß abgefilmte TV-Bilder, sondern häufig auch Aufnahmen direkt von der Tribüne. Smartphones mit immer besseren Kameras und eine schnelle mobile Datenverbindungen machen es möglich. Der frühere RTL-Manager Helmut Thoma kündigte vor Kurzem sogar an, für einen Lokalsender mit Smartphone-Reportern aus den Bundesliga-Stadien berichten zu wollen. 
 ""Für die Aufnahmen aus dem Stadion greift nicht das Urheber-, sondern das Hausrecht "", erklärt Frey. Ein Urheberrecht auf Tore oder Spielzüge vonseiten der Spieler, Vereine oder Ligen gibt es nicht. Stattdessen müssen die jeweiligen Betreiber des Stadions entscheiden, ob die Zuschauer Kameras mitbringen und benutzen dürfen. Da die Vereine aber Absprachen mit der Liga treffen, spiegeln die Allgemeinen Geschäftsbedingungen meist deren Anforderungen wider. 
Die Ticket-AGB von Borussia Dortmund etwa untersagen die Verbreitung von Ton- und Bildaufnahmen im Netz – allerdings nur zu kommerziellen Zwecken. Ob ein Vine-Account oder eine Facebook-Seite mit mehreren Tausend Followern dazu zählt, dürfte schwer nachzuweisen sein. Bis jetzt scheint jedenfalls die DFL die Aufnahmen von Zuschauern zu dulden. In der Premier League sind die Regeln strenger. Der FC Arsenal beispielsweise untersagt in seinen AGB sämtliche Aufnahmen und verbietet sogar noch einmal explizit die Verbreitung in sozialen Netzwerken. 
Es bleibt die Frage nach der Durchsetzung solcher Regeln und wie die von der Premier League angekündigten automatischen Erkennungsverfahren funktionieren sollen. Auf YouTube gibt es das Content-ID-System, mit dem Rechteinhaber den Katalog der Plattform automatisch scannen und illegal kopierte Inhalte flaggen können. Für Vine oder Fotohoster gibt es dieses automatische System nicht, auch wenn Unternehmen wie Tumblr in Software investieren, die unter anderem Markenlogos erkennen soll und eines Tages auch für andere Zwecke zum Einsatz kommen könnte. 
Bis es soweit ist, dürfte den Ligen und Verbänden nur der direkte Kontakt mit den jeweiligen Diensten bleiben, um geschützte Inhalte löschen zu lassen. Dass die Uploader strafrechtlich belangt werden, scheint indes unwahrscheinlich. Die Premier League hat dem Branchenportal The Lawyer mitgeteilt, einzelne Uploader nicht verklagen zu wollen, solange die Verstöße nicht in einem größeren Rahmen geschehen.  ""Jeden Verstoß abzumahnen, wäre schon logistisch Wahnsinn "", glaubt auch Anwalt Dieter Frey. Er kann sich aber vorstellen, dass die Rechteinhaber in Zukunft immer bessere Erkennungssoftware nutzen werden, um unerlaubt verbreitete Inhalte finden und löschen zu können. 
Unwahrscheinlich scheint dagegen eine andere Lösung, nämlich dass die Ligen eines Tages den Nutzen der neuen Dienste im Netz erkennen und von sich aus zeitnah kurze Clips der Highlights im Netz posten. Im Fall von YouTube hat es schließlich auch eine Weile gedauert: Im Jahr 2007 verklagte die Premier League den Videodienst erstmals wegen unerlaubter Livebilder. Vergangenen Herbst zog sie die Klage nach sechs Jahren ohne Erfolg von sich aus zurück. Einer der Gründe: Ein komplettes Verbot der Aufnahmen hätte auch die YouTube-Kanäle der Vereine betroffen. Und auf diese Option wollte dann doch niemand verzichten."	technik
"Das nächste große Ding, die nächste Killer-App: Seit Twitter im Jahr 2007 auf dem Festival seinen Durchbruch feiern konnte, nutzt die Technik- und Start-up-Branche das South by Southwest (SXSW) im texanischen Austin als Versuchsfeld für neue Dienste und Anwendungen. Zwar schaffen es nur wenige Apps, auch über das Festival hinaus relevant zu sein, doch zwischen Hype-Cycle und Bullshit-Bingo zeigen sich gelegentlich die Trends der Zukunft. 
Ein Thema in diesem Jahr ist mobiles Livestreaming. Bereits in der Woche vor dem SXSW machte eine neue iOS-App auf sich aufmerksam. Sie heißt Meerkat, wie das Erdmännchen, und ermöglicht, direkt vom Smartphone aus Videos zu streamen. Das Besondere ist die Anbindung an Twitter: Über das soziale Netzwerk senden die Meerkat-Nutzer ihre Streams an ihre Follower. Die können sie anschließend kommentieren. Die einfache Bedienung und Verknüpfung mit einem bestehenden Netzwerk ist Meerkats Stärke im Vergleich zu anderen Livestreaming-Diensten. 
Am Anfang der Meerkat-Tweets taucht der Hinweis  ""Live Now "" auf. Wer auf den folgenden Link klickt, sieht zurzeit vor allem Aufnahmen von neugierigen Nutzern vor den Computern. Doch die Möglichkeiten sind riesig, glauben die Entwickler: Meerkat sei ebenso für Interviews wie für Aufnahmen von Veranstaltungen, für spontane Promi-Auftritte oder für witzige oder interessante Situationen aus dem Alltag gedacht. Das Eishockeyteam Miami Dolphins hat ebenso mit der App experimentiert wie die Castingshow American Idol.  ""Jeder hat eine Geschichte zu erzählen "", sagt Meerkat-Gründer Ben Rubin gegenüber dem Guardian. 
Jedenfalls wenn Twitter mitspielt. Am Freitag schränkte das Netzwerk den Meerkat-Entwicklern bereits eine wichtige Funktion ein. Neue Meerkat-Nutzer können nicht mehr ihre Twitter-Follower in die App importieren und sie automatisch über anstehende Streams benachrichtigen. Künftig müssen sie ihre Twitter-Timeline im Blick behalten. Das ist bei vielen Followern eher mühselig, Videos können untergehen. 
Rubin bedauert die Entscheidung, sieht darin aber nicht das Ende der App. Tatsächlich konnte der Dienst über das Wochenende mindestens weitere 30.000 Nutzer gewinnen. Dabei war Meerkat ursprünglich ein Nebenprojekt, wie viele gute Ideen. Nachdem Rubin vergangenen Dezember seinen Video-Dienst Yevvo aufgab, arbeiteten er und ein kleines Entwicklerteam an zwei neuen Apps, eine davon war Meerkat. Die erste Version wurde in nur acht Wochen entwickelt und kam in einem ersten Test so gut an, dass Rubin und seine Investoren jetzt verstärkt in Meerkat investieren. 
Auf dem SXSW präsentieren andere Unternehmen ähnliche Ideen. Die App stre.am etwa ist für den Innovation Award des Festivals nominiert. Sie verbindet ebenfalls Livestreaming mit einer Chat-Funktion und ermöglicht zudem, Videos zu archivieren und ausgewählte Highlights für 24 Stunden nach der Aufnahme zur Verfügung zu stellen. Der Dienst Hang /w preist die Integration mit Facebook und Twitter an, hier können Streamer mit ihren Live-Aufnahmen Geld verdienen. 
Twitter selbst könnte demnächst ebenfalls in den Markt einsteigen. Bereits im Januar übernahm Twitter den Video-Dienst Periscope. Den gibt es zurzeit nur in einer geschlossenen Beta-Version, aber er funktioniert offenbar ähnlich wie Meerkat. Einige Experten vermuten, dass Twitter Meerkat nur deshalb ausbremst, um später in diesem Jahr einen eigenen Live-Service zu starten und bis dahin nicht zu viele Nutzer an die Konkurrenz zu verlieren. 
Ist Livestreaming das nächste große Ding? Die These ist schon älter, meist blieb es bei der Behauptung. Doch der Erfolg von Plattformen wie twitch.tv hat in den vergangenen beiden Jahren zu einer neuen Livestreaming-Kultur im Netz geführt. Nun geht es darum, das Streaming mobiltauglich zu machen, es von der Schreibtisch- und Studiobühne hinaus in den Alltag zu bringen. 
Der Zeitpunkt ist günstig. Die Kameras von Smartphones taugen inzwischen für gute Aufnahmen, dank LTE ist die mobile Datenverbindung kein Problem mehr. Was fehlt, sind die richtigen Anwendungen und der Weg, Streams auch an ein Publikum außerhalb der eigenen Freundesliste zu bringen. Mit Twitter, Facebook und Google erkennen die großen Netzwerke das Potenzial. 
Dabei klingt Livestreaming von jedermann zu jederzeit zunächst wie eine schlechte Idee: Verwackelte, grobkörnige Aufnahmen in schlechtem Licht, vertikale Videos, übersteuerter Ton, Windrauschen und banaler Alltag. 
 
Dazu kommen die rechtlichen Bedenken. Zu Beginn des Jahres sorgte die Plattform YouNow für Ärger bei Daten- und Jugendschützern, nachdem zahlreiche Jugendliche in Deutschland den Dienst für sich entdeckten. Minderjährige quasselten unbeschwert aus dem Kinderzimmer mit fremden Zuschauern und verrieten dabei so manches private Detail. Anwälte verwiesen außerdem auf das Persönlichkeitsrecht, wonach das Filmen von unwissenden Personen in der Öffentlichkeit verboten ist. 
Dennoch zeigt der plötzliche Erfolg von YouNow ebenso wie der aktuelle Hype um Meerkat: Das Interesse an Livestreams aus dem Alltag ist vorhanden. Und es ist denkbar, dass Twitter der Weg ist, genau das einem größeren Publikum schmackhaft zu machen. Schließlich ist kein anderes soziales Netzwerk so sehr auf den einzelnen Moment aus. Casey Newton vom Technikportal The Verge schreibt:  ""Live-Aufnahmen fühlen sich einfach so an, als gehörten sie auf Twitter. "" Das Unternehmen sei längst  ""ein Betriebssystem für Nachrichten "". 
Tatsächlich scheinen Live-Videos nur das nächste Puzzleteil für Twitters Echtzeit-Mechanik zu sein. Gelänge es Twitter, die Funktion in seinen Dienst zu integrieren, könnte das der Durchbruch für spontane Live-Aufnahmen sein – nicht nur für Journalisten. Das Technikmagazin Mashable sieht Anwendungsmöglichkeiten im Showgeschäft, für Makler, Unternehmen und zu Werbezwecken. Und natürlich dürfte auch die Pornobranche nicht abgeneigt sein. Für Twitter selbst geht es darum, ein nachhaltiges Geschäftsmodell zu finden. 
Einen Vorreiter gäbe es. Als Twitter vor etwas mehr als zwei Jahren den Kurzvideodienst Vine einführte, sorgte er für viele Fragen: Was soll man bloß in maximal sechs Sekunden filmen? Werden unsere Timelines mit banalen Home-Videos geflutet? Inzwischen ist Vine ein lebendiges soziales Netzwerk, das seine eigenen Stars hervorgebracht hat und die Grenzen des Urheberrechts herausfordert. 
Noch ist Twitter nicht so weit. Und auch Meerkat ist zum jetzigen Zeitpunkt wenig mehr als ein kurzlebiger Hype. Ob die App in einem Jahr oder auch nur einem Monat noch eine Rolle spielt, ist fraglich. Doch ganz außer Acht lassen sollte man die Glaskugel des South by Southwest nicht: Der Trend geht zum Bewegtbild. Live und in Farbe."	technik
"Quizfrage: Wie viele letzte Chancen bekommt ein Smartphone-Hersteller? Die Antwort ist: ziemlich viele, jedenfalls wenn er HTC heißt. Bereits 2013 war die Rede von der letzten Chance für das taiwanische Unternehmen, denn ungefähr so lange strauchelt es bereits finanziell. In diesem Sommer kündigte HTC mehr als 2.000 Mitarbeitern, rund 15 Prozent der Belegschaft. Zuvor hatte man einen Verlust von 250 Millionen US-Dollar gemeldet – für das vorangegangene Quartal, wohlgemerkt. Ein Grund sind die schlechten Verkaufszahlen der Smartphones. Ein Problem, das noch andere Android-Hersteller betrifft. 
Für HTC ist es allerdings ein besonderes Dilemma, denn anders als Mischkonzerne wie Samsung oder Sony ist das Unternehmen auf seine Smartphone-Sparte angewiesen; mit der Actionkamera RE und dem noch in der Testphase befindlichen Virtual-Reality-Headset Vive lassen sich kaum Umsätze generieren. Deshalb versucht es HTC jetzt noch einmal. Am Dienstagabend präsentierte HTC das One A9 als Auftakt einer neuen Produktreihe. Neues Modell, neue letzte Chance sozusagen. 
Das One A9 sei eine  ""gute Alternative "" zum iPhone 6, wurde CEO Cher Wang am Tag der Vorstellung in einem internen Brief an die Mitarbeiter zitiert. Wie ernst sie es meinte, zeigte sich am Abend. Wie bereits vorab veröffentlichte Bilder vermuten ließen, unterscheidet sich das A9 äußerlich nämlich tatsächlich kaum von den Apple-Geräten. Der Rahmen und die Rückseite sind aus Metall und wahlweise in grau, silber, gold oder granatrot erhältlich. Auffällig sind die Antennenstreifen aus Kunststoff, die fast an der gleichen Stelle verarbeitet sind wie im iPhone 6, die Kamera dagegen ist mittig angeordnet und ragt ein wenig aus dem Gehäuse hervor. 
Das Display des One A9 ist fünf Zoll groß und liefert dabei eine Full-HD-Auflösung von 1.920 mal 1.080 Pixeln. Das ist die gleiche Auflösung wie in HTCs aktuellem Oberklasse-Modell One M9, allerdings ist sie geringer als die des etwas größeren Samsung Galaxy S6. Den Trend zu einem zweiten Gerät in Phablet-Größe macht HTC nicht mit. Display und Auflösung dürften die Mehrheit der Käufer absolut befriedigen. 
HTC will das A9 nicht als direkten Konkurrenten zum M9 positionieren. Qualcomms Achtkernprozessor Snapdragon 617 ist zwar neu, kommt in Sachen Leistung aber nicht an den des im M9 heran. Abstriche gibt es auch beim Akku: 2.150 Milliamperestunden sind kein Spitzenwert, allerdings sollen sowohl Rechen- als auch Akkuleistung mit aktuellen Topgeräten mithalten können. Je nach Region gibt es 16 oder 32 Gigabyte internen Speicher und zwei oder drei Gigabyte Arbeitsspeicher. Und, zumindest das ist ein Pluspunkt zu anderen aktuellen Geräten, einen Slot für externe SD-Karten. 
Auch bei der Kamera gibt es Unterschiede zum M9: Mit 13 Megapixeln liegt die Kamera des A9 unter den 20 Megapixeln des M9, allerdings ist die Blende etwas größer und soll somit bessere Aufnahmen bei geringeren Lichtverhältnissen liefern – etwas, das erste Tests zeigen müssen, zumal HTC mit seiner Kameratechnik in den vergangenen Jahren nicht immer glücklich aussah. Ambitionierte Fotografen können auf Wunsch im RAW-Modus aufnehmen und die Dateien gleich auf dem Smartphone bearbeiten. 
Überraschend viel Wert legt HTC auf den Sound. Der Kopfhöreranschluss soll genug Leistung bringen, um auch die größten Kopfhörer versorgen zu können, im A9 selbst ist ein besonderer Audiochip verbaut, der Musik von 16 auf 24-Bit hochskalieren kann. Das scheint eher eine Funktion für die Nische zu sein, aber ein paar Besonderheiten muss HTC natürlich liefern. 
Insgesamt bleibt nämlich die Frage, ob HTC mit dem One A9 punkten kann oder ob das taiwanische Unternehmen mit einem Smartphone, das so augenscheinlich dem iPhone nachempfunden ist, nicht die eigene Identität gefährdet und somit komplett in der Irrelevanz verschwindet. 
Dass Smartphone-Hersteller gegenseitige Trends kopieren, war schon immer der Fall. Samsung hatte sich mit dem Galaxy S6 von Apple inspirieren lassen, die wiederum zuletzt einige Hardware-Trends von Microsoft aufgegriffen haben. Andere, wie Xiaomi, haben von Beginn an auf erfolgreiche Klone gesetzt, um sie dann zum deutlich günstigeren Preis zu verkaufen. Tatsächlich näherten sich viele Android-Hersteller in den vergangenen Jahren optisch wie technisch an; die großen Experimente sind vor allem Startups vorbehalten. 
HTC galt lange Zeit als einer der innovativsten und mutigsten Hersteller. Das HTC Legend aus dem Jahr 2010 war das erste Smartphone mit einem Rahmen aus Metall, lange bevor Apple und Samsung nachzogen. Gewissermaßen hat also Apple zuerst HTC kopiert. Das One M7 gewann noch 2013 den Preis für das beste neue Mobilgerät und die folgenden beiden Generationen bauten auf dem gleichen erfolgreichen Design auf. Allerdings hatten sie einige kleinere Unzulänglichkeiten, etwa die erwähnte Kamera. Das One M9 konnte diese zwar weitestgehend ausmerzen, den Rückstand auf die Android-Konkurrenz von Sony oder Samsung aber nicht verkürzen. 
 
 ""Extreme Situationen rufen nach extremen Maßnahmen "", kommentiert Vlad Savov von The Verge nun HTCs Entscheidung, die eigenen Design-Maxime zugunsten eines iPhone-Klons aufzugeben. Das muss seiner Meinung nach gar nicht das Schlechteste sein, schließlich ist der Look des iPhones bewährt und der Erfolg von Herstellern wie Xiaomi hat gezeigt, dass gut geklont schon halb gewonnen ist. David Pierce von Wired sieht das ähnlich:  ""Es ist wie ein iPhone, läuft aber mit Android - ist doch ein prima Verkaufsargument "", schreibt Pierce. 
Möglicherweise liegt die echte Stärke des One A9 ohnehin nicht in der Hardware. Das Gerät wird nämlich, wenn es Anfang November weltweit ausgeliefert wird, das erste mit der neuen Android-Version 6.0 Marshmallow sein. Jedenfalls das erste jenseits der von Google selbst hergestellten Nexus-Modelle. Android 6.0 bietet einige große Neuerungen, etwa einzelne Berechtigungen für Apps. In dieser Hinsicht hätte HTC einen Vorsprung vor der Konkurrenz, denn es dürfte eine Weile dauern, bis etwa Samsung seinen jetzigen Modellen die neue Android-Version nachliefert. 
Gleichzeitig habe HTC eine  ""enge Zusammenarbeit "" mit Google geplant. Zwar wird das One A9 ebenfalls mit der angepassten Android-Oberfläche namens Sense ausgeliefert. Die soll sich aber deutlich dezenter als die anderer Hersteller präsentieren, HTC spricht selbst von  ""Android 6.0 mit Sense "", was die Hierarchie offenlegt. Die ersten Tester sind jedenfalls von der Performance des Systems überzeugt. Mehr noch: HTC verspricht, dass sämtliche von Google bereitgestellten Android-Updates binnen zwei Wochen auch auf dem A9 erscheinen sollen. 
Das wäre eine ziemlich große Sache. In diesem Jahr haben Android-Sicherheitslücken wie Stagefright gezeigt, wie problematisch die verzögerte Update-Philosophie von Herstellern wie Samsung oder Sony sein kann. Die geloben zwar mittlerweile ebenfalls Besserung, doch keiner äußerte sich bislang so deutlich wie HTC. Für die Kunden ist die Abkehr vom eigenen, aufgeblähten Android hin zur  ""Reinform "" von Google eine gute Nachricht und sie macht das One A9 durchaus attraktiv. 
Attraktiv ist auch der Preis, jedenfalls in den USA. Dort wird das One A9 ab 399 US-Dollar erhältlich sein, was deutlich geringer ist als der Preis des iPhone 6 oder des One M9. The Verge berichtet allerdings, dass es etwa in Großbritannien deutlich teurer sein wird, golem.de schreibt von 580 Euro für Deutschland, was nicht mehr wirklich günstig wäre. Dennoch ist HTCs Strategie für seine mutmaßlich letzte Chance, das eigene Smartphone-Geschäft zu beleben, am Ende klar: Die Oberklasse machen Apple, Samsung und Sony mittlerweile unter sich aus. HTC dagegen vertraut auf geringere Preise, die schnellen Software-Updates von Google – und die optischen Reize des iPhones."	technik
"Zwei Nexus-Smartphones, ein Tablet-Laptop-Hybrid und zwei neue Chromesticks: Google hat am Dienstag in San Francisco eine Reihe von Geräte vorgestellt, mit denen das Unternehmen nicht zuletzt zwei seiner wichtigsten Plattformen attraktiver machen will: Android und Google Play. 
Google-Manager Dave Burke stellte zunächst das Nexus 5X und das Nexus 6P vor, die neue Generation der Nexus-Smartphones mit dem Betriebssystem Android 6.0 alias Marshmallow. 
Das 5X wird von LG hergestellt, hat ein 5,2 Zoll großes Full-HD-Display mit 1.920 mal 1.080 Pixeln, einen auf zwei Gigahertz getakteten Snapdragon-808-Prozessor, zwei Gigabyte Arbeitsspeicher und wahlweise 16 oder 32 Gigabyte Flash-Speicher – aber keinen Steckplatz für microSD-Karten. 
Das 6P hat einen 5,7 Zoll großen AMOLED-Bildschirm mit 2.560 mal 1.440 Pixeln, einen Snapdragon 810 mit drei Gigabyte Arbeitsspeicher und wahlweise 32, 64 oder 128 Gigabyte Festspeicher. 
Es gibt eine Reihe von Gemeinsamkeiten: Beide Geräte werden über einen Stecker vom Typ USB-C aufgeladen, was laut Burke doppelt so schnell gehen soll wie bei einem iPhone 6 Plus (gemeint war aber wahrscheinlich ein iPhone 6s Plus.) 
Beide haben eine 12,3-Megapixel-Hauptkamera mit besonders großen Pixeln, die viel Licht auch in dunklen Umgebungen aufnehmen können sollen. Diesen Ansatz verfolgte schon HTC mit seinen  ""Ultrapixeln "" recht erfolglos. Burke aber zeigte anhand eines Beispielfotos, dass die neuen Nexus-Geräte in der Abenddämmerung bessere Fotos machen als ein iPhone 6s Plus. Ob das wirklich so ist und ob die Nexus-Geräte dafür bei Sonnenlicht chancenlos sind, werden Tests noch zeigen. 
Die Funktion Smart Burst entspricht ungefähr den Live Photos von Apple im neuen iPhone 6s: Die Nexus-Smartphones machen damit eine Reihe von Fotos in schneller Folge und bauen diese zu einem animierten GIF zusammen. 
Auf der Rückseite der neuen Smartphones ist ein Fingerabdrucksensor platziert. Der entsperrt die Geräte in weniger als 600 Millisekunden, verspricht Google. Außerdem dient er zum Verifizieren von Bezahlvorgängen mit Android Pay in Geschäften (das es in Deutschland nicht gibt) und im Appstore Google Play. 
Die wichtigste Gemeinsamkeit ist aber natürlich das Betriebssystem: Das Nexus 5X und das 6P werden die ersten Smartphones mit vorinstalliertem Android 6.0 alias Marshmallow sein. Dessen Funktionen sind durch die verschiedenen Entwicklerversionen bereits seit Längerem bekannt, hier noch einmal einige der wichtigsten Neuerungen und Funktionen: 
Das Nexus 6P wird je nach Speicherausstattung 649, 699 oder 799 Euro kosten. Das 5X wird in der Version mit 16 Gigabyte Speicher 479 Euro kosten, der Europreis für die 32-Gigabyte-Version ist noch unbekannt. Beide können ab sofort in mehreren Ländern im Google Store vorbestellt werden, jedoch noch nicht in Deutschland. Das dürfte sich in einigen Wochen aber ändern. Im Preis enthalten ist ein dreimonatiges Probe-Abo für Google Play Music, was aber eher als Lockangebot zu verstehen ist. Verständlich, angesichts zahlreicher Konkurrenten von Spotify über Apple Music bis zu Aldi life. Wohl auch deshalb kündigte Google ein Familien-Abo für Google Play Music an: Für 14,99 Dollar können bis zu sechs Familienmitglieder auf Googles Musikangebot zugreifen. 
Android 6.0 alias Marshmallow wird Google in der kommenden Woche veröffentlichen, jedoch nur für einige ältere Nexus-Geräte. Bis die Hersteller anderer Android-Geräte das Upgrade anbieten, dürfte es noch Wochen oder gar Monate dauern. 
Seit 2010 gibt es die Nexus-Reihe. Beliebt war sie stets, weil sie ordentliche bis sehr gute Hardware mit Googles neuestem Betriebssystem zu einem günstigen Preis bot. Wie attraktiv sind nun die neuen, gar nicht mehr so günstigen Nexus-Modelle? 
 
Nicht besonders, hieß es auf zdnet.com schon vor einigen Tagen: Seit Hersteller wie Samsung weniger Bloatware in ihre Android-Oberflächen steckten und Apps wie die Kamerasteuerung in ihrem Funktionsumfang sogar erweiterten, sei das pure Android der Nexus-Modelle nicht mehr das überlegene, argumentiert Autor Larry Dignan. Außerdem biete auch Motorola im Moto X ein weitgehend unverändertes Android. Abgesehen davon gibt es mit dem Honor 7 oder dem OnePlus Two mittlerweile auch andere Android-Geräte mit Oberklasse-Ausstattung zum Mittelklasse-Preis. 
Eine Kernfunktion von Android Marshmallow, die selektive Genehmigung von App-Berechtigungen, ist auch kein Alleinstellungsmerkmal. Andere Betriebssysteme können das schon länger. 
Sollte die Kamera weniger überwältigend sein, als Google behauptet, bliebe den Nexus-Modellen nur ein echter Vorteil: Die Nexus-Modelle werden vorerst die einzigen Androiden sein, die Googles neue monatliche Sicherheitsupdates definitiv bekommen. Nachdem die Stagefright-Sicherheitslücke – von ihren Entdeckern ganz unbescheiden  ""die Mutter aller Android-Verwundbarkeiten "" genannt – bekannt geworden war, hatte Google verkündet, diese Updates einzuführen. Hersteller wie Samsung und LG wollen sie zwar ebenfalls an ihre Kunden weitergeben, brauchen dafür aber die Hilfe vieler Mobilfunkbetreiber in aller Welt. 
Allerdings sind solche Sicherheitsfragen kein mehrheitsfähiges Verkaufsargument. Die neuen Nexus-Smartphones werden es also möglicherweise schwer haben, an die früheren Erfolge der Reihe anzuknüpfen. 
Die zweite Version von Googles Streaming-Stick für Fernseher unterstützt mehr WLAN-Standards und soll die Übertragungsqualität damit deutlich verbessern. Mit der aufgehübschten Chromecast-App soll es einfacher sein, Inhalte aus verschiedenen Quellen wie den Smartphone-Apps von Netflix, Hulu oder Google Play schnell zu finden und zu organisieren. 
Ganz neu ist Chromecast Audio. Über einen 3,5-Millimeter-Anschluss wird der mit einem Lautsprecher in der Wohnung verbunden. Dann können Nutzer ihre Musik, Hörbücher oder Podcasts aus Google Music Play vom Smartphone auf diesen Lautsprecher streamen. Das geht auch mit iOS-Apps, der Chromecast Audio versteht sich also auch mit iPhones. 
Beide Chromecast-Sticks werden  ""in Kürze "" im Google Store und im Elektronikhandel für je 39 Euro zu kaufen sein. 
Die vielleicht größte Überraschung des Abends heißt Pixel C. Das C steht für convertible – umwandelbar. Das Gerät ist als Mischung aus Tablet und Laptop gedacht. Es hat einen zehn Zoll großen Touchscreen mit 2.560 mal 1.800 Pixeln, einen Nvidias X1-Prozessor mit vier Kernen, drei Gigabyte Arbeitsspeicher und wahlweise 32 oder 64 Gigabyte Flash-Speicher. Das Betriebssystem ist wiederum Android 6.0. 
Als Zubehör wird es eine Ansteck-Tastatur geben, die sich, wenn sie nicht gebraucht wird, auf der Rückseite und der Vorderseite befestigen lässt. Auf der Vorderseite heißt: das Gerät ist im Ruhezustand, das Display ist verdeckt. Auf der Rückseite heißt: die Tastatur wird per Induktion geladen. Magneten halten sie jeweils in ihrer Position. 
Mit seinem Metallgehäuse sieht das Pixel C wie eine edle Variante von Microsofts Surface aus. Ob es den Spagat zwischen Tablet und Laptop besser hinkriegt, müssen Tests zeigen. 
Das Pixel C soll rechtzeitig zum Weihnachtsgeschäft in den Handel kommen. Es wird 499 bis 599 Dollar kosten, die Tastatur noch einmal 149. Wann und zu welchem Preis das Gerät in Deutschland erhältlich sein wird, gab Google noch nicht bekannt."	technik
"Wenn im Sommer, ab Juni oder Juli, die ersten Model X von Tesla auf deutschen Straßen fahren, werden sie einiges an Aufmerksamkeit erregen. Dass der SUV bis 250 km/h elektrisch fährt, dürfte dabei auf den ersten Blick für die Umstehenden weniger spannend sein als die Performance im Stand, wenn sich die Falcon Wings genannten Türen öffnen. Es ist schon ein spezielles Schauspiel, wie sich die doppelt abknickenden Flügeltüren gemächlich in die Höhe schieben.  ""Das hat sonst keiner "" scheint eine Maxime zu sein, auf die man bei Tesla besonderen Wert legt. 
Beim Model S, dem ersten in größeren Stückzahlen hergestellten Fahrzeug des amerikanischen E-Autobauers, waren es unter anderem die atemberaubende Beschleunigung aus dem Stand (von 0 auf 100 km/h in drei Sekunden) und der riesige Bildschirm in der Mittelkonsole, die die Limousine mit dem Haben-Will-Faktor versahen. Beides ist nun im Model X Standard. Der SUV verfügt in der 131.000 Euro teuren Version P90D über 391 kW (532 PS) und rast in 3,4 Sekunden auf 100 km/h. Der riesige Bildschirm, über den fast alle Fahrzeugfunktionen gesteuert werden, ist wieder serienmäßig. 
Auf dem Genfer Autosalon gewährt Tesla der europäischen Öffentlichkeit zurzeit zum ersten Mal einen Blick in den siebensitzigen E-SUV. Die Verarbeitung macht einen guten Eindruck, die hochwertig aussehenden Applikationen aus offenporigem Holz haben sich die Amerikaner bei anderen Premiumherstellern abgeschaut. 
Ein Highlight im Cockpit ist die weit nach oben gezogene Windschutzscheibe, auf die man bei Tesla besonders stolz ist. Für Fahrer und Beifahrer ergibt das ein sehr luftiges Raumgefühl. Wie sehr allerdings beim Fahren eventueller Sonnenschein stört, bleibt abzuwarten. Als Schutz vor zu viel Licht ist die Scheibe zum Teil getönt, zudem kann man eine herkömmliche Sonnenblende auf der Scheibe arretieren – was im ansonsten netten Innenraum allerdings ziemlich hässlich ist und nach Billiglösung aussieht. 
Beim Model X wurden Wünsche von Model-S-Kunden berücksichtigt. So gibt es nun zwischen Fahrer- und Beifahrersitz eine Mittekonsole mit Becherhalter und Ablagefach, zudem können die Sitze nicht nur geheizt, sondern auch gekühlt werden. Der Kunde kann zwischen einer Konfiguration mit sieben oder etwas luftiger gestellten sechs Sitzen wählen. Weil die Sessel in der mittleren Reihe auf einzelnen Podesten angebracht sind, lassen sie sich einzeln verschieben und justieren. Sind die beiden hinteren Sitze umgeklappt, beträgt das Kofferraumvolumen (inklusive des Fachs unter der Motorhaube) beachtliche 2.180 Liter. 
Die schon erwähnten Flügeltüren sollen einen komfortablen Zutritt zu den beiden hinteren Sitzreihen bieten. Und in der Tat: Weil sich die Türen komplett nach oben wegfalten, gelingt der Einstieg mindestens so bequem wie durch die Schiebetür eines großen Vans. Angst, dass die Türen in engen Parklücken andere Autos beschädigen, muss man laut Tesla nicht haben: Sensoren an der Türseite scannen vorher die Umgebung und öffnen nur, wenn genügend Platz vorhanden ist – oder stoppen, bevor die Garagendecke erreicht ist. Ausreichend, damit sich die Konstruktion nach oben falten kann, sind 30 Zentimeter Abstand zwischen Tür und Nachbarauto. Mit einer herkömmlichen Tür müsste man sich schon ziemlich schlängeln, um dort herauszukommen. 
Auch die vorderen, sich herkömmlich öffnenden Türen bieten einen Clou: Nähert sich der Fahrer mit dem Schlüssel in der Tasche, öffnet sich die Fahrertür automatisch. Man nimmt Platz, tritt die Bremse – und die Tür schließt sich wieder, ohne dass der Fahrer einen Befehl dazu geben müsste. Auch hier sorgen Sensoren dafür, dass die Tür nirgendwo aneckt. Dass die Portale dabei nicht so satt ins Schloss fallen wie bei anderen Premiumherstellern, dürfte den Tesla-Käufer bei so viel Show wenig stören. 
 
Wie bei Neufahrzeugen dieser Preisklasse üblich, ist das Model X serienmäßig zudem mit Kamera und Radarsensoren ausgestattet, die Spurhalte- oder Notbremsassistent verwirklichen. Für 2.600 Euro Aufpreis verfügt das Fahrzeug über autonome Fahrfunktionen. Damit fährt es bis 150 km/h automatisch, hält den Abstand zum Vordermann und die Spur, bremst im Stop-and-go-Verkehr bis zum Stillstand und fährt wieder los, ohne dass der Fahrer etwas tun müsste. Blinkt dieser, scannt das Model X, ob der Spurwechsel gefahrlos möglich ist, und wechselt dann von alleine die Spur. Eine Hand muss der Fahrer allerdings immer am Lenkrad lassen, sonst deaktiviert sich das System irgendwann von selbst. 
Nach Angaben von Tesla funktioniert der Autopilot auf Autobahnen und Landstraßen. Auch die autonomen Parkfunktionen sind im Paket enthalten. Weil die Hardware bereits an Bord ist, kann der Kunde den Autopiloten auch nachträglich freischalten per Software-Update, dann für 3.300 Euro. 
Tesla fahren ist generell kein günstiger Spaß: Die Basisversion des Model X mit dem Namen 70D kostet mindestens 93.000 Euro. Die zwei Ziffern weisen auf die 70 kWh große Batterie im Fahrzeug hin, die 400 Kilometer Reichweite gewährt. Das D steht für das Dual-Motor-Konzept: Allradantrieb durch einen E-Motor an der Vorder- und einen an der Hinterachse. Ausgehend vom Antrieb des Model S dürfte die Leistung hier bei deutlich über 300 PS liegen. Den Standardsprint absolviert der SUV in 6,2 Sekunden, 225 km/h wird er maximal schnell. Mit der größeren, 90-kWh-Batterie kommt der SUV auf 470 km Reichweite. 
Mit dem Model X als zweitem Modell im Portfolio dürfte sich Teslas Wachstumskurs in Deutschland fortsetzen. 2014 konnten die Amerikaner gut 800 Fahrzeuge absetzen, im vergangenen Jahr waren es bereits rund 1.600.  ""Es wird ein Jahr mit bedeutendem Wachstum für uns "", glaubt auch Deutschland-Geschäftsführer Jochen Rudat."	technik
"Das B in Apple steht für Bescheidenheit. Kein anderes Unternehmen der Branche beherrscht die Selbstbeweihräucherung, die Superlative und die Show so gut wie Apple. Das bleibt nicht ohne Wirkung. Mehrere Millionen Menschen haben das iPhone 6s oder 6s Plus bereits vorbestellt, ohne es jemals in der Hand gehabt zu haben. War das voreilig? Nach einigen Tagen mit den beiden neuen Modellen, die am 25. September in den Handel kommen, brauche ich nun ungefähr 2.000 Wörter für meine Antwort. 
Auf den ersten Blick sehen das iPhone 6s und das 6s Plus exakt so aus wie ihre Vorgänger. Sie sind 0,1 bis 0,2 Millimeter länger, breiter und dicker, aber das ist zu vernachlässigen. Sie sind allerdings spürbar schwerer. Das iPhone 6s wiegt 143 Gramm, das sind 14 Gramm mehr als das, was das iPhone 6 auf die Waage bringt. Das iPhone 6s Plus wiegt mit 192 Gramm sogar 20 Gramm mehr als der Vorgänger und ist damit mit Abstand das schwerste Smartphone, das ich je in der Hand und in der Hosentasche hatte. Ein schöner Brocken, aber eben ein Brocken. Das liegt vor allem am Display, wie The Verge herausgefunden hat. 
Das Gehäuse ist jetzt aus raumfahrterprobtem 7.000er Aluminium. Die Alu-Zink-Legierung soll wohl ein zweites Bendgate verhindern. Ob das klappt, darf gerne jemand anderes überprüfen. 
Update 1: Jemand hat es überprüft. Ergebnis: Das iPhone 6s ist verdammt stabil. 
An der Displaygröße und der Auflösung hat sich im Vergleich zum iPhone 6 nichts geändert; die neuen Modelle brechen also keine Pixeldichte-Rekorde. Aber High-End-Smartphones bewegen sich mit ihren ppi-Werten ohnehin seit Jahren in Bereichen, in denen das menschliche Auge praktisch keinen Unterschied mehr erkennen kann. Es kommt also auf andere Qualitäten an: Helligkeit, Kontrast, Farben. Und da ist die Android-Konkurrenz zum Teil an Apple vorbeigezogen, zumindest laut Laborwerten. Im Alltag ist die Situation aber weniger eindeutig. Im Vergleich zu Samsungs hochgelobtem AMOLED-Display im Galaxy S6 etwa zeigt das LC-Display des iPhone 6s Plus Farben etwas zurückhaltender, weniger leuchtend. Man könnte auch sagen: realistischer. Und heller erscheint das Galaxy S6 auch nicht, obwohl es das laut Labortest tun müsste. Das iPhone 6s hat also ein ausgezeichnetes Display, aber sicherlich nicht das insgesamt beste auf dem Markt. 
Auch hier ist Apple unter Zugzwang geraten: Die Kameras zum Beispiel im Samsung Galaxy S6 und im LG G4 sind denen im iPhone 6 ebenbürtig, mitunter überlegen, wie verschiedene Tester festgestellt haben. Das konnte Apple nicht auf sich sitzen lassen. Die Superlative, Sie wissen schon. 
Deshalb hat das iPhone 6s erstmals eine zwölf-Megapixel-Kamera, nachdem Apple sich jahrelang geweigert hatte, den Pixelwettbewerb der Konkurrenz mitzumachen und stets bei acht Megapixeln geblieben und damit gut gefahren war. Mehr Pixel allein garantieren aber noch keine besseren Fotos, zumal sie auf dem Sensor so eng zusammenliegen, dass es zu gewissen unerwünschten Überlappungseffekten beim Lichteinfall kommen kann – Crosstalk genannt. Apple will deshalb einen Weg gefunden haben, die einzelnen Pixel so zu isolieren, dass es keinen Crosstalk gibt. Samsungs Isocell lässt grüßen. 
Nach einem zugegebenermaßen nur amateurhaften Vergleich (siehe Fotostrecke) von iPhone 6s, 6s Plus, 6 und Samsung Galaxy S6 habe ich insgesamt den Eindruck, dass sich Apples Anstrengungen gelohnt haben. Die höhere Auflösung der 6s-Kamera im Vergleich zum iPhone 6 wird beim Zoomen natürlich schnell sichtbar. Die Kontraste wirken zudem schärfer. Die Farben erscheinen etwas kräftiger als beim iPhone 6 und natürlicher als beim Galaxy S6, das zumindest im Automatik-Modus einen gewissen Hang zur Überbelichtung hat. Das gilt für Aufnahmen in natürlichem wie auch künstlichem Licht. Kurz gesagt: Bessere Fotos macht derzeit wohl keine Smartphone-Kamera. 
Wie professionellere Tester urteilen, werde ich nachreichen. Was diese professionellen Tester aber in jedem Fall weiterhin vermissen werden, sind manuelle Einstellungen zu Verschlusszeit, Blendenöffnung und Iso-Werten in der Standard-App. Dafür braucht man externe Apps wie Manual und Camera+. Apple selbst vertraut fast vollständig auf seine Automatik, Nutzer können vor einer Aufnahme lediglich mit einem Schieberegler festlegen, ob ein Bild heller oder dunkler werden soll. 
Update 2: Die Kollegen von golem.de halten die Kamera des Galaxy S6 für besser. TIME dagegen hat das iPhone 6s einem Fotografen in die Hand gedrückt, damit er aus einem Hubschrauber heraus Foros von der New Yorker Skyline machen kann. (Hat da einer Medienkrise gesagt?) Sein Fazit: Sehr gute Bildqualität, sehr realistische Farben. 
Das iPhone 6s kann jetzt auch Videos in 4K-Auflösung aufnehmen. Samsung, HTC, LG und andere lassen grüßen. Im 6s Plus steckt zudem ein optischer Bildstabilisator auch für Videoaufnahmen. Den habe ich bei einer einhändigen Fahrradfahrt über das Tempelhofer Feld getestet, konnte aber keinen signifikanten Unterschied zum 6s feststellen. 
Eine Anmerkung zu 4K: Die günstigste Version des iPhone 6s hat nur 16 Gigabyte Speicher, abzüglich Betriebssystem und systemeigener Apps. Wer circa 30 Minuten lang in 4K aufnimmt, hat den Speicher komplett gefüllt. 
Die Frontkamera für Selfies und Facetime-Videochats wurde ebenfalls aufgewertet. Aus der 1,2-Megapixel-Kamera ist eine mit fünf Megapixeln geworden, damit auch iPhone-Selfies endlich eine nennenswerte Auflösung erreichen. Damit sie auch bei schlechteren Lichtverhältnissen gelingen, benutzt Apple das Display als Blitzlicht. Ein erster Blitz analysiert das Umgebungslicht, an das sich der Bildschirm dann beim folgenden eigenen Aufleuchten anpasst. LG lässt grüßen. 
Der Effekt: Selfies mit dem 6s in dunkler Umgebung sind überraschend dunkel. Und das ist auch gut so. Denn das iPhone 6 hellt solche Bilder zu stark auf. Das sieht meist unrealistisch aus und hat mehr Bildrauschen zur Folge, als das beim 6s der Fall ist. 
Und noch etwas hat sich Apple bei der Konkurrenz abgeschaut: Live Photos. In der Standardeinstellung speichern beide Kameras nicht nur ein Standbild, sondern auch die 1,5 Sekunden vor und nach dem Druck auf den Auslöser – natürlich nur, wenn der Fotograf das Motiv lange genug im Sucher hat. Das daraus entstandene Minivideo, das eher wie ein animiertes GIF mit Ton wirkt, schaut man sich an, indem man mit dem Finger etwas fester darauf drückt. Live Photos lassen sich aber auch auf einem MacBook mit Apples kommendem Betriebssystem OS X El Capitan betrachten. Dort wird es einen Live-Icon geben, über das Nutzer nur mit der Maus fahren müssen. 
 
In der Praxis sehen die bewegten Bilder selten so toll aus wie jene, die Apple während der iPhone-Vorstellung in San Francisco zeigte: Kinder, die sich in Pose werfen, ein Wasserfall, der im Bildhintergrund rauscht – so etwas muss der Fotograf planen und komponieren. Für schnelle Schnappschüsse eignen sich Live Photos eher nicht. Außerdem muss man das iPhone bei der Aufnahme extrem ruhig halten oder am besten gleich ein Stativ nutzen. Denn ein wackeliges Dreisekundenvideo sieht meist einfach nur dilettantisch aus. 
Gespeichert wird neben dem Live Photo übrigens immer auch das einzelne Standbild. Und Abschalten lässt sich die Funktion im iPhone 6s auch. Apple ist nicht das erste Unternehmen, das Fotos zum Leben erwecken will: HTC lässt ... Sie wissen schon. Zoe heißt die Kamerafunktion bei den Taiwanern, eingeführt wurde sie mit HTC One vor zweieinhalb Jahren. 
Live Photos ist eine Funktion, die sozusagen die rechte Maustaste des iPhone 6s benutzt: 3D Touch heißt die Technik, sie ist die Weiterentwicklung von Force Touch in der Apple Watch und im MacBook-Trackpad. Die Idee: Das Display unterscheidet, wie stark jemand drückt, was dann verschiedene Effekte auslöst, verbunden mit einem haptischen Feedback. 
Ein etwas festerer Druck auf ein App-Icon öffnet kurze Menüs mit bis zu vier direkten Links auf typische Aktionen. Die Mail-App zum Beispiel gibt mir unter anderem die Möglichkeit, eine neue Mail zu schreiben, ohne dass ich die App vorher öffnen muss. Quick Actions heißen die Abkürzungen bei Apple. 
Innerhalb der App bewirkt ein fester Druck auf einen Link in einer Mail, dass sich die entsprechende Website in einem Pop-up-Fenster öffnet, sozusagen als Vorschau. Ein zweiter fester Druck öffnet den Link im Safari-Browser. Peek und Pop nennt Apple das – Reinschauen und Aufmachen. 
Rund 20 System-Apps unterstützen 3D Touch zunächst, darunter wie erwähnt die Kamera, Notizen, der App Store, Kontakte, Musik und der Kalender. Quick Actions sparen allenfalls Sekundenbruchteile, haben aber den Vorteil, dass sich häufig benötigte Aktionen mit einem Daumen durchführen lassen, ohne dass man auf winzige Buttons zielen muss. Wirklich etwas zügiger funktioniert der Wechsel zwischen verschiedenen Apps mit Peek und Pop. 
Kreative Einsatzmöglichkeiten für 3D Touch werden sicherlich externe App-Entwickler finden. In Games etwa. Bisher ist nur ein Spiel bekannt, das 3D Touch einsetzen wird: Warhammer 40.000: Freeblade. 
Noch ist die Technik insgesamt ein wenig unterwältigend. Das Prinzip ist zwar einleuchtend und recht intuitiv. Was aber fehlt, ist die Möglichkeit, es zu konfigurieren. Ich würde gerne eine eigene Auswahl von bis zu vier Shortcuts pro App zusammenstellen können, oder die Auswahl weglassen und stattdessen eine einzige Aktion festlegen, die mit dem festen Druck auf ein Icon sofort ausgelöst wird. Leider werden das auch Entwickler von Drittanbieter-Apps nicht einrichten können, wie Apple auf Nachfrage mitteilt. Nur innerhalb von Apps sind sie freier, was die Ausgestaltung von 3D Touch angeht. 
Dennoch hat die Technik Potenzial. Genug jedenfalls, das auch andere Hersteller das Prinzip aufgreifen werden. Huawei tut das bereits. 
Es gibt vier Gründe, warum das iPhone 6s schneller sein kann als das iPhone 6. Der erste ist der neue Chip, A9 genannt. Apple gibt an, dass seine CPU-Leistung bis zu 70 Prozent über der des A8 liegt, seine GPU-Leistung sogar 90 Prozent. Im Test mit zwei verschiedenen Benchmark-Apps kann ich letzteres nachvollziehen, ersteres nicht ganz. 
Der zweite Grund ist LTE Advanced: Damit ist es theoretisch möglich, eine Bandbreite von bis zu 300 Megabit pro Sekunde zu erzielen – doppelt so viel wie im Vorgänger. In der Praxis müssen deutsche Kunden dafür vorerst aber bestimmte Mobilfunkverträge bei der Telekom abschließen, die als bisher einziger Anbieter bis zu 300 Mbit/s im Downstream anbietet, und das auch nur in einigen Gebieten und unter optimalen Bedingungen. Bei Vodafone sind maximal 225 Mbit/s möglich. 
Drittens sollen im WLAN nun bis zu 866 Mbit/s möglich sein, auch das ist doppelt so viel wie bisher. Und auch das ist ein theoretischer Wert, der nur selten wirklich erreicht werden dürfte. Aber immerhin lädt das iPhone 6s Websites im Test deutlich schneller als das iPhone 6 und auch als Samsungs Galaxy S6. 
Ein Hinweis an dieser Stelle zum Betriebssystem iOS 9: Standardmäßig voreingestellt ist die neue Funktion WLAN Assist. Bei schlechtem Empfang über WLAN schaltet sie automatisch auf die Datenübertragung per Mobilfunk um. Wer das in den Einstellungen ganz unten unter Mobiles Netz nicht deaktiviert, erschöpft unter Umständen sein Datenvolumen ziemlich rasch. 
Viertens erlaubt iOS 9 erstmals den Einsatz von Werbe- und Trackingblockern im Safari-Browser. Was zwischenzeitlich zur Folge hatte, dass gleich drei kostenpflichtige Blocker die Top-Charts im App Store anführten. Ich habe auch einen installiert – und er funktioniert bestens. Websites laden in Safari noch einmal merklich schneller, zudem geht die Werbung nicht mehr vom monatlichen Datenvolumen ab. Wer unterwegs viel surft, entsprechend schnell an die Volumengrenze stößt und dann nachbucht, kann mit einem Adblocker also Geld sparen. 
An dieser Stelle muss ich als Angestellter eines werbefinanzierten Onlineangebots natürlich darauf hinweisen, dass Sie meinen Arbeitsplatz gefährden, wenn Sie Adblocker verwenden. Einer der Anbieter hat das mittlerweile eingesehen und seinen Adblocker wieder aus dem App Store entfernt. Aber Sie können ja in den Filterregeln eine Ausnahme für ZEIT ONLINE und alle anderen Medienangebote einrichten, die Ihnen am Herzen liegen. 
Obwohl die Akkus im iPhone 6s und 6s Plus etwas kleiner sind als die in den Vorgängermodellen, sollen sie länger halten, verspricht Apple. Der Prozessor und iOS 9 sollen das möglich machen. 
Meine Testgeräte hielten bei zurückhaltender Nutzung zwei 18-Stunden-Tage durch, bevor der Akku leer war. Zurückhaltend heißt: Automatische Helligkeit des Displays, Browsen mit Safari in der S-Bahn, regelmäßige Twitternutzung, das eine oder andere Foto, ein paar Dutzend E-Mails und Kurznachrichten und vielleicht noch ein wenig mehr, und nachts muss das Gerät ausgeschaltet sein. Intensivere Nutzung, also eine Stunde 4K-Videos streamen, eine Stunde Asphalt 8 spielen sowie ein Benchmark-Test zusätzlich zum vorherigen Programm lassen den Akkustand innerhalb von 18 Stunden auf unter 30 Prozent sinken – das dürfte als akzeptabler Wert durchgehen. 
Für den Notfall gibt es in iOS 9 einen Stromsparmodus. Der schaltet unter anderem diverse Hintergrundprozesse ab, drosselt den Prozessor und dimmt das Display bei Inaktivität schnell ab. So hält der Akku noch ein paar Stunden länger durch. 
Das iPhone 6s kostet zwischen 739 und 959 Euro und damit mindestens 140 bis 250 Euro mehr als die derzeit besten Android-Smartphones. Das 6s Plus ist mit 849 bis 1.069 Euro noch teurer. Die günstigsten Versionen mit nur 16 Gigabyte Speicher sind zudem eigentlich nicht mehr zeitgemäß, zumal iPhones keinen Steckplatz für SD-Karten haben. Apple-Fans hat das alles aber noch nie abgeschreckt. Alle anderen müssen für sich entscheiden, ob sie den Apple-Aufschlag bezahlen wollen oder für deutlich weniger Geld eines der in vielerlei Hinsicht ebenbürtigen Android-Topgeräte kaufen. 
Am iPhone 6s und 6s Plus jedenfalls mag vieles nicht ganz so niedagewesen sein, wie das Unternehmen behauptet. Das ist aber auch nicht entscheidend. Wichtiger ist, dass die Elemente, die neu im iPhone sind, auch funktionieren. Und das tun sie. 
3D Touch löst zwar keinen rechten Wow-Effekt aus, weil es effektiv kaum Zeit spart und sich nicht individuell anpassen lässt. Aber es ist eine einleuchtende, intuitive Erweiterung von Multitouch. Wer es ein paar Hundert Mal benutzt hat, wird sich fragen, wieso es das bisher noch nicht in einem Smartphone gab. 
Sehr erfreulich ist auch die Kamera. Menschen, die hinreichend wohlhabend und betriebssystemagnostisch sind und einfach nur eine möglichst gute Kamera in ihrem Smartphone haben wollen, wären allein deshalb bestens mit dem iPhone 6s bedient. Allerdings müssen sie auf manuelle Einstellungen verzichten können. Auch bei Apple ist eben immer noch Luft nach oben."	technik
"Nach fünfeinhalb Jahren hat der Ölpreis wieder die Marke von 50 US-Dollar unterschritten: Am Mittwoch kostete ein Fass Brent aus der Nordsee (159 Liter) zwischenzeitlich 49,66 Dollar, so wenig wie zuletzt im Mai 2009. Auch die US-Sorte WTI war für weniger als 50 Dollar je Fass zu haben. Seit Sommer haben sich die Ölpreise mehr als halbiert – im Juni kostete etwa ein Fass Brent-Öl noch rund 115 Dollar. 
Was Aktionäre von Erdölkonzernen besorgt, erfreut die Autofahrer: Parallel zum Ölpreis fällt auch der Spritpreis. Ein Liter Super E10 kostete laut Mineralölwirtschaftsverband im Schnitt zuletzt rund 1,27 Euro, ein Liter Diesel etwa 1,15 Euro. Wie teuer die Kraftstoffe noch vor einem halben Jahr waren, zeigt die Infografik, die das Portal Statista für ZEIT ONLINE erstellt hat: Zum Jahreshoch 2014 im Juni lag der vom Automobilclub ADAC ermittelte Durchschnittspreis für E10-Superbenzin bei 1,558 Euro je Liter, der für Diesel bei 1,385 Euro. 
 ""Preiswerter als 2014 war Tanken zuletzt im Jahr 2010 "", ist das Fazit des ADAC. Gebremst wird der Effekt allerdings durch die Schwäche des Euros: Hätte er gegenüber dem Dollar nicht ebenfalls deutlich an Wert verloren, wären die Preise an den Tankstellen noch stärker gesunken. Denn Rohöl wird in Dollar gehandelt, und mit einem schwächeren Euro wird der Import von Öl und Derivaten teurer. 
Für das günstige Öl gibt es zahlreiche Gründe. Einer davon ist das hohe Angebot auf dem Weltmarkt, ausgelöst durch den Schieferöl-Boom in den USA. Außerdem halten die Opec-Länder weiterhin an ihren hohen Förderquoten fest, früher drosselte etwa Saudi-Arabien bei einem Überangebot seine Mengen. Zudem wächst die Wirtschaft in wichtigen Regionen wie Europa und China nur verhalten, die Nachfrage ist entsprechend schwach. 
Eine schnelle Trendwende ist derzeit nicht abzusehen. Im Dezember prognostizierte die Internationale Energieagentur IEA, dass sich das Überangebot in der ersten Jahreshälfte 2015 sogar noch vergrößern wird. Die Fachleute gehen zudem von einer sinkenden Nachfrage aus. Das wird sich auch im Ölpreis widerspiegeln: Inzwischen halten Experten einen Preis von 40 US-Dollar je Fass Rohöl für immer wahrscheinlicher."	technik
"Bild schlug Alarm:  ""Benzinpreis explodiert: Bald 1 Mark! "" Die Schlagzeile vom 1. November 1973 taucht heute auf Humor-Websites auf, weil das Benzin längst dreimal so viel kostet. Damals jedoch war die Angst groß, die Ölkrise könnte das Land lähmen. Jahrelang hatte sich außer ein paar Politologen kaum jemand den Kopf darüber zerbrochen, dass der wichtigste Rohstoff für die Wirtschaft der Industrieländer zu einem großen Teil aus politisch nicht besonders stabilen Regionen kam. Und plötzlich kam der Konflikt im Nahen Osten an deutschen Tankstellen an. 
Ägypten und Syrien hatten am 6. Oktober 1973, dem jüdischen Feiertag Jom Kippur, Israel angegriffen, um den Sinai und die Golanhöhen zurückzuerobern. Weil vor allem die USA, aber auch andere westliche Länder Israel politisch unterstützten, drosselte die arabisch dominierte Organisation der Erdöl exportierenden Länder (Opec) die Fördermengen drastisch. Der Ölpreis stieg rasch von rund drei US-Dollar pro Barrel (159 Liter) auf mehr als fünf Dollar, im folgenden Jahr sogar auf mehr als zwölf US-Dollar – heute sind es um die 100 Dollar. 
Gegen die USA und die Niederlande hatte die Opec wegen derer angeblich so israelfreundlicher Politik sogar einen Boykott verhängt.  ""Die Araber drehen uns den Hahn zu "", titelten auch deutsche Zeitungen und fragten:  ""Gehen in Europa die Lichter aus? "" Selbst die sonst so besonnene ZEIT sorgte sich, ob  ""Ein Zeitalter des Mangels? "" bevorstehe. 
Europa steckte ohnehin in einer Wirtschaftskrise. Den Regierenden war schon im Herbst 1973 klar: Das teure Öl würde die Lage zusätzlich verschlimmern. Unternehmen drohte die Pleite, sie entließen Beschäftigte oder fuhren Kurzarbeit. 
Die Bundesregierung versuchte gegenzusteuern, unter anderem mit einer Kampagne zum Energiesparen. Und am 9. November verabschiedete der Bundestag im Eiltempo das Energiesicherungsgesetz. Es erlaubte die sonst nur für den Verteidigungsfall vorgesehene Rationierung von Öl und Benzin und verhängte ein Tempolimit von 100 Kilometern pro Stunde auf Autobahnen, 80 auf Land- und Bundesstraßen. 
Außerdem ordnete das Gesetz ganztägige Fahrverbote an, bis dahin undenkbar im Land von Mercedes, Opel und Volkswagen. Allerdings beschränkte sich die Regierung von Bundeskanzler Willy Brandt (SPD) zunächst auf vier autofreie Sonntage: am 25. November und den drei folgenden Adventssonntagen. Rechtzeitig zum Familienbesuch an Weihnachten sollte das Fahrverbot wieder vorbei sein. 
Weil die DDR im Rat für gegenseitige Wirtschaftshilfe mit Russland und den sozialistischen  ""Bruderstaaten "" von günstigeren Preisen profitierte, blieb die Ölkrise in den Siebzigern aus. Die Chemieindustrie profitierte davon, preiswerter an Erdöl-basierte Chemierohstoffe und Treibstoff zu kommen. Erst Anfang der achtziger Jahre kehrte sich die Situation um. Jetzt setzte Ostdeutschland verstärkt auf heimische Braunkohle. Die Staatsbahn setzte sogar wieder alte kohlebefeuerte Dampflokomotiven auf die Schienen. 
In Österreich galt ein radikaleres Fahrverbot: Jedes Auto musste einen Tag pro Woche aussetzen. Ein Aufkleber auf der Windschutzscheibe zeigte an, an welchem Wochentag es in der Garage bleiben musste. Im Februar wurden einwöchige Zusatzferien zwischen den Schulhalbjahren eingeführt, die heute noch als  ""Semesterferien "" existieren, umgangssprachlich mancherorts noch  ""Energieferien "" genannt werden. Bundeskanzler Bruno Kreisky (SPÖ) rief sogar die Männer dazu auf, sich stromsparend nass zu rasieren. 
Ganz leer blieb es nicht auf den Straßen. Taxis, Linienbusse und natürlich Ärzte, Polizei und Rettungsdienste waren vom Verbot ausgenommen. Und wer nachweisen konnte, dass er zum Beispiel verderbliche Ware zu fahren hatte, konnte eine Ausnahmeerlaubnis beantragen. Weil sich am ersten auto- und motorradfreien Sonntag ganze 1.300 Fahrer ohne einen solchen Schein erwischen ließen, wurde das Bußgeld für den 2. Dezember von 80 auf 500 Mark erhöht. Jetzt ertappten die Kontrolleure nur noch 222 Verbotssünder. 
 
In Essen demonstrierte die kommunistische DKP gegen das Fahrverbot: An den hohen Benzinpreisen sei nur die Profitgier der Ölmultis schuld, diese seien zu enteignen. Den geplanten Autokorso allerdings musste die DKP sein lassen. 
Für die meisten Deutschen war der erste autofreie Sonntag vor allem ein Wandertag, der zu ausgedehnten Spaziergängen und Radtouren auf leeren Autobahnen und Landstraßen einlud. Der Bürgermeister der Gemeinde Künzell bei Fulda rief seine 12.000 Bürger zu einem  ""Ölsparwandertag "" auf; erfolgreiche Wanderer bekamen am Ziel einen vollen Kanister Benzin. Die Wanderlust nahm allerdings schnell wieder ab. An den folgenden Sonntagen beantragten mehr Fahrzeughalter Ausnahmegenehmigungen. Am 16. Dezember gab es sogar wieder einzelne Staus. 
Schätzungen zufolge brachte das Sonntagsfahrverbot eine Ersparnis beim Benzinverbrauch von 7 bis 12 Prozent. Doch vor allem führten die leeren Straßen die Abhängigkeit von Energie – und das bedeutete damals vor allem fossile Brennstoffe – deutlich vor Augen. Laut einer Umfrage schränkten auf die Sparappelle der Regierung hin tatsächlich 70 Prozent der Bevölkerung ihren Energieverbrauch ein. Viele sparten Strom, drosselten die Heizung und hielten das Tempolimit ein. 
Erst allmählich lernten die westlichen Staaten aus den Ölkrisen der 1970er Jahre und reduzierten ihre Abhängigkeit von arabischem Erdöl. Sie trieben die Offshore-Förderung etwa in der Nordsee voran, investierten in Atomkraftwerke, regenerative Energiequellen und effizientere Energienutzung. Zudem legten sie strategische Reserven an, um weniger erpressbar zu sein. Auch, weil sich Produktionsprozesse verändert haben, ist die Weltwirtschaft heute weniger anfällig für Ölpreisschwankungen. 
Die US-Regierung erwog 1973 offenbar einen Militärschlag gegen Ölförderstaaten für den Fall, dass sich die Krise verschärfen sollte. Das geht aus britischen Dokumenten vor, die im Jahr 2003 veröffentlicht wurden und bis dahin geheim waren. Nach damaliger Einschätzung der britischen Geheimdienste bereitete Washington einen Plan vor, in Saudi-Arabien und Kuwait mit Luftlandetruppen Ölförderanlagen anzugreifen und zu besetzen. Großbritannien rechnete damit, zur Unterstützung aufgefordert zu werden. 
In zahlreichen Kommunen und Regionen gibt es heute wieder autofreie Sonntage. Oft haben sie einen touristischen Aspekt wie die Sperrung der Straßen in den Flusstälern der Mosel ( ""Happy Mosel "") oder des Rheins ( ""Tal Total ""). Eine Initiative, die einen autofreien Sonntag in Berlin durchsetzen wollte, scheiterte. 
Viele Kommunen in Europa beteiligen sich am Autofreien Tag am 22. September. In Deutschland rufen viele Verbände für den dritten Sonntag im Juni zum Aktionstag  ""Mobil ohne Auto "" auf. 
Doch der Jom-Kippur-Krieg war nach wenigen Wochen schon wieder vorbei und mündete in Friedensverhandlungen. Am ersten Weihnachtsfeiertag verkündete Kuwait, die Ölförderung werde wieder erhöht. Und damit war auch Schluss mit dem Fahrverbot. 
Zunächst erwog die Bundesregierung noch, sonntags abwechselnd Autos mit geraden und ungeraden Nummern auf dem Kennzeichen fahren zu lassen, dann verschwand auch diese Idee. Im Gegensatz zu anderen europäischen Staaten schaffte Deutschland sogar das Tempolimit auf Autobahnen wieder ab. Es war, als hätte es die Ölkrise nie gegeben. 
Diese Vergesslichkeit rächte sich schon wenige Jahre später: 1979 stürzte der zweite Ölpreisschock, ausgelöst von der islamischen Revolution im Iran und dem Ersten Golfkrieg, Europa und die USA in eine tiefe Wirtschaftskrise. An eine Neuauflage von Fahrverbot und Tempolimit traute sich die sozialliberale Koalition unter Helmut Schmidt (SPD) jedoch nicht heran."	technik
"Und sie bewegt sich doch, die Union! Im März 2015 hatte die große Koalition einen Gesetzentwurf zur Änderung des Telemediengesetzes vorgestellt, mit dem Ziel, Hotspot-Betreibern die nötige Rechtssicherheit zu geben und so für mehr offene WLANs in Deutschland zu sorgen. Schnell war klar, dass der Entwurf genau das verhindern würde, weil er den WLAN-Anbietern bestimmte Maßnahmen vorgeschrieben hätte, um Rechtsverstöße der Nutzer zu unterbinden. Diese Maßnahmen hätten dazu geführt, dass WLANs nicht wirklich offen und Anbieter nicht wirklich rechtlich abgesichert gewesen wären. Aber es war eben das, was mit der Union machbar war. 
Jetzt haben sich die Berichterstatter und Sprecher der zuständigen Arbeitsgruppen im Bundestag darauf geeinigt, diese Maßnahmen ersatzlos aus dem Gesetzentwurf zu streichen und damit die unselige Störerhaftung, die es so nirgendwo sonst gibt, endlich zu beerdigen. Auch die Union zieht plötzlich mit. Im Parlament hat sich damit eine Mehrheit gefunden, die es im Kabinett nicht gab. 
Alles, was es letztlich brauchte, um aus einem völlig sinnlosen einen sinnvollen Gesetzentwurf zu machen, waren die Argumente von Opposition und den Netzpolitikern aus der eigenen Koalition, von Verbraucherschützern, Freifunkern, den Bürgerrechtlern der Digitalen Gesellschaft, dem Einzelhandel, Telekommunikationsanbietern, vom Bundesrat, der EU-Kommission, von den Branchenverbänden eco und Bitkom, vom zuständigen Generalanwalt am Europäischen Gerichtshof, von den Fachleuten im Wirtschafts, Justiz- und im Innenministerium sowie ein angebliches Machtwort der Bundeskanzlerin. Mehr nicht. 
Von zentraler Bedeutung war wohl das Gutachten des EuGH-Generalanwalts vom März in einem Fall zur Störerhaftung. Laut dieses Gutachtens sind zumindest Gewerbetreibende, die der Öffentlichkeit ein kostenloses WLAN zur Verfügung stellen, nicht für das verantwortlich, was die Nutzer damit tun. 
Die Richter am EuGH folgen den Einschätzungen der Generalanwälte meistens. Deshalb war absehbar, dass sie mit ihrem Urteil den Gesetzentwurf der Bundesregierung als europarechtswidrig dastehen lassen würden – zumal das deutsche Recht nicht zwischen gewerblichen und privaten Hotspot-Anbietern unterscheidet. Diese Peinlichkeit wollten sich möglicherweise auch die letzten Widerständler ersparen. 
Wenn es jetzt so läuft, wie es sich die Fachpolitiker im Bundestag wünschen, wird der geänderte Entwurf schon in der kommenden Sitzungswoche beschlossen und tritt, sofern der Bundesrat zustimmt, im Herbst in Kraft. Das alles hätte auch ein Jahr schneller gehen können, aber die Netzpolitik der großen Koalition ist eben nicht von Tempo geprägt. Immerhin: Es bewegt sich was, und jetzt auch noch in die richtige Richtung."	technik
"Selten haben sich Computer so schön geirrt wie zuletzt die in Googles Labor. Mitte Juni hatten Google-Forscher eindrucksvoll demonstriert, welch fantastische Bilder künstliche neuronale Netze erzeugen, wenn man ihnen ihre verborgenen Suchmuster entlockt. Die surrealen Werke gingen um die Welt, woraufhin Google einen Code veröffentlichte, mit dem sich diese Netzwerke auch am heimischen Rechner simulieren lassen. 
Seither gibt es geradezu einen Wettstreit um die spektakulärsten Foto- und sogar Videomanipulationen, auf Twitter werden sie verbreitet unter dem Hashtag #DeepDream. Ein wenig in den Hintergrund gerückt ist die Frage, was die Google-Forscher mit ihren  ""Traumbildern "" eigentlich bezweckten. Dabei ist die Antwort ebenso faszinierend: Sie wollten besser verstehen, wie die von ihnen selbst geschaffene künstliche Intelligenz überhaupt funktioniert. 
Inception heißt das Netzwerk, das die zauberhaften Welten geschaffen hat. Im vorigen Jahr hat Inception den Large Scale Visual Recognition Challenge gewonnen, in dem neuronale Netze darum wettstreiten, die meisten Fotos richtig zu klassifizieren; ist es ein Gesicht, ein Hund, ein Vogel, ein Auto, eine Landschaft? Bei dieser Sortierarbeit setzte das Netzwerk von Google  ""einen neuen Standard "", wie die Schöpfer von Inception stolz in einem Fachartikel schrieben. 
 ""Dabei gibt es die Art von Netzwerken schon lange, "" sagt Aditya Khosla, Forscher am Labor für Computerwissenschaften und künstliche Intelligenz des Massachusetts Institute of Technology (MIT). Khosla hat ein zweites Netzwerk mitentwickelt, das die Google-Forscher neben Inception verwendet haben, um ihre Traumbilder zu erschaffen. Er weiß also genau, was neuronale Netzwerke können.  ""Klar, es gab in den letzten paar Jahren massive Verbesserungen bei der Kategorisierung von Objekten, neuerdings auch einige von Google und Facebook. Aber wirklich neu oder überraschend ist an der Visualisierung von Google nichts "", sagt er. 
Das Konzept neuronaler Netze erdachten die Neurowissenschaftler Warren McCullogh und Walter Pitts von der Universität Chicago im Jahr 1943. Statt Transistoren schlugen McCullogh und Pitts als Recheneinheiten künstliche Nervenzellen (Neurone) vor, die miteinander zu Schaltkreisen verbunden sind. Anders als bei Transistoren, die Nullen und Einsen verrechnen, schicken künstliche Neurone erst dann ein Signal ab, wenn die Summe ihrer Inputs einen gewissen Schwellenwert überschreitet. Sie arbeiten also nicht mit binärer Logik, wie jeder heutige Computer, sondern mit Schwellenwert-Logik. Allerdings werden heutige künstliche neuronale Netze auf Computern simuliert, die binäre Logik stellt also die Schwellenwert-Logik dar. 
Wer verstehen will, wie Inception und ähnliche Netzwerke arbeiten, schaut sich am besten den generellen Aufbau künstlicher neuronaler Netze an. Ihre Architektur folgt fast immer denselben Prinzipien: Hunderte oder Tausende künstliche Neuronen sitzen in übereinanderliegenden Schichten und sind über (simulierte) Leitungen verbunden. Ein Neuron kann die Nachbarn in seiner eigenen Schicht und Zellen der darüberliegenden Schicht über seine Leitungen aktivieren. Die oberste oder Input-Schicht funktioniert gleichsam als Sensor, der mit jenen Daten gefüttert wird, die das Netzwerk sortieren soll. Im Fall von Inception sind das Bilder, in anderen Netzwerken können das aber auch Geräusche sein. Jeder Bildpunkt aktiviert genau ein Neuron in der Input-Schicht. Die tiefste oder Output-Schicht dagegen hat meist nur eine Handvoll Neurone, für jede Bild-Kategorie eins. Diese Neurone zeigen an, zu welcher Kategorie ein Bild gehört, das der Input-Schicht präsentiert wurde. 
Doch bevor so ein Netzwerk diese Aufgabe gut erledigt, muss es trainiert werden.  ""Mit jedem Bild läuft eine Aktivitätswelle durch das ganze Netzwerk, von der Input- zur Output-Schicht "", sagt Khosla.  ""Wenn die Output-Schicht das Bild aber der falschen Kategorie zuordnet, bekommt sie das mitgeteilt und schickt ein Fehler-Signal zurück durch das Netzwerk. Das nennt man beaufsichtigtes Lernen. "" Das Fehler-Signal führe dazu, das sich die Leitungen zwischen den Neuronen im gesamten Netzwerk so anpassen, dass der Fehler weniger wahrscheinlich wird. Durch diese Fehlerbehebung werden die künstlichen Neurone sozusagen auf bestimmte Bildeigenschaften abgerichtet. 
Googles Inception gehört zu den sogenannten Konvolutionsnetzwerken. In dieser Art von Netzwerk reagieren die Neurone der zweiten Schicht nach erfolgreichem Training auf Hell-Dunkel-Kontraste in einer bestimmten Orientierung, also etwa auf die geraden Kanten eines Hausumrisses oder von Fenstern. Die nächsttiefere Schicht reagiert auf Kombinationen dieser Kanten, zum Beispiel hausähnliche Konturen und so weiter. Je tiefer die Schicht im Netzwerk, desto komplexer sind die Strukturen im Bild, auf die dann die Neurone reagieren. 
Bei tiefen neuronalen Netzwerken (Deep Neural Networks) wie Inception mit seinen 22 Schichten ist jedoch oft nicht klar, auf welche Formen genau sich die Neurone in den tieferen Schichten einschießen. Das ist der Punkt, an dem die Google-Forscher nicht genau vorhersagen können, was ihre künstliche Intelligenz tut. Im neuronalen Netz sind zwar alle Regeln definiert und mathematisch simpel. Jedes Neuron aber vollführt eine nicht-lineare Funktion: Es kann auf kleine Input-Änderungen mit starken Output-Änderungen reagieren. So verhält sich das gesamte Netzwerk auf nicht-lineare Weise, und es lässt zwar in der Summe, aber nicht im Einzelnen vorherberechnen, was im Netzwerk beim Training geschieht. 
 ""Deshalb haben viele Gruppen in den letzten Jahren Methoden entwickelt, um die Eigenschaften der tiefen Neurone visuell darzustellen, "" sagt Khosla. So wollen sie verstehen, wann und wie Fehlinterpretationen ausgelöst werden. ""Dabei zeigt man einem gut trainierten Netzwerk ein Bild und lässt die Aktivitätswelle durch das Netz laufen. Anstatt aber ein Fehlersignal zurückzuschicken, überaktiviert man die Zellen in jener tiefen Schicht, deren Eigenschaften einen interessieren. Das so manipulierteSignal läuft dann durch das Netzwerk zurück bis zur Input-Schicht. "" So werden im Originalbild genau jene Eigenschaften überzeichnet, die die tiefe Schicht am stärksten angesprochen haben. 
 
So erzeugen auch die Google-Programmierer ihre  ""Deep-Dream ""-Bilder, mit einer kleinen Abwandlung: Sie füttern dem Netzwerk das überzeichnete Bild mit den visualisierten Eigenschaften der gewählten Schicht erneut. Die Neurone in der Schicht bekommen also in dem überzeichneten Bild mehr von den Formen zu  ""sehen "", auf die sie ohnehin schon spezialisiert waren, und überzeichnen diese Eigenschaften ein weiteres Mal. Dieser Prozess wird so lange wiederholt, bis die Formen, welche die gewählte Schicht  ""erkennt "", ganz klar auf dem Originalbild erscheinen. 
So macht das Visualisierungswerkzeug von Google aus Wolken Vögel und aus Bäumen Gebäude. Ist eine Schicht im Netzwerk auf Vogelformen spezialisiert, reagiert sie auf alles im Wolken-Bild was auch nur im geringsten nach einem Vogel aussieht. Mit jedem Update des Bildes wird dann die Region im Originalbild, die nach einem Vogel aussieht, etwas mehr zum Vogel umgezeichnet. Nach vielen Wiederholungen ersteht so aus einer Wolke etwas, das an einen Kakadu erinnert. 
 ""Dadurch sehen die Bilder spektakulär aus, "" sagt Khosla.  ""Ich würde diese Methode aber eher als Spielzeug bezeichnen. "" Google leiste zwar sehr gute Arbeit bei der Kategorisierung von Bildern, aber eine große Errungenschaft sei die Visualisierung nicht.  ""Mir jedenfalls hat das in meinem Verständnis von Konvolutionsnetzwerken nicht sehr geholfen. "" 
Die Methode, tiefe Schichten zu aktivieren, um die Lieblingsformen der dort versteckten Neurone zu überzeichnen, wurde auch nicht von Google erfunden, sondern von einer Forschungsgruppe aus Oxford und im Jahr 2013 erstmals vorgestellt.  ""Solche oder ähnliche Bilder gibt es schon länger. Ich glaube sie sind erst jetzt so bekannt geworden, weil es Google war, das sie vorgestellt hat. Menschen außerhalb meines Forschungsgebietes sehen in den Bilder etwas Spektakuläres, auch weil der Name Inception an den gleichnamigen Kinofilm erinnert. Innerhalb der Forschungsgemeinde beeindruckt das aber eigentlich niemanden. "" 
Bedenken, die Bilder der Netzwerk- ""Träume "" deuteten darauf hin, dass Maschinen zum menschlichen Geist aufschließen, seien ebenfalls unangebracht. Zwar ließ sich Professor Kunihiko Fukushima aus Tokio, als er Anfang der achtziger Jahre die Konvolutionsnetzwerke vorschlug, vom Aufbau des visuellen Gehirns inspirieren. Aber von denkenden Maschinen mit Bewusstsein und Intention sind Netzwerke wie Inception noch immer so weit entfernt wie zu Zeiten Fukushimas "", sagt Khota."	technik
"Eine Führung durch eine Gedenkstätte und dazwischen Besucher mit dem Smartphone auf Pokémon-Jagd – gegen dieses Szenario wehren sich die ersten Einrichtungen in den USA, seit der Hype um die neue Augmented-Reality-App Pokémon Go begonnen hat. 
So forderte der Nationalfriedhof Arlington in Washington die Besucher bei Twitter auf, die Suche nach den kleinen Monsterfiguren zu unterlassen, weil es unangemessen sei. Auch das Holocaust-Museum in der US-Hauptstadt appellierte an die Besucher, respektvoll beim Einsatz der Technik zu sein. Man versuche, den Ort aus dem Spiel entfernen zu lassen, sagte ein Sprecher dem Sender NPR. 
Bei der App sind Pokémon-Figuren an diversen Orten platziert, ein Spieler muss in der Nähe sein, um sie auf dem Smartphone-Bildschirm zu sehen und fangen zu können. Die Entwickler, Nintendos Pokémon Company und die Spielefirma Niantic Labs, haben aber die Möglichkeit geschaffen, unangemessene Orte zu melden. 
Niantic, damals noch als Google-Tochter, hatte im vergangenen Jahr schon ähnlichen Ärger in Deutschland. In dem ebenfalls ortsbasierten Spiel Ingress wurden einige der Portale, um die Nutzer kämpfen müssen, bei ehemaligen Nazi-Konzentrationslagern platziert. Nach Kritik wurden sie schnell aus der Nähe der Gedenkstätten entfernt. Pokémon Go greift auch auf Datenbanken von Ingress zurück. 
In Deutschland ist Pokémon Go ab sofort im Play Store von Google für Android-Geräte sowie in Apples App Store für iOS-Geräte verfügbar. 
Unterdessen meldete die Polizei der A&M University in Texas auch einen Auffahrunfall im Zusammenhang mit dem Spiel. Ein Fahrer habe seinen Wagen regelwidrig abgestellt, um auszusteigen und ein Pokémon zu fangen, erklärte die Polizei bei Twitter. Ein anderes Auto sei von hinten aufgefahren. 
In Amsterdam forderte das Akademisch-medizinische Zentrum (AMC) Besucher auf, beim Spielen von Pokémon Go nicht mehr in nicht-öffentliche Bereiche des Krankenhauses einzudringen. Einige Spieler hätten sich sogar im Keller des Krankenhauses wiedergefunden, wo unter anderem Kleidung desinfiziert wird.  ""Es gibt tatsächlich ein krankes Pokémon im AMC, aber wir sorgen gut für es. Bitte besucht es nicht "", teilte die Klinik über den Kurznachrichtendienst Twitter mit. Beigefügt war ein Bild der Pokémon-Figur Pikachu neben einer Taschentücherbox. Die niederländische Bahnaufsicht ProRail bat Nintendo außerdem um Änderungen an der App, nachdem manche Spieler unwissentlich auf Gleise geraten waren. 
Eine erste Aktualisierung von Pokémon Go wurde bereits vorgenommen, um die übermäßigen Zugriffsrechte auf Google-Profile einzuschränken. Am Vortag war herausgekommen, dass das Spiel bei der Anmeldung mit Google den Zugriff auf alle Inhalte der Konten anfragte. Die App-Entwickler von der Firma Niantic Labs erklärten, das gehe auf einen Fehler zurück und es seien nie andere Informationen als Nutzername und E-Mail-Adresse abgerufen worden. Unklar blieb, welche Daten die App von Google überhaupt bekommen hätte. Der Umgang der App mit dem Datenschutz hatte dennoch starke Kritik ausgelöst."	technik
"Wer sich das Twitter-Profil von Recep Tayyip Erdoğan anschaut, stolpert über zwei Dinge. Zum einen sind da die zahllosen Bilder von Wahlkampfveranstaltungen. Fahnenfuchtelnde Massen, Präsident Erdoğan steht mittendrin, in Antalya, Kars und Istanbul. Er schert sich offenbar nicht um die Verpflichtung zur Neutralität in seinem Amt. 
Ein zweites Mal stolpert man darüber, dass Erdoğan überhaupt twittert. Vor etwa einem Jahr hatte er noch versprochen Twitter –  ""die Bedrohung der Gesellschaft "" – alsbald auszulöschen. Heute hat er mehr Follower als ZEIT ONLINE und die britische BBC zusammen. 
Wo man Widerspruch vermutet, herrscht Kalkül. Die Wähler von Erdoğans Partei AKP wohnen vor allem in ländlichen Gegenden und nutzen traditionelle Medien wie das Fernsehen. Twitter hingegen zieht die junge Generation in den Städten an. Da haben Erdoğan und die konservative AKP Nachholbedarf. Wird die Kritik auf Twitter etwas heftiger, reagieren Tausende Trolle, die Erdoğan ergeben sind. Hinzu kommt, dass soziale Netzwerke wie Twitter ohnehin weit schwieriger zu kontrollieren sind als traditionelle Medien. 
Große Teile der traditionellen Massenmedien hat die AKP unter ihrer Kontrolle, sei es durch Übernahme oder Gummiparagrafen. Erdoğan hat erst vor wenigen Tagen Can Dündar wegen Spionage angezeigt. Dündar ist Chefredakteur der regierungskritischen Zeitung Cumhuriyet. 
Soziale Netzwerke und das Internet sind dagegen dezentral und weit schwieriger zu kontrollieren. Deutlich wurde das vor allem während der Gezi-Proteste: Damals teilten Demonstranten vor allem über Twitter Informationen und organisierten neue Aktionen. Erdoğan reagierte mit teilweisen Blockaden von Twitter und Störsendern im Gezi-Park, die das mobile Internet lahmlegten. 
Heute twittern 17 Prozent aller Türken, etwa 50 Prozent sind in sozialen Netzwerken angemeldet. Die sozialen Netzwerke haben sich als alternative Medien etabliert. Mit ihrem Aufstieg begann auch die türkische Internetzensur, schreiben die türkischen Wissenschaftler Mustafa Akgül und Melih Kırlıdoğ in einer im Internet Policy Review veröffentlichten Studie. 
Twitter startete im Jahr 2006. Zur gleichen Zeit beriet die türkische Regierung über das Gesetz 5651. Als Hauptgrund, den Zugang zum Netz beschränken zu wollen, gab man damals Kinderpornografie an. Ein Jahr später trat das Gesetz in Kraft. Heute ermöglicht es den türkischen Behörden eine weitgehende Zensur des Internets. Die Türkei spielt in dieser Hinsicht in einer Liga mit dem Iran und China. Im Press Freedom Index von Reporter ohne Grenzen liegt die Türkei mittlerweile hinter Staaten wie dem Irak und Afghanistan. 
Zentraler Pfeiler der türkischen Zensur ist noch heute das Gesetz 5651. Es regelt, unter welchen Umständen die Presidency of Telecommunication and Communication (TIB), eine Abteilung der türkischen Kommunikationsbehörde, Websites blockieren kann. Die TIB kann sich dem Gesetz nach an die Provider halten, wenn es um  ""schädliche Seiten "" geht. Zum Beispiel, wenn sie Glücksspiele oder Drogen anbieten, das Türkentum beleidigen oder jegliche Form von Pornografie, Prostitution oder Homosexualität zeigen. 84 Prozent aller Seiten werden Akgül und Kırlıdoğ zufolge wegen erotischer Darstellungen blockiert. 
 
Die türkische Regierung hat seit Inkrafttreten des Gesetzes im Jahr 2007 über 80.000 Internetseiten gesperrt. Zu den bekanntesten gehören YouTube, Alibaba.com, Wordpress.com und Richarddawkins.net. Das listet die Seite Engelliweb auf. Davon fallen allein 30.000 Sperrungen in den Zeitraum der vergangenen zehn Monate, also erst in den Präsidentschaftswahlkampf und jetzt in den für die Parlamentswahlen. Zuletzt war zum Beispiel die Seite des Atheistenverbandes betroffen. Aber selbst die Zahlen von Engelliweb sind nur Schätzungen. Die türkische Behörde TIB gibt offiziell keine Auskunft über die Zahl der gesperrten Seiten. Dienste wie YouTube wurden bis 2014 mehr als 30 Mal zeitweise abgestellt. 
Im vergangenen Jahr baute die türkische Regierung die Kontrolle weiter aus. Im Parlament verabschiedete die AKP-Mehrheit einen Zusatz zum Gesetz 5651. Seitdem bedarf die Blockade einer Internetseite mit  ""schädlichem Inhalt "" erst im Nachhinein einer richterlichen Genehmigung. Das Verfassungsgericht hatte den Zusatz zwar zwischenzeitlich als nicht verfassungskonform bezeichnet und gekippt. Die AKP wartete daraufhin jedoch einfach den Ruhestand einiger Richter ab und verabschiedete das Gesetz erneut. 
Der Zusatz zum Gesetz 5651 schreibt den türkischen Providern zum einen vor, sich in einer Union zu organisieren, so dass Seiten flächendeckender zensiert werden können. Zum anderen sind sie verpflichtet, die Daten türkischer Internetnutzer auf Vorrat zu speichern. Anders als in Deutschland geht es hierbei nicht um Wochen, sondern um ein bis zwei Jahre. Türkische Behörden können, wenn sie einen Gerichtsbeschluss vorlegen, auf diese Daten zugreifen. 
Akgül und Kırlıdoğ schreiben in ihrer Studie, dass selbst diese Maßnahmen viele Türken nicht von bestimmten Seiten abhalten. Techniken zur Umgehung der Zensur seien weit verbreitet. Dazu gehören etwa Anonymisierungsdienste wie das Tor-Netzwerk oder sogenannte VPN-Tunnel. Wie man einen solchen Tunnel einrichtet, wird hier beschrieben. 
Die Autoren vermuten eine zweite Absicht hinter den Gesetzen. Mit ihnen schützen sich die Regierenden auch selbst, etwa vor Korruptionsvorwürfen. Deutlich wird das am Transparenzbericht von Twitter. 2014 stellten türkische Behörden und Gerichte demnach mehr Löschanfragen als alle anderen Staaten zusammen. Akgül und Kırlıdoğ stellten fest, dass die Konten von Regierungskritikern, die über Korruption berichteten, nicht mehr in dem Netzwerk erreichbar waren. Akgül und Kırlıdoğ schreiben weiter, dass die Türkei bei Korruptionsvorwürfen rigider vorgeht als die chinesische Regierung. 
Auf der anderen Seite liegt die Türkei bei der Zahl reiner Informationsanfragen an Twitter eher im Mittelfeld. Eine mögliche Schlussfolgerung ist, dass die Regierung weniger an Strafverfolgung als an Meinungshoheit interessiert ist. Ein ähnliches Bild ergibt sich im Transparenzbericht von Facebook. Auch hier ließ die Türkei zusammen mit Indien die meisten Seiten sperren. 
Zudem strebt die türkische Regierung offenbar auch eine detaillierte Kontrolle von Netzinhalten an. Verhandlungen mit den Firmen Procera Networks und Net Clean sind dokumentiert. Bestätigt hat die Regierung das jedoch nicht. Im Wesentlichen geht es dabei um sogenannte Deep Paket Inspection (DPI). 
Im Internet werden Daten in Form von kleinen Paketen verschickt. Normalerweise werden diese Pakete bei den Internetanbietern nur auf Absender und Empfänger kontrolliert, sogenannte Metadaten. Bei der DPI werden die Pakete aber auch auf ihren Inhalt hin kontrolliert und können nach Gutdünken aussortiert oder verfolgt werden. Ähnliche Techniken sollen auch der Iran und China einsetzen. 
Das Fazit der Forscher ist weniger trüb, als die Fakten vermuten lassen. Zwar sei der Staat imstande, das Netz zu zensieren, umfassend werde ihm das aber in absehbarer Zeit nicht gelingen. Die Dämonisierung von sozialen Netzwerken könne aber dazu führen, dass weniger gebildete Menschen alternativen Medien weniger offen gegenüberstehen. Mit anderen Worten: Die türkische Regierung hält ihre Wähler von kritischen Inhalten fern. Die teilweise Kontrolle sozialer Netzwerke nutzt die Regierung wiederum, um Kritik an der Regierung klein zu halten."	technik
"Die unendlichen Weiten des Weltraums gibt es in Computerspielen meist nur häppchenweise. Mal darf man Raumschiffe zusammenbauen, mal darf man sie fliegen, mal darf man sich als Weltraumsoldat auf entfernten Planeten herumschlagen. Eine Rund-um-die-Uhr-Simulation des Astronautenalltags aber fehlt noch. 
Game-Designer Chris Roberts will genau das ändern. Sein Crowdfunding-Großprojekt Star Citizen soll alles auf einmal bieten: nicht nur ein Science-Fiction-Universum, in dem Abertausende Spieler miteinander Handel treiben, kämpfen, um die Wette fliegen und neue Inhalte erschaffen. Sondern auch eine Einzelspieler-Kampagne (Squadron 42) mit packender Story, inspiriert von Roberts' Neunziger-Jahre-Hit Wing Commander. All das erleben Spieler aus der Ich-Perspektive: Sie sollen voll und ganz ins Astronautenleben eintauchen. 
So jedenfalls sehen die Pläne für Star Citizen aus. Doch mehr als vier Jahre nach dem Start des Projekts ist es noch weit von seiner Fertigstellung entfernt. Den ursprünglich geplanten Erscheinungstermin 2014 konnte Roberts' Firma Cloud Imperium Games nicht einhalten. Stattdessen durften die Crowdfunder nach und nach einzelne, aber noch unfertige Spielbestandteile ausprobieren – etwa das Hangar-Modul, das eigene Raumschiffe begehbar machte (2013), oder auch den Schlachtensimulator Arena Commander von 2014. Derweil sammelte das Projekt eifrig Crowdfunding-Gelder: Erst über Kickstarter, dann über die Website von Roberts Space Industries. 
Mitte Dezember erreichte Star Citizen 100 Millionen US-Dollar Crowdfunding-Kapital. Erzielt wurden die Einnahmen vor allem durch den Verkauf von Starter-Packages, Raumschiffen und Merchandise-Produkten. Die 100 Millionen sind deutlich mehr, als andere schwarmfinanzierte Games bisher eingesammelt haben. Mit der Summe und Entwicklungsdauer wächst aber auch die Kritik an den Machern: Die Vorwürfe reichen von Missmanagement über Verschwendung bis Betrug. In zahllosen Foren und Kommentarspalten diskutieren Spielefans darüber, ob Roberts sich mit dem Projekt verhoben hat, ob Star Citizen jemals fertig wird – oder ob die Crowdfunder vielleicht sogar einem scam, einem Schwindel, aufgesessen sind. 
Längst geht es nicht mehr nur darum, was aus Star Citizen wird. Sondern ganz grundsätzlich um die Frage, ob schwarmfinanzierte Spiele dieser Größenordnung erfolgreich sein können. Mit dreistelligen Millionensummen hantieren normalerweise nur Publisher von AAA-Blockbustern, sei es nun Destiny, GTA V oder Max Payne 3. Solche Spiele entstehen meist unter großer Geheimhaltung, jeder Screenshot oder Trailer wird von den Fans begierig aufgesogen, die einzelnen Produktionsschritte sind verborgen, der Konsument zahlt seine 60-70 Euro und vielleicht noch etwas Geld für spätere Downloadinhalte. 
Crowdfunding-Projekte sind – zumindest theoretisch – der Gegenentwurf: Die Spieler werden stark in die Entwicklung einbezogen und beeinflussen mit ihren Investitionen, welche Bestandteile überhaupt verwirklicht werden. Crowdfunding lebt also von der Idee, dass es bessere, weil den Spielerwünschen entsprechende, Games hervorbringt. Dass die Spieleproduktion letztendlich demokratischer wird. 
 
Allerdings ist Crowdfunding anfällig für den sogenannten feature creep. Dahinter steckt die Tendenz, mit wachsender Geldsumme immer mehr Funktionen hinzuzufügen und die Entwicklungszeit dadurch unnötig in die Länge zu ziehen. Besonders häufig tritt der feature creep auf, wenn Crowdfunding mehr Geld in die Kasse spült als erwartet – das Adventure Broken Age beispielsweise verzögerte sich um mehr als ein Jahr. Chris Roberts hat immer wieder bestritten, dass Star Citizen unter seinem Kapital leidet. Im gleichen Atemzug verteidigt er die Dimension des Projekts:  ""Star Citizen ist wichtig, weil es groß ist, weil es ein kühner Traum ist. Es ist etwas, dass sich sonst niemand zutraut. Ihr habt Star Citizen nicht unterstützt, weil ihr haben wollt, was ihr schon kennt. "" Natürlich sei das Spiel weitaus größer, als er sich das 2012 vorgestellt hatte. Das sei aber absolut nichts Schlechtes. 
Viele Star-Citizen-Unterstützer stimmen ihm zu. Ein Spiel mit diesem Umfang und Anspruch brauche nun mal ausreichend Entwicklungszeit, sagen sie – und bewundern den Firmenchef für seinen visionären Mut. Mittlerweile arbeiten weit über 200 Menschen an dem Spiel, die Studios sitzen in Austin, Santa Monica, Frankfurt und im britischen Wilmslow, weitere Firmen übernehmen Auftragsarbeiten wie die Gestaltung der Künstlichen Intelligenz. Im ersten Halbjahr 2016 soll Teil 1 der Solo-Kampagne Squadron 42 erscheinen, die in Wilmslow entwickelt wird; zum Cast gehören Hollywood-Stars wie Gary Oldman, Mark Hamill und Gillian Anderson. Zudem haben gerade erst alle Crowdfunder die Version Alpha 2.0 von Star Citizen erhalten. Umfang: 32 Gigabyte. 
Alpha 2.0 bietet einen ersten Eindruck vom zusammenhängenden Spieluniversum. Das Abenteuer beginnt in Port Olisar unweit des Gasgiganten Crusader. Vom Mannschaftsquartier aus erkunden Spieler entweder die Station oder laufen geradewegs zum Landeplatz, um dort – allein oder mit Crew – ein Raumschiff zu starten. Sie kämpfen dann in Dogfights gegen Piraten-Geschwader oder erkunden verlassene Raumstationen, in denen erstmals auch das Egoshooter-Modul zum Einsatz kommt. In Previews der Alpha 2.0 schwärmen Spieler vom nahtlosen Übergang zwischen den verschiedenen Spielebenen – ein Vorgeschmack darauf, was Star Citizen einmal von Konkurrenten wie Elite: Dangerous abheben soll. Die 2014 erschienene Simulation hat gerade erst ein Modul zur Planetenerkundung erhalten. 
Star Citizen ist am Ende eine Rechnung mit vielen Unbekannten. Besonders knifflig ist die Frage, wie Roberts eines Tages das Gleichgewicht zwischen den Spielern wahren will. Die Starter-Packs, die Cloud Imperium Games verkauft, gibt es nämlich bereits ab 35 US-Dollar – man kann aber auch ein Komplettpaket für 18.000 US-Dollar erwerben und hat dann deutlich leistungsfähigere Raumschiffe zur Verfügung. Nun ist natürlich jedem selbst überlassen, wie viel Geld er für virtuelle Gegenstände in einem noch unfertigen Spiel ausgibt. Doch wenn das Multiplayer-Universum erst einmal startet, könnte die dann herrschende Raumschiff-Klassengesellschaft für einigen Ärger sorgen. Star Citizen ist und bleibt ein spannendes Experiment."	technik
"Zwar kostet ein Pkw mit Dieselantrieb in der Anschaffung mehr als ein vergleichbares Auto mit Ottomotor, doch wer viel mit dem Wagen unterwegs ist, fährt den Mehrpreis schnell wieder rein. Der Kraftstoff ist vergleichsweise günstig, und weil in einem Liter Diesel mehr Energie steckt als in einem Liter Benzin, ist der Verbrauch in aller Regel auch niedriger. 
Hinzu kommt, dass der deutsche Staat Diesel an der Tankstelle fördert. Die Energiesteuer für Benzin liegt bei rund 65 Cent je Liter, auf Diesel ruht eine Abgabenlast von 47 Cent. So spart ein Dieselfahrer gegenüber dem Benzinfahrer pro Liter 18 Cent. 
Die seit Jahren bestehende Besserstellung von Diesel hat dazu geführt, dass die Nachfrage nach Dieselfahrzeugen in Deutschland kräftig gestiegen ist. 1990 machte der Dieselantrieb bei den Neuverkäufen gerade mal elf Prozent aus – inzwischen liegt der Anteil bei 48 Prozent (Januar bis September 2015). 
Dabei gibt es aber große Unterschiede zwischen den Herstellern. Bei neuzugelassenen Autos von BMW und Audi macht Diesel inzwischen fast 70 Prozent aus, und auch mehr als die Hälfte der Neuwagen von VW und Mercedes hat mittlerweile einen Dieselmotor. Bei Opel beträgt der Dieselanteil hingegen rund 30 Prozent, bei dem für Hybridantriebe bekannten Hersteller Toyota machen Diesel-Pkw auf dem deutschen Markt nur 21 Prozent aus. 
Dass die Top 3 der Dieselverkäufer (Audi, BMW, Mercedes) genau die Hersteller sind, die vor allem als Anbieter von Firmenwagen attraktiv sind, verwundert nicht: Wie beschrieben lohnt sich das Dieselfahren vor allem für Leute, die viel unterwegs sind. Und für die Hersteller vor allem großer Autos ist der Dieselantrieb relevant, weil er aufgrund des geringeren Verbrauchs auf 100 Kilometer auch weniger CO2 ausstößt – der Diesel spielt eine wichtige Rolle, um die strenger werdenden CO2-Grenzwerte zu erreichen. 
Der wachsende Anteil des Dieselantriebs bei den Neuwagen hat dazu geführt, dass auch der Pkw-Bestand immer diesellastiger geworden ist. Der Anteil der Pkw, die hierzulande mit Diesel fahren, ist von 2006 bis 2015 von 21,9 auf 31,2 Prozent gestiegen. Entsprechend sank der Benzinanteil von 77,9 auf 67,2 Prozent. Wie sich der Dieselanteil auf die wichtigsten Automarken verteilt, zeigt die folgende Infografik. 
Umweltschützer kritisieren seit Langem die steuerliche Bevorzugung von Dieselkraftstoff, die vor allem für Spediteure und Handwerker eingeführt wurde. Angesichts des VW-Dieselskandals hat die Kritik daran wieder zugenommen. Schließlich macht der Diesel trotz besserer CO2-Bilanz genügend andere Probleme, mit Feinstaub und mit Stickoxiden, die jetzt im Mittelpunkt des VW-Skandals stehen. In Dieselmotoren ist es schwieriger und damit aufwändiger, die Abgase sauber zu bekommen; zudem sind heute auch Benzinmotoren vergleichsweise CO2-arm geworden. 
In Frankreich, wo Dieselkraftstoff ebenfalls geringer besteuert wird, will die Regierung ab 2016 die Steuer auf Diesel anheben und die Abgaben auf Benzin senken, um beide binnen fünf Jahren anzugleichen. In Deutschland stehe eine Veränderung der Dieselsteuer dagegen  ""nicht auf der Agenda "", machte Bundesumweltministerin Barbara Hendricks (SPD) klar."	technik
"Wer 1,1 Promille Alkohol im Blut hat, ist als Autofahrer absolut verkehrsuntüchtig und macht sich strafbar. Doch schon die 0,5-Promille-Marke definiert den Gefahrengrenzwert, der zu einem Bußgeld von mindestens 500 Euro führen kann. Im Gegensatz dazu ist für Radfahrer aktuell die absolute Verkehrsuntüchtigkeit bei 1,6 Promille festgelegt. Demnach dürfen Radler viel mehr trinken und sich dann theoretisch noch auf das Velo schwingen, ohne ein Bußgeld zu fürchten. 
Das könnte sich allerdings ändern. Der Allgemeine Deutsche Fahrrad Club (ADFC) und weitere Vereine und Institutionen fordern, dass der Gesetzgeber auch für Radfahrer einen zusätzlichen Gefahrengrenzwert von 1,1 Promille als Bußgeldtatbestand in das Straßenverkehrsgesetz aufnimmt. Zwar können sich Radler genauso wie Autofahrer schon ab 0,3 Promille wegen relativer Fahrunsicherheit strafbar machen, wenn ein alkoholtypischer Fahrfehler festgestellt wird. In der Praxis gelingt dieser Nachweis ohne Unfall allerdings nur selten. 
Der Höchstwert wird außerdem oft falsch gedeutet: 1,6 Promille ist nicht als Grenze zu verstehen, bis zu der man sicher auf dem Rad unterwegs ist – so betrunken kann niemand mehr sicher Rad fahren. Deshalb macht man sich am Lenker ab 1,6 Promille auch strafbar, ohne dass eine alkoholbedingte Fahrunsicherheit nachgewiesen werden muss. Wer nicht erheblich an Alkohol gewöhnt ist, kann in der Regel ab diesem Wert sein Gefährt meist nicht einmal wiederfinden oder aufschließen. 
Selbst 1,0 Promille geht über einen leichten Rausch hinaus und wird bei geselligen Anlässen nur selten erreicht. Ab diesem Wert der Alkoholisierung wird das Radfahren deutlich gefährlicher. Von den alkoholisierten Radfahrern verunglückten rund 85 Prozent mit 1,1 Promille und mehr. Insgesamt ist der Anteil der Fahrradunfälle mit Alkoholeinfluss 2013 allerdings auf weniger als 4,4 Prozent gesunken. 
Beim Fahrrad werden nicht so hohe Anforderungen an den Fahrer gestellt wie beim Pkw, und von ihnen geht auch nicht so eine hohe Gefährdung aus. Bei der Promillegrenze steht hier besonders der Eigenschutz im Vordergrund, denn für berauschte Radler endet eine Kollision trotz geringerer Geschwindigkeit meist schwerer. Beispielsweise fallen alkoholisierte Radler im Vergleich zu nüchternen Fahrern rund drei Mal häufiger mit dem Gesicht auf den Asphalt, berichtet Roland Huhn vom ADFC. Der Gesetzesvorschlag des Radverbands soll Verkehrsunfälle verhindern und die Eigenverantwortung der Radler fördern."	technik
"Das Gegenteil von gut ist gut gemeint – trifft dieser Satz auch auf jene Verordnung über Maßnahmen zum Zugang zum offenen Internet zu, über die das EU-Parlament am morgigen Dienstag abstimmt? Internetunternehmen und Risikokapitalgeber aus Europa und den USA befürchten es, ebenso Bürgerrechtsorganisationen, die deutschen Landesmedienanstalten und auch der Erfinder des World Wide Web, Sir Tim Berners-Lee. 
 ""Das World Wide Web hat sich zu einer machtvollen und universellen Plattform entwickelt, weil ich es als offenes Netzwerk aufbauen konnte, das alle Datenpakete gleich behandelt. Dieses Prinzip der Netzneutralität hat dafür gesorgt, dass das Internet seit seiner Entstehung ein freier und offener Raum geblieben ist. Falls der Entwurf der Verordnung in seiner jetzigen Fassung beschlossen wird, sind Innovation, freie Meinungsäußerung und Privatsphäre sowie Europas Fähigkeiten, in der digitalen Wirtschaft eine führende Rolle zu spielen, bedroht "", schreibt Berners-Lee. 
Weniger idealistisch, in der Sache aber ähnlich sehen das Firmen von BitTorrent, Etsy, Foursquare, Kickstarter, Netflix, Reddit bis Vimeo und Investoren wie Union Square Ventures. In einem Brief an die EU-Abgeordneten schreiben sie:  ""Der Entwurf beinhaltet vier große Probleme, die Netzneutralität unterminieren und auch der europäischen Technikindustrie schaden könnten "" – womit sie natürlich auch sich selbst meinen. 
Im Einzelnen seien das die sogenannten Spezialdienste, das Zero-Rating, Datenverkehrskategorien sowie das Trafficmanagement bei drohender Netzüberlastung. Das sind genau jene Problemfelder, die schon die deutsche Stanford-Professorin Barbara van Schewick im Entwurf als solche erkannt hatte. Sie ist auch die Initiatorin des offenen Briefs. 
Und darum geht es: Bei Spezialdiensten denken Befürworter an gesonderte Datenkanäle für Telemedizin und selbstfahrende Autos, Internetprovider und Kritiker dagegen an HD-Videos, für deren ruckelfreie Übertragung die Nutzer zur Kasse gebeten werden. Zero-Rating heißt: Anbieter von Internetinhalten, etwa von Streamingdiensten, können mit Providern aushandeln, dass ihre Inhalte nicht auf das monatliche Datenvolumen der Kunden angerechnet werden. Zudem dürfen Provider dem Entwurf zufolge künftig Datenverkehrskategorien einführen, also bestimmte Inhalte wie zum Beispiel P2P-Verkehr in eigene Klassen einsortieren und unterschiedlich behandeln. Und eine gezielte Drosselung bestimmter Datenübertragungen kann auch bei  ""drohender Netzüberlastung "" beginnen, nicht erst bei einer tatsächlichen Überlastung. 
 ""Diese Probleme gefährden die Zukunft von Innovationen durch Start-ups und wirtschaftliches Wachstum in der EU "", schreiben die Verfasser des offenen Briefs. Sie befürchten, dass nur Unternehmen wie Google und andere etablierte Branchengrößen genug Geld haben, um mit Providern über Zero-Rating und Spezialdienste zu verhandeln und dann bevorzugt behandelt werden – was faktisch zu einem Zwei-Klassen-Internet führen würde. Start-ups wären chancenlos und müssten mit einem  ""Standardnetz "" Vorlieb nehmen. Und auch auf Nutzerseite würde gelten: Nur wer genug Geld hat, kann sich ein Premium-Internet leisten. 
Genau das soll die Verordnung eigentlich verhindern. So heißt es gleich im ersten einleitenden Absatz des Entwurfs:  ""Mit dieser Verordnung sollen gemeinsame Regeln zur Wahrung der gleichberechtigten und nichtdiskriminierenden Behandlung des Datenverkehrs bei der Bereitstellung von Internetzugangsdiensten und damit verbundener Rechte der Endnutzer geschaffen werden. Mit der Verordnung sollen die Endnutzer geschützt und es soll gleichzeitig gewährleistet werden, dass das 'Ökosystem' des Internets weiterhin als Innovationsmotor funktionieren kann. "" 
Doch einige widersprüchliche und schwammige Formulierungen im Entwurf – wann zum Beispiel beginnt eine  ""drohende Netzüberlastung ""? – gefährden dieses Ziel und lassen den Internetprovidern viel Spielraum für neue Geschäftsmodelle. Die argumentieren, dass sie diesen Raum auch brauchen, weil sie ansonsten auf den Kosten für den Ausbau der Infrastruktur sitzen bleiben, während andere mit den Inhalten viel Geld verdienen. 
Die Kritiker rufen die EU-Parlamentarier nun auf, mit dem Verordnungsentwurf auch eine Reihe von Änderungsanträgen zu beschließen, mit denen die Schlupflöcher gleich wieder gestopft und die Netzneutralität im Berners-Lee'schen Sinne gesichert würde. Den Änderungen müssten anschließend aber auch die Länder noch einmal zustimmen. 
Sollten die Änderungsanträge nicht angenommen werden, kann allenfalls noch das Gremium Europäischer Regulierungsstellen für elektronische Kommunikation (Gerek) nachbessern. Es soll nämlich Leitlinien festlegen, nach denen die nationalen Regulierungsbehörden die Verordnung umsetzen beziehungsweise die Umsetzung überwachen. In Deutschland wäre das die Bundesnetzagentur. 
Weil die EU-Kommission das Thema Netzneutralität aber in einem Paket mit der geplanten, jedoch keinesfalls gesicherten Abschaffung der Roaminggebühren in der EU verknüpft hat, ist ohnehin völlig offen, ob eine Mehrheit der Abgeordneten willens ist, den jetzigen Kompromiss am Dienstag noch einmal infrage zu stellen."	technik
"Trecker-Motor, Ölbrenner, Rußschleuder: Der Diesel hatte lange ein schlechtes Image – von dem er sich in den vergangenen Jahren mühsam lösen konnte. Nun hat der Skandal um Prüfmanipulationen den Ruf wieder beschädigt. 
Trotzdem gibt es ein paar Gründe, die weiterhin für den nach Rudolf Diesel benannten Motor sprechen. Diesel erfand 1893 das Selbstzünder-Verfahren, nach dem das Triebwerk prinzipiell bis heute funktioniert. Welche Vorteile hat der Motor? 
Weil Dieselkraftstoff mehr Energie enthält als Benzin, braucht ein Dieselfahrzeug für vergleichbare Leistungen weniger Sprit als ein Wagen mit Ottomotor. Hinzu kommt, dass in Deutschland Diesel an der Zapfsäule staatlich subventioniert ist. Der Steuervorteil gegenüber Benzin liegt bei fast 30 Cent je Liter. 
Dadurch ist der Diesel-Pkw ein Kostensparmodell – vor allem für Vielfahrer, bei denen sich der höhere Anschaffungspreis sowie die Mehrkosten bei Steuer und Versicherung schnell amortisieren. Ausspielen kann der Diesel seine Verbrauchsvorteile nicht nur bei Langstreckenfahrten auf der Autobahn, sondern – wie jeder Taxifahrer weiß – vor allem auch im Stadtverkehr. 
Der im Vergleich zu Benzinern geringere Verbrauch führt dazu, dass auch weniger Kohlendioxid aus dem Auspuff kommt. Das erfreut nicht nur die Umwelt, sondern auch die deutsche Autoindustrie. Dieser hilft der Diesel nämlich beim Erreichen der künftigen CO2-Grenzwerte. So stößt beispielsweise ein Volkswagen Phaeton mit Ottomotor gemäß Normzyklus mindestens 265 Gramm CO2 pro Kilometer aus, der Diesel emittiert 41 Gramm weniger. 
Der Unterschied macht für VW auch in der Bilanz viel aus, denn die EU plant für das Ende des Jahrzehnts Strafzahlungen, wenn verkaufte Neuwagen die Grenzwerte überschreiten: 95 Euro für jedes zusätzliche Gramm CO2. Das ergibt zwischen den beiden Motorversionen des Phaeton eine Differenz von mehreren Tausend Euro. Die deutschen Hersteller mit ihren großen und schweren Autos können es sich daher kaum leisten, den Diesel sterben zu lassen. 
In fast jeder Hinsicht ist ein Dieselmotor dreckiger als ein Benziner. Bei der Verbrennung entstehen mehr Partikel, mehr Stickoxide und auch mehr Aldehyde. Sofern die Systeme zur Abgasreinigung funktionieren, ist der moderne Diesel aber keine schmutzige Technik. Und immerhin bei der Kohlenmonoxid-Emission schneidet er besser ab: Mit zehn Gramm CO pro verbranntem Liter Kraftstoff ist der Selbstzünder zehnmal sauberer als ein Benziner. Allerdings: Im Vergleich mit Ruß und Stickoxiden gilt CO als aktuell wenig kritisch. Die Grenzwerte in der Atemluft werden in der Regel deutlich unterschritten. 
Bei vergleichbarer Leistung wirken Dieselmotoren oft kräftiger als Ottomotoren. Das liegt in der Regel an ihrem früh einsetzenden maximalen Drehmoment, was vor allem dem mittlerweile obligatorischen Turbolader geschuldet ist. Ein hohes Drehmoment bei vergleichsweise geringen Umdrehungszahlen führt dazu, dass ein Fahrer den Wagen kraftvoll beschleunigen kann, ohne die Gangschaltung betätigen zu müssen. Zugleich hilft dies, Sprit zu sparen, da bei relativ niedriger Drehzahl der Motor mehr Kraft bietet. 
Gegenüber einem ähnlich starken Ottomotor ohne Turbo bietet der Dieselmotor in der Regel mehr Fahrspaß. Doch der Trend zu Aufladung und Downsizing beim Benziner hat den Vorteil für den Diesel verringert: Auch die Turbo-Ottos sind mittlerweile bei niedrigen Drehzahlen durchzugsstark. Allerdings büßen sie durch die Zusatztechnik auch einen Teil ihres traditionellen Kostenvorteils gegenüber dem Diesel ein. 
Doofe Unfälle passieren. Kürzlich ging der Fall eines Autofahrers durch die Boulevardmedien, der nach einer Spinne im Tank seines Wagens suchte und für bessere Sicht ein Feuerzeug zückte – die Sache ging übel aus, weil der Mann offenbar einen Benziner fuhr. Denn Ottokraftstoff verdampft schon bei Temperaturen weit unter null Grad, die gasförmigen Schwaden sind dann hochentzündlich. Schon ein kleiner Funke kann ausreichen. 
Beim Diesel dagegen liegt der sogenannte Flammpunkt bei frühestens 55 Grad. Erst bei dieser Temperatur verdampft er, und nur dann lässt er sich ohne Weiteres mit einem Streichholz oder Feuerzeug entzünden. Bei Raumtemperatur ist das versehentliche Anzünden von Diesel daher kaum möglich. 
Der Siegeszug des Dieselmotors, erzählt in 14 Karten: 
"	technik
"Keine Sorge, hieß es monatelang, die Kommunikationsdaten von Deutschen werden nicht vom Bundesnachrichtendienst überwacht, man filtere sie zuverlässig aus. Offensichtlich war das eine Lüge. Der BND ist gar nicht in der Lage, genau zu unterscheiden, welche Kommunikation von Deutschen stammt und welche nicht – und er wusste das auch. Das zumindest berichten Süddeutsche Zeitung, NDR und WDR und berufen sich dabei auf streng geheime Akten des NSA-Ausschusses des Bundestages. 
Der BND darf, so steht es im sogenannten G-10-Gesetz, die Kommunikation von Ausländern überwachen und darin nach verdächtigen Dingen suchen. Als erste Berichte auftauchten, der BND habe den in Frankfurt ansässigen Internetknotenpunkt De-CIX ausgespäht, gab es sofort den Verdacht, dass dabei auch massenhaft deutsche Daten in seinen Schleppnetzen hängen geblieben sein müssen. Das wurde bislang immer dementiert. 
Doch die Akten belegen nun, dass eine  ""absolute und fehlerfreie "" Trennung zwischen deutscher und ausländischer Telekommunikation nicht möglich gewesen sei. Eigentlich sollte ein vom BND konstruiertes Programm namens DAFIS die Daten deutscher Internetnutzer herausfiltern, so wie es das Gesetz fordert. BND-interne Prüfungen zeigten aber schon am Anfang, dass mindestens fünf Prozent der deutschen Kommunikationsdaten nicht aussortiert werden konnten. Das habe sich bis zuletzt nicht geändert, schreibt die Süddeutsche Zeitung. 
Die Kooperation fand zwischen 2004 und 2008 unter dem Codenamen Eikonal statt. Weil die NSA keinen direkten Zugang zu dem Internetknotenpunkt bekommen sollte, zweigte der BND die Daten ab und leitete sie an die NSA weiter, wie der WDR schreibt. Dazwischen sollte ein spezielles Programm dafür sorgen, dass Daten deutscher Staatsbürger herausgefiltert werden, um Rechtsverstößen vorzubeugen. Was misslang. 
Trotzdem wurden die Daten – auch das belegen die Akten nun – an die NSA weitergeleitet. Jahrelang wurden demnach Daten deutscher Staatsbürger an den US-Geheimdienst übermittelt. 
Über beide Fakten wurde weder die für die Überwachung zuständige G-10-Kommission informiert noch das Parlamentarische Kontrollgremium des Bundestages, das ebenfalls für die Geheimdienste zuständig ist. 
Der BND, dieses Bild verfestigt sich mehr und mehr, ist demnach nahezu eine Unterabteilung der NSA. Der deutsche Geheimdienst filtert von der NSA gelieferte Daten nach von der NSA gelieferten Stichworten beziehungsweise  ""Selektoren "", und er tut das mit von der NSA gelieferter Software. Anschließend leitet er die Ergebnisse an die NSA zurück. Ganz offensichtlich hat er dabei deutsches Recht verletzt und seine Befugnisse weit überschritten. Mitglieder des Untersuchungsausschusses des Bundestages fordern daher eine Reform des Dienstes und der parlamentarischen Kontrolle."	technik
"Der Bundesnachrichtendienst (BND) überwacht den Internetverkehr, so viel ist schon länger klar. Aber was für Daten saugt er dabei eigentlich ab? Und was passiert mit ihnen? Der Untersuchungsausschuss des Bundestages hat diese Fragen zumindest zum Teil beantworten können. 
Hier die bisher bekannten und teils beunruhigenden Fakten: 
Der BND hat mindestens zwei große und mehrere kleinere Datenbanken. Die erste große heißt InBe, ein Akronym von Inhaltliche Bearbeitung. Darin werden Telefongespräche, E-Mails und Faxe gesammelt, in denen vor allem Ausländer Dinge besprochen haben, die aus Sicht des BND für Deutschland gefährlich und relevant sind. Wie groß diese Datenbank ist und wie viele einzelne Gespräche darin liegen, ist nicht bekannt. Einzige Aussage dazu: Aktuell seien  ""mehrere Hunderttausend Daten "" darin gespeichert, auch von deutschen Staatsbürgern. Was  ""Daten "" in diesem Zusammenhang bedeutet, ist unklar. 
Welche Schlüsse aus diesen Daten gezogen werden, wurde zumindest umrissen. Demnach wird darin automatisch nach bestimmten Schlagwörtern, Hinweisen, Punkten gesucht. Von der Software als auffällig identifizierte Mitschnitte werden dann von einem Mitarbeiter mit entsprechenden Sprachkenntnissen angesehen und ausgewertet. Sind sie nach seiner Meinung interessant, erstellt er oder sie daraus ein  ""Meldungsvorprodukt "", eine Art Zusammenfassung des Inhalts. Die geht nach Pullach, wo sie von Auswertern weiterverarbeitet wird. 
Man kann dabei den Eindruck gewinnen, dass der BND der Menge an Kommunikationsinhalten nicht Herr wird. Es sind zu viele, der Nachrichtendienst hat offenbar nicht genug Leute mit den entsprechenden Sprachkenntnissen, um alles zu analysieren. Inhaltsanalyse scheint daher beim BND seit einiger Zeit kein Schwerpunkt mehr zu sein. 
Das führt zur zweiten großen Datenbank, zu VerAS. Die Abkürzung steht für Verkehrsanalysesystem. Darin werden Metadaten gesammelt. Also nicht die Inhalte von Gesprächen, E-Mails oder SMS, sondern alles, was als Daten um diese Kommunikation herum anfällt: Wer kommunizierte mit wem, wann und wo tat er das, wie lange und womit et cetera. 
Es ist das gleiche Prinzip, nachdem die Vorratsdatenspeicherung funktionieren sollte. Wenn man weiß, wer mit wem wie oft redete, kann man Beziehungen erkennen, auf Pläne schließen, ja ganze Netzwerke aufklären. Ziel ist es, entweder auf noch unbekannte Menschen aufmerksam zu werden, die möglicherweise etwas planen, oder mehr über bereits bekannte Verdächtige zu erfahren. Beispielsweise soll VerAS jemanden anhand seines Kommunikationsprofils wiederfinden können, auch wenn er ein Handy benutzt, dessen Nummer der BND noch gar nicht kennt. 
VerAS beweist, wie aussagekräftig Metadaten sind und wie viel mit ihnen möglich ist. Allein dadurch, dass Metadaten strukturiert vorliegen und sich leichter bearbeiten lassen als Kommunikationsinhalte, ergibt sich ein Vorteil. Denn es muss niemand den Zusammenhang verstehen, um daraus Schlüsse ziehen zu können. Metadaten lügen nicht. Projekte wie die Visualisierung der Vorratsdaten des Grünen-Politikers Malte Spitz oder der Metadaten des Bits-of-Freedom-Anwalts Ton Siedsma haben das schon öffentlich demonstriert. Der BND nutzt so etwas längst für seine tägliche Arbeit. 
Er geht dabei auch weiter, als es beispielsweise die NSA tut. In den Daten kann bis in die vierte und fünfte Kontaktebene gesucht werden, also beispielsweise nach dem Lebensgefährten der Mutter der Freundin des Anwalts des Verdächtigen. Wie sinnvoll das ist und wie oft das tatsächlich getan wird, ist nicht klar. Aber es ist technisch möglich. Das ist erstaunlich genug: Der NSA ist es nur erlaubt, drei Ebenen tief zu suchen (also bis zur Freundin des Anwalts des Verdächtigen), künftig soll es sogar auf zwei Ebenen beschränkt werden, also auf den Anwalt des Verdächtigen. 
Die Software des BND sucht dazu in dieser Datenbank nach bestimmten Selektoren, wie sie intern genannt werden, also nach einzelnen Merkmalen. Das kann eine Handynummer sein, eine IP-Adresse, ein Name, ein Datum, ein Ort. 
Solche Selektoren können vom BND selbst kommen. Der große Teil aber kommt wohl von der NSA. Mehrmals am Tag lade der BND von einem Server der NSA Selektoren herunter und gebe sie in seine Datenbank ein, hieß es im Untersuchungsausschuss. Anschließend werden die Ergebnisse nach Pullach zur Auswertung geschickt und von dort aus zum Teil auch weiter an die NSA. Auftragsdatenverarbeitung für die Amerikaner also. 
Woher kommen die Daten für VerAS? Wie bei InBe sind nicht alle Quellen bekannt. Sicher ist dass sie ebenfalls aus dem Abhören von Kommunikationssatelliten und dem Belauschen  ""leitungsgebundener Kommunikation "" stammen, also aus Internetkabeln. Der Internetknoten De-CIX in Frankfurt war eine Quelle. Von den Antennen Bad Aiblings abgefangene Satellitenkommunikation eine zweite, in Ländern wie Afghanistan mitgeschnittene Kommunikation eine dritte. Aber es gibt – soviel ist sicher – noch weitere Quellen. Es scheint, dass der BND direkt oder indirekt auch Zugang zu Seekabeln besitzt und dort Internetkommunikation in unbekannter Größenordnung mitschneidet. 
 
Sicher ist auch, dass es eine gewaltige Menge an Daten sein muss, die beim BND gespeichert sind. Wie viele genau? Ebenfalls unklar, das hat der Untersuchungsausschuss bislang noch nicht herausbekommen. Sicher ist nur, dass in dieser Sammlung verbotenerweise auch Daten von Deutschen sind. Zwar gibt es diverse technische Filter, um das zu verhindern – ihre genaue Funktion ist geheim – aber sie funktionieren alles andere als perfekt. 
Beunruhigend ist außerdem, dass der BND die beiden großen Datenbanken jahrelang betrieben hat, ohne dass es die dazu vom Gesetz geforderte Genehmigung und Kontrolle gab. Laut Paragraf 6 des BND-Gesetzes und Paragraf 14 des Bundesverfassungsschutzgesetzes muss der BND seinen eigenen Datenschutzbeauftragten einbeziehen, wenn er eine  ""automatisierte Datei mit personenbezogenen Daten "" plant. Außerdem muss er dazu auch den Bundesdatenschutzbeauftragten anhören und warten, bis das Bundeskanzleramt die Datei genehmigt hat. Zusätzlich dazu muss er  ""in angemessenen Abständen "" prüfen, ob die Datenbanken weiter gebraucht werden. 
Nichts davon ist geschehen. Die BND-Datenschutzbeauftragte sagte, sie habe erst von der Existenz der Datenbanken erfahren, als sie schon eine Weile dort gearbeitet habe. Der Bundesdatenschutzbeauftragte kannte sie gar nicht und das Kanzleramt hat sie nie offiziell genehmigt. Auf Veranlassung der BND-Datenschutzbeauftragten sind die vorgeschriebenen Prüfverfahren inzwischen im Gange. Zumindest teilweise. Ein Verfahren läuft wie vorgeschrieben in Abstimmung mit Bundesdatenschützern und Kanzleramt, das andere bislang nur  ""BND-intern "". 
Der BND war aufgrund einer sehr eigenwilligen Rechtsauffassung trotzdem der Meinung, dass alles seine Richtigkeit hat. Es handele sich, so lautete die Argumentation, bei den eingesammelten Daten allein um Daten aus dem Ausland. Es gebe keine Datenerhebung im Inland, ja sie würden sogar bei Satelliten abgesaugt, also im Weltraum. Das BND-Gesetz finde daher keine Anwendung. Was allerdings bedeutet, dass der BND ganz ohne gesetzliche Grundlage persönliche Informationen – auch von Deutschen – verarbeitet. 
Diese Überzeugung wird daher im Bundestag mit bitterem Spott  ""Weltraumtheorie "" genannt. Sie zeigt, wie unabhängig der Bundesnachrichtendienst handelt und wie sehr er zu einem eigenen Staat im Staate und damit einem rechtsfernen, ja einem rechtsfreien Raum geworden ist. 
Der CDU-Obmann im Untersuchungsausschuss, Roderich Kiesewetter, sagte, die Dateianordnungsverfahren seien offenbar nicht  ""im Fokus der Behörde gewesen "". Das sei aber auch nicht so dramatisch, er könne nicht erkennen, dass Daten illegal weitergegeben worden seien und es gebe ja schließlich noch andere rechtliche Bestimmungen, an die sich Beamte zu halten hätten. Es war so sicher nicht gemeint, aber es klang wie: Hoffentlich tun sie es auch. Wissen kann der Untersuchungsausschuss das nicht zur Gänze, auch weil ihm immer noch viele Akten vorenthalten werden oder er sie nur geschwärzt bekommt."	technik
"Es muss kein VW Passat, Audi A6 oder eine Mercedes E-Klasse als Dienstfahrzeug sein. Besonders in größeren Städten erreicht man mit dem Rad oftmals viel schneller sein Büro. Als ernst zu nehmende Alternative wurden Fahrräder allerdings lange nicht betrachtet, weil Diensträder nur für Geschäftsfahrten genutzt werden durften, für den Weg nach Hause oder für Freizeitaktivitäten aber nicht. 
Inzwischen ist das Dienstfahrrad dem Dienstauto jedoch steuerlich gleichgestellt und eine private Nutzung unter Anwendung der Ein-Prozent-Regelung erlaubt. Dabei müssen Dienstfahrzeugnutzer ein Prozent des Bruttopreises ihres Gefährts als monatliche Einnahmen versteuern – auch das Dienstrad gilt als Form der Gehaltsumwandlung beziehungsweise der Mitarbeitermotivation. 
Einige große Unternehmen wie Bayer oder DHL bieten inzwischen ihren Angestellten eine ganze Fahrradflotte als Alternative zum Dienstauto an. So fördert der Arbeitgeber nebenbei auch die Mobilität und Fitness des Mitarbeiters. Zudem können Unternehmen in ihrer Außenwirkung vom positiv besetzten Image des Fahrrades profitieren. 
Wer bislang als Angestellter nicht damit gerechnet hat, an ein teures Rennrad oder Carbonrad zu kommen, dem kann die Regelung nun eine Tür öffnen. Denen, die ohnehin mit einem Rad geliebäugelt haben, spendiert der Staat gewissermaßen einen Teil des Kaufpreises. Arbeitgeber und Arbeitnehmer können sich die Kosten teilen, sie können es aber natürlich auch jeweils selbst finanzieren. 
Wenn der Angestellte Kosten übernimmt, dann wird ein Teil seines Bruttogehalts für die monatliche Ratenzahlung oder die Leasinggebühr abgezweigt. Damit wandelt der Arbeitnehmer einen kleinen Teil seines Entgelts in eine Sachleistung um und spart Steuern und Sozialabgaben. Am größten ist die Ersparnis für den Angestellten natürlich, wenn der Arbeitgeber die Anschaffungskosten übernimmt. Wer sein Firmenvelo mit anderen teilen möchte, sollte dies vorher mit dem Arbeitgeber klären und am besten schriftlich in einer Vereinbarung festhalten. 
Die Dienstradregelung schließt jedes Fahrrad mit Pedalantrieb ein, auch E-Bikes. Die Räder mit Elektroantrieb bieten sich besonders als Firmenvelo an, denn auf ihnen kommt man auch im Anzug kaum ins Schwitzen. Allerdings können nicht alle elektrisch unterstützten Fahrräder als Diensträder genutzt werden. Eine Ausnahme bilden S-Pedelecs, da sie mit ihrer Motorunterstützung bis 45 km/h nicht mehr als Fahrrad, sondern als Kraftfahrzeug gelten. 
Man kann die schnellen E-Bikes dann natürlich zum  ""Dienstkraftfahrzeug "" machen. Damit gilt dann eine weitere Regelung: Fahrten zwischen Wohnung und Arbeitsplatz werden als zusätzlicher geldwerter Vorteil betrachtet und mit 0,03 Prozent des Kaufpreises pro Entfernungskilometer auf das Bruttogehalt aufgeschlagen. 
Das firmeneigene Fahrrad, das man nun auch privat nutzen kann, muss selbstverständlich den Vorschriften der Straßenverkehrs-Zulassungsordnung entsprechen, also eine Klingel, zwei Bremsen, Beleuchtung, Rücklichter und Reflektorstreifen haben. Was mit dem Fahrrad verbunden ist, zählt zur Ausstattung und wird daher als Bestandteil der Anschaffungskosten betrachtet. Dazu zählt auch der Akku eines E-Bikes. Dagegen sind eine Fahrradtasche, Regenbekleidung oder ein Helm Zubehör und müssen vom Angestellten privat angeschafft werden. Das Fahrradschloss bildet eine Ausnahme: Hier gibt es keine rechtliche Regelung über den Träger der Kosten. 
Die Reparatur- und Wartungskosten eines Dienstrades können vom Arbeitgeber übernommen werden oder sind in Form eines Reparaturkostenschutzes im Leasingangebot enthalten. Ansonsten ist der Arbeitnehmer in der Verantwortung, sich um Wartung und Reparatur zu kümmern."	technik
"Immerhin sind sie kreativ, diese Kriminellen: Das mTAN-Verfahren zur Absicherung des Onlinebankings wurde schon mehrfach ausgetrickst, immer wieder mussten die Banken und Mobilfunkanbieter nachbessern. Aber nun haben Betrüger doch noch eine neue Masche entdeckt. So konnten sie insgesamt mehr als eine Million Euro von Telekom-Kunden in Deutschland stehlen. Das berichtet die Süddeutsche Zeitung. Im Bericht ist von Fällen  ""im mittleren zweistelligen Bereich "" bei verschiedenen Banken die Rede. 
Eigentlich soll das mTAN-Verfahren vor Kriminellen schützen: Onlinebanking-Kunden müssen sich auf der Website der Bank einloggen und bekommen für jede Überweisungen, die sie tätigen wollen, eine einmal gültige Transaktionsnummer auf ihr Handy geschickt. Ein Angreifer müsste also in irgendeiner Form sowohl den Computer als auch das Handy des Kunden kontrollieren oder sich in die jeweiligen Datenübertragungen einklinken können, um selbst Überweisungen tätigen zu können. 
Genau das ist in diesen Fällen gelungen. Wobei der Angriffsweg einige Besonderheiten aufweist: 
Die SZ berichtet nun von einem Fall, in dem einem Postbank-Kunden auf diese Weise mehr als 30.000 Euro gestohlen wurden. Die Täter überwiesen zunächst drei höhere Beträge vom Tagesgeld- auf das Girokonto des Opfers. Anschließend überwiesen sie sich das Geld in neun verschieden hohen Tranchen auf ihre eigenen Konten, um das Überweisungslimit des Kunden zu umgehen. 
Die Telekom will mittlerweile ihre Maßnahmen zur Identifikation von Händlern und Mobilfunkshop-Betreibern verschärft haben, so dass ein Betrug mit diesem Trick nicht mehr möglich sei. 
Als Alternative zum mTAN-Verfahren bietet sich das Chip-TAN-Verfahren an: Onlinebanking-Kunden müssen sich dazu kleines Gerät kaufen, das Transaktionsnummer in Verbindung mit der EC-Karte generiert. Das Chip-TAN-Verfahren konnte bisher nur in Fällen ausgehebelt werden, die noch etwas spezieller sind als die nun bekannt gewordenen."	technik
"Metadaten helfen Amerikas Geheimdiensten beim Töten. Und der Bundesnachrichtendienst hilft der NSA und der CIA, genau solche Metadaten zu sammeln. Nicht gezielt, sondern massenhaft. Viele Millionen Metadaten fischt der BND ab und reicht sie an die amerikanischen Dienste weiter. Genauer: 220 Millionen jeden Tag. 
Im deutschen Auslandsgeheimdienst vollzieht sich ein Paradigmenwechsel. Statt einzelnen Verdächtigen nachzuforschen, setzt der BND auf Massenüberwachung. Recherchen von ZEIT ONLINE zeigen nun zum ersten Mal, wie umfangreich dieser Umbau ist und wie problematisch. 
Früher belauschten Spione Menschen, sie kopierten heimlich Briefe und hörten Telefonate ab. Sie wollten wissen, was die Leute sagen, was sie miteinander verabreden und sich gegenseitig weitererzählen. Bis heute bestimmt der mithörende Agent mit den Kopfhörern auf den Ohren die Vorstellung davon, wie Überwachung funktioniert. Doch das ist die Vergangenheit. 
Die Spione der Gegenwart interessieren sich für ganz andere Spuren: Metadaten. Aus ihnen können die Geheimdienste herauslesen, wer wann wo mit wem und wie lange kommunizierte. Jede E-Mail trägt solche Metadaten, jede SMS, jedes digitale Bild, jede WhatsApp-Nachricht. Wer sie interpretieren kann, weiß nicht nur, was Menschen einander erzählen. Metadaten verraten viel mehr: Wo Menschen gerade sind, woher sie kamen, was sie im Moment tun, sogar was sie planen. Sie enttarnen jedes Versteck und jeden heimlichen Kontakt.  ""We kill people based on metadata "", sagte der frühere NSA- und CIA-Chef Michael Hayden 2014. Wer die passenden Metadaten kennt, weiß, wohin er die tödliche Drohne schicken muss. 
Genau so gehen NSA und CIA vor. Die menschlichen Ziele, auf die amerikanische Drohnen im Jemen, in Somalia oder Afghanistan ihre Hellfire-Raketen abschießen, werden mit ebensolchen weltweit mitgeschnittenen Metadaten ermittelt – mit GPS-Standortkoordinaten, mit Kommunikationsmustern, mit Kennungen von Mobiltelefonen. Anhand dieser Informationen lassen sich auch Profile erstellen und Muster im Verhalten der Zielperson erkennen. So können die Geheimdienste mit großer Sicherheit voraussagen, was eine bestimmte Person als Nächstes tun wird, wo sie sich zu einem bestimmten Zeitpunkt aufhalten wird. Für die NSA sind Metadaten eine der wichtigsten Informationsquellen. 
Auch im BND weiß man schon lange um die Macht der Metadaten. Seit dem 11. September 2001 wird dort überlegt, die Arbeit stärker auf solche Daten zu stützen. Seit 2002 nahmen diese Überlegungen Kontur an, belegen Aktenvermerke des Dienstes. Sie zeigen auch, dass der BND inzwischen große Teile seiner Überwachung auf die Auswertung von Metadaten umgestellt hat. 
ZEIT ONLINE hat von geheimen Akten des Auslandsnachrichtendienstes erfahren, aus denen hervorgeht, dass fünf Dienststellen daran beteiligt sind, Metadaten in großem Stil zu sammeln. In den BND-Außenstellen in Schöningen, Rheinhausen, Bad Aibling und Gablingen laufen in aller Welt abgesaugte Metadaten ein, 220 Millionen davon an jedem einzelnen Tag. Zwischen einer Woche und sechs Monaten werden sie dort gespeichert und nach bislang unbekannten Kriterien sortiert. Die Daten werden aber nicht nur gesammelt. Sie werden auch genutzt, um Verdächtige zu beobachten und zu verfolgen. 
Woher der BND die Daten genau bezieht, ist noch unklar. Der NSA-Untersuchungsausschuss des Bundestages hat aufgedeckt, dass der Geheimdienst sowohl Satellitenkommunikation als auch Internetkabel abhört. Die 220 Millionen Metadaten sind nur ein Teil dessen, was bei diesen Abhöraktionen anfällt. Sicher ist, dass die Metadaten allein aus  ""ausländischen Wählverkehren "" stammen, also aus Telefonaten und SMS, die über Mobilfunk und Satelliten geführt und verschickt wurden. 
 
Von diesen 220 Millionen Daten, die jeden Tag anfallen, wird ein Prozent dauerhaft archiviert. Zwei Millionen Metadaten landen also in einer fünften Dienststelle. Dort werden sie in einer Datenbank für  ""Langfristanalysen "" für zehn Jahre abgelegt. In diesem Langfrist-Speicher ist noch keine Internetkommunikation enthalten, keine Daten aus sozialen Netzwerken, keine E-Mails. Auch für diese interessiert sich der BND und sammelt sie in bislang noch unbekanntem Umfang. Allein der Telefondatenspeicher enthält jedoch schon  ""circa 11 Milliarden Einträge pro Jahr "". 
Doch offensichtlich ist das dem BND längst noch nicht genug. ZEIT ONLINE konnte weitere Geheimakten einsehen. Dort finden sich Hinweise, dass der Geheimdienst noch viel mehr solcher Informationen sammeln will. So hat der BND schon vor einigen Monaten im Bundestag beantragt, seinen Etat um 300 Millionen Euro aufzustocken. Mit diesem Geld soll die elektronische Ausstattung hochgerüstet werden. Der Name des Projekts: Strategische Initiative Technik (SIT). Darin findet sich ein Programm mit dem Namen EASD. Die Abkürzung steht für  ""Echtzeitanalyse von Streaming-Daten "". Wie die Akten belegen, will der BND fast 700.000 Euro investieren, um eine spezielle Datenbank-Software namens Hana zu installieren. Hergestellt wird sie vom deutschen Software-Konzern SAP. 
Das System Hana ist eine sogenannte In-Memory-Datenbank. Alle dort gespeicherte Daten liegen nicht auf Festplatten, sondern im sogenannten RAM, dem Arbeitsspeicher. Auf diesen kann innerhalb von Millisekunden zugegriffen werden. Hana kann deshalb Suchanfragen in einer Sekunde beantworten, für die ein festplattengestütztes System mindestens einen halben Tag brauchen würde. Fragt ein Agent ein solches System: Wo ist uns dieser Terrorist schon einmal aufgefallen?, erhält er unmittelbar ein Ergebnis. Wichtiger allerdings ist, dass Hana auch komplexe Anfragen fast ebenso schnell verarbeiten kann. Das heißt, die Datenbank kann viele verschiedene Daten miteinander verknüpfen, um Muster herauszulesen. Solche Technik braucht man, wenn man Metadaten in großem Stil auswerten will. 
Vielen Bürgern ist nicht bewusst, wie aussagekräftig Metadaten sind. Der BND tut einiges dafür, dass das so bleibt. Während der Anhörungen vor dem NSA-Untersuchungsausschuss sprechen die Geheimdienstler beispielsweise konsequent von  ""Routineverkehren "", wenn sie Metadaten meinen. Das klingt nach schlechtem Sex und soll verschleiern, dass sich dahinter eine flächendeckende, anlasslose und massenhafte Überwachung verbirgt. 
Außerdem argumentieren die Agenten, dass sie solche Routineverkehre überall in der Welt ohne Beschränkungen absaugen und nach Gutdünken verwenden dürfen. Der ehemalige Bundesdatenschutzbeauftragte Peter Schaar teilt diese Ansicht ganz und gar nicht. Er ist der Meinung, dass auch Metadaten durch das Grundrecht des Brief- und Postgeheimnisses geschützt sind. 
Aber selbst wenn Metadaten nicht unter dem Schutz des Grundgesetzes stehen sollten, müsste sich der BND eine Langfrist-Datenbank, die nach dem Gesetz eine  ""automatisierte Datei "" ist, vom Kanzleramt genehmigen lassen. Über jede automatisierte Datei, in der Daten länger als sechs Monate gespeichert werden sollen, muss außerdem die Bundesbeauftragte für Datenschutz informiert werden. 
Leider ist nicht zu erfahren, ob Letzteres geschehen ist. Dateianordnungsverfahren zu geheimen Dateien sind ebenfalls geheim, lautet die Antwort der Bundesbeauftragten auf entsprechende Fragen. Man könne sich dazu nicht äußern und die Existenz einer solchen Anordnung weder dementieren noch bestätigen. Es bleibt also nur die Hoffnung, dass die Datensammlung ordentlich genehmigt wurde. Eine kleine Hoffnung. Denn der BND hat schon andere Datenbanken teilweise über Jahre hinweg ohne die gesetzlich vorgeschriebene Zustimmung der Kontrollbehörden betrieben. 
Die Nebelmaschine des BND funktioniert offenbar auch gegenüber der Bundesregierung. Es war 2013, in der Zeit der großen Aufregung um die Enthüllungen Edward Snowdens. Im Juni und Juli des Jahres versicherten Regierungsmitglieder wie Kanzleramtsminister Ronald Pofalla immer wieder, NSA und BND hielten sich an deutsche Gesetze. 
Diskutiert wurden damals jedoch nur Inhaltsdaten, also Gesprächsmitschnitte sowie die Inhalte von Faxen und E-Mails. Um Metadaten ging es noch gar nicht. Erst Mitte August 2013 deutet der Grünen-Politiker Hans-Christian Ströbele erstmals an, dass es noch eine zweite Überwachungsebene geben könnte. Nach einer Sitzung des Parlamentarischen Kontrollgremiums sagte er, er habe erfahren, dass der Bundesnachrichtendienst  ""Hunderte von Millionen von Informationen von Kommunikationsverbindungen "" aus der Auslandsaufklärung speichere und an die USA weiterleite. Die Dimension des Problems jedoch erkannte auch Ströbele damals offenbar nicht. 
Im BND war man wohl ganz zufrieden damit, dass sich die öffentliche Debatte an anderen Themen abkämpfte und wirkte darauf hin, dass sich das auch nicht änderte. Ein Beispiel: ZEIT ONLINE hat Kenntnis von geheimen Akten, aus denen hervorgeht, dass der BND die Information über die Anzahl der abgefischten Metadaten für die Sitzung des Parlamentarischen Kontrollgremiums am 26. Juni 2013 zusammengestellt hatte. Doch selbst vor dem geheim tagenden und sehr verschwiegenen Kontrollgremium wollte der Dienst seine Aktivitäten nicht direkt offenbaren. Die Informationen sollten nur  ""reaktiv "" vorgetragen werden, ist in der Akte vermerkt – also nur, wenn die Parlamentarier gezielt danach fragen. 
Die Geheimdienstler gehen oft so vor. Damit erschweren sie es den Parlamentariern gewaltig, ihrer Kontrollaufgabe nachzukommen. Denn auf diese Weise können die Abgeordneten die Geheimdienstler nur noch dazu zwingen, zuzugeben, was die Kontrolleure ohnehin schon wissen.  ""Sie können ja nur nach konkreten Vorgängen fragen, wenn sie davon schon Kenntnis haben "", sagt Gisela Piltz. Sie war für die FDP im Bundestag und zur fraglichen Zeit Mitglied im Parlamentarischen Kontrollgremium.  ""Es war immer schwierig, umfassende Informationen zu erhalten, offensichtlich trauen die Nachrichtendienste auch Parlamentariern nicht unbegrenzt. "" 
Ähnlich geht es gegenwärtig im NSA-Untersuchungsausschuss zu. Ein Unterabteilungsleiter des BND namens W.K. hatte beispielsweise im November 2014 im Ausschuss zum ersten Mal eine Zahl genannt: 500 Millionen. Das sei die Menge an Metadaten, die vom BND an die NSA gesandt werde – jeden Monat. Die große Menge erkläre sich dadurch, dass ein einzelnes Telefonat schon Dutzende von Metadaten enthalte. Vermitteln wollte er: So viel ist das alles nicht. 
Tatsächlich steckt schon hinter diesen 500 Millionen Metadaten des Herrn K. ein Gegenwert von vielen Millionen Telefonaten oder SMS. Und das ist nur ein Bruchteil dessen, was wahrscheinlich übermittelt wird. Denn die 500 Millionen im Monat beziehen sich allein auf das Satellitenabhörprogramm in Bad Aibling. Wie viele der 220 Millionen täglich gesammelten Metadaten aus den anderen Abhörprogrammen in die USA gesendet werden, ist bislang nicht öffentlich geworden. W.K. verlor darüber kein Wort. 
Nachtrag: Bei der Sitzung des NSA-Untersuchungsausschusses am 5. Februar 2015 sagte der gleiche W.K. vom BND, dass nur ein kleiner Teil der gesammelten Daten an die USA geschickt werde. Wie viele genau, wollte er nicht beziffern. Zitat:  ""Das ist weniger als ein Promillebereich, der an die USA geht. "" Das wären im Zweifel immernoch mehrere hunderttausend bis Millionen Metadaten am Tag. 
FDP-Politikerin Piltz und auch verschiedene Mitglieder des NSA-Ausschusses fordern daher längst eine Überarbeitung der Geheimdienstkontrolle. Sie wünschen sich mehr Leute, mehr Sachverstand. Vor allem aber: mehr Einblick in die Arbeit der Agenten. 
Eine englischsprachige Fassung des Artikels findet sich hier. 
Haben Sie Informationen zu diesem Thema? Oder zu anderen Vorgängen in Politik und Wirtschaft, von denen Sie finden, dass die Öffentlichkeit sie erfahren sollte? Wir sind dankbar für jeden Hinweis. Dokumente, Daten oder Fotos können Sie hier in unserem anonymen Briefkasten deponieren."	technik
"Alle warten auf Google. Auf dem Mobile World Congress in Barcelona möchte das US-Unternehmen Yezz die ersten Module für das modulare Smartphone Project Ara vorstellen. Die gibt es auch – hinter einer Glasscheibe. Zwar hält Marketing-Manager Maurizio Sole Festa von der Mutterfirma DDM Brands ein Gerät in den Händen. Es handelt sich aber lediglich um ein Mock-up, mit dem das Prinzip erklärt werden soll. Die einzigen beiden funktionierenden Geräte in Barcelona sind im Besitz von Google. Ob die Google-Vertreter an diesem Tag noch einmal bei Yezz vorbeischauen, weiß Festa aber angeblich selbst nicht so genau. 
Die Situation ist durchaus kurios und zeigt: Bei Project Ara läuft nichts ohne Google. Gemeinsam mit der damaligen Tochter Motorola hat der Konzern das Projekt vor zwei Jahren offiziell ins Leben gerufen. Etwas später präsentierte der Niederländer Dave Hakkens sein Phonebloks-Konzept. Inzwischen arbeiten Google, einige frühere Motorola-Ingenieure und Hakkens zusammen: Die Idee ist ein Smartphone, das mit verschiedenen Modulen nach Belieben auf- und umgerüstet werden kann. 
DDM Brands und Yezz möchten zu den ersten gehören, die sowohl Module als auch die entsprechenden Endoskelette anbieten. So heißen die Gehäuse, auf die die Module gesteckt werden.  ""Wir glauben an die Individualität der Entwickler "", sagt Festa,  ""und möchten gleichzeitig den Nutzern Kreativität erlauben. Warum sollte es nicht Spaß machen, sein persönliches Smartphone zusammenzustellen? "" 
Von den ernsteren Gesichtspunkten ganz zu schweigen: Weniger Elektroschrott durch einfache Aufrüstung, neue Nutzerschichten dank preiswerter Konfigurationen und individuelle Einsatzgebiete durch spezielle Module. All das verspricht Project Ara, jedenfalls theoretisch. Die Kritiker zweifeln, ob das Zusammenspiel aus Hard- und Software tatsächlich so reibungslos funktionieren wird. 
Einige der Möglichkeiten, die Yezz in Aussicht stellt, sind Kameramodule zwischen zwei und 20 Megapixeln, die jeweils für die Vor- und Rückseite konzipiert sind und im Zusammenspiel 360°-Aufnahmen ermöglichen. Ein Smart-Cover-Modul, dass hinten eingeklinkt wird und sich dann über die Vorderseite legt. Ein Modul, das mit einem Spiele-Controller verknüpft ist und eines mit dem NFC-Chip. 
Die in Barcelona vorgestellten Module und Endoskelette von Yezz orientierten sich am Referenz-Design von Google. Dennoch soll jeder Hersteller später seine Module nach eigenem Ermessen herstellen können. Project Ara ist Open-Source, die Dokumentation frei verfügbar. Toshiba soll das sogenannte APU-Modul zum Start liefern. Es ist theoretisch das einzige, das die Nutzer benötigen, um das Gerät in Betrieb zu nehmen. Es enthält das Betriebssystem, den Arbeitsspeicher und Prozessor, der die einzelnen Module verwaltet. 
Damit die einzelnen Module auch von verschiedenen Herstellern funktionieren, entwickelt Google eine speziell angepasste Android-Version. Bevor Module in den offiziellen Verkauf gehen, müssen sie von Google zertifiziert werden. Der eigentliche Verkauf soll dann über einen Marktplatz geschehen, ähnlich wie Googles Play Store. 
Besonders schick sieht das Mock-up eines Project-Ara-Smartphones übrigens nicht aus. Zwar können die einzelnen Module auf Wunsch bedruckt werden, was der Rückseite des Geräts aber eher den Look einer Klowand mit Aufklebern verleiht. Zwischen den einzelnen Modulen sind kleine Rillen, das APU-Modul dagegen ragt jedenfalls im Prototyp etwas hervor. 
Und wie soll das eigentlich alles halten, etwa wenn das Gerät einmal aus der Tasche oder auf den Tisch fällt? Festa grinst bei dieser Frage. Das Geheimnis soll in elektropermanenten Magneten liegen, die zwischen Modul und Endoskelett für zusätzlichen Halt sorgen. Sie benötigen dazu zwar keine dauerhafte Stromversorgung, trotzdem ist die Akkuleistung von Project Ara immer noch ein kleines Problem. Ein Tag ohne Steckdose soll inzwischen aber möglich sein, sagt Festa. 
Noch in diesem Jahr möchten Google und Yezz einen ersten Testlauf in Puerto Rico machen. Dann soll auch der entsprechende Web-Store online gehen. Denkbar ist, dass Google nicht nur einzelne Endoskelette und Module, sondern bereits fixe Konfigurationen anbietet. Yezz spricht von drei Modellen für drei verschiedene Preisklassen: Die erste soll etwa 50 Euro kosten, die dritte etwa 300 Euro. 
Dazu muss aber Google mitspielen. Dass sich das Unternehmen in Barcelona mit einem funktionierenden Prototypen zurückhält, hat seine Gründe: Vergangenes Jahr präsentierte Google auf seiner Entwicklerkonferenz I/O bereits einen Prototypen. Der hängte sich allerdings nach dem Anschalten während der Präsentation auf – zum Spott der Anwesenden. 
Update: In einer früheren Version dieses Artikels hatten wir fälschlicherweise berichtet, die Stromversorgung der Elektropermanentmagneten würde im Project-Ara-Smartphone zu Akkuproblemen führen. Das ist so nicht richtig, wir haben die Passage geändert."	technik
"Die zweite Generation des Smartphones fürs vergleichsweise gute Gewissen ist fast fertig. Seit ein paar Tagen können Interessenten das Fairphone 2 vorbestellen, die Auslieferung der ersten 15.000 Geräte beginnt im November. Knapp 530 Euro kostet es, und damit deutlich mehr als das Vorgängermodell, von dem bis heute gut 61.000 Stück verkauft wurden. Das liegt zum Teil an der ordentlichen, aber nicht überwältigenden Ausstattung des Android-Smartphones, vor allem aber an der Bauweise und den verwendeten Materialien. 
Möglichst viele der im Fairphone verarbeiteten Rohstoffe sollen aus Minen stammen, deren Erträge nachweislich nicht aus konfliktbelasteten Gegenden kommen oder die von den Fairphone-Machern aus anderen Gründen als vergleichsweise vorbildlich eingestuft werden. Das klappt ansatzweise mit Wolfram und Zinn und soll irgendwann auch für Gold gelten. Ein Smartphone aus 100 Prozent fair gehandelten Materialien wird es nie geben, das geben die Entwickler selbst zu. Sie lernen aber beständig dazu und teilen ihre Erkenntnisse mit den potenziellen Kunden. Das schließt eine Auflistung ein, die anzeigt, welches Bauteil und welcher Teil der Produktion wie viel Geld pro Gerät kostet (hier für das erste Modell). Das  ""fair "" im Namen mag sich besser verkaufen, aber  ""transparent "" trifft es eher. 
Der Clou am neuen Gerät: Auch Laien sollen zentrale Module des Fairphone 2 ganz einfach auswechseln können, damit das Gerät möglichst lange hält. Mindestens fünf Jahre lang – das wünschen sich die Entwickler jedenfalls. Am Freitag präsentierte Fairphone-Mitgründer Miquel Ballester einen Prototyp in Berlin. Optisch und auch technisch werde sich bis zur Auslieferung noch einiges verändern, sagt Ballester. Aber ZEIT ONLINE konnte schon einmal ausprobieren, wie sich die einzelnen Module entfernen lassen. 
Zu den austauschbaren Teilen gehören die Hülle aus Gummi, das Display, der Akku, die Basiseinheit mit Prozessor und Speicher, die Hauptkamera und das Mikrofon plus Lautsprecher. Sieben Module werden es insgesamt sein, die Hülle mitgezählt. Um eines der tiefer liegenden auszutauschen, muss man erst die Hülle abziehen, dann den Akku herausnehmen und schließlich mit zwei Schiebern das Display entriegeln. Danach lässt es sich nach vorne schieben und schließlich abnehmen. Am Protoptyp war das noch recht hakelig, wirkte dafür aber stabil. 
Auf der unteren Hälfte sind die weiteren Module befestigt. Sie lassen sich mit einem kleinen Kreuzschraubendreher abschrauben, die entsprechenden Schrauben werden eindeutig markiert sein. Außerdem wird es kleine Symbole auf jedem Modul geben, die dem Nutzer zeigen, um welche Teile es sich handelt. Der Austausch eines Moduls sollte, so viel zeigte der kleine Test, in ein bis maximal drei Minuten möglich sein. 
Wer sich mit der Materie auskennt, wird einzelne Module auch öffnen und deren Bestandteile reparieren können. Laien empfehle er das aber nicht, sagte Ballester. 
Wer ein neues Modul braucht, wird es im Onlineshop von Fairphone bestellen müssen. Ob es in Zukunft bessere Module, also zum Beispiel eine Kamera mit höherer Auflösung geben wird, konnte Ballester nicht sagen. Fairphone sei als Firma gerade einmal zwei Jahre alt, da könne er nicht wissen, was in den kommenden zwei Jahren passiert. 
Die wichtigsten technischen Details des Geräts sind seit geraumer Zeit bekannt: Das Gerät hat einen fünf Zoll großen Touchscreen mit einer Auflösung von 1.920 mal 1.080 Pixeln, zwei Gigabyte Arbeitsspeicher, 32 Gigabyte Flash-Speicher und einen Steckplatz für Speicherkarten. Der Quad-Core-Prozessor ist ein Snapdragon 801 von Qualcomm. Die Hauptkamera hat eine Auflösung von acht Megapixeln. Das Fairphone 2 beherrscht LTE und bekommt zwei SIM-Karten-Steckplätze. 
Das Betriebssystem wird Android 5.1 alias Lollipop sein. Ein Upgrade auf Android M, das im Herbst erscheint, dürfte früher oder später möglich sein. Details dazu und zur Software insgesamt will Fairphone in naher Zukunft veröffentlichen. Ein offenes Geheimnis ist, dass Fairphone auch mit Jolla und Canonical redet, um eines Tages möglicherweise alternative Betriebssysteme anbieten zu können."	technik
"Der Computerwissenschaftler Eben Upton war Doktorand an der Cambridge University, als er eine Beobachtung machte : Der Großteil der jungen Informatikstudenten wusste gar nicht, wie ein Computer eigentlich funktioniert. Den Grund hatte er schnell ausgemacht. Viele Kinder und Jugendliche lernen zwar den Umgang mit Software, aber kaum jemand zeigt ihnen den Aufbau der dazugehörenden Geräte. Eine Lösung musste her und Upton und seine Kollegen wollten sie finden. Einen preiswerten Computer, der klein und robust genug ist, um ihn täglich im Rucksack zu transportieren, und der gleichzeitig viele Möglichkeiten bietet. Das war 2006. Die Idee des Raspberry Pi war geboren. 
Seit Beginn dieses Jahres ist das Raspberry Pi erhältlich . Dabei handelt es sich um einen sogenannten Einplatinen-Computer. Alle für den Betrieb erforderlichen Komponenten befinden sich auf einer einzigen Leiterplatte. Das spart vor allem Platz. Das Raspberry Pi ist gerade einmal so groß wie eine Kreditkarte und bietet doch zahlreiche Anschlussmöglichkeiten. Das neuere B-Modell besitzt einen Audio- und zwei Videoausgänge, einen Slot für SD-Speicherkarten, einen Netzwerkanschluss sowie zwei USB-Anschlüsse. Die zentrale Schaltstelle ist ein Chip auf Basis der ARM-Architektur , die auch in Smartphones vorkommt. Das Raspberry Pi ist zudem sehr günstig: Knapp 30 Euro kostet das System. 
Da das Raspberry Pi keine Festplatte besitzt, muss das Betriebssystem vorher auf einer SD-Karte installiert werden. Zurzeit unterstützt das System vor allem diverse Linux-Distributionen. Es gibt aber bereits Portierungen von mobilen Betriebssystemen wie Android oder ChromeOS, was ganz im Sinne der Erfinder liegt: Denn das Raspberry Pi soll nicht nur das Interesse von Hobby-Entwicklern wecken, sondern ganz nebenbei eine neue Kultur in der Hardwareentwicklung vorantreiben. 
Open-Source-Hardware 
Den nächsten Schritt auf diesem Weg hat das Team jetzt im offiziellen Blog vorgestellt . Als letztes fehlendes Puzzlestück hat der Chiphersteller Broadcom, dessen Technik im Raspberry Pi steckt, den Grafiktreiber als Open Source veröffentlicht. Der Vorteil quelloffener Treiber ist, dass sie die Verwendung der Hardware mit neuen Betriebssystemen erleichtern. Treiber können angepasst werden, ohne sie vorher mühsam nachkonstruieren zu müssen, oder sich eine meist beschränkte Erlaubnis der Hersteller zu besorgen. Entsprechend euphorisch geben sich die Raspberry-Pi-Macher im Blog. 
Nicht ganz so überzeugt sind dagegen die Nutzer. Bereits kurz nach der Ankündigung bemängelten einige , dass allenfalls die Hälfte des Grafiktreibers quelloffen ist. Die andere Hälfte, die Firmware, werde weiterhin automatisch beim Systemstart geladen und sei deshalb vor neugierigen Einblicken geschützt. Das Lob an Broadcom, wo Eben Upton nicht ganz zufällig inzwischen arbeitet, sei deshalb zu positiv ausgefallen. Andere Nutzer sehen in der Entscheidung dagegen einen guten Kompromiss zwischen den Ansprüchen von Herstellern und Entwicklern. 
Das Team des Raspberry Pi versucht sich derweil als Schlichter: Broadcom habe genau die Teile als Open Source veröffentlicht, die für die Portierung auf andere Systeme erforderlich seien und sei damit der erste Hersteller von ARM-Chips, der das tut. 
 
Der Raspberry-Entwickler Alex Bradbury hofft, dass auch andere Chiphersteller dem Beispiel Broadcoms folgen werden. Anders als bei Software, wo das Prinzip Open Source weit verbreitet ist, sind viele Hardware-Treiber geschlossen und bieten nur die Funktionen, die vom Hersteller gewünscht sind. Vor allem Grafikchips unterliegen meist starken Beschränkungen und mangelnder Dokumentation , die nicht selten auch Patenten und dem Konkurrenzdruck geschuldet sind. 
Im Fall des Raspberry-Pi aber sei die Geheimniskrämerei gar nicht notwendig, sagte Bradbury gegenüber ZDNet . Die Architektur sei so entwickelt, dass Broadcom durch die teilweise Offenlegung der Treiber keine Nachteile entstehen. 
Stattdessen überwiegen die Vorteile, glaubt Bradbury. Und tatsächlich können offene Treiber nicht nur die Restriktionen umgehen. Vor allem können so die Funktionen der Hardware verbessert werden, da sie jeder auf einzelne Systeme oder Anwendungsmöglichkeiten optimieren kann. 
Hilf dir selbst – und damit allen 
Unabhängige Projekte wie das Raspberry Pi können somit der Anfang sein, der dazu führt, dass sich auch die Hardware zunehmend den Nutzern öffnet. Viele Smartphones nutzen ARM-Chips, und auch dort sind es häufig proprietäre Treiber, die Entwickler in ihren Ideen bremsen. Sollten Hersteller wie Broadcom ihnen zumindest teilweise entgegen kommen, könnten am Ende auch andere Nutzer von den Ergebnissen profitieren. 
In diesem Sinne folgt das Raspberry Pi, wie auch ähnliche Projekte wie Arduino , das Cubie - und das Beagleboard , dem Credo der sogenannten Maker-Bewegung. Die hat sich, wie der Name sagt, dem Selbermachen verpflichtet: 3D-Druck, Gadgets, Roboter oder Computertechnik – hilf dir selbst, ist das Motto. Und gib Ideen und Ergebnisse zurück in die Community. Denn nur so können andere darauf aufbauen. 
Der Erfolg der noch jungen Hardware gibt dem Konzept des Open-Source-Rechners jedenfalls Recht. War der Mini-Computer ursprünglich für Schüler gedacht, ist er inzwischen vor allem unter Bastlern beliebt. Upton schätzt, dass inzwischen rund 400.000 Exemplare verkauft wurden. Immer mehr Anwendungen tauchen im Netz auf: Als mobile Jukebox etwa, als Sendeeinheit eines Wetterballons in 30 Kilometer Höhe, als Synthesizer oder als Add-On für Digitalkameras . Mit der Erschließung neuer Betriebssysteme dürften es noch viel mehr werden."	technik
"Eine der wenigen wirklich auffälligen Neuerungen am iPhone 6s ist etwas, das Computernutzer seit Jahrzehnten als Rechtsklick kennen. Was ein Klick auf die rechte Taste einer Maus auslöst, nämlich die Anzeige eines Menüs von kontextabhängigen Optionen, war auf Smartphone-Displays bisher nur über dezidierte Buttons möglich. Die wiederum mussten Nutzer in der Regel innerhalb einer App suchen. 
Apples 3D Touch genannte Technik ändert das: Ein fester Druck auf ein App-Icon oder bestimmte Bereiche innerhalb einer App öffnet Menüs mit Shortcuts zu beliebten Funktionen und erlaubt noch einige andere direkte Interaktionen, die bisher mehrere Schritte erforderten. 
Was unter der Bezeichnung Force Touch bereits in der Apple Watch und im Trackpad der aktuellen MacBooks steckt, hat Apple damit unter neuem Namen und mit erweitertem Funktionsumfang auch ins iPhone gepackt. Das Unternehmen geht wenig bescheiden davon aus, dass 3D Touch die Bedienung von Geräten ähnlich massiv verändern wird wie die Einführung von Multitouch-Gesten vor acht Jahren. 
Apple war damals nicht das erste und einzige Unternehmen, das Multitouch als Feature verkaufte. Und Apple ist auch jetzt nicht allein mit seiner Innovation. Aber es gibt eben nur ein Gerät, das sich irgendwo zwischen 50 und 90 Millionen Mal im Quartal verkauft und die Technik damit schlagartig als neuen Standard etablieren kann: das iPhone. 
Wenn es nach Chris Harrison geht, wird es dafür auch langsam Zeit. Harrison leitet die Future Interfaces Group am Human Computer Interaction Institute der Carnegie-Mellon-Universität in Pittsburgh, Pennsylvania. Er hat dort neue Eingabe- und Interaktionsmethoden unter anderem für IBM, AT&T und Microsoft entwickelt, darunter auch einige, die mehr oder weniger an 3D Touch erinnern. Für Apple hat er nie gearbeitet oder geforscht. Er sagt:  ""Seit Jahren wiederhole ich wie ein Papagei, dass sich der Mensch-Computer-Input weiterentwickeln muss, von Multitouch zu rich touch. "" 
Erste druckempfindliche Bildschirme habe es schließlich bereits Mitte der siebziger Jahre gegeben. Welche Vorteile sie bieten, wissen Forscher wie er also schon länger, als es iPhones gibt. Dennoch hat es bis 2015 gedauert, bis die Technik in Smartphones angekommen ist. 
Wer so ein Handy oder Tablet besitzt, sagt Harrison, ist mittlerweile ein Experte im Umgang mit diesen Geräteklassen, wird aber nicht mehr besser:  ""Nun ist die Zeit gekommen, ein paar besondere Funktionen einzuführen für all jene, die lernen wollen. Ich vergleiche es mit Tastatur-Shortcuts: Müssen wir Strg + C und Strg + V zum Kopieren und Einfügen beherrschen? – Nein. Aber wenn wir es gelernt haben, sind wir schneller und effizienter. "" 
Effizienz ist ein Begriff, den man auch bei Apple gerne verwendet, um 3D Touch anzupreisen. Im Alltag sieht das dann so aus: Statt zum Beispiel die Kamera-App mit einem Tap zu öffnen und dann den kleinen Button zu suchen, mit dem man von der Haupt- auf die Frontkamera wechselt, um ein Selfie zu machen, klicken iPhone-6s-Besitzer einmal kräftig auf das Icon der Kamera-App. Dadurch öffnet sich ein Minimenü mit Shortcuts zu vier möglichen Aktionen, darunter auch die Selfie-Option. Wer dann den Finger nicht vom Display nimmt, sondern direkt auf diese Option rutscht, ist minimal schneller im Selfie-Modus als über den bisherigen Weg. 
Das klingt schrecklich banal. Aber weil es solche Abkürzungen auch in Apple Maps, Fotos, Nachrichten, Mail und anderen systemeigenen Apps gibt, werden Poweruser eifrig davon Gebrauch machen. Chris Harrison sagt:  ""Müssen iPhone-Besitzer das nutzen? – Nein. Werden sie es dennoch häufig tun? – Vielleicht nicht. Aber wenn, dann werden sie es lieben. "" 
 
Bisher hat Apple nur drei externe Apps genannt, die 3D Touch integriert haben: Facebook, Instagram und Dropbox. Weitere werden spätestens nach dem Verkaufsstart des iPhone 6s und 6s Plus am 25. September bekannt werden. Oftmals dürften Entwickler 3D Touch einfach zum schnellen Zugriff auf grundlegende Funktionen einer App nutzen. Kreativere Einsatzmöglichkeiten sind aber vor allem im Rahmen von Games vorstellbar: Gas geben in einem Rennspiel etwa durch stärkeres Drücken auf das Display. 
Je kreativer und vielfältiger die Optionen ausfallen werden, desto mehr werden andere Hersteller eigene Versionen von 3D Touch implementieren wollen. Andernfalls müssen sie damit leben, dass die iOS-Version einer App der Android- oder Windows-10-Version überlegen ist. 
Huawei hat mit dem Mate S schon mal vorgelegt: Noch vor Apples Vorstellung des iPhone 6s hatte das Unternehmen aus China sein Mate S auf der Ifa in Berlin präsentiert, dessen Top-Version ebenfalls ein druckempfindliches Display hat. Auch Huawei nennt das ein wenig dreist Force Touch, womit der Name nicht mehr Apple-exklusiv ist. Die Technik ermöglicht unter anderem das Hineinzoomen in ein Foto, wenn man stärker aufs Display drückt. Das wiederum ist bisher weder mit 3D Touch noch mit Apples Force Touch möglich. 
Zudem kann Huaweis Top-Smartphone unterscheiden, ob Nutzer die Fingerkuppe oder den Knöchel auf das Display drücken; Knuckle Sense heißt letzteres. Damit ist es unter anderem möglich, den Anfangsbuchstaben einer App auf den Bildschirm zu schreiben, damit diese sich dann öffnet. Chris Harrison hat selbst eine vergleichbare Technik entwickelt und Finger Sense getauft. Er geht davon aus, dass jetzt viele Smartphone-Hersteller anfangen werden, alternative Eingabemöglichkeiten zu integrieren:  ""Technologien wie 3D Touch sind die erste Welle neuer Touch-Erfahrungen. "" 
Auch Apple selbst hat noch Potenzial. Zum einen haben noch längst nicht alle denkbaren Bereiche auf dem iPhone 6s eine 3D-Touch-Funktion. Welche Flächen und Icons 3D Touch unterstützen und welche nicht, müssen Nutzer durch Ausprobieren selbst herausfinden. Zum anderen wäre 3D Touch auch im iPad ziemlich praktisch. Das dürfte Apple nachreichen, sobald das Unternehmen ein gutes Verkaufsargument für die nächste iPad-Generation braucht."	technik
"Apple stellt Ping, sein mehr oder weniger soziales Netzwerk für Musik , ein. Das berichtet All Things Digital , das Techblog des Wall Street Journal , unter Berufung auf unternehmensnahe Kreise. Demnach wird der im September 2010 gestartete Dienst durch Apples Partnerschaften mit Twitter und Facebook ersetzt. 
Ping wird aktuell als Bestandteil von iTunes 10.6.3 und iOS 6 Beta ausgeliefert. Mit dem nächsten größeren Update von iTunes im Herbst 2012 soll der Dienst aber eingestellt werden. 
Apple hatte mit iTunes 10 eine neue Version seiner Musikverkaufssoftware vorgestellt, die erstmals mit einem sozialen Netzwerk verbunden war. Ping ist auf Musikfans zugeschnitten und informiert Freunde nicht nur über gerade laufende Songs, sondern soll anhand der Songauswahl auch Menschen mit gleichen Musikinteressen finden. Eine Folgen-Funktion ähnlich wie bei Twitter gibt es bei Ping auch. Sie informiert über Updates ausgewählter Accounts. 
Seit November 2010 können Nutzer von Ping und Twitter ihre Accounts auf den beiden Diensten verbinden . Wird innerhalb von Ping mitgeteilt, welche Musik gerade spielt und welche Musik gemocht wird, landet dies auch im eigenen Twitterfeed. 
Nur ein Verkaufskanal, kein soziales Netzwerk 
Kauft oder empfiehlt ein Nutzer einen Song, sehen seine Freunde bei Ping das, und der Kauf-Button leuchtet direkt darunter auf. Kritiker warfen Ping deshalb von Anfang an vor, kein echtes soziales Netzwerk, sondern ein verlängerter Verkaufskanal für Apples iTunes-Angebote zu sein, der wenig Interaktion für seine Mitglieder anbot. 
Auf dem Branchenkongress D10 hatte Apple-Chef Tim Cook den Misserfolg bereits eingestanden:  ""Wir haben es mit Ping versucht, und ich denke, die Nutzer haben abgestimmt und gesagt: 'Es lohnt sich nicht, hier viel Energie reinzustecken.' "" 
Erschienen bei golem.de"	technik
"Die Kanzlerin mag es gerne grundsätzlich, wenn es um die Digitalisierung in Deutschland geht.  ""Das Internet ist für uns alle Neuland "", hatte Angela Merkel vor zwei Jahren gesagt, als US-Präsident Barack Obama in Berlin zu Besuch war und die Spionage-Tätigkeiten der USA Thema wurden. 
Nun scheint sie schon ein bisschen mehr angekommen zu sein in diesem Neuland, denn sie findet fast schon liebevolle Worte für Facebook. So praktisch, so nützlich, einfach schön. 
Was sie denn von dem sozialen Netzwerk halte, fragte sie ein Besucher des Evangelischen Kirchentages in Stuttgart. Und Merkel antwortete:  ""Es ist schön, dass man es hat. "" So schön, wie man ein Auto hat  ""oder eine ordentliche Waschmaschine "". 
Und noch mehr: Facebook sei eine Bereicherung, wenn es darum gehe, anders als im realen Leben mit Menschen Kontakte zu knüpfen. Um dann aber wieder einzuschränken:  ""Aber auch Facebook wird nicht das ganze Leben glücklich machen. "" 
Für die Kirchentagsbesucher hat sie noch eine Tipp parat:  ""Aber man darf aus der puren Existenz von Facebook nicht entnehmen, dass ich automatisch tolle Freunde habe. ""
""Die Apple Watch benimmt sich wie ein Tamagotchi. Die Älteren unter Ihnen werden sich erinnern: Tamagotchis waren Spielzeuge aus den Neunziger Jahren. Gemacht, um zu nerven. Ständig verlangten die Fantasiewesen in ihrem Plastikei nach Aufmerksamkeit, wollten gefüttert, bespaßt und von ihrer Pixelkacke befreit werden. Wer ein Tamagotchi besaß, hatte einen Erziehungsauftrag, aber in Wahrheit erzog das Tamagotchi seinen Besitzer. Die Apple Watch tut genau das, wenn man sie nicht bremst. 
Hier soll es deshalb weniger um die technischen Daten der Apple Watch gehen. Wichtiger ist, was dieses Gerät mit mir macht, wie ich das finde und was sich daraus für die Zukunft ergibt. Denn eines muss man sich immer wieder vor Augen führen: Dies ist die erste Apple Watch. Vergleichen Sie mal das erste iPhone mit dem iPhone 6 und bedenken Sie, dass zwischen beide Modellen nur sieben Jahre lagen. Sieben Jahre, in denen wir nicht nur die Technik verändert haben, sondern die Technik auch uns. Die Frage lautet deshalb zunächst einmal: Wer benutzt hier wen? 
Diese Frage stellt sich, weil Apple mit seiner Smartwatch auch ein bestimmtes Menschenbild verkauft. Der Apple-Watch-Träger hat demnach von Montag bis Sonntag den gleichen Tagesablauf, will immer über alles informiert sein, will immer sofort auf alles antworten können und springt einmal pro Stunde auf, wenn seine Uhr ihn daran erinnert, woran Apple-CEO Tim Cook neuerdings glaubt: Sitzen ist der neue Krebs. Nachdem zwei Apple-Mitarbeiter mich beim Einrichten der Apple Watch beraten hatte, passte sie jedenfalls zum Tagesablauf von Tim Cook. Was aber, wenn mir der Tagesablauf von Tilman Rammstedt besser gefällt? 
Wer zunächst alles aktiviert beziehungsweise in der Standardeinstellung belässt, was die Apple Watch hergibt, beschwört einen Handgelenksdämon herauf. Ein Tamagotchi, das unterschiedslos jede empfangene E-Mail anzeigt, das klingelt und vibriert, wenn es das iPhone in der Hosentasche auch tut, das selbst an faulen Sonntagen alle paar Stunden mit einem deutlichen Klopfen zu mehr Bewegung mahnt und alle paar Minuten den Puls misst. 
Ich habe einige Tage gebraucht, um in der Begleit-App auf dem iPhone alle relevanten Einstellungsmöglichkeiten zu finden und der Apple Watch ihr anerzogenes Verhalten weitgehend abzugewöhnen. Leider geht das oftmals nur nach dem Prinzip Ganz oder Gar Nicht. Ausnahmen und Unregelmäßigkeiten sind nicht vorgesehen. In vielen Fällen ist Gar Nicht aber zumindest besser als Ganz. Seit ich meine Smartwatch weniger  ""smart "" gemacht und das Gefühl habe, einen Assistenten am Handgelenk zu tragen und nicht mehr ein aufmerksamkeitsheischendes Monster, trage ich sie gerne. 
Sehr gerne sogar. Um es deutlich zu machen: Von allen Smartwatches, die ich bisher getestet habe, gefällt mir die Apple Watch am besten. Deshalb ist dies auch kein Verriss, Tamagotchi-Vergleich hin oder her. 
38,6 x 33,3 x 10,5 Millimeter in allen kleinen Ausführungen 
42 x 35,9 x 10,5 Millimeter in allen großen Ausführungen 
Die Apple Watch Sport mit Aluminiumgehäuse wiegt ohne Armband 25 Gramm in der kleinen und 30 Gramm in der großen Version. 
Die Apple Watch mit Edelstahlgehäuse wiegt 40 beziehungsweise 50 Gramm. 
Die goldene Apple Watch Editon wiegt zwischen 55 und 69 Gramm. 
Sprachsteuerung mit Siri 
drehbare  ""digitale Krone "" zum Scrollen 
Force Touch 
Pulsmesser, Beschleunigungssensor, Gyrosensor, Umgebungslichtsensor 
Lautsprecher und Mikrofon, Telefonieren ist möglich 
Verbindung zum iPhone über Bluetooth 4.0 
Batterielaufzeit 18 Stunden (Herstellerangabe) 
Einstellungen erfolgen über die Begleit-App auf dem iPhone. Voraussetzung ist ein iPhone 5, 5c, 5s oder 6 beziehungsweise 6 plus. 
Technisch sind alle Apple-Watch-Modelle gleich. Die Preisunterschiede ergeben sich allein aus den verschiedenen Größen, Armbändern sowie Gehäusematerialien und dem verwendeten Glas. 
Das günstigste Modell ist die Apple Watch Sport mit Aluminiumgehäuse in der 38mm-Ausführung. Sie kostet 399 Euro. Die 42mm-Version kostet 449 Euro. 
Die Apple Watch mit Edelstahlgehäuse kostet zwischen 649 und 1249 Euro, je nach Größe und Armband. 
Die Apple Watch Edition in Roségold oder Gelbgold kostet zwischen 11.000 und 18.000 Euro. 
Zunächst einmal steht mir die Apple Watch gut, wie ich finde. Als Tim Cook sie erstmals der Öffentlichkeit vorstellte, sah ich sie nur auf Fotos und war wenig beeindruckt. Ein bisschen klobig, dachte ich. Und vor allem: rechteckig. Ich mochte rechteckige Armbanduhren nie. Jetzt trage ich die 42 Millimeter hohe, gut einen Zentimeter dicke Edelstahlversion, die mit Edelstahl-Gliederarmband stolze 125 Gramm wiegt – und finde sie schön. Nicht so schön, wie ich das iPhone 6 im Vergleich zum ersten iPhone finde. Aber schöner als alle anderen Smartwatches, die ich bisher ausprobiert habe. Einige meiner Kollegen ebenfalls. Auch sie können zunächst kaum glauben, dass dies die Apple Watch ist. Auch sie hatten sich eine klobige Computeruhr vorgestellt. Nicht etwas, das auf den ersten Blick einigermaßen unauffällig aussieht, auf den zweiten dann doch wie irgendetwas Neuartiges mit Bildschirm und auf den dritten annähernd so teuer, wie sie wirklich ist. Was Uhren tun dürfen. 
Und teuer ist sie. Das Modell, das Apple mir zur Verfügung gestellt hat, kostet 1.149 Euro. Mir ist egal, dass das in der Welt der Armbanduhren nicht viel ist, dass Apple-Geräte immer überteuert sind, dass ich sie ja nicht kaufen muss und so weiter. Ich finde sie trotzdem zu teuer. 
Aber zurück zu ihren Stärken. Da sind als nächstes die Standards. Wie alle Geräte dieser Klasse soll die Apple Watch dafür sorgen, dass ich mein Smartphone weniger oft aus der Tasche ziehe, entsperre, irgendetwas nachschaue und dann noch irgendetwas anderes, weil ich gerade schon dabei bin und wie sehen eigentlich die Reaktionen auf meinen Tweet von vorhin aus und ... oh, ich glaube, ich hätte eben aus der U-Bahn aussteigen müssen. 
Diese Aufgabe erfüllt die Apple Watch. Ich sehe, ob unter meinen neuen E-Mails eine wichtige ist und lasse das Smartphone in der Tasche, wenn das nicht der Fall ist. Ich falle nicht mehr auf das Phantomvibrieren am Oberschenkel herein, wenn ich fälschlicherweise glaube, eine Kurznachricht bekommen zu haben. Wenn ich das iPhone zum Navigieren benutze, reicht es meist, auf die haptischen Hinweise der Uhr zu achten, um zu erkennen, wann ich abbiegen muss und in welche Richtung. All das können andere auch. Die Navigation mit der Moto 360 von Motorola zum Beispiel hat mir ähnlich gut gefallen. 
Zwei Dinge dagegen haben die Konkurrenten (noch) nicht: Force Touch und die sogenannte Taptic Engine. Beide nutzt Apple bisher nur zaghaft, aber ich glaube, dass sie in künftigen Versionen der Apple Watch sehr viel mächtiger werden. 
 
Mit Force Touch hat Apple quasi den rechten Mausklick auf die Smartwatch gebracht. Ein fester Druck auf das Display öffnet Einstellungen oder ermöglicht neue Aktionen. Auf dem kleinen Touchscreen einer Armbanduhr ermöglicht das eine ganze neue Bedienebene. Eine, die mir besser gefällt als das Drehen an der  ""digitalen Krone "" der Apple Watch, weil sie weniger Feingefühl erfordert und deshalb zum Beispiel im Gehen praktischer ist. Es wird spannend sein zu sehen, inwieweit Apple den Force Touch (den ja auch das Trackpad des neuen MacBooks beherrscht) für Drittanbieter-Apps öffnet und was Entwickler daraus machen. Und ob andere Hersteller nachziehen und die Technik kopieren. 
Noch viel mehr gilt das für die Taptic Engine. Wo andere Smartwatches einfach nur vibrieren, klopft und pulsiert die Apple Watch am Handgelenk des Trägers, dass sie fast lebendig wirkt. Selten hat sich ein Stück Technik so sehr wie ein Teil des menschlichen Körpers angefühlt. Es steht natürlich jedem frei, die Vorstellung gruselig zu finden. Aber man muss kein Cyborg sein wollen, um diese Art der Interaktion faszinierend zu finden. Oder nützlich, wie die fast taubblinde MollyWatt. Sie erklärt in diesem Blogeintrag, wie ihr die Taptic Engine im Alltag hilft. 
Bisher können sich Apple-Watch-Besitzer untereinander Klopfzeichen und auch den eigenen Herzschlag senden, aber Apple fällt nicht viel mehr ein, als das zu einer romantischen Geste zu erklären. Dabei könnten die morse-artigen Klopfzeichen und das – am besten auch steuerbare – Pulsieren ein eigener Kommunikationskanal werden. Abhängig vom Sender und dem Kontext können solche Signale ganz verschiedene Bedeutungen annehmen, die sich nur den beiden Beteiligten erschließen. 
Ich glaube, die Technik könnte einen wortlosen Slang, eine Jugendsprache oder eine vergleichbar codierte Kommunikationsweise hervorbringen. Nebenbei würde ein Gerät, das uns antippt und sich manchmal fast wie etwas Organisches bewegt, noch mehr ein Teil von uns, als es Smartphones heute schon sind. Anders und zugegebenermaßen ziemlich einseitig-euphorisiert ausgedrückt: Apple hat die Chance, die Gesellschaft erneut zu verändern. 
Voraussetzung wäre, dass Apple den Zugriff auf die Taptic Engine auch Drittentwicklern erlaubt und am besten sogar eine plattformübergreifende Kommunikation ermöglicht, damit andere Hersteller ihre Version der Taptic Engine – sofern sie eine entwickeln – mit der von Apple kompatibel gestalten können. Ob irgendetwas davon wirklich passieren wird, steht natürlich in den Sternen. Aber man wird ja wohl noch davon träumen dürfen, dass Wearables mehr sein können als gezähmte Tamagotchis. 
Ich habe Apple übrigens gefragt, ob die Klopfzeichen verschlüsselt transportiert werden, so wie iMessages auch. Dann könnte man sogar eine hübsche alternative Messenger-App daraus machen. Leider hat Apple meine Frage bisher nicht beantwortet. 
Viele Daten und Details habe ich nicht erwähnt, über sie wurde woanders genug geschrieben. Nur so viel sei gesagt: Eine an meine Bedürfnisse angepasste Apple Watch hält zwei volle Tage durch, bevor ich den Akku aufladen muss. Also deutlich länger, als Apple verspricht. Die Spracheingabe mit Siri funktioniert gut, kommt für mich aber nur infrage, wenn ich mich unbeobachtet fühle. Als Fitnesstracker funktioniert die Apple Watch so gut oder schlecht wie andere Geräte auch. Apps gibt es jetzt schon Tausende – dazu werde ich einen eigenen Artikel schreiben. 
Nach mehreren Tagen mit der Apple Watch lautet mein vorläufiges Fazit: Die Apple Watch 2 oder 3 werde ich vielleicht kaufen. Ich hoffe, dass sie flacher und vielleicht ein wenig runder werden. Dass sie das Potenzial der Taptic Enginge besser ausnutzen (auch wenn dann vielleicht ein Phantomklopfen das Phantomklingeln der heutigen Zeit ersetzen wird). Und dass sie viel kleinteiligere Einstellungsmöglichkeiten bekommen. 
Ich möchte zum Beispiel bestimmen können, über wessen E-Mails und Kurznachrichten ich informiert werde und wen meine Uhr zu ignorieren hat. * Oder dass sie lernt, wessen Nachrichten ich sofort lese und wessen nicht. Ich möchte an Wochenenden auch ein anderes  ""Aktivitätsziel "" haben dürfen als unter der Woche. 
Ich will meine Apple Watch erziehen und gestalten können und nicht, dass sie das mit mir versucht. Wenn ich das wollte, könnte ich mir auch gleich die App Tamagotchi Classic installieren. Die wurde gerade aktualisiert, damit man sie auch über die Apple Watch benutzen kann. Beziehungsweise damit man sich über die Apple Watch benutzen lassen kann. 
* Update: Ein Leser hat mich darauf hingewiesen, dass ich durchaus einstellen kann, wessen E-Mails die Apple Watch anzeigt und wessen nicht. Dazu muss ich in der Begleitapp auf dem iPhone festlegen, dass nur die Mails meiner VIP-Kontakte auf der Uhr angezeigt werden sollen. Wer ein VIP-Kontakt ist, bestimme ich vorher selbst. Die Einstellung ist zu finden im Menü Meine Uhr unter Mail und dort unter Mail einschließen."	technik
"Auf genau 152 Nutzerkonten verteilt sich meine digitale Identität. Das geht so weit, dass ich sogar meine Gasrechnung online einsehen kann. Die digitale Prasserei tut nicht weh, hat aber einen großen Nachteil: Jedes dieser Konten braucht ein Passwort. Sicherheitsexperten raten zu einem, das nicht in Wörterbüchern steht, mindestens zwölf Zeichen hat und natürlich aus Zahlen, Klein- und Großbuchstaben sowie Sonderzeichen besteht. Man sollte es nicht für mehrere Dienste verwenden und bitte im Halbjahresturnus wechseln. Ich behaupte, nicht einmal neunjährige Gedächtnisweltmeister hätten Spaß daran, sich diesem Irrsinn auszusetzen. Passwörter überfordern uns. 
Bislang schütze ich meine digitale Identität mit den immer gleichen und miserablen Wörtern, womit ich, wie diese Liste und diese Statistik zeigen, mehrheitsfähig bin. Das macht meine Passwörter allerdings zu einer leichten Beute für Angreifer, die mit gigabytegroßen Wörterbuchlisten und starken Rechnern vorgehen. Dagegen will ich meine digitale Identität künftig besser schützen. Die Frage ist nur, wie? 
Ginge es nach der Fido Allianz, sollen Nutzer und starke Passwörter wieder zusammen finden. In der Allianz arbeiten seit 2012 Unternehmen wie Google, Microsoft und VISA an neuen Entwürfen. Vor allem die sogenannte Zwei-Faktor-Authentifizierung will die Allianz verbreiten. Die Idee: Man kombiniert ein leichter zu merkendes Passwort mit einem weiteren Faktor und erhält so ein starkes Passwort. 
Der weitere Faktor kann ein Fingerabdruck, ein einmaliger Code (Einmalpasswort) oder ein Token sein. Token, die meist in Form einer Chipkarte oder eines USB-Sticks vorkommen, gibt es schon länger, bislang vor allem in Unternehmen. Dort sichern sie meist den Zugang zu besonders sensiblen Daten oder Räumen. Token sind in der Regel weniger fälschungsanfällig als Fingerabdrücke oder andere biometrische Merkmale und der Nutzer hat – im Gegensatz zum versandten Einmalpasswort – alles selbst in der Hand. 
Die Fido-Allianz will deshalb Token voranbringen, wie den Yubikey der Firma Yubico. Der sieht aus wie ein schmaler USB-Stick und kann einfach am Schlüsselbund getragen werden. Jeder Rechner erkennt den kleinen Token als normale Tastatur. Mit einem Druck auf seinen Sensor gibt der Yubikey ein Passwort aus. Das kann entweder ein sehr langes Passwort zur ständigen Verwendung oder ein kurzes Einmalpasswort sein. Letzteres lässt sich dann für besagte Zwei-Faktor-Authentifizierung nutzen. Nutzer wählen eine der beiden Funktionen, indem sie den kleinen Sensor des Yubikeys lang oder kurz drücken. Google Mail sowie die Mail-Anbieter mailbox.org und Posteo unterstützen schon heute den Yubikey. Microsoft hat angekündigt, mit Windows 10 die Ideen der Fido Allianz zu unterstützen. 
Der Yubikey soll auch meine digitale Identität künftig besser schützen. Ich bestelle den Yubikey Neo für rund 50 Euro. Es gibt auch andere Modelle, aber der Neo hat einen Vorteil: Er lässt sich sowohl per USB an meinen Rechner andocken, als auch per drahtloser Nahfeldkommunikation, kurz NFC, benutzen. Ich könnte damit also auch auf meinem NFC-fähigen Smartphone gute Passwörter nutzen. 
Der erste Funktionstest ist simpel: Ich stecke den Yubikey in den USB-Slot meines Rechners und drücke das kleine Sensorenfeld mit der grünen Diode. Der Token spuckt sofort kleine Einmalpasswörter aus. So ist der Yubikey voreingestellt. Man testet das einfach im Editor oder Notepad. 
Der nächste Schritt ist schon komplizierter. Um den Yubikey auf meine Bedürfnisse anzupassen, brauche ich das Yubikey Personalization Tool. Das Programm läuft auf Windows, Mac OS und Linux, die Installation geht flott und ist intuitiv. Aber dann geht es los mit den vielen kryptischen Begriffen und Abkürzungen, die den Yubikey zu einem sinnvollen, aber auch komplexen Werkzeug machen. 
Ich öffne das Yubikey Personalization Tool und sehe in der oberen Leiste die Möglichkeiten des Tokens angezeigt. Dort befinden sich auch die zwei Grundfunktionen: Im static mode gibt der Yubikey das lange Passwort aus, im oath-hotp-mode hingegen das kurze Einmalpasswort. Zwischen beiden Funktionen kann ich – wie oben erwähnt – wechseln, in dem ich den Sensor lang oder kurz drücke. Das lange Passwort des static modes kann ich im Prinzip für jeden Onlinedienst verwenden. Jeden Onlinedienst mit demselben Passwort zu sichern, ist aber nicht besonders klug. Denn gerät das Passwort einmal in falsche Hände, zum Beispiel durch einen Hack oder aus Unachtsamkeit, stehen Angreifern gleich alle meiner Dienste offen. 
 
Der oath-hotp-mode ist da sicherer. (OATH steht für den Standard Open Authentification, HOTP steht für HMAC-based One-time Password Algorithm.) Ihn kann ich für die Zwei-Faktor-Authentifizierung nutzen. Auf der Yubico-Seite gibt es eine kleine Auswahl jener Dienste, die den oath-hotp-mode unterstützen, darunter Dropbox und Evernote. Man geht dann zum Beispiel auf die Dropbox-Seite, gibt seinen Nutzernamen und sein normales Passwort ein. Anschließend wird man aufgefordert, ein zusätzliches Einmalpasswort einzugeben. Das würde dann der Yubikey beisteuern. 
Aber von meinen 152 Diensten sind die wenigsten aufgeführt. Ich beschließe daher, den Yubikey im oath-hotp-mode anders zu nutzen: Ich lege mir einen Passwortmanager zu, parke in dessen verschlüsselter Datenbank meine 152 Zugangsdaten und nutze den Yubikey nur für die zweifache Sicherung dieser Datenbank. 
Der Passwortmanager meiner Wahl wird KeePass. Anders als die Konkurrenz ist sein Code quelloffen und damit theoretisch von jedermann auf Fehler oder Hintertüren überprüfbar. Außerdem gibt es KeePass für alle gängigen Betriebssysteme und Smartphones. Die Installation ist selbsterklärend und ich lege meine 152 Dienste dort an. Was natürlich einige Zeit dauert. Praktischerweise kann KeePass auch gleich sichere Passwörter für jeden meiner Onlinedienste generieren. Da ich sie mir eh nicht mehr merken muss, dürfen sie auch höllenschwer sein. Die KeePass-Datenbank selbst ist mit dem als sicher geltenden Advanced Encryption Standard (AES-256) verschlüsselt. 
Um diese Datenbank voller Passwörter gut zu schützen, habe ich mir zunächst ein besonders schweres Passwort ausgedacht. Es hat 40 Zeichen und wenn ich es falsch eingebe, brauche ich fast eine Minute, bis ich mit dem zweiten Versuch fertig bin, auf dem Smartphone gerne auch zwei. Der Yubikey kann diese Aufgabe für mich übernehmen. Ich stelle den Yubikey also im ersten seiner beiden sogenannten Slots auf static mode. Dabei hilft mir das Video auf der Yubico-Seite. Letztlich läuft es darauf hinaus, dass ich mein langes Passwort eingebe und Write Configuration drücke. Das war einfacher als gedacht. In der Bilderstrecke ist der Prozess noch einmal ausführlicher erklärt: 
Als kleines Schmankerl habe ich auf dem Yubikey nur 36 Zeichen meines Passworts verstaut. Die letzten vier Zeichen gebe ich per Hand ein. Sollte mein Yubikey verloren gehen, ist mein Generalpasswort damit noch nicht komplett in fremden Händen. 
Ich könnte an dieser Stelle aufhören: Um auf meine digitale Identität zuzugreifen, öffne ich also mit KeePass meine Passwort-Datenbank, stecke den Yubikey ein, drücke ihn kurz und gebe meine vier letzten Zeichen dazu. Aber noch einmal: Das ist keine Zwei-Faktor-Authentifizierung, sondern erleichtert nur die Eingabe eines sehr langen, komplexen Passworts. 
Im Alltag erweist sich die Kombination von KeePass und Yubikey anfangs dennoch als gewöhnungsbedürftig. Gerade wenn es schnell gehen muss – und das muss es ja eigentlich immer – nervt der Aufwand. Schnell merke ich aber auch, dass ich viel Zeit spare, weil ich nie Passwörter raten muss und dank KeePass immer weiß, bei welchem Dienst ich mich mit welchem Fantasienamen oder welcher Mail angemeldet habe. Wirkliche wichtige Passwörter sind zudem endlich richtig stark, ohne dass ich sie mir merken müsste. 
Noch besser wäre es natürlich, KeePass mit Zwei-Faktor-Authentifizierung zu nutzen. Sowohl KeePass als auch der Yubikey unterstützen das. Zuerst bereite ich den Yubikey vor. 
Dafür muss ich nun den zweiten Slot einrichten. Ich stecke den Yubikey in meinen Rechner und öffne das Yubikey Personalization Tool. Dort wähle ich anschließend den zweiten Slot aus und in den oberen Reitern den oath-hotp-mode. Wie genau die Einstellung vonstattengeht, ist oben in der Fotostrecke beschrieben. 
Dabei wird unter anderem ein sogenannter Secret Key erstellt, eine lange, wirre Zeichenfolge. Dieser Secret Key enthält das Geheimnis, aus dem der Yubikey immer wieder Einmalpasswörter generiert. Wichtig ist, dass man sich den Secret Key unabhängig von KeePass merken beziehungsweise ihn irgendwo speichern sollte. Denn geht später einmal etwas schief, komme ich mit dem Secret Key doch noch an meine Datenbank bei KeePass. 
Ist der Yubikey eingerichtet, muss ich außerdem noch KeePass für den Yubikey im oath-hotp-mode einrichten. Dafür lade ich die Erweiterung OtpKeyProv herunter und entpacke die enthaltenen Dateien in den Ordner von KeePass. Das ist auch schon die Installation. Nach wenigen Schritten und unter Verwendung des Secret Key, siehe Fotostrecke, ist meine digitale Identität mit zwei Faktoren gesichert. 
Diese Methode hat nun alle Vorteile gebündelt: Einen Passwort-Manager mit starken Passwörtern, gesichert durch einen Token mit einem sehr langen Passwort und einem Einmalpasswort. Wenn ich KeePass starte, gebe ich nun erst mein sehr langes Passwort via Yubikey ein, indem ich drei bis vier Sekunden den Sensor gedrückt halte. Im zweiten Schritt kommt nun noch mein Einmalpasswort dazu. Ich drücke dazu den Sensor des Yubikeys nur kurz. Fertig. 
Gleichzeitig zeigt sich in diesem Modus aber auch eine Schwäche: Man ist komplett vom Yubikey abhängig. Wird der Yubikey beschädigt, zerstört, verlegt oder gestohlen, verliert man alle Passwörter in der Passwort-Datenbank. 
Auf seiner Seite empfiehlt der Hersteller deshalb, einen zweiten Yubikey genau gleich zu konfigurieren. Das ist aber zum einen kostspielig und zum anderen im Falle des oath-hotp-mode auch unmöglich, denn jeder Secret Key ist einmalig. Wer den Yubikey also derart nutzt und dann verliert, ist vollends auf die Hilfe der unterstützenden Dienste angewiesen. KeePass zum Beispiel hat für genau diesen Fall einen Wiederherstellungsmodus (recovery mode). Dafür wird der Secret Key des Yubikey benötigt. 
Will ich den Yubikey via NFC auf meinem Smartphone benutzen, muss ich mich für eine seiner beiden Funktionen entscheiden, also entweder das starke, aber immer gleiche Passwort oder die kurzen Einmalpasswörter. Beides geht nicht. Denn anders als im normalen Modus gibt der Yubikey die Passwörter automatisch bei der Verbindung mit dem Smartphone ab. Den Sensor brauche ich nicht drücken, kann aber so auch nicht beeinflussen, welche Funktion der Yubikey übernimmt. 
Die Einrichtung ist wieder so wenig selbsterklärend, dass es auf manche abschreckend wirken dürfte. Ich muss dafür im Yubikey Personalization Tool die NFC-Funktion meines Yubikeys einstellen. Das geht so: Den Yubikey per USB in den Rechner stöpseln, das Yubikey Personalization Tool öffnen und unter dem Reiter Tools auf NDEF Programming gehen. (NDEF steht für NFC Data Exchange Format.) Dort werden – wie bekannt – die zwei Funktionen des Yubikey angezeigt. Der static mode für das lange Passwort und der oath-hotp-mode für die kurzen Einmalpasswörter. Ich wähle den oath-hotp-mode und drücke dann Program. Das wars. 
Der Yubikey greift nun automatisch auf die Einstellung zurück, die ich bereits vorher für den oath-hotp-mode gewählt habe, siehe oben. Halte ich den Yubikey also an mein Smartphone, sendet er ein achtstelliges Einmalpasswort via NFC an das Gerät. Theoretisch. 
Aber wenn ich nach der Konfiguration den Yubikey an mein Smartphone halte, öffnet sich im Firefox-Browser die Yubico-Seite, in der Adresszeile ist mein Einmalpasswort angehängt.Das hilft mir erst einmal nicht weiter. Um das Passwort auch in den Zwischenspeicher zu legen und so meine KeePass-Datenbank auf dem Smartphone öffnen zu können, muss ich zusätzlich die App YubiClip installieren. Gesagt, getan. Halte ich nun den Yubikey an mein Smartphone, wird das Einmalpasswort automatisch in die Zwischenablage kopiert. Ich kann es dann in die KeePass-Apps Keepass2Android oder KeePassDroid einfügen und so auf meine Passwortdatenbank zugreifen. 
Aber 152 Zugänge auf einem Smartphone zu speichern, das mir ebenso schnell abhanden kommen könnte wie den Yubikey, fühlt sich trotzdem falsch an. Für den mobilen Gebrauch habe ich daher eine eigene Datenbank mit nur wenigen Passwörtern darin eingerichtet. Einen Teil seiner digitalen Identität lieber zu Hause zu lassen, ist vielleicht grundsätzlich eine gute Idee."	technik
"Seit im Januar mein etwas in die Jahre gekommenes Samsung Galaxy S3 den Geist aufgab, suche ich nach einem neuen Android-Smartphone. Auf dem Mobile World Congress konnte ich im März das Galaxy S6 begutachten, das nicht nur mich in Sachen Design und Ausstattung überzeugte. Doch während ich noch hadere und warte, bis der Preis etwas fällt, taucht eine mögliche Alternative auf: Das neue LG G4 galt schon vorab als ein Geheimtipp des Sommers und tatsächlich macht es vieles richtig und einiges sogar richtig gut. 
Über Pfingsten konnte ich das neue Oberklasse-Gerät des südkoreanischen Herstellers testen. LG bietet gegen Aufpreis eine Abdeckung aus Echtleder an, die sich von der Smartphone-Einheitskleidung unterscheiden soll. Tatsächlich ist das Gewand – neben dem Holz-Cover des Motorola Moto X – einer der größten Hingucker, jedenfalls im Neuzustand: LG betont, dass sich das Leder im Laufe der Zeit abnutzt und so zu einem  ""individuellen Look "" führe. Anders gesagt: Es wird vermutlich speckig, und ob das dann immer noch so schick aussieht, ist wohl Geschmackssache. 
Wie im Vorgänger sind auch im G4 die Anschalt- und Lautstärke-Knöpfe auf der Rückseite und nicht, wie in den meisten Smartphones, an der Seite angebracht. Das gibt dem Gerät gemeinsam mit den nur leicht abgerundeten Ecken einen recht schnittigen Look. Zudem ist es LG gelungen, die Kameralinse in das Gehäuse einzuebnen – die hässliche Hervorhebung wie im iPhone 6 oder dem Galaxy S6 gibt es nicht, allerdings ist das G4 auch etwas dicker und zudem leicht gekrümmt. Das fällt aber erst beim zweiten Blick auf. 
Nicht verstecken kann das G4 seine Größe. Es ist nicht nur deutlich größer als mein altes S3, sondern auch noch einmal schwerer und größer als das Galaxy S6 und das iPhone 6, wie entsprechende Vergleichsbilder zeigen. Das 5,5 Zoll große Display hat eine Auflösung von 2.560 mal 1.440 Pixeln und soll mit einer neuen Technik einen größeren Farbraum abdecken als die Konkurrenz. Ob das Normalanwendern auffällt, sei dahingestellt. In jeden Fall ist das Display sehr hell und einem Smartphone dieser Preisklasse würdig. 
Für mich persönlich ist die Größe dennoch ein Problem, zumal Samsung die gleiche Auflösung im Galaxy S6 in ein dezenteres 5,1-Zoll-Display verpackt. So sehr mir das Design gefällt, so sehr stechen die Nachteile ins Auge, beziehungsweise in der Hose: Das kantige Gehäuse drückt in engeren Hosentaschen schon mal unangenehm und vielleicht hat ein Bekannter nicht unrecht, wenn er sagt:  ""Es ist vielleicht doch ein Handy für die Handtasche. "" 
Aber wen drückt schon die Jeans, wenn die inneren Werte stimmen. Eines der wichtigsten Kriterien für mich ist mittlerweile die Kamera. Mit dem Galaxy S6 hat Samsung die neue Referenzkamera für Android-Handys geliefert, und ausgerechnet hier will LG angreifen. Die 16-Megapixel-Kamera im G4 hat eine Blende von f/1.8, was noch einmal größer ist als jene im S6. Vereinfacht ausgedrückt: Die Kamera fängt mehr Licht ein und macht damit bessere Bilder bei schlechten Lichtverhältnissen. Jedenfalls theoretisch. Auch der Bildstabilisator und die Sensorgröße wurden noch einmal erweitert. 
Wie immer sind richtige Vergleiche zwischen Kameras nur unter Laborbedingungen möglich. Ich kann deshalb nur meine subjektive Meinung äußern, und die ist eindeutig: Das G4 macht exzellente Bilder, gibt die Farben genau wider und überzeugt gleichermaßen bei Sonnenlicht, bei Detailaufnahmen und auch in der Dämmerung liefert es noch passable Aufnahmen. Erste Vergleichstests zwischen dem G4, dem S6 und dem iPhone 6 gibt es unter anderem auf den Seiten von Android Central, Gotta be Mobile, Cnet und Green Bot. Einige bewerten die Kamera das G4 besser als die des S6 in bestimmten Situationen, im Test von Tom's Guide dagegen hat Samsung weiterhin die Nase vorn. 
Mich jedenfalls hat nicht nur mein erstes Bild im Panoramamodus begeistert. LG bietet Smartphone-Fotografen etwas an, für das andere Hersteller externe Apps benötigen: einen anständigen manuellen Modus. Während der Auto-Modus für die meisten Anwender reichen dürfte, können ambitionierte Fotografen in der Kamera-App Werte wie die ISO-Zahl und Belichtungszeit verändern. Zudem unterstützt das G4 die Aufnahme im unbearbeiteten RAW-Format. Solche Details zeigen, dass es LG ernst meint mit seiner Kamera. Nur einen kleinen Nachteil gibt es: Der Fokus braucht bisweilen etwas lange, bis er das Bild scharfgestellt hat. 
 
Neben der Kamera ist auch der Rest der Technik auf dem aktuellen Stand. Drei Gigabyte Arbeitsspeicher und ein Sechskernprozessor sind Standard für die Oberklasse, auch wenn der Prozessor in Benchmark-Tests nicht an die Achtkern-Konkurrenz von Samsung oder HTC herankommt. Überraschend ist, dass LG wie bereits beim Vorgänger auf einen Fingerabdruck-Sensor verzichtet. Damit zeigen die Koreaner, dass sie nicht jeden Trend mitmachen müssen, was mir wieder sympathisch ist. 
Gegen den aktuellen Smartphone-Trend spricht auch die Tatsache, dass sich die Rückseite des G4 abnehmen lässt und somit den Akku und einen externen SD-Speicherslot offenlegt. Viele Nutzer kritisierten Samsungs Entscheidung, diese Austauschmöglichkeiten im S6 einem Unibody-Design zu opfern. Ich persönlich habe noch nie einen Akku in einem Smartphone gewechselt, weshalb mir die Option weniger wichtig ist. Hauptsache, der Akku hält lange. 
Ausgerechnet hier zeigt das G4 zwei Gesichter. Nachdem ich in den ersten beiden Tagen das Gerät kaum genutzt habe, war ich überrascht, wie viel Akkuleistung noch übrig war. Ganz anders sah es bei intensiver Nutzung aus: Nach einer Weile mit aktiver Kamera, Twitter, Instagram und viel Zeit in den Menüs ging die Anzeige schnell von 100 auf etwa 70 herunter. Auch andere Tester bemerkten, dass der 3.000 mAh Akku auf dem Papier besser abschneidet als in der Praxis. Andere, wie etwa Engadget oder Netzwelt, lobten die Leistung dagegen, was ein klares Urteil schwierig macht. 
Bleibt noch die Software. Auch LG kann es sich natürlich nicht verkneifen, seine eigene Android-Oberfläche mitzuliefern. Die sieht im Vergleich zu Samsungs TouchWiz nüchterner und auch etwas futuristischer aus. Generell gefällt sie mir ganz gut, auch wenn sich einige erweiterte Einstellungen etwas verstecken. Vom Hersteller mitgelieferte, aber eigentlich überflüssige Software gibt es nur wenig, nimmt man die Google-Dienste einmal aus. Einige unnötige Apps lassen sich erfreulicherweise sogar komplett deinstallieren. 
Das Smart Bulletin ist ein zusätzlicher Homescreen, der verschiedene persönliche Dienste, etwa Wetter- und Ortsdaten sowie Kalendereinträge bündelt – Google Now lässt grüßen. Mit LG Health gibt es zudem die fast schon obligatorische Fitness-App, um die ich aber einen großen Bogen gemacht habe. Clever sind dagegen die sogenannten Smart Settings: Hier können die Nutzer verschiedene Standorte mit Einstellungen verknüpfen. So erkennt das G4, ob der Nutzer sich zu Hause, am Arbeitsplatz oder unterwegs befindet und aktiviert entsprechend das WLAN oder mobile Datennetz. Hilfreich – auch wenn ich aus Datenschutz- und Akkuleistungsgründen nur ungern ständig das GPS aktiviert lasse. 
Auch sonst hat LG einige nette Kniffe im Angebot. Wer etwa zweimal auf das Display klopft, aktiviert oder deaktiviert den Lockscreen (wie schon in den Vorgängermodellen). Die Front-Kamera hat einen Selfie-Modus, der auslöst, wenn die Nutzer aus der offenen Hand eine Faust ballen, und wer zweimal die Lautstärketaste drückt, löst sofort die Kamera aus, ohne vorbei das Display entsperren zu müssen. Hier zeigen sich leider die Nachteile der rückseitigen Knöpfe: In der Praxis ist das schnelle Drücken bei gleichzeitiger Ausrichtung fummelig. 
Nach fünf Tagen im Einsatz ist mir das LG G4 durchaus ans Herz gewachsen. Das Design sieht nicht nach dem nächsten iPhone-Klon aus (hör zu, Samsung!), die Verarbeitung wirkt, jedenfalls in der Ledervariante, sehr wertig – trotz des Plastikrahmens. In jedem Fall ist das G4 mal  ""etwas anderes "" und deshalb attraktiv. 
Das Highlight aber ist die Kamera, die es mit den besten Smartphone-Kameras auf dem derzeitigen Markt aufnehmen kann und durch Bildqualität und Einstellungsmöglichkeiten überzeugt. Ein paar Punkte in der B-Note gibt es für den austauschbaren Akku und das funktionale Android, das zum Verkaufsstart in der Version 5.1 Lollipop ausgeliefert wird. 
Ob das G4 mein nächstes Smartphone wird, ist dennoch fraglich. Während ich mit dem wankelmütigen Akku leben könnte, ist die Größe doch ein Problem. Möglicherweise sind mir 5,5-Zoll-Displays einfach einen Tick zu groß. Deshalb bleibt für mich persönlich das Galaxy S6 mit seiner etwas kleineren Bauweise und ähnlich guter Kamera die erste Wahl. Für alle Nutzer, die große Taschen haben, ist das G4 allerdings eine ordentliche Alternative. Die hat ihren Preis: Zum Verkaufsstart im Juni kostet die Leder-Version 699 Euro, die Variante mit Plastik-Cover gibt es für 649 Euro."	technik
"In der Türkei ist am Montag landesweit der Zugang zu Twitter und YouTube blockiert worden. Die Onlineausgabe der Zeitung Hürriyet meldete unter Berufung auf einen Sprecher des Verbandes der türkischen Internetprovider, die Sperre sei von der Staatsanwaltschaft angeordnet worden. 
Grund für die Sperren sei die Verbreitung eines Fotos aus der vergangenen Woche: Darauf war der von Linksextremisten in einem Gerichtsgebäude als Geisel genommene Istanbuler Staatsanwalt Mehmet Selim Kiraz zu sehen, wie er mit einer Pistole bedroht wurde. Kiraz, der zu den Gezi-Protesten ermittelte, war vergangene Woche aus einem Gerichtsgebäude in Istanbul entführt worden. Der Staatsanwalt sowie die beiden Geiselnehmer starben, als die Polizei versuchte, ihn zu befreien. 
Ein Sprecher von Präsident Recep Tayyip Erdoğan gab den Medien die Schuld an den erneuten Zugangssperren. Wären die Medien nach der Geiselnahme ihrer Verantwortung gerecht geworden und hätten das Foto nicht verbreitet,  ""wäre das Thema jetzt nicht auf der Tagesordnung "". 
Zunächst wurde auch der Zugang zu Facebook blockiert. Die Sperre wurde nach kurzer Zeit wieder aufgehoben. Laut Hürriyet begründete die türkische Internetbehörde BTK dies damit, dass die beanstandeten Fotos von Facebook gelöscht worden seien. 
Bereits im März vergangenen Jahres wurde im Vorfeld von Kommunalwahlen der Zugang zu Twitter und YouTube zeitweise blockiert. Auf den Internetseiten waren Tonaufzeichnungen veröffentlicht worden, die Korruptionsvorwürfe im Umfeld des damaligen Ministerpräsidenten Erdoğan belegen sollten. Das Verfassungsgericht hatte dies als Verletzung der Grundrechte gewertet. Erdoğan hatte allerdings am vergangenen Freitag ein Gesetzespaket unterzeichnet, das unter anderem eine schärfere Kontrolle des Internets ermöglicht."	technik
"In seiner Stellungnahme zeigt sich das für die Aufsicht von Verwertungsgesellschaften zuständige Deutsche Patent- und Markenamt (DPMA) am Donnerstagabend kurz angebunden: Die zuständige Schiedsstelle habe im Streit um das Leistungsschutzrecht Einigungsvorschläge unterbreitet, die prinzipiell die Zahlungspflicht von Google unterstützen.  ""Den Tarif der VG Media hält sie unter einschränkender Auslegung für anwendbar "", heißt es in der offiziellen Mitteilung. 
Google war wie mehrere andere Suchmaschinenbetreiber gegen den im vergangenen Jahr vorgestellten Tarif der Verwertungsgesellschaft VG Media vorgegangen. Die beanspruchte für die Verlage insgesamt elf Prozent der Umsätze einer Suchmaschine – beziehungsweise etwas über sechs Prozent für die von ihr vertretenen Angebote. Allein im Fall Google wären dies nach Kalkulation der Gesellschaft über 300 Millionen Euro pro Jahr, die sich die 230 Verlagsangebote in der VG Media teilen könnten. 
Hier hatten die Verlagsvertreter offenkundig zu hoch gepokert, denn diesen Geldregen wollte die Schiedskammer den Verlagen nicht zugestehen. Sowohl die Bemessungsgrundlage als auch die Höhe des Tarifs seien nicht angemessen, heißt es in der Mitteilung. Das bedeutet, die Verleger können nicht einfach alle Umsätze einer Suchmaschine zugrunde legen und müssen sich zudem mit geringeren Anteilen begnügen. Welche Beträge und Abrechnungsformen angemessen sein könnten, veröffentlichte das DPMA allerdings nicht. 
Ob Google und die anderen überhaupt zahlen müssen, ist aber noch nicht ausgemacht. Denn gleichzeitig öffnet die Schiedskammer ein Schlupfloch: So sollen Textausschnitte mit bis zu sieben Wörtern – plus Suchbegriffe – weiterhin kostenlos sein. 
In der Vergangenheit hatte sich gerade Google rigoros Zahlungspflichten von Verlegerseite entzogen. Auf der deutschsprachigen Nachrichtensuche erscheinen nur Angebote, die den Konzern vom Leistungsschutzrecht freigestellt haben. Als in Spanien ein ähnliches Leistungsschutzrecht wie in Deutschland in Kraft trat, hat der Konzern die dortige Ausgabe von Google News kurzerhand abgeschaltet. 
In seiner ersten Reaktion präsentiert sich die VG Media als Sieger. Geschäftsführer Markus Runde stellt fest:  ""Das Recht ist anwendbar. Google verwertet im Sinne des Urheberrechtsgesetzes die Presseerzeugnisse in den verschiedenen Google-Oberflächen. "" Damit seien wichtige Fragen geklärt. Das letzte Wort sei aber nicht gesprochen. Der Schiedsspruch ist nur verbindlich, wenn keine Seite widerspricht. Zudem sei es nötig, dass Google interne Daten offenlege, um einen neuen Tarif auszuhandeln, betont die VG Media. 
Google interpretiert die Entscheidungen der Schiedskammer gegenteilig.  ""Nach dem Bundeskartellamt hat nun auch die Schiedsstelle die Anträge der VG Media zurückgewiesen und in aller Deutlichkeit auf die Widersprüchlichkeit des Leistungsschutzrechts hingewiesen "", sagte Google-Sprecher Kay Oberbeck ZEIT ONLINE. Die Juristen des Konzerns prüfen nun die Entscheidung im Detail. 
Seit der Verabschiedung des unter Fachleuten umstrittenen Leistungsschutzrechtes vor zwei Jahren greift die VG Media insbesondere Google von zwei Seiten an. Einerseits will die Verwertungsgesellschaft dafür kassieren, wenn der Konzern auf öffentlich zugängliche Nachrichten zugreift. Gleichzeitig bestehen die Verlage aber auch darauf, dass Google ihre Angebote nicht aus den eigenen Suche entfernen darf oder sie gegenüber anderen Angeboten benachteiligt. Dies begründen sie mit der marktbeherrschenden Stellung Googles. Ergebnis wäre eine gesetzliche Zahlungspflicht für den US-Konzern, egal, ob er die Leistungen der Presseverleger nutzen will oder nicht. 
Erst vor zwei Wochen hatte das Bundeskartellamt zum zweiten Mal die Eröffnung eines von der VG Media geforderten Missbrauchsverfahrens gegen Google abgelehnt.  ""Wir haben Google hingegen deutlich gemacht, dass eine Totalauslistung einzelner Verleger einen Verstoß gegen das kartellrechtliche Diskriminierungsverbot darstellen könnte "", erklärte Kartellamts-Präsident Andreas Mundt. Da die Verlage derzeit jedoch rechtlich gegen Google vorgehen, sei das Verhalten des Konzerns zumindest vorläufig gerechtfertigt. In der Stellungnahme heißt es:  ""Auch ein marktbeherrschendes Unternehmen kann kartellrechtlich nicht dazu verpflichtet werden, bei einer ungeklärten Rechtslage ein erhebliches Schadensersatzrisiko einzugehen ""."	technik
"Als bei einem Autogipfel der Bundesregierung 2010 das Ziel ausgegeben wurde, im Jahr 2020 sollten in Deutschland eine Million Autos zumindest zum Teil elektrisch fahren, schien das Stichjahr noch lange hin, und die ersten Autohersteller hatten mit Elektro-Konzeptautos Interesse am alternativen Fahren geweckt. Inzwischen ist Ernüchterung eingekehrt. 
Jetzt sind es nicht mal mehr vier Jahre, bis das Ziel erreicht sein soll – doch Deutschland ist davon meilenweit entfernt. Die Chance, ein Elektroauto zu Gesicht zu bekommen, ist nach wie vor äußerst gering. Anfang dieses Jahres waren gerade einmal 0,06 Prozent aller rund 45 Millionen Pkw in Deutschland ausschließlich mit Strom unterwegs. In Zahlen: 25.502 Elektroautos. Dazu kommen noch 130.365 Hybridautos (0,29 Prozent), also solche mit einem Elektro- und einem Verbrennungsmotor. 
Experten bezweifeln, dass die Million bis 2020 überhaupt noch zu schaffen ist. An dem politischen Ziel kann man grundsätzlich Kritik üben. Die Autoindustrie wäre auch nicht gleich am Ende, wenn in vier Jahren nur 600.000 Elektroautos unterwegs wären. Nimmt man das Ziel aber ernst, dann werden Politik und Industrie noch mehr tun müssen, um es zu erreichen. Am Dienstagabend berät Bundeskanzlerin Angela Merkel mit ihrem Kabinett und den Vorstandschefs verschiedener Autohersteller über einen Kaufzuschuss, an dem sich nach dem Willen von Finanzminister Wolfgang Schäuble auch die Unternehmen beteiligen sollen. 
Ob die Maßnahme bei dem Treffen wirklich beschlossen wird, ist offen. Aber wie vernünftig ist sie? Um diese Frage zu klären, muss man die Grundfrage beantworten: Woran liegt es, dass nicht schon viel mehr Elektroautos gekauft werden? 
Dafür gibt es mehrere Erklärungsversuche. Der eine sieht ein Angebotsproblem. Tatsächlich gibt es derzeit nicht viele Elektroautomodelle, bei Hybridfahrzeugen sieht es besser aus. Hinzu kommt, dass abgesehen vom kalifornischen Newcomer Tesla die Hersteller wenig Werbung für ihre E-Autos machen. Die etablierten Autobauer haben ihre Werbebudgets für diese Modelle zuletzt sogar gekürzt. Das verstärkt den Eindruck, es gebe so gut wie keine Elektroautos am Markt. 
Der Grund: Die Hersteller verdienen derzeit nichts an den Stromern. Weil es keinen Massenmarkt gibt, ist die Fertigung vergleichsweise teuer, und die traditionellen Autobauer haben kaum Interesse, das Segment voranzubringen – schließlich verkaufen sie weiterhin ihre am Markt etablierten Benzin- und Dieselfahrzeuge. Sie gehen die Elektromobilität nur halbherzig an, um damit zumindest die wenig ambitionierten CO2-Grenzwerte der nächsten Jahre zu erfüllen. 
Darum ist von den deutschen Herstellern aktuell BMW der einzige, der mit dem i3 ein von Beginn an als Elektroauto konzipiertes Modell im Angebot an. Ansonsten bieten Volkswagen und Daimler Elektroversionen bekannter Modelle an (VW e-Golf, VW e-Up, Mercedes B-Klasse; ab Jahresende auch wieder Smart electric drive). Das Angebot wird sich in den kommenden Jahren aber verbessern. Opel etwa will im nächsten Jahr mit dem Ampera-e ein neues reines E-Auto auf den Markt bringen. Das weltweit meistverkaufte Elektroauto stammt aber aus Japan: Es ist der Nissan Leaf. 
Der zweite Erklärungsversuch für den fehlenden Durchbruch der Elektromobilität betrachtet die Nachfrageseite. Die Gründe für die mäßige Kundenakzeptanz sind bekannt. Die Stromer sind in der Anschaffung erheblich teurer als ein Verbrenner-Pendant. Ein Beispiel: Der viertürige VW e-Golf kostet in der Grundausstattung 34.900 Euro, ein ähnlich motorisierter Golf, der mit Benzin fährt, kostet in vergleichbarer Ausstattung als Viertürer dagegen nur ab 23.250 Euro. Bei über 11.000 Euro Differenz und einem aktuell niedrigen Benzinpreis erhöhen auch 5.000 Euro Anschaffungsprämie, die gerade im Gespräch sind, den Kaufanreiz nur gering. 
 
Außerdem stecken die E-Fahrzeuge in einem Nutzungsparadox fest: Die bisher beschränkte Reichweite spräche für einen Einsatz auf eher kürzeren Strecken vor allem im urbanen Raum – doch genau dort haben viele Menschen, die als potenzielle Nutzer in Betracht kämen, keine Garage, in der sie ihr Elektroauto an der Steckdose laden könnten. Und Ladesäulen gibt es in Wohngebieten noch viel zu wenige. Auf dem Land hingegen, wo Nutzer ihren Stromer in der eigenen Garage laden könnten, brauchen vor allem Pendler Autos mit großer Reichweite, die sie E-Autos nicht zutrauen. 
Zwar haben einzelne Hersteller die Akkus in ihren Elektrofahrzeugen verbessert, sodass diese mit einer Stromladung eine längere Strecke zurücklegen können als bisher. Beim Nissan Leaf etwa von 200 auf 250 Kilometer. Das liegt aber schlicht an größeren Batterien. Auch das Tesla Model S hat ja nur deshalb eine so große Reichweite von 500 Kilometern, weil darin entsprechend viele Akkuzellen verbaut sind – was sich aber eben auf den Preis niederschlägt. 
Teure Anschaffung und eine vergleichsweise geringe Reichweite: Schon dies macht Elektroautos noch wenig attraktiv. Zwar rechnen Experten in den nächsten Jahren mit sinkenden Batteriepreisen und einer verbesserten Akkutechnik. Bosch verspricht bis 2020 eine doppelt so hohe Batteriekapazität in der Golf-Klasse wie heute. 
Die Frage bleibt aber auch dann: Wo soll man sein Elektroauto aufladen? Reine Elektroautos sind zumindest lokal emissionslos und haben deshalb in Ballungsräumen ihren größten gesellschaftlichen Nutzen, wenn dort gesundheitsschädliche Abgase vermieden werden. Doch ohne einen Ausbau der öffentlichen Ladeinfrastruktur wird dort die Nachfrage nach E-Autos auch mit einer Kaufprämie kaum steigen. Wer keine Ladesäule in der Nähe der Wohnung hat, schafft sich auch bei 5.000 Euro Zuschuss kein Elektroauto an. 
Wenn die Regierung also die Elektromobilität finanziell fördern will, sind Mittel für die Errichtung weiterer öffentlicher Ladepunkte noch wichtiger als eine Kaufbeihilfe. Dazu zählen Schnellladesäulen entlang der Autobahnen, aber auch normale Ladesäulen gerade in urbanen Räumen. 
Zum Jahresende 2015 gab es laut dem Bundesverband der Energie- und Wasserwirtschaft (BDEW) deutschlandweit 5.836 öffentlich zugängliche Ladepunkte. Bundesverkehrsminister Alexander Dobrindt (CSU) will die Zahl in den nächsten Jahren kräftig erhöhen. 15.000 zusätzliche Ladesäulen schweben ihm vor. Dafür soll der Bund rund 300 Millionen Euro investieren. Auch darum wird es am Dienstagabend beim Autogipfel im Kanzleramt gehen. Die von Dobrindt ins Gespräch gebrachte Zahl ist ambitioniert. Selbst die EU geht für Deutschland von einem Bedarf von nur etwa 8.000 Ladesäulen im Jahr 2020 aus. Mindestens ebenso wichtig ist es, das Abrechnungswirrwarr zu beenden. Bisher sind die Tarifsysteme an den Ladesäulen uneinheitlich und verbraucherunfreundlich. 
Der Großteil der Autos wird allerdings auch in Zukunft außerhalb der Städte gefahren, wo das Aufladen zu Hause Grundlage des elektrischen Fahrens sein wird. Um künftig die Netze nicht zu überlasten und das Laden vernünftig steuern zu können, sind in den privaten Garagen sogenannte Wallboxes notwendig, an denen die Elektroautos angeschlossen werden. Ihre Verbreitung könnte die Regierung mit Beihilfen fördern. 
Kurzum: Staatliche Förderung sollte ein Mix sein, in dem die Stärkung der Infrastruktur – öffentlich wie privat – mindestens ebenso wichtig ist wie die direkte Kaufunterstützung, zumal Subventionen für Ladesäulen letztlich nachhaltiger sind. Dazu zählt außerdem eine weitere Förderung des Ausbaus erneuerbarer Energien. Denn nur dann sind Elektroautos auch aus Umweltsicht sinnvoll. Betreibt man sie heute mit dem deutschen Strommix, in dem mehr als 40 Prozent aus Kohlekraftwerken kommt, dann schneiden die E-Autos nicht besser ab als Pkw mit Benzin- oder Dieselmotor. 
Eines aber ist klar: Selbst wenn im Jahr 2020 tatsächlich eine Million Elektroautos in Deutschland unterwegs sein sollten – auffallen werden sie auch dann nicht. Bleibt der Gesamtbestand bis dahin konstant bei rund 45 Millionen Pkw, werden die Stromer dann nicht mehr als gut zwei Prozent ausmachen. In anderen Ländern steht es um die Elektromobilität übrigens auch nicht besser. Bei allem Hype um Tesla: In den USA hatten von den Neuwagen zuletzt auch nur 0,3 Prozent einen Elektromotor – trotz einer staatlichen Förderung."	technik
"Facebook hat es vorgemacht: Das Unternehmen möchte gegen Hass vorgehen, beziehungsweise vorgehen lassen, indem es die Verantwortung auf die Nutzer überträgt. Sie sollen sich Gewaltaufrufen entgegenstellen und Hetze mit Argumenten entkräften. Counterspeech heißt das Konzept. Experten sind von dessen Wirksamkeit nicht überzeugt. Aber es kann sehr vorteilhaft sein, jedenfalls für Facebook. Schließlich muss es dann weniger häufig entscheiden, welche Posts und Profile es löscht. 
Klingt gut, dachte man sich wohl bei Twitter und hat den Kampf gegen Hassbotschaften ebenfalls ausgelagert. In seinem neuen Trust & Safety Council sind mehr als 40 Organisationen und Experten vertreten, etwa Jugendschutz.net oder EU Kids Online. In einem Blogpost erklärte das Unternehmen am Dienstag, dass sie zusammen mit Twitter eine freie und sichere Meinungsäußerung gewährleisten sollen. 
Als amerikanisches Unternehmen verweist Twitter gerne auf die in den USA durch den ersten Zusatzartikel der Verfassung sehr weitgehende Meinungsfreiheit. Was in anderen Ländern als Volksverhetzung verboten wird, kann in den USA durchgehen. Wie auch Facebook profitiert Twitter davon, schließlich muss das Unternehmen wenige Ressourcen aufwenden, um Tweets und Konten löschen zu lassen. 
Die Hauptverantwortung liegt bei den Nutzern. Sie sollen missbräuchliches Verhalten melden und bei Drohungen die Polizei einschalten. Denn  ""Websites können eine Drohung nicht untersuchen und beurteilen "", heißt es in den Regeln von Twitter. Erst wenn die Behörden aktiv werden, könne man mit ihnen zusammenarbeiten. Zusätzlich schult das Unternehmen Nichtregierungsorganisationen (NGO), damit sie auf Twitter eine positive Geschichte erzählen. Als Beispiel für sogenannte Counternarratives nennt Twitter den Hashtag #NotInMyName, mit dem sich Muslime nach den Anschlägen von Paris im November gegen Gewalt im Namen des Islam bekannt hatten. 
Nutzer, NGOs und Experten sollen also gegen Hass im Internet vorgehen. Und Twitter? Das Unternehmen löscht nur dann, wenn es sein muss. Doch die US-Technikfirmen sollen bei der Terrorbekämpfung helfen, das wünscht sich die US-Regierung. Twitter gerät so unter Druck. Über soziale Netzwerke wird häufig Propaganda verteilt, es sei sogar die erste Wahl für die Terrorgruppe  ""Islamischer Staat "" (IS), heißt es in einer Studie, die im Dezember erschienen ist. Twitter hatte daraufhin angekündigt zu handeln. 
Ende Dezember hat das Unternehmen seine Richtlinien geändert und die Gewaltdefinition um  ""Drohungen oder Aufforderungen zu Terrorismus "" erweitert. Außerdem hat Twitter seit Mitte 2015 über 125.000 Konten entfernt, die im Zusammenhang mit dem IS standen und mehr Mitarbeiter eingestellt, die Beschwerden der Nutzer verfolgen sollen. Genaue Zahlen nennt Twitter nicht. 
Auch andere Staaten wollen ihre Ansprüche geltend machen. In halbjährlichen Transparenzberichten führt das Unternehmen auf, welche Staaten Inhalte entfernen lassen wollen. So hat Twitter im ersten Halbjahr 2015 genau 1.003 Löschanträge erhalten. Die meisten stammen aus der Türkei, nämlich 718. Twitter hat 60 Prozent der türkischen Anträge widersprochen, konnte sich aber nur in jedem 20. Fall durchsetzen. Schon mehrfach wurde der Zugang zu Twitter in der Türkei gesperrt. Aus Deutschland kamen im selben Zeitraum 14 Löschanträge, die hauptsächlich den Jugendschutz betrafen. 
Das Unternehmen selbst teilt mit, es gebe keinen  ""magischen Algorithmus "", der terroristische Inhalte oder Hetze finden könne. Dem Unternehmen bleibt deshalb nur die Wahl, seine Angestellten entscheiden zu lassen, was stehen bleiben darf und was nicht, oder eben die Nutzer gegen den Hass arbeiten zu lassen."	technik
"Einmal pusten, bitte – erst dann springt der Motor an. Allerdings nur, wenn der Alkoholgehalt im Atem des Fahrers stimmt. Menschen, die wiederholt wegen Trunkenheit am Steuer aufgefallen sind oder dieser vorbeugen wollen, können sich einen sogenannten Alcolock in ihr Auto einbauen lassen. Plumpst der Fahrer nach einem Gelage in den Sitz, kann er es sich getrost bequem machen, doch den Wagen bekommt er dann nicht an. 
Vorreiter bei den sensorischen Zündsperren ist Schweden. Dort darf ein Arbeitgeber im Arbeitsvertrag von Mitarbeitern die Nutzung eines Alcolocks verlangen.  ""Insofern gibt es eine rechtliche Grundlage "", sagt Volvo-Deutschland-Sprecher Olaf Meidt. Der Staat an sich verlange die Nutzung von seinen Bürgern allerdings nicht. Volvo machte mit seinem entsprechenden Gerät, dem Alcoguard, früh auf sich aufmerksam. 
In Deutschland ist der rechtliche Rahmen dafür noch nicht abgesteckt.  ""Es gibt immer noch einige Hürden zu nehmen "", sagt Simone Klipp von der Bundesanstalt für Straßenwesen (BASt). Die Wissenschaftlerin hat anhand von amerikanischen Studien die langfristige Wirkung der Alcolocks erforscht. Ihr Befund:  ""Die Studien haben gezeigt, dass man mit den Geräten gefährliche Alkoholfahrten wirksam unterbinden kann "", sagt Klipp und macht sich für eine Einführung in Deutschland stark.  ""Natürlich werden die Geräte nicht flächendeckend zur Pflicht werden, das ist auch gar nicht mit dem Grundgesetz vereinbar "", schränkt sie ein. Aber sie seien eine sinnvolle Ergänzung zu bereits vorhandenen Maßnahmen wie dem Führerscheinentzug oder der Fahreignungsprüfung. 
Wer derweil auf freiwillige Selbstkontrolle setzt, kann auch in Deutschland nachrüsten. Als einziger Autobauer bietet zwar Volvo für rund 1000 Euro werksseitig einen Promilletester, doch Besitzer von Fahrzeugen anderer Marken können auf Drittanbieter wie Dräger oder ACS zurückgreifen. Deren Produkte sind allerdings noch teurer: 1500 bis 2000 Euro werden verlangt. So richtig kommt zumindest Volvos Alcoguard aber nicht an.  ""Die Nachfrage in Deutschland ist sehr gering "", sagt Volvo-Sprecher Meidt über das firmeneigene Gerät. Seit der Einführung in Deutschland vor einem Jahr hätten sich nur sehr wenige Kunden dafür entschieden. 
Die Akzeptanz für Alcolocks in der Bevölkerung ist allerdings sehr hoch. Nach einer Umfrage des Technik-Branchenverbands Bitkom ist jeder Zweite für einen grundsätzlichen Alkoholtest. Nur vier Prozent lehnten einen Promille-Check vor jeder Fahrt ab.  ""Bei jedem fünften Verkehrsunfall mit Personenschaden ist Alkohol im Spiel "", sagt Bitkom-Präsident August-Wilhelm Scheer. Die große Zustimmung in der Bevölkerung für standardmäßige Alkoholtests freue ihn daher sehr. 
 
Weniger Freude dürfte dagegen bei denen aufkommen, die sich einen Vorteil bei der Kfz-Versicherung versprechen. Die Chancen für private Nachrüster auf Rabatte bei der Auto-Haftpflicht oder dem Kasko-Schutz stuft Christian Lübke vom Gesamtverband der Versicherer als gering ein – schon jetzt lägen die Durchschnittsprämien in der Kraftfahrtversicherung auf dem Niveau der 1980er Jahre. Allerdings sei es jedem Versicherer freigestellt, welche Rabatte er gewährt. 
Angesichts der recht teuren Geräte dürfte sich das Nachrüsten zumindest finanziell also kaum lohnen. Flottenmanager können theoretisch mehr rausholen. Zum einen ist ihr Verhandlungsspielraum mit der Versicherung größer, zum anderen bekommen sie wegen der höheren Stückzahlen die Alcolocks selbst preiswerter. 
Wer sich aus ideellen oder selbsttherapeutischen Erwägungen dennoch für einen Alcolock entscheidet, sollte wissen: Auch die Geräte selbst arbeiten nicht fehlerfrei. Die Alcolocks seien zwar gut gegen verschiedene Manipulationsversuche geschützt, versichern die Hersteller. Aber es kam schon vereinzelt zu Fehlalarmen: Das Gerät versagt dann die Fahrt, obwohl man keinen Alkohol getrunken hat. Außerdem zeichnen die Promilletester jeden Versuch auf, was zumindest die Frage nach dem Umgang mit diesen Daten aufwirft."	technik
"Sie fühlen sich als die harten Jungs unter den Radfahrern, naturverbunden und hart im Nehmen. Mountainbiker sind da unterwegs, wo sich die  ""Warmduscher "" mit ihren Tourenrädern gar nicht erst hinwagen. In diesen Kreisen war eine elektrische Unterstützung beim Pedalieren vor nicht allzu langer Zeit vollkommen undenkbar. Doch so wie mehr und mehr Komfortelemente an die Mountainbikes (MTBs) kamen, um das Radeln über Stock und Stein angenehmer zu machen, erobert sich auch der Elektromotor am Bike seinen Platz. 
 ""Lithium und Laktat sind längst keine Gegensätze mehr "", formuliert Gunnar Fehlau vom Pressedienst Fahrrad. Vor allem, seit die Pedelecs das Image als Rentner-Räder abgelegt haben, entdeckt eine neue Zielgruppe den Reiz des Radfahrens mit elektrischem Rückenwind. Offensichtlich ist die Vorstellung, mit sanfter Hilfe lange Anstiege abseits der befestigten Wege zu meistern und nicht ganz außer Atem den Gipfel zu erreichen, zu verlockend. 
Der elektrische Antrieb erweitert zudem die sportlichen Kapazitäten und macht noch steilere Anstiege erreichbar. Und schließlich hat sich das einst plumpe Design der E-Räder gewandelt, ist cool und sportiv geworden. Akkus und Motoren sind inzwischen so integriert, dass die meisten Zeitgenossen – ausgenommen die Puristen unter den Mountainbikern – die Hilfe gar nicht erkennen und höchstens anerkennend dem Radler hinterherblicken. 
Allerdings ist das Vergnügen, elektrisch unterstützt über Waldwege zu heizen, nicht gerade preiswert. Unter 2.500 Euro ist kaum ein entsprechendes Rad im Programm der Hersteller. Doch auch die MTBs ohne Motor gehören nicht gerade zu den Sonderangeboten, zumindest sofern sie eine aufwendige Ausstattung bieten. Die Kundschaft ist also durchaus sensibilisiert für höhere Kosten, und so folgt die Industrie dem Trend, dass  ""sich die Elektrifizierung im gehobenen Segment immer stärker durchsetzen wird "", sagt Fehlau. Angesichts des schwachen Euro werden sich die Preise in der Branche, die traditionell in Asien arbeiten lässt und in US-Dollar abrechnet, ohnehin nach oben bewegen. 
Wie ein dynamisches Rad aussehen kann, zeigt der Schweizer E-Bike-Hersteller Flyer mit dem Goroc, das mindestens 3.100 Euro kostet. Wie fast alle Räder in diesem Segment wird es von einem dezent montierten Mittelmotor angetrieben. Der stammt in diesem Fall von Bosch. Die niedrige Platzierung erzeugt einen tiefen Schwerpunkt, was sich positiv auf das Fahrverhalten auswirkt. 
Mit einem Gewicht von 20 Kilogramm gehört das Goroc zu den Leichtgewichten in seiner Klasse. Der Grund: Das Rad ist ein sogenanntes Hardtail, das heißt, das Hinterrad ist nicht gefedert. Dadurch ist es tendenziell leichter. Das Goroc besitzt eine vordere Federgabel mit einem Federweg von 100 bis 120 Millimetern. Die Schaltung bietet zehn Übersetzungen – das klingt nicht nach viel, ist aber auch für schweres Gelände vollkommen ausreichend. 
Doppelt so viele Gänge bietet das SDURO AllMtn RC von Haibike ab 4.000 Euro. Der Mittelmotor von Yahama erlaubt die Montage eines Doppelkettenblatts, was den Gipfelsturm deutlich erleichtert. Das noch mal mindestens 300 Euro teurere Xduro FatSix von Haibike mit Bosch-Antrieb und besonders dicken Reifen hat sogar ein Navigationssystem, mit dem sich der Radler auch in unbekanntes Gelände wagen kann. 
Weniger für Wege in Wald und Flur, sondern für den Pendler, der am Wochenende auch mal sportlich unterwegs sein will, ist das Charger GT45 HS von Blue Label gedacht, der Zweitmarke des Radherstellers Riese & Müller. Die blau-gelbe Lackierung des Fahrrads soll an die Gulf-Porsche erinnern, die einst in Le Mans siegten. Voriges Jahr gewann Blue Label damit den Red-Dot-Award für Produktdesign. Das Charger GT45 HS wird bis zu 45 km/h schnell, braucht deshalb ein Versicherungskennzeichen und darf nur mit Helm und Mofa-Führerschein gefahren werden. Für diejenigen, denen es weniger schnell gehen darf, gibt es auch eine gedrosselte Version. 
Beobachter der Branche wie Gunnar Fehlau gehen davon aus, dass Fahrräder mit Elektromotor auch in der kommenden Saison Stütze des Fachhandels bleiben werden – vor allem, weil auch urbane Pedelecs inzwischen ein modernes, dynamisches Design zeigen. Bleibt als letztes Segment noch das Rennrad, von dem es bisher nur wenige elektrifizierte Exemplare gibt. Doch  ""in zwei bis drei Jahren kommen auch elektrische Renner auf den Markt "", ist sich Fehlau sicher."	technik
"Wer am Samstag in einer größeren Stadt bummeln geht, ist happige Preise gewöhnt: eine Kugel Eis 1,50 Euro, das kleine Mineralwasser im Café 2,50 Euro und die Portion Spaghetti Carbonara – die bei uns im Dorf neun Euro kostet – schlägt mal eben mit zwölf Euro zu Buche. Da ist es doch nur logisch, dass auch der – in den Augen des Herstellers einzig wahre – Stadtwagen mit zu seinem Revier passenden Preisen vorfährt. 
Die Rede ist natürlich von der neuen, dritten Generation des Smart Fortwo. Mit der derzeit stärksten Motorisierung kostet er mindestens 11.790 Euro. Und dann steht er noch relativ nackt da. Unser mit jeder Menge Extras ausgerüsteter Testwagen übersprang locker die 15.000-Euro-Marke. Dafür gibt es auch schon Basismodelle in der deutlich größeren Kompaktklasse. Zumal der Smart-Händler über Rabatte kaum mit sich reden lässt, der – sagen wir mal – Kia- oder Peugeot-Händler schon. 
Gut, er kostet also. Aber für das viele Geld erhält man auch ein einmaliges Auto: Mit seiner Kürze von 2,70 Metern fährt der Smart in seinem eigenen Segment. Seit Toyota den (30 Zentimeter längeren) iQ vom Markt genommen hat, gibt es gar keine Alternativen mehr. Als Stadtwagen ist der Smart einfach unschlagbar. Der Wendekreis ist mit seinen knapp sieben Metern unfassbar klein. Unfassbar, weil man es als Fahrer manchmal selbst nicht glauben kann, in welch engen Straßen man das Mikro-Auto problemlos in einem Zug wenden kann. 
Und das natürlich leidige Thema Parken. Der Smart ist so lang geblieben wie sein Vorgänger. Auch mit dem durfte man eigentlich schon nicht mehr quer einparken, weil ein Stück der 2,70 Meter dann doch auf die Straße ragt. Aber in vielen Fällen wird es knöllchenfrei toleriert. Und selbst wenn nicht: Mit dem Fortwo rangiert man in Lücken, an denen Nicht-Smart-Fahrer achselzuckend vorbeifahren müssen. 
Diese Vorteile des Konzepts hat sich der Smart in der dritten Generation bewahrt. Genauso wichtig ist jedoch, dass er einige Nachteile der alten nicht mehr hat. Da wäre in erster Linie die lahme Halbautomatik zu nennen, die durch ein aufpreispflichtiges Doppelkupplungsgetriebe ersetzt wurde. Unser Testwagen hatte die serienmäßige Fünfgang-Handschaltung an Bord. Über die gibt es nichts zu meckern. Zudem verfügt der Smart jetzt über so etwas wie Fahrkomfort. Vorbei die Zeiten, wo seine Fahrer hilflos hoppelnd jeder Straßenrille ausgeliefert waren. 
Dieser Fortschritt ist allerdings relativ. Der gleichgebliebene Radstand von 1,87 Meter und die schon aus Sicherheitsgründen knackige Gesamtauslegung dürfen keinen Komfort wie bei Klein- oder gar Kompaktwagen erwarten lassen. Doch ist die Straße einigermaßen in Ordnung, gibt es im Smart keine Rückenprobleme – auch deshalb nicht, weil die Sitze sehr gut sind. 
 
Parken und das Wenden in engen Straßen machen in jedem Fortwo Spaß. Freude am Fahren kommt jedoch nur mit der stärksten Motorisierung auf. Der 0,9-Liter-Motor arbeitet mit Turbounterstützung und leistet 66 kW (90 PS). Damit lässt sich im rund 900 Kilogramm schweren Smart ein flotter Ampelstart hinlegen. Man kommt sich fast wie in einem Elektroauto vor, das bauartbedingt ja ebenfalls für Schnellstarts bekannt ist. Der Smart erreicht nach 3,5 Sekunden Tempo 50, bis 100 km/h vergehen nur 10,4 Sekunden. So zügig bewegt, kommt man naturgemäß nicht mit dem Normverbrauch von 4,2 Litern auf 100 Kilometer aus – im Test erreichten wir im Schnitt 5,8 Liter. 
Erst auf der Autobahn wird der Zweisitzer elektronisch bei 155 km/h eingebremst. Aus Sicherheitsgründen, denn der Motor gibt einem das Gefühl, dass es auch noch einige km/h schneller gegangen wäre. Aber was soll's, mehr Tempo braucht man nicht, und der Zweisitzer soll ja in erster Linie ein Stadtauto sein. Außerhalb der City macht sich im übrigen die nervöse Lenkung des Smart negativ bemerkbar. Sie reagiert gefühlt zu schnell und zu stark auf kleinste Bewegungen. Was in der Stadt sinnvoll ist, erfordert außerhalb derselben Konzentration, damit man nicht überzieht. 
Und wie sieht es mit dem Platz in einem so kleinen Fahrzeug aus? Zwei Personen sitzen ausgezeichnet, selbst wenn sie über 1,90 Meter messen. Im Vergleich zum Vorgänger macht sich die um zehn Zentimeter gewachsene Breite positiv bemerkbar – man kommt sich mit den Ellenbogen nicht mehr in die Quere. 
In den Kofferraum passen zwischen 260 und 350 Liter. Das hört sich erst mal richtig viel an, doch in der Praxis ist mehr als ein normaler Einkauf nicht drin. Praktisch ist das einzeln zu öffnende, hochklappbare Heckfenster für ein oder zwei Einkaufstüten. Wer mehr einpacken will, muss den unteren Teil extra entriegeln und nach unten wegklappen. 
Kurzum: Auch die Neuauflage des Smart Fortwo ist weiterhin kurz und wendig. Dazu kommen die erheblich bessere Schaltung, mehr Fahrkomfort und mehr Bewegungsfreiheit im Innenraum. Der Turbomotor ist eine Wucht. Auf der Minusseite stehen der hohe Preis sowie die teilweise billig wirkenden und nicht an jeder Stelle souverän verarbeiteten Materialien. 
Technische Daten 
Motorbauart: 0,9-Liter-Turbobenziner, Dreizylinder Leistung: 66 kW (90 PS) Beschleunigung (0-100 km/h): 10,4 s Höchstgeschwindigkeit: 155 km/h (abgeregelt) Normverbrauch: 4,2 Liter je 100 km CO2-Emission: 97 g/km Testverbrauch: 5,8 Liter Abgasnorm: Euro 6 Preis: ab 11.790 Euro"	technik
"Im Juni 2013 begannen der britische Guardian und die amerikanische Washington Post, geheime Dokumente zu veröffentlichen, die sie vom früheren NSA-Mitarbeiter Edward Snowden bekommen hatten. Wie die Kontaktaufnahme von Snowden und den beteiligten Journalisten ablief, steht hier. Snowden selbst wurde in den USA der Spionage angeklagt und floh nach Russland ins Exil. Die beiden Zeitungen wurden im April 2014 für ihre Enthüllungen mit dem Pulitzer-Preis ausgezeichnet. 
Die von Snowden entwendeten Dokumente enthüllen ein weltweites Netz von Spionagesystemen. Sie zeigen, dass die amerikanische National Security Agency (NSA), die britischen Government Communications Headquarters (GHCQ) und ihre Partnerdienste jede Form elektronischer Kommunikation überwachen wollen. Die wichtigsten Fakten im Überblick: 
Wer überwacht, spioniert und hackt: Im Zentrum des Skandals stehen die NSA und der britische Geheimdienst GCHQ. Zu den engsten Partnern der USA und Großbritannien gehören Kanada, Australien und Neuseeland, zusammen bilden sie die Five Eyes. 
Weitere Länder arbeiten mit diesen fünf zusammen, darunter Deutschland, Schweden, Frankreich, Belgien oder auch Japan und Südkorea. Sie alle profitieren von den Erkenntnissen insbesondere der Five Eyes und liefern ihnen eigene Informationen. 
Wer überwacht, ausspioniert und gehackt wird: 
Die Ziele: Die Geheimdienste wollen Verdächtige finden, die sie bisher nicht kannten. Dazu analysieren sie die vielen Metadaten, also wer wann mit wem in Kontakt steht.  ""Man braucht den Heuhaufen, um darin die Nadel zu finden "", hat der ehemalige NSA-Direktor Keith Alexander das Prinzip beschrieben. 
Die NSA und die GCHQ spionieren aber auch gezielt einzelne Unternehmen und Spitzenpolitiker aus. Es geht ihnen also nicht nur um die Terrorbekämpfung, sondern auch um die eigenen politischen und wirtschaftlichen Interessen. 
Ein ganz anderes Ziel für die nächsten Jahre ist es, kommerzielle und andere Verschlüsselungssysteme zu brechen – mithilfe von Technik oder auch Spionen, die in Unternehmen für Verschlüsselungstechnik eingeschleust werden sollen. Das geht aus einem  ""mission statement "" für die Zeit von 2012 bis 2016 hervor, das die New York Times veröffentlicht hat. 
 
Die Informationsquellen: 
Die bisher bekannt gewordenen Programme von NSA und GCHQ zeigen, wie umfassend die Geheimdienste jede Form von elektronischer Kommunikation unterwandert haben. Die folgende Liste zeigt, welche Datenquellen mit welchen Methoden angezapft werden und wie die Programme intern heißen. 
Weitere NSA-Programme und ihre Codenamen hat die Seite Electrospaces gesammelt. Die Bürgerrechtsbewegung ACLU hat alle bisher veröffentlichten Snowden-Dokumente durchsuchbar aufgelistet. Die französische Bürgerrechtsorganisation La Quadrature du Net hat unter nsa-observer.net eine Liste der knapp 300 bisher bekannten NSA-Überwachungsprogramme zusammengestellt. 
Die Reaktionen in den USA: 
Politisch hat sich seit Beginn der Enthüllungen nicht viel getan. Zwar hat zunächst eine offiziell unabhängige Expertengruppe die Praktiken der NSA unter die Lupe genommen und dem US-Präsidenten Barack Obama 46 Änderungsvorschläge unterbreitet. Aber bisher hat es nur zwei gesetzliche Neuerungen gegeben. Eine ist eher positiv zu werten, die andere eher negativ: 
Am 1. Juni 2015 lief Abschnitt 215 des Patriot Acts aus, der seit Oktober 2001 die (umstrittene) rechtliche Grundlage für die Sammlung aller Telefonverbindungsdaten der US-Bürger bildete. Am 3. Juni ist dafür der USA Freedom Act inkraft getreten, der die Datenspeicherung neu regelt. Seit Dezember 2015 hält nicht mehr die NSA die Daten vor, sondern die Provider. Die Regierung muss beim Foreign Intelligence Surveillance Court (Fisc) eng gefasste Anträge stellen, um Daten einsehen und analysieren zu dürfen. Auch darf die NSA nur noch die Daten von Kontakten von Verdächtigen analysieren, nicht mehr von Kontakten von Kontakten von Verdächtigen (two hops statt three hops). Geheime Interpretationen von Gesetzen durch den Fisc soll es nicht mehr geben. Der oberste Geheimdienstdirektor muss Entscheidungen des Gerichts, in denen Gesetze gedeutet werden, darauf prüfen, ob sie komplett oder in Teilen veröffentlicht werden können. Das neue Gesetz erlaubt es dem Fisc zudem, externe Experten anzuhören, um sich zum Beispiel technische Vorgänge und deren Auswirkungen auf die Privatsphäre von Überwachungszielen erklären zu lassen. Der USA Freedom Act enthält allerdings mehrere Ausnahmen und Sonderfälle, die einige der Neuerungen gleich wieder abschwächen. 
Ende 2014 hat der Kongress zudem einen Gesetzentwurf verabschiedet, der die Befugnisse der NSA noch ausbaut, statt sie einzudämmen.Entscheidend ist Absatz 309 im Intelligence Authorization Act for Fiscal Year 2015: Darin geht es um  ""unabsichtlich "" und ohne richterliche Beschlüsse erlangte Telefon- und Internetdaten von Ausländern und US-Bürgern. Diese Daten dürfen US-Geheimdienste künftig maximal fünf Jahre aufbewahren, mit zahlreichen Ausnahmen. Handelt es sich zum Beispiel um verschlüsselte Kommunikation, gilt die Fünf-Jahres-Grenze nicht mehr. Es gibt bereits seit 1981 die präsidiale Verfügung 12333, nach der die Geheimdienste das alles schon dürfen – ohne irgendeine parlamentarische Kontrolle. Der Kongress hat die Befugnisse aus dieser Verfügung in Gesetzesform gegossen, sich dabei aber keine Kontrollmöglichkeit eingeräumt. Dianne Feinstein, die Vorsitzende des Geheimdienstausschusses des Senats hatte offenbar dafür gesorgt, dass der Passus nach einer ersten Abstimmung im Repräsentantenhaus nachträglich in den Entwurf geschrieben wurde. Dann hat der Senat den Entwurf abgesegnet. Anschließen hat auch das Repräsantantenhaus der veränderten Fassung zugestimmt – vermutlich, ohne die Änderung bemerkt oder verstanden zu haben. Das Gesetz trat nach der Unterschrift von Präsident Obama am 19. Dezember 2014 inkraft. 
Die wohl größten Veränderungen gibt es auf technischer Ebene: Unternehmen wie Google, Apple und Microsoft bauen nach und nach ihre Verschlüsselungstechniken aus, um die Daten ihrer Kunden besser zu schützen. Der Anteil an verschlüsselten Internetverbindungen hat sich weltweit deutlich erhöht. Das macht die Massenüberwachung durch Geheimdienste mindestens aufwendiger und teurer. 
In Deutschland: Die Bundesregierung hat erst scharf auf die Enthüllungen reagiert, als klar war, dass auch die Bundeskanzlerin ein Ziel der NSA ist. Angela Merkel rief US-Präsident Barack Obama an und verlangte  ""Aufklärung über den Gesamtumfang "" der US-Spionage in Deutschland. Der damalige Außenminister Guido Westerwelle bestellte den US-Botschafter ein. Wirklich geändert hat sich seitdem aber wenig: 
Mehrere Bundesbürger haben juristische Schritte eingeleitet, um gegen die Überwachung durch in- und ausländische Geheimdienste vorzugehen: 
Großbritannien: Die parlamentarische Aufsicht über die britischen Geheimdienste MI5, MI6 und GCHQ ist schwach und ineffektiv. So steht es im Bericht des Ausschuss für innere Angelegenheiten im britischen Unterhaus, über den heise.de berichtet. In Zukunft soll für die Überprüfung der Geheimdienste nicht mehr nur der Geheimdienstausschuss zuständig sein. Allerdings hat die britische Regierung im Eilverfahren das DRIP-Gesetz durch das Parlament gebracht, das den Geheimdiensten nach Ansicht von Rechtsexperten neue Befugnisse verschafft und eine Vorratsdatenspeicherung wieder einführt. Es soll 2016 durch das Gesetz Investigatory Powers Bill abgelöst werden. Der Entwurf dazu wurde im November 2015 der Öffentlichkeit vorgestellt und enthält neben der nachträglichen Legitimierung von GCHQ-Überwachungspraktiken auch fragwürdige Regelungen zum Thema Verschlüsselung sowie eine auf Internetnutzung ausgeweitete Vorratsdatenspeicherung. 
Amnesty International zieht gegen die britische Regierung wegen deren Massenüberwachungspraktiken vor den Europäischen Gerichtshof für Menschenrechte. 
Österreich: Weil der BND im Auftrag der NSA möglicherweise auch österreichische Behörden ausspioniert hat, hat die österreichische Regierung Anzeige gegen Unbekannt erstattet. 
Internationale Ebene: Das EU-Parlament hat den LIBE-Ausschuss beauftragt, den NSA-Skandal und seine Auswirkungen auf die EU-Bürger zu untersuchen. Der Ausschuss hat dazu Dutzende Experten angehört – darunter Techniker, Datenschützer, Aktivisten, Journalisten und Politiker. Der mit einigen Änderungen vom Ausschuss angenommene Entwurf des Abschlussberichts stellt die Legalität und das Ausmaß der NSA-Überwachungsprogramme infrage, unterstellt der NSA auch Wirtschaftsspionage, verurteilt die anlasslose Massenüberwachung von Menschen in aller Welt und beinhaltet eine lange Liste von Forderungen. Die richten sich nicht nur gegen die USA, sondern auch gegen EU-Mitgliedstaaten – darunter auch Deutschland, das seine Geheimdienstgesetze überprüfen solle. Der Bericht wurde am 12. März 2014 vom Plenum angenommen. 
Das EU-Parlament hat zudem eine Resolution verabschiedet, in der EU-Kommission und Ministerrat aufgefordert werden, das Swift-Abkommen auszusetzen. Es erlaubt den US-Behörden, unter bestimmten Voraussetzungen die Kontodaten europäischer Bürger abzufragen. Die EU-Kommission weist das Ansinnen des EU-Parlaments zurück und will weder das Swift-, noch das Fluggastdaten-Abkommen mit den USA stoppen. Über Letzteres stimmt das EU-Parlament im Februar 2016 ab. 
Die UN-Vollversammlung hat im Dezember 2013 eine Resolution zum Schutz der Privatsphäre angenommen. Die Resolution geht auf einen Vorstoß von Deutschland und Brasilien zurück und ist eine direkte Reaktion auf die Spionageaffäre des US-Geheimdienstes NSA. Allerdings wurde die Resolution auf Druck der USA und der anderen Mitglieder der Five Eyes im Vorfeld abgeschwächt."	technik
"Seit den Enthüllungen von Edward Snowden über die Spähprogramme der NSA bemüht sich Google um mehr Sicherheit. Im vergangenen September stellte der Konzern sämtliche Suchanfragen auf die gesicherte HTTPS-Verbindung um. Erst kürzlich gab es Spekulationen, ob Google sichere Websites künftig höher in seinem Ranking platzieren würde, was wiederum Website-Betreiber anspornen könnte, auf mehr Sicherheit zu setzen. Und Ende März machte Google bei seinem E-Mail-Dienst Gmail die HTTPS-Verbindung zur Pflicht und stärkte außerdem die Verschlüsselung zwischen seinen Datenzentren. 
Das größte Projekt für die Daten- und Abhörsicherheit der Nutzer aber könnte noch kommen. Wie die Website Venturebeat von Insidern erfahren haben will, plant Google, das Verschlüsselungsprogramm PGP besser mit Gmail zu verzahnen, also zumindest die Nutzung der komplexen, aber sicheren Ende-zu-Ende-Verschlüsselung zu erleichtern. Details sind nicht bekannt, doch sollte es soweit kommen, wäre das ein Schritt, um Verschlüsselung der breiten Masse zugänglich machen. Geschätzte 500 Millionen Gmail-Konten gibt es bereits. 
PGP: Gute Idee, schlechte Verbreitung 
PGP ( ""Pretty Good Privacy "") ist für Internetverhältnisse steinalt. Bereits seit 1991 gibt es die Software. Sie verschlüsselt die Inhalte einer Mail so, dass nur Sender und Empfänger sie lesen können. Wer sie unterwegs abfängt, bekommt nur Zahlen- und Buchstabensalat zu sehen. 
Dazu benötigen die Nutzer jeweils ein Schlüsselpaar: einen privaten und einen öffentlichen Schlüssel. Den privaten kennt nur der jeweilige Nutzer, er ist in der Regel zusätzlich mit einem Passwort gesichert. Den öffentlichen Schlüssel dagegen stellen die Nutzer entweder auf ihre Website, als Signatur in ihre E-Mail oder auf einen sogenannten Keyserver, eine Art Telefonbuch für PGP-Nutzer. 
Das ist wichtig, denn um jemandem eine E-Mail mit PGP zu schicken, muss der Sender den öffentlichen Schlüssel des Empfängers kennen. Landet die verschlüsselte E-Mail bei einem Empfänger, kann dieser, und nur dieser, sie mit seinem privaten Schlüssel wieder entschlüsseln. Zusätzliche Signaturen verstärken die Echtheit der Nachricht. 
Wie schwierig ist es, sich anonym im Internet zu bewegen, E-Mails zu verschlüsseln, die eigene Privatsphäre zu schützen und Daten sicher zu speichern? Wie alltags- und laientauglich sind die entsprechenden Programme? 
In der Serie  ""Mein digitaler Schutzschild "" beantwortet ZEIT ONLINE diese Fragen. Digital-Redakteur Patrick Beuth hat ein Notebook mit der nötigen Software ausgerüstet und seine Erfahrungen dokumentiert. Er hat dazu Handbücher gelesen, Wikis und Anleitungen, und er hat Hacker und andere Experten um Rat gebeten. 
Das Ergebnis ist eine Schritt-für-Schritt-Anleitung für diejenigen, die noch keine Erfahrung mit Linux, Anonymisierungssoftware oder Verschlüsselung haben – und das ändern möchten. 
Teil 1: Ubuntu (Linux) als Betriebssystem 
Teil 2: Anonymes Surfen mit Tor 
Teil 3: Anonymes Surfen mit VPN 
Teil 4: Ein anonymes E-Mail-Konto Einrichten mit Hushmail und Tor 
Teil 5: E-Mails verschlüsseln mit Enigmail / OpenPGP 
Teil 6: Daten auf der Festplatte mit TrueCrypt verschlüsseln 
 
Die Serie Mein digitales Schutzschild gibt es auch als E-Book. Erfahren Sie in dieser für eReader hochwertig aufbereiteten Fassung, wie Sie Ihre Daten auf dem PC und im Internet besser schützen können. 
Unser E-Book steht Ihnen dabei als EPUB-Version für Ihren eReader, sowie als MOBI-Version für Ihr Kindle Lesegerät von Amazon zur Verfügung. 
Entdecken Sie auch weitere E-Books von ZEIT ONLINE unter www.zeit.de/ebooks. 
So einfach das Prinzip ist, so aufwändig ist die Umsetzung. Auch Lukas Pitschl, Entwickler des Verschlüsselungpakets GPGTools, kennt das Problem:  ""Es gibt nur wenige Mailprogramme, die PGP von Haus aus unterstützen, was den Einstieg schwierig macht "", sagt Pitschl im Gespräch mit ZEIT ONLINE. Die Nutzer müssen deshalb nämlich auf zusätzliche Software wie eben GPGTools für Apple Mail oder Enigmail für Thunderbird zurückgreifen, die für die Verschlüsselung und die Verwaltung der Schlüssel sorgen. 
Ein zusätzliches Hindernis ist, dass PGP für jedes Gerät eingerichtet werden muss. Mal eben die verschlüsselten E-Mails auf dem Smartphone oder einem öffentlichen Rechner abrufen ist nicht mehr möglich, ohne auch dort einen privaten Schlüssel einzurichten – für die meisten Nutzer ist das nicht praktikabel. 
Wer hat Zugriff auf den privaten Schlüssel? 
Wie könnte Google den Prozess vereinfachen? Für Pitschl wäre die einfachste Lösung, dass Google die PGP-Option einfach in seine bestehenden Dienste und Apps integriert.  ""Bereits bei der Registrierung eines neuen Kontos könnte das Schlüsselpaar erstellt werden "", sagt Pitschl,  ""und das selbst, wenn die Verschlüsselung standardmäßig deaktiviert ist. "" Bei Bedarf könnte man die Verschlüsselung jederzeit aktivieren und müsste sich nicht mehr lange mit der Einrichtung beschäftigen. 
Der private Schlüssel läge in diesem Fall etwa in einem Plugin für den Chrome-Browser lokal auf dem Rechner. Ähnliche Versuche von Drittanbietern gibt es bereits. Auf mobilen Geräten könnte die offizielle Gmail-App gleich alle nötigen Funktionen zur Erstellung von Schlüsseln und Signaturen bereithalten. Der Vorteil: Nutzer müssen keine zusätzliche Software installieren – wären allerdings auch an die offizielle App gebunden. 
 
Eine zweite Lösung wäre anspruchsvoller, könnte aber den Trend zu Wearables bestätigen. Der private PGP-Schlüssel befände sich in diesem Fall auf einem Schlüsselanhänger, wie ihn Google angeblich testet. Für jede E-Mail, die verschlüsselt verschickt wird, müsste der Sender den Schlüsselanhänger in den USB-Port stecken oder, im Fall von mobilen Geräten, über den eingebauten NFC-Chip kommunizieren. Wie das Wall Street Journal berichtet, experimentiere Google mit der Technik bereits im Umfeld von Gmail. Auch in der Kryptoszene gibt es ähnliche Ideen bereits: Sogenannte Smartcards dienen zur Aufbewahrung privater Schlüssel. 
In jedem Fall würde Google als Keyserver fungieren, und somit automatisch die öffentlichen Schlüssel zweier Gmail-Nutzer zuweisen. Die Nutzer müssten nicht mehr vor dem Senden der Nachricht nach dem öffentlichen Key des Empfängers suchen: Wenn dieser ebenfalls die PGP-Option aktiviert hat, erkennt Google das und initiiert die Verschlüsselung automatisch. Für Lukas Pitschl wäre das der wohl größte Vorteil:  ""Wenn prinzipiell alle Mails zwischen Gmail-Nutzern von den offiziellen Apps automatisch ver- und entschlüsselt werden, merken die Nutzer das gar nicht mehr. "" 
In jedem Fall aber wäre es notwendig, dass Google nicht auf die privaten Schlüssel der Nutzer zugreifen kann. Denn das würde die Ende-zu-Ende-Verschlüsselung ad absurdum führen, da Google die Mails auf dem Transportweg öffnen könnte, um die Inhalte zu durchsuchen. Ähnlich, wie es auch bei der De-Mail der Fall ist. 
Gmail scannt E-Mails zu Werbezwecken 
Ausgerechnet die Inhalte von E-Mails sind ein wichtiger Bestandteil des personalisierten Anzeigengeschäfts von Google. Erst vergangene Woche hatte das Unternehmen mittels einer Änderung in den AGBs bestätigt, was ohnehin längst bekannt war: Dass man die E-Mails der Nutzer automatisch scanne. Wörtlich heißt es in den AGBs nun:  ""Unser automatisches System analysiert Ihre Inhalte (inklusive E-Mails), um Ihnen relevante Funktionen wie auf Sie zugeschnittene Suchergebnisse, Werbung sowie Spam- und Schadsoftwareerkennung zur Verfügung zu stellen. "" 
Auch Lukas Pitschl glaubt nicht daran, dass Google tatsächlich daran gelegen ist, den kompletten E-Mail-Verkehr seiner Nutzer zu verschlüsseln. 500 Millionen Menschen die sichere Ende-zu-Ende-Verschlüsselung zumindest etwas näherzubringen, wäre zwar ein Erfolg und könnte andere Anbieter anspornen.  ""In jedem Fall aber müsste man Google vertrauen, dass sie die privaten Schlüssel nicht doch auf ihren Servern lagern "", sagt Pitschl."	technik
"Der US-Geheimdienst NSA arbeitet laut Washington Post an einem Quantencomputer. Diese neue Computertechnologie könnte dem Geheimdienst ermöglichen, an bislang verschlüsselte Informationen zu gelangen. Das berichtet die Zeitung unter Berufung auf Dokumente des Whistleblowers Edward Snowden. Demnach betreibt der Geheimdienst ein fast 80 Millionen Dollar teures Forschungsprogramm zum Bau eines  ""kryptologisch nützlichen Quantencomputers "". 
Die NSA forscht dem Bericht zufolge in geheimer Zusammenarbeit mit einem Labor in College Park im Bundesstaat Maryland. Die Auswirkungen seien gravierend: Gängige Verschlüsselungstypen seien gegenüber Quantencomputern hinfällig – Daten aus Politik, Medizin und Wirtschaft könnten dadurch problemlos zugänglich werden. 
Die zugespielten Dokumente legten aber nahe, dass der Geheimdienst keinen Vorsprung vor der Wissenschaft habe. Ein Einsatz der Technologie sei daher wohl erst in mehreren Jahren möglich. 
Quantencomputer basieren auf einer Abwandlung des üblichen Binärsystems. Während ein Bit eines üblichen Computers auf Nullen und Einsen basiert, verwendet ein Quantencomputer sogenannte Qubits, die sowohl Null als auch Eins sind, und Berechnungen somit theoretisch um ein Vielfaches schneller durchführen können. 
Die Physiker Serge Haroche und David J. Wineland wurden 2012 für ihre Untersuchungen von Quantenzuständen mit dem Nobelpreis ausgezeichnet. 
Bislang wurden Geräte, die die Quantentechnologie nutzen, überwiegend zu wissenschaftlichen Zwecken eingesetzt. Künftig sollen sie auch die Entwicklung von Medikamenten fördern. Ein kanadisches Unternehmen hatte 2011 behauptet, den ersten kommerziellen Quantencomputer entwickelt zu haben, was Forscher jedoch anzweifelten."	technik
"Verschlüsselung wird zum Politikum. China plant ein Anti-Terror-Gesetz, das schwerwiegende Folgen für US-Unternehmen wie Google, Microsoft und Apple hätte. Diese wären gezwungen, den chinesischen Behörden Zugriff auf kryptografische Schlüssel zu geben und Hintertüren in ihre Programme einzubauen. US-Präsident Barack Obama ist alarmiert. 
Er habe das Thema bereits gegenüber Präsident Xi Jinping angesprochen, sagte Obama im Interview mit Reuters.  ""Wir haben sehr deutlich gemacht, dass sie das ändern müssen, wenn sie mit den USA Geschäfte machen wollen. "" Der Gesetzentwurf zwinge alle ausländischen Unternehmen, der chinesischen Regierung Mechanismen an die Hand zu geben, um ihre Kunden auszuspionieren, bemängelte der US-Präsident.  ""Sie können sich vorstellen, dass die Konzerne dazu nicht bereit sein werden. "" 
Was er nicht sagte: Obamas eigene Behörden, namentlich das FBI und die NSA, wollen solche Hintertüren in den Produkten der US-Firmen ebenfalls haben. Bereits im vergangenen Oktober sagte FBI-Direktor James Comey, die zunehmende Verschlüsselung von Kommunikation und Geräten drohe,  ""uns an einen sehr dunklen Ort zu führen "". 
Comey beklagte, dass es in vielen Onlinediensten keine Abhörschnittstellen gibt, und dass Apple und Google bestimmte Nutzerdaten so auf ihren mobilen Geräten verschlüsseln wollten, dass auch die Firmen selbst nicht mehr darauf zugreifen können, selbst wenn sie physischen Zugriff auf das Gerät haben. Er fordert ein Gesetz, das die Unternehmen zum Einbau von Hintertüren verpflichtet. 
Vor Kurzem sprang ihm NSA-Direktor Michael Rogers zur Seite. Auch er will einen einfachen Zugang zu den Kundendaten von Apple, Microsoft, Google und den anderen US-Technikunternehmen. Und auch er versuchte, die Hintertüren schönzureden, indem er sie einfach als Vordertüren bezeichnete – schließlich solle die absichtlich lückenhafte Verschlüsselung ganz offiziell gesetzlich geregelt werden. 
Doch die US-Unternehmen denken dabei nicht nur an die eigene Regierung, sondern eben auch an die chinesische. Yahoos Sicherheitschef Alex Stamos fragte den NSA-Direktor mehrfach, ob er die eigenen Sicherheitsvorkehrungen auch für andere Regierungen aushebeln soll, wenn die ein entsprechendes Gesetz beschließen. Rogers hatte darauf nur eine Nicht-Antwort:  ""Ich denke, wir können uns da durcharbeiten "", sagte er immer wieder. Präsident Obama steht nun vor der schwierigen Aufgabe, eine bessere Antwort finden zu müssen. So wie die Unternehmen, die sich nun überlegen müssen, ob sie ihr Chinageschäft aufs Spiel setzen. 
Was tun derweil die US-Unternehmen? Die Snowden-Enthüllungen haben ihnen geschadet, sie stehen seit dem Bekanntwerden von PRISM mitunter sogar als Kollaborateure da, weil sie sich auf technischer und rechtlicher Ebene nicht ausreichend gegen die Begehrlichkeiten der Geheimdienste gewehrt haben. Mit neuen Verschlüsselungstechniken und anderen mehr oder weniger wirkungsvollen Maßnahmen versuchen sie, das Vertrauen der Kunden vor allem im Ausland zurückzugewinnen. 
Apple-CEO Tim Cook etwa gibt sich gegenüber den Forderungen von Comey und Rogers unnachgiebig. Er will an der hintertürlosen Verschlüsselung von iPhones und iPad festhalten. Bei einem Treffen mit Obama im Weißen Haus sagte er an die US-Regierung gerichtet:  ""Wir leben immer noch in einer Welt, in der nicht alle Menschen gleich behandelt werden. Zu viele Menschen glauben, ihre Religion nicht frei ausüben, ihre Meinung nicht frei äußern und nicht lieben zu dürfen, wen sie wollen. Einer Welt, in der solche Informationen den Unterschied zwischen Leben und Tod bedeuten können. Wenn wir es nicht schaffen, alles in unserer Macht stehende zu tun, um das Recht auf Privatsphäre zu schützen, riskieren wir den Verlust von etwas viel Wertvollerem als Geld. Wir riskieren unseren way of life. "" 
Microsoft kämpft öffentlichkeitswirksam vor Gericht gegen die US-Regierung. Das Unternehmen will verhindern, Kundendaten an US-Strafverfolger herausgeben zu müssen, die auf einem Server in Irland gespeichert sind. 
Und Google? Im September hatte das Unternehmen versprochen, in der kommenden Version des Betriebssystems Android – also Version 5.0 alias Lollipop – den Hauptspeicher standardmäßig so zu verschlüsseln, dass nur noch der Besitzer des Geräts ihn entschlüsseln kann. Bis dahin war das allenfalls eine Option in Android, die Nutzer auf Wunsch aktivieren konnten. Im Oktober hatte Google die Ankündigung im offiziellen Blog wiederholt. Nun sieht es so aus, als ob Google das nicht ganz ernst gemeint hat. Den Redakteuren von Ars Technica ist nämlich aufgefallen, dass neue Lollipop-Geräte keineswegs mit Default-Verschlüsselung ausgeliefert werden. Einzige Ausnahme: Googles eigene Geräte Nexus 6 und Nexus 9. 
Ein Blick in Googles aktuelle Richtlinien für Hardware-Hersteller zeigt: Der Absatz zur Verschlüsselung (Punkt 9.9) wurde präzisiert, aber wie schon in einer früheren Version verpflichtet Google niemanden, sie standardmäßig zu aktivieren. Zwar  ""empfiehlt "" Google  ""dringend "", das zu tun, weil es in einer zukünftigen Version von Android wahrscheinlich verpflichtend sein würde. Aber Ars Technica vermutet, dass Google den Herstellern noch etwas Zeit geben will, um ihre Hardware zum Beispiel mit speziellen Chips anzupassen. Die Verschlüsselung kann nämlich dazu führen, dass ein Gerät langsamer arbeitet. Offenbar haben Google und seine Hardware-Partner Angst, Nutzer würden eher zu einem etwas schnelleren als zu einem etwas sichereren Smartphone greifen. Bisher hat sich Google dazu noch nicht geäußert. 
Update: Mittlerweile gibt es ein Statement von Google:  ""Durch die Performance-Probleme auf einigen Android-Partner-Geräten liegt noch keine standardmäßige Verschlüsselung bei jedem neuen Gerät mit Lollipop vor. Unsere neuen Nexus-Geräte sind jedoch standardmäßig verschlüsselt und Android-Nutzer mit Jelly Bean und höhere Versionen haben die Möglichkeit, die Daten auf ihren Geräten in  ""Einstellungen "" unter dem Menüpunkt  ""Sicherheit "" und dem Unterpunkt  ""Verschlüsselung "" zu verschlüsseln. ""
""Tanja Langes Problem liegt noch Jahre in der Zukunft, aber sie ist trotzdem spät dran. Die deutsche Professorin für Mathematik an der Technischen Universität Eindhoven sucht nach Verschlüsselungsverfahren, die einem Quantencomputer standhalten würden. 
Wenn es erst einmal einen solchen Computer gibt, in zehn oder 15 Jahren vielleicht, wird er viele heute verwendete kryptografische Verfahren mit einem Schlag obsolet machen. Er wird alles, was dann noch mit ihnen verschlüsselt wird, sehr schnell entschlüsseln können: jede mit PGP gesicherte E-Mail und jede Übertragung von Passwörtern und anderen sensiblen Daten im Internet über das Protokoll SSL beziehungsweise TLS. Das  ""https "" in der Adresszeile des Browsers steht dann nicht mehr für secure, sondern für sinnlos. 
Aber auch alles, was heute verschlüsselt wird, ist dann nachträglich gefährdet: Bekanntlich darf die NSA verschlüsselte Daten, die sie nicht knacken kann, beliebig lange aufbewahren. Mit einem Quantencomputer könnte sie diese doch noch entschlüsseln. Deshalb hat es Tanja Lange so eilig. 
Die Snowden-Enthüllungen haben der Welt vor Augen geführt, welches Ausmaß die Überwachung des Internets angenommen hat, welch einen Aufwand die NSA betreibt, um Datenpakete rund um den Globus abzufangen und auszuwerten. Doch nicht nur Geheimdienste nutzen die Schwachstellen der Technik und die Sorglosigkeit der Nutzer aus. Andere staatliche Stellen benutzen Überwachungstechnik zur Zensur des Netzes. Kriminelle versuchen, an Zugangsdaten oder andere verwertbare Informationen zu gelangen. Und nicht wenige Unternehmen nutzen die vielfältigen Analyseverfahren, um das Verhalten und die Interessen von Internetnutzern zu beobachten. 
Eine Möglichkeit, vieles davon zu verhindern oder zumindest erheblich zu erschweren, ist die Verschlüsselung von Nachrichten und Datenübertragungen. Dafür gibt es etablierte Protokolle und Programme wie PGP und SSL/TLS. 
Yan Zhu hat es sich zur Aufgabe gemacht, diese lange bekannte Technik möglichst weit zu verbreiten. Als Entwicklerin bei Yahoo und bei der Bürgerrechtsbewegung Electronic Frontier Foundation (EFF) betreut sie deshalb Projekte wie Yahoo End-to-End, Let's Encrypt und HTTPS Everywhere. 
Zum Artikel  ""Verschlüsselung: Kampf dem Klartext "" 
Heutige Verschlüsselungsverfahren wie PGP und SSL/TLS basieren auf jahrzehntealten Methoden und Algorithmen. Sie sind nicht perfekt, hinreichend gut ausgestatte und motivierte Angreifer können sie überwinden. Aber insbesondere die mühelose, passive Massenüberwachung im Internet durch Geheimdienste können sie weitgehend unmöglich machen. Jedenfalls solange, bis es Computer gibt, die heutige kryptografische Verfahren brechen. 
Quantencomputer werden das schaffen. Die Fachwelt rechnet damit, dass es in zehn bis 15 Jahren praxistaugliche Quantencomputer geben wird. Unternehmen wie IBM und Google arbeiten daran, aber auch die NSA versucht, einen solchen Computer zu entwickeln. Er ist ausdrücklich dafür gedacht, verschlüsselte Daten zu entschlüsseln. 
Tanja Lange sucht deshalb Algorithmen, die künftig zur Verschlüsselung eingesetzt werden können und die auch einem Quantencomputer standhalten würden. Die deutsche Mathematikerin an der Technischen Universität Eindhoven leitet dazu das von der EU-Kommission geförderte Projekt PQCRYPTO, das steht für Post-Quantum-Cryptography. 
Zum Artikel  ""Kryptografie: Verschlüsseln für den Tag X "" 
 ""Es geht nicht darum, etwas zu entwickeln, das wir in zehn oder 15 Jahren einsetzen wollen, sondern um etwas, das wir jetzt schon einsetzen müssten "", sagt Lange. Die renommierte Kryptografin hat ein persönliches Interesse daran, dass es schnell geht. Sie legt großen Wert auf ihre Privatsphäre, spricht äußerst ungern über sich selbst und mag nicht einmal ihren Geburtsort veröffentlicht sehen. 
Um ihrem Ziel möglichst bald näher zu kommen, leitet sie das Projekt PQCRYPTO, an dem Universitäten und Unternehmen aus elf Ländern beteiligt sind, darunter die Bundesdruckerei, die TU Darmstadt und die Ruhr-Uni Bochum. PQCRYPTO steht für Post-Quantum-Cryptography, also kryptografische Verfahren für das Zeitalter nach der Einführung von Quantencomputern, mit denen  ""jedes Datenpaket im Internet abgesichert werden kann "", wie Lange sagt. Die EU-Kommission fördert das Projekt mit 3,9 Millionen Euro. 
Dass es eines Tages Quantencomputer gibt, die ganz anders funktionieren als heutige Rechner und bei bestimmten Rechenoperationen um ein Vielfaches schneller sind, ist unter Forschern weitgehend unstrittig. Die Frage ist nur noch, wann es so weit sein wird. Abwehrmaßnahmen gegen solche bisher nicht-existenten Maschinen zu suchen, ist deshalb nicht verrückt, sondern vernünftig. 
Das weiß auch die NSA. Einerseits versucht sie selbst, einen  ""kryptologisch nützlichen Quantencomputer "" zu bauen, wie es in den Snowden-Dokumenten heißt. Andererseits ahnt oder weiß der US-Geheimdienst, dass er nicht der einzige ist. Mitte August hat die NSA deshalb ein bemerkenswertes Dokument veröffentlicht. Darin empfiehlt sie ihren Partnern in der Regierung und der Wirtschaft dringend, zur Verschlüsselung geheimer Unterlagen die aktuellen Standards der NSA zu nutzen. Wer die noch nicht implementiert habe, möge bitte keine größeren Anstrengungen zur Umstellung mehr unternehmen und sich lieber schon auf den anstehenden Wechsel auf post-quantenkryptografische Standards vorbereiten. 
 
Das Rennen derjenigen, die kryptografische Verfahren mithilfe quantenphysikalischer Eigenheiten brechen wollen gegen jene, die das mit neuen Verfahren zu verhindern versuchen, begann bereits im Jahr 1994. Damals veröffentlichte der Mathematiker Peter Shor einen Algorithmus, der auf einem Quantencomputer jede zusammengesetzte Zahl faktorisieren kann. 
Um zu verstehen, was das bedeutet, muss man wissen, auf welchen mathematischen Annahmen heutige asymmetrische Verschlüsselungsverfahren wie RSA oder ECC beruhen und wie ein Quantencomputer funktioniert. 
RSA – das steht für die Erfinder Rivest, Shamir und Adleman – wird zum Beispiel zur E-Mail-Verschlüsselung, aber auch in der Transportverschlüsselung TLS verwendet, mit der zum Beispiel Onlinebanking- und Shoppingseiten abgesichert sind. Es basiert auf einer Einwegfunktion, also einer Berechnung, die in eine Richtung einfach, in die Gegenrichtung aber extrem schwierig ist. 
Im Fall von RSA ist das die Multiplikation von hinreichend großen Primzahlen. Die ist leicht. Das Produkt der Primzahlen kann jeder sehen und zum Verschlüsseln benutzen, es ist also der öffentliche Teil eines kryptografischen Schlüsselpaars. Aber die große Zahl wieder in ihre Primfaktoren zu zerlegen, sie zu faktorisieren, ist aufwendig. So aufwendig, dass es heutige Computer bei Weitem überfordert. Den geheimen Schlüssel des Schlüsselpaars, der aus den verwendeten Primzahlen besteht und zum Entschlüsseln benutzt wird, können Angreifer deshalb nicht berechnen, er ist sicher. 
Ähnlich ist es in der Elliptic Curve Cryptography, kurz ECC. Die Erzeugung kryptografischer Schlüssel beruht hier auf dem sogenannten diskreten Logarithmus-Problem, das noch schwieriger zu lösen ist als die Faktorisierung. ECC kommt deshalb bei vergleichbarer Sicherheit mit kürzeren Schlüsseln aus als RSA und beansprucht damit weniger Rechenleistung vom Anwender. 
Shors Algorithmus könnte auf einem Quantencomputer sowohl die Faktorisierung radikal beschleunigen, als auch das diskrete Logarithmus-Problem schnell genug lösen, um ECC zu gefährden. Der Algorithmus macht sich dabei zwei besondere Eigenschaften eines Quantencomputers zunutze. Sie heißen Superposition und Verschränkung. 
Ein normaler Computer rechnet mit Bits, die immer den Wert null oder eins haben. Ein Quantencomputer hingegen nutzt sogenannte Qubits. Deren Wert liegt mit einer gewissen Wahrscheinlichkeit irgendwo zwischen null und eins. Genau weiß man das erst, wenn man ihn misst. Bis dahin hat ein Qubit, vereinfacht gesagt, mehrere Werte gleichzeitig, was mehrere Rechenvorgänge des Computers gleichzeitig ermöglicht. Das ist die Superposition. 
Wenn mehrere Qubits auch noch quantenphysikalisch miteinander verschränkt sind, beeinflussen sich die Wahrscheinlichkeiten ihrer Zustände gegenseitig. Das hat zur Folge, dass die Zahl der gleichzeitig möglichen Rechenvorgänge noch einmal dramatisch zunimmt. Der Quantencomputer rechnet also mit extrem viel mehr Möglichkeiten als ein herkömmlicher. Voraussetzung dafür ist allerdings eine stabile Verschränkung möglichst vieler Qubits, und daran wird derzeit mit verschiedenen Ansätzen geforscht. 
Tanja Lange und ihr Team sind nicht die einzigen, die nun nach einem geeigneten Ersatz für die Verfahren hinter RSA und ECC suchen, damit große Teile des Internetverkehrs auch in den kommenden Jahrzehnten sicher verschlüsselt werden können. Auch Microsoft und der niederländische Chipproduzent NXP versuchen, die Transportverschlüsselung TLS mit neuen Algorithmen auf das Post-Quanten-Zeitalter vorzubereiten. 
Es gibt solche Algorithmen und Verfahren im Prinzip auch schon, zum Beispiel das McEliece-System, erfunden im Jahr 1978. Es verwendet sogenannte fehlerkorrigierende Codes in verschachtelter Darstellung. Der Sender baut zum Verschlüsseln Fehler in seine Nachricht ein, und nur wer den versteckten, effizienten Algorithmus zur Fehlerkorrektur kennt, kann die ursprüngliche Nachricht zurückgewinnen. Dieser geheime Algorithmus ist der private Schlüssel, die verschachtelte Darstellung des Codes der öffentliche. 
Doch die bisher infrage kommenden Verfahren sind noch nicht alltagstauglich, sagt Lange – aus zwei Gründen:  ""Warum haben wir noch keine Post-Quantenkryptografie in unseren Smartphones? Vor allem, weil die Schlüssel zu groß sind. "" Das verzögert den Ver- und Entschlüsselungsprozess beziehungsweise den Aufbau einer verschlüsselten Verbindung, außerdem passen große Schlüssel nicht in die derzeit akzeptierten Internetprotokolle. Das zweite Hindernis:  ""Für manche Systeme dauert die Berechnung zu lange. "" Derart rechenintensive Algorithmen kosten Energie, was spätestens in einem Smartphone mit entsprechend kleinem Akku zum Problem wird. Damit kämpft derzeit auch das Microsoft-NXP-Team. 
Wer  ""hinreichend paranoid oder sicherheitssensitiv "" ist, sagt Lange und meint sich damit wohl auch selbst, könnte Post-Quanten-Verfahren heute schon nutzen, solange es im kleinen Maßstab geschieht, etwa zur E-Mail-Verschlüsselung oder für Sicherungskopien. Denn in diesem Fall muss ein Schlüssel immer nur einmal heruntergeladen werden. Da macht es nichts, wenn er ein paar Megabyte groß ist. 
Aber für den globalen Maßstab, für die Verschlüsselung großer Teile des täglichen Internetverkehrs, hat die Forschung noch keine passende Lösung. Nicht zuletzt, weil ein optimales Verschlüsselungssystem bei jedem Verbindungsaufbau neue Schlüssel generiert. Forward Secrecy heißt das. Wenn die alle um ein Vielfaches größer sind als die heute verwendeten, würde das Internet mehr oder weniger unbenutzbar. 
Zudem sollen die künftigen Verfahren am besten auch noch gegen sogenannte Seitenkanalangriffe geschützt sein. Das heißt, ein Angreifer soll zum Beispiel nicht aus der elektromagnetischen Abstrahlung eines Computers auf die geheimen Schlüssel schließen können. Es gibt faszinierend kreative Ansätze für solche Angriffe, entsprechend schwierig sind die Vorkehrungen dagegen. Innerhalb von PQCRYPTO befassen sich deshalb Experten der RU Nijmegen und der Ruhr-Uni Bochum damit. 
Das Projekt macht Fortschritte.  ""Wir haben ein paar Kandidaten, die wir als post-quanten-sicher einschätzen "", sagt Lange. Am heutigen Montag hat sie eine entsprechende, wenn auch kurze Liste veröffentlicht. Sie beinhaltet Verweise auf Verschlüsselungsverfahren, die so sicher sind, dass sie auch 30 Jahren noch nicht geknackt werden können. Massentauglich sind die aber noch nicht. 
Bis 2018, wenn die EU-Förderung ausläuft, soll die Liste nicht nur Vorschläge, sondern fertige Bibliotheken beinhalten, die in Verschlüsselungssoftware implementiert werden können. Die Projektteilnehmer sind zudem gut mit Standardisierungseinrichtungen wie ISO, IETF und NIST vernetzt, sodass ihre Lösungen irgendwann auch zu Standards erklärt werden können. Bis die aber weit verbreitet sind, dauert es oft Jahre. Es ist nicht auszuschließen, dass es Forschern vorher gelingt, einen echten, praktisch einsetzbaren Quantencomputer zu entwickeln. 
Tanja Lange findet die Vorstellung an sich  ""ganz toll "", schließlich können Quantencomputer auch für bessere Wettervorhersagen, das Durchsuchen großer Datenbestände oder für medizinische Forschungszwecke eingesetzt werden:  ""Ich möchte auf jeden Fall, dass Quantencomputer existieren. Wir müssen eben nur akzeptieren, dass im Nebeneffekt die heutige Kryptografie darunter leidet. ""
""Rund 30 Millionen aktive Kunden haben die United-Internet-Marken GMX und Web.de zusammen. Das sind 30 Millionen Menschen, die ab sofort Ende-zu-Ende-verschlüsselte E-Mails senden und empfangen können. Also E-Mails, die nur Sender und Empfänger lesen können, während Schnüffler im selben Netzwerk oder auch die E-Mail-Provider selbst allenfalls kryptische Zeichenfolgen erkennen können. 
Vor einigen Tagen haben die beiden Mailprovider eine entsprechende Möglichkeit eingeführt. Sie basiert auf dem OpenPGP-Format, funktioniert in den Browsern Chrome und Firefox sowie den Android- und iOS-Apps der Anbieter und ist darüberhinaus kompatibel mit anderen PGP-Systemen. 
PGP steht für Pretty Good Privacy, eine Verschlüsselungssoftware aus den neunziger Jahren. OpenPGP ist eine Art Open-Source-Version von PGP, die als Grundlage für viele E-Mail-Verschlüsselungsprogramme dient. Beides gilt bis heute als sicher, aber auch mitunter als schwer zu bedienen und einzurichten. 
Die deutschen Anbieter haben die Installation und Nutzung vergleichsweise simpel gestaltet. Es ist ein ambitionierter und löblicher Versuch, die verschlüsselte E-Mail zum Standard zu machen. Und im Großen und Ganzen klappt das gut, wie unser Praxistest zeigt. 
Eingerichtet wird die PGP-Verschlüsselung im Webmailer von GMX oder Web.de mit wenigen Schritten. Nach dem Einloggen auf gmx.net oder eben web.de müssen sich Nutzer über den Menüpunkt Einstellungen zu Verschlüsselung und schließlich zu Verschlüsselte Kommunikation einrichten durchklicken. Oder sie klicken auf das neue Schlosssymbol neben dem Button E-Mail schreiben oben links im Browser. Der erste Klick öffnet dann den Einrichtungsassistenten. 
Ein Pop-up-Fenster zeigt nun die notwendigen Schritte zur Einrichtung an. Der erste besteht im Download der Browsererweiterung Mailvelope, die es für den Firefox- und den Chrome-Browser gibt. Mit Mailvelope werden Schlüsselpaare erzeugt und verwaltet sowie Mails ver- und entschlüsselt. (siehe Infobox oder Mailvelope) Die Erweiterung ist mit zwei Klicks und nach wenigen Sekunden installiert, das Ganze ist selbsterklärend. 
Anschließend werden Nutzer aufgefordert, ein Passwort für die Verschlüsselung einzugeben. Mit dem wird der nun im Hintergrund von Mailvelope generierte private Schlüssel gesichert. Das bedeutet: Um eine Mail zu entschlüsseln oder eine eigene Mail zu signieren, brauchen Nutzer später den lokal in ihrem Browser gespeicherten privaten Schlüssel und dieses Passwort. 
Wer die Einrichtung an dieser Stelle beendet, kann bereits ver- und entschlüsseln. Der soeben erzeugte private Schlüssel verbleibt lokal auf dem jeweiligen Gerät gespeichert, der öffentliche Schlüssel landet auf einem Server von United Internet. Damit ist sichergestellt, dass sich Nutzer von GMX und Web.de untereinander reibungslos verschlüsselte Mails schicken können. 
Die Mailprovider empfehlen aber noch einen dritten Schritt: die Sicherung der Schlüssel. 
In einem neuen Fenster erscheinen dazu ein Wiederherstellungscode und ein QR-Code, die Nutzer ausdrucken und sicher verwahren sollen. Benötigt wird der Wiederherstellungscode zum einen, wenn man sein Verschlüsselungspasswort vergisst. Zum anderen können Nutzer mit dem Wiederherstellungscode oder mit dem QR-Code auch auf anderen PCs sowie auf dem Smartphone oder Tablet die soeben erzeugten Schlüssel nutzen. Diese Möglichkeit zur Übertragung eines privaten Schlüssels auf weitere Geräte ist in anderen PGP-Systemen mitunter komplex, hier ist es vergleichsweise gut gelöst. 
Dazu erzeugt Mailvelope an dieser Stelle einen Container, der den privaten und den öffentlichen Schlüssel des Nutzers sowie sein Verschlüsselungspasswort enthält. Der Container wird mit einem zufälligen 26-stelligen Passwort nach dem Standard AES-256 verschlüsselt und anschließend in einem gesonderten Bereich auf den Servern von United Internet abgelegt. Das Passwort aber wird nur lokal angezeigt, Nutzer sollen es ausdrucken oder abschreiben und sicher verwahren. Die Mailprovider bekommen es nicht zu sehen und können den Container auf ihren Servern deshalb unter keinen Umständen öffnen. Mailvelope ist Open-Source-Software, ebenso wie die dazugehörige OpenPGP-Implementierung für JavaScript. Experten können sich also davon überzeugen, dass die Software wirklich nur das tut, was sie soll. 
 
 
Um die Verschlüsselung auf einem zweiten PC zu nutzen, müssen die Nutzer dort die Browsererweiterung Mailvelope installieren und sich auf gmx.net oder web.de einloggen. Über die Menüpunkte Einstellungen, Sicherheit und Verschlüsselung gelangt man zu einem Feld, in dem man den Wiederherstellungscode eintippen kann. Mehr ist zur Einrichtung nicht nötig. 
Auf dem Smartphone muss die jeweilige App des Mailanbieters installiert sein. Über das Menü Einstellungen gelangen Nutzer zum Punkt Verschlüsselung aktivieren, dort werden sie aufgefordert, den Wiederherstellungscode einzutippen oder den QR-Code einzuscannen (dafür wird eine QR-Scanner-App benötigt). 
Neben dem Button E-Mail schreiben oben links im Webmailer von GMX und Web.de ist ein Schlosssymbol zu sehen. Um eine verschlüsselte Mail zu verschicken, muss man es anklicken und anschließend den Empfänger eintragen. 
Hat man diesem Empfänger zuvor noch keine verschlüsselte Mail geschickt, muss man ihn in einem Pop-up-Fenster als Kontakt zur verschlüsselten Kommunikation bestätigen. 
United Internet setzt auf das Prinzip der asymmetrischen Verschlüsselung: Nutzer erzeugen mit Mailvelope ein Schlüsselpaar aus einem öffentlichen und einem privaten Schlüssel. Der private bleibt lokal auf ihrem Rechner (im Firefox hier und in Chrome hier), der öffentliche wird auf einem Schlüsselserver hinterlegt, auf den GMX und Web.de zugreifen können. 
Mit dem öffentlichen Schlüssel des Empfängers verschlüsseln Nutzer ihre Mails. Im Verbund von GMX und Web.de funktioniert das reibungslos, weil alle öffentlichen Schlüssel auf dem gemeinsamen Schlüsselserver liegen. 
Mit dem privaten entschlüsselt der Empfänger eine Mail wieder. Der private Schlüssel kann außerdem zum kryptografischen Signieren einer Mail verwendet werden. Anhand der Signatur kann ein Empfänger überprüfen, ob eine Mail wirklich vom angeblichen Absender kommt oder ob sie manipuliert wurde. 
Für Nutzer, die sich mit PGP auskennen: Erzeugt werden Schlüssel mit einer Länge von 4.096 Bit. Sie haben kein Ablaufdatum und können nicht für ungültig erklärt werden. 
Die Entwickler von Mailvelope haben ihre Browsererweiterung in mehreren Tests überprüfen lassen, deren Ergebnisse gibt es hier. 
Um das System möglichst sicher zu machen, sollten Nutzer erstens ihr Mailkonto mit einem starken Passwort – zum Beispiel mithilfe eines Passwortmanagers – schützen. Eine Zwei-Faktor-Authentifizierung bieten GMX und Web.de leider nicht an. Zweitens sollten Nutzer ihre Geräte vor dem Zugriff durch Unbefugte schützen, sie also nicht unbeaufsichtigt herumstehen lassen. Drittens sollten sie auch den privaten Schlüssel mit einem starken Passwort sichern. Das muss allerdings beim Entschlüsseln und Signieren immer wieder eingegeben werden. 
Abschließend klickt man rechts auf Verschlüsselt senden und wird dazu nach dem eigenen Schlüsselpasswort gefragt. 
In der App von GMX und Web.de gibt es einen Schieberegler, mit dem die Verschlüsselung beim Schreiben einer Mail aktiviert werden kann. Auch hier müssen neue Empfänger innerhalb des Verbundes von GMX und Web.de kurz bestätigt werden. 
Im Test funktionierte das wie erwünscht. Alle Geräte, auf denen die Verschlüsselung eingerichtet war, konnten innerhalb des United-Internet-Verbunds verschlüsselte Mails senden und entschlüsseln. Geräte, auf denen sie nicht eingerichtet war, sowie Thunderbird als alternativer Zugang zum Mailpostfach, konnten die verschlüsselten Mails nicht entschlüsseln. 
Komplizierter ist die verschlüsselte Kommunikation mit Kontakten, die weder GMX, noch web.de nutzen. Beide Seiten müssen zunächst wie bei PGP üblich ihre öffentlichen Schlüssel manuell austauschen. Die Nutzer von GMX und Web.de tun das über Mailvelope. Sie klicken dazu das Symbol der Erweiterung oben im Browser an, dann auf Optionen, dann auf Schlüssel anzeigen. Dann klicken sie auf den gewünschten Schlüssel und im Pop-up-Fenster auf Exportieren. Wichtig: Dunkelblau markiert sein sollte jetzt nur der öffentliche Schlüssel. Mit dem Button Herunterladen lässt er sich lokal speichern und dann per Mail an den gewünschten Empfänger schicken. Der muss ihn dann in sein PGP-System importieren. 
Umgekehrt müssen sie den öffentlichen Schlüssel ihres Gegenübers bekommen und entsprechend über Mailvelope importieren. Erst danach ist die verschlüsselte Kommunikation möglich – und dann auch nur auf jenem Gerät, auf dem der öffentliche Schlüssel des anderen importiert wurde. 
Die Ende-zu-Ende-Verschlüsselung von GMX und Web.de ist sehr schnell auch auf mehreren Geräten installiert und zumindest innerhalb der drei Anbieter einfach zu benutzen. So einfach, wie eine auf PGP basierende Verschlüsselung vielleicht nie zuvor war. Gut gelöst ist vor allem der Weg, den privaten Schlüssel auf mehreren Geräten zu benutzen, um zum Beispiel auch unterwegs auf dem Smartphone ver- und entschlüsseln zu können. Nur wer seinen privaten Schlüssel bestmöglich schützen will, wird ihn prinzipiell nicht auf fremde Server hochladen und auf mehrere Geräte verteilen wollen. Wer sich erst später dafür entscheidet, sein Schlüsselpaar nicht auf den Servern zu sichern, kann die Sicherung im Webmailer auch nachträglich löschen, verliert damit aber auch die Möglichkeit zur Wiederherstellung des privaten Schlüssels. 
Nutzer sollten sich aber zumindest mit dem groben Prinzip der asymmetrischen Verschlüsselung befassen, um zu verstehen, was da weitgehend im Hintergrund mit ihren Mails geschieht, wo ihre Schlüssel liegen und was sie brauchen, um Mails zu verschlüsseln, zu signieren oder zu entschlüsseln. In dieser Hinsicht wird jede auf PGP basierende Lösung mehr vom Nutzer verlangen als Messenger-Apps wie Signal, TextSecure, Threema oder mit Abstrichen auch WhatsApp, die nach der Ersteinrichtung die Verschlüsselung komplett im Hintergrund laufen lassen."	technik
"Das Verbot von Neuwagenkäufen ist auf Kuba seit Jahresbeginn zwar außer Kraft, doch das dürfte das Straßenbild auf der Karibikinsel kaum verändern. Die staatlichen Autohändler verlangen horrende Preise, die die meisten Kubaner kaum bezahlen können. Kurz nach Inkrafttreten der neuen Regelung schossen die Preise in astronomische Höhen. Ein Beipiel: Für den Kleinwagen Kia Rio, der in Deutschland für knapp 11.000 Euro angeboten wird, verlangen die Händler auf Kuba umgerechnet etwa 30.800 Euro. 
Hinzu kommt: Üblich sind auf Kuba Löhne von umgerechnet etwa 15 Euro im Monat. Ein ganzes Berufsleben würde da nicht ausreichen, um sich einen Kia Rio leisten zu können. Die Mittelklasse-Limousine Peugeot 508 ist mit einem Preis von umgerechnet rund 193.000 Euro in Kuba erst recht jenseits aller Vorstellungskraft. 
 ""Selbst wenn alle meine Angehörigen hier und drüben in Miami mitmachen, kriegen wir eine solche Summe nicht zusammen "", sagt der 28-jährige Musiker Gilbert Losada.  ""Wir werden abwarten, ob sie mit den verrückten Preisen runtergehen. Wir sind wirklich enttäuscht. "" 
Das Verbot, einen Neuwagen zu kaufen, galt seit 1959. Nur mit einer Sondergenehmigung des Transportministeriums durfte man ein Neufahrzeug von staatlichen Händlern erwerben – es konnte Jahre dauern, den nötigen Stempel zu bekommen. Deswegen kurven auf Kuba immer noch viele jahrzehntealte amerikanische Straßenkreuzer herum. Daneben blühte für Neuwagen ein Schwarzmarkt mit überhöhten Preisen. Der Verkauf von Gebrauchtwagen an Privatleute wurde bereits 2011 erlaubt. 
Staat legt den Preis fest 
Direkt importieren dürfen Kubaner Autos aber immer noch nicht. Das bleibt dem staatlichen Monopol vorbehalten, das damit auch den Marktwert der Autos bestimmen darf. Und der liegt nun selbst für bescheidene Kleinwagen fast auf dem Niveau eines Ferrari. 
Erschwingliche Alternativen waren bei einem Gebrauchtwagenhändler im Westen von Havanna auch nicht zu erblicken. Das billigste Auto war ein BMW aus dem Jahr 1997 für umgerechnet gut 10.600 Euro. Für einen Minivan von Hyundai, Baujahr 2009, sollten potenzielle Käufer immer noch 74.000 Euro bezahlen. 
Wer sich in Kuba ein Auto kauft, wird also wohl auch in Zukunft auf einen der zahlreichen US-Oldtimer aus den Jahren vor der Revolution zurückgreifen. Die Ära der Fünfziger-Jahre-Kreuzer geht auf Kuba also doch nicht so schnell zu Ende wie von manchem erwartet."	technik
"Kubaner dürfen ab sofort ganz legal ein aus dem Ausland importiertes Auto oder Motorrad kaufen. Bislang war das nur Diplomaten, ausländischen Firmen und einigen wenigen Staatsbediensteten erlaubt. Nur mit einer speziellen Erlaubnis der Behörden konnten importierte Fahrzeuge erworben werden, der Schwarzmarkt für diese Genehmigungen war ein ertragreiches Geschäft. Die Regierung vergab die Sondergenehmigungen oft an Regierungsbeamte oder Ärzte. 
Alle anderen mussten sich mit Autos, die vor der Revolution 1959 gebaut wurden, oder mit Importen aus der Sowjetunion aus den 1970er und 1980er Jahren begnügen. Die neue Regelung ist Teil der von Staatschef Raúl Castro angekündigten Reformen zur wirtschaftlichen Öffnung des Landes. 
Kuba hatte kurz nach der Revolution von 1959 den freien Autohandel verboten. Seit Oktober 2011 ist Privatpersonen der Handel mit alten Gebrauchtwagen wieder eingeschränkt erlaubt. Die Regelung galt nur für Autos, die vor 1959 gebaut wurden. 
Der Erwerb eines neuen Autos wird trotzdem für viele Kubaner unerreichbar bleiben. Am Tag vor dem Inkrafttreten des neuen Gesetzes wurden chinesische Gebrauchtwagen von 2010 für rund 25.000 US-Dollar angeboten, ein südkoreanischer Hyundai für 37.000 Dollar. Der staatliche Monatslohn liegt allerdings bei nur etwa 20 US-Dollar."	technik
"Die Sonne scheint, die Innenstadt ist voll, Parkplätze sind nicht in Sicht – wie gern wäre mancher urbaner Autofahrer da mit dem Roller unterwegs. Doch der Umstieg ist denkbar einfach. Ein Pkw-Führerschein reicht aus, und Vespas und andere Motorroller gibt es mittlerweile so mobil wie beim Carsharing für die Kurzzeitmiete. Doch einige Dinge sind vor dem Losbrausen zu beachten. 
Inzwischen bieten in einigen Städten Unternehmen stationsunabhängig Roller an, beispielsweise in München oder Hamburg. Das heißt, die Zweiräder stehen am Straßenrand, und man bucht und bezahlt sie über das Smartphone. Nach der Fahrt stellt man sie einfach wieder an der Straße ab. So sind die Roller auch bei einer Städtereise eine günstige und praktische Alternative zum Auto – die Südeuropäer leben es in ihrer Heimat vor. 
Der Pkw-Führerschein (Klasse B) umfasst auch die Klasse AM. Das bedeutet: Man darf auch kleine Scooter fahren. Diese dürfen durch ihre Bauart maximal eine Geschwindigkeit von 45 km/h erreichen; der Motor darf höchstens 50 Kubikzentimeter Hubraum haben, für Elektromotoren gilt eine Grenze von 4 kW Leistung. Das gilt nach Auskunft des ADAC auch im EU-Ausland. 
In Deutschland und auch im EU-Ausland gilt Helmpflicht. Viele Vermieter bieten einen Kopfschutz mit an. Hier muss die Größe passen, außerdem sollte das Modell nach ECE R 22/05 geprüft und mit einem entsprechenden Label gekennzeichnet sein. Wer häufiger fährt, sollte über die Anschaffung eines eigenen Helms nachdenken – das ist die beste Lösung. Neben dem Kopf sollten auch alle anderen Körperteile geschützt sein. Also: keine nackte Haut, denn selbst ein Sturz mit geringem Tempo kann gefährliche Hautabschürfungen mit sich bringen. Scooterfahrer sollten in jedem Fall Handschuhe tragen. 
Empfehlenswert für alle, die noch nie zuvor einen Roller gefahren sind, ist ein bisschen Training vor der ersten Fahrt, beispielsweise auf einem wenig befahrenen Parkplatz. Bremsen, Kurven fahren und ausweichen ist mit einem Roller anders als mit einem Fahrrad:  ""Ein Roller hat mit 100 bis 130 Kilogramm wesentlich mehr Masse mit entsprechenden Auswirkungen auf die Dynamik "", sagt Lars Krause vom TÜV Süd. 
So sollte beispielsweise der richtige Bremsdruck an der Vorder- und Hinterbremse geübt werden: Wer vorn zu stark bremst, kann wegrutschen; wer nur hinten verzögert, läuft Gefahr, auf das Hindernis aufzufahren, denn das hat kaum Wirkung. Für das Umfahren von Hindernissen gilt: Bremsen, Griffe lösen und umfahren, dann wieder bremsen.  ""Nur die wenigsten der kleinen Roller haben ABS. Bremsen und ausweichen müssen deswegen besonders geübt sein. Fahrrad-Erfahrung reicht nicht! "", warnt Krause. 
Auch das Fahren in Schräglage sollte geübt werden, insbesondere, wenn noch ein Sozius dabei ist. Auch der Beifahrer muss sich mit in die Kurve legen, damit der Fahrer das Fahrzeug beherrschen kann. Erfahrung braucht es auch für das Lesen der Straße.  ""Selbst kleinste Verunreinigungen führen schnell zum Sturz "", sagt Motorradexperte Krause. 
Vor der Abfahrt empfiehlt es sich, den Roller auf Beschädigungen überprüfen. Offensichtliche Schäden, wie verbogene Spiegel oder Bremshebel, weisen auf einen Unfall hin oder darauf, dass der Roller umgefallen ist.  ""Umfaller sind besonders gefährlich, weil dabei Luft ins Bremssystem gelangen kann "", sagt Krause. Die Reifen sollten nicht weniger als ein Millimeter Profil haben und keine Beschädigungen aufweisen. Bei zu wenig Druck die nächste Tankstelle anfahren und auffüllen – die Werte stehen in der Betriebsanleitung unter dem Sitz."	technik
"Das Versprechen ist Wirklichkeit geworden. Von Oktober an liefert Toyota, der nach Stückzahlen gemessen größte Autohersteller der Welt, ein mit Wasserstoff betriebenes Brennstoffzellenauto nach Deutschland. Der Mirai fährt immer elektrisch, den Strom dafür macht er selbst. Eine erste ausführliche Tour in einem Vorserienmodell macht offensichtlich: Dieses Antriebskonzept steht nicht in Konkurrenz zu batterieelektrischen Fahrzeugen. Vielmehr ist der Mirai ein ernst zu nehmender, leiser Gegner für das konventionelle Alleskönnerauto mit Verbrennungsmotor. 
Das liegt an der Flexibilität. Und entscheidend am Gefühl. Hier hängt man nicht mental am Stromkabel und plant schon die nächste Ladezwangspause. Stattdessen fährt man so frei wie gewohnt, nur ohne das Brummen und die Abgase eines Benzin- oder Dieselmotors. Laut Toyota verbraucht der Mirai rund 760 Gramm Wasserstoff auf 100 Kilometer, was bei fünf Kilogramm Tankvolumen eine theoretische Reichweite von 658 Kilometern ergibt. Praktisch kamen wir im Test auf ein Kilogramm pro 100 Kilometer – also die Möglichkeit, mit einer Tankfüllung 500 Kilometer weit zu kommen. 
Der Mirai – der Name bedeutet aus dem Japanischen übersetzt  ""Zukunft "" – funktioniert erstaunlich unspektakulär. Wer irgendwann den Vollhybrid Prius gefahren ist, fühlt sich sofort zu Hause. Der winzige Schalthebel wird mit Fingerspitzenkraft nach dem Start in die Position D wie Drive gezogen. Jetzt ist das Auto anfahrbereit, und der Joystick wippt zurück in seine Ausgangsposition. Danach bietet die Lenkung, analog zu anderen Toyotas der letzten Jahre, eine direkte Rückmeldung, befindet sich aber wie der Rest dennoch auf der komfortablen Seite. 
Schnell geht natürlich auch. Der 113 kW (155 PS) starke Elektromotor zieht die mit 4,89 Metern Länge ausgewachsene Limousine zügig nach vorne. Ein Sportwagen ist der Mirai damit nicht, eher ein kraftvoller Gleiter für den Alltag. Auf gehobenem Niveau allerdings: Der Grundpreis von 78.540 Euro ist zugleich der Endpreis. Die Vollausstattung ist inklusive. Ledersitze, Navigationssystem, Luxussoundanlage. 
Auffällig ist, wie Menschen, die weder technik- noch autobegeistert sind, sondern nur von A nach B kommen wollen, auf dieses Elektroauto reagieren: Warum denn nicht gleich so? Keine Umstellung im Alltag, einfach in drei Minuten volltanken und weiter geht es in den Strand- oder Skiurlaub. 
Eine Antwort auf die Frage ist die bisher rudimentäre Infrastruktur. Zwar gibt es in Großstädten wie Hamburg und Berlin sowie in Süddeutschland einige Wasserstofftankstellen. Dazwischen, zum Beispiel in Niedersachsen, klaffen Lücken. Das von der Nationalen Organisation Wasserstoff- und Brennstoffzellentechnologie (NOW) koordinierte Aufbauprogramm sieht 400 Zapfsäulen bis 2023 vor. Genug für den Anfang, nicht genug für einen Boom. 
Nötig wären wohl 2.000, damit jeder versorgt ist. Selbst wenn im pessimistischen Szenario zwei Millionen Euro pro Tankstelle zugrunde gelegt werden (vereinfacht gesagt wird eine weitere Säule dazugestellt), kommen hier insgesamt lediglich vier Milliarden Euro zusammen. Das ist bloß scheinbar viel Geld. Denn erstens zahlen deutsche Autofahrer jährlich rund 70 Milliarden Euro für Benzin und Diesel, von denen mehr als die Hälfte Steuern und Abgaben sind. Und zweitens fallen die Investitionskosten nicht auf einen Schlag an. Sie würden über viele Jahre gestreckt werden. Entscheidend ist also nicht das Geld, sondern der Wille. 
 
Kritiker monieren gerne, dass der Wirkungsgrad im Vergleich zu batterieelektrischen Autos schlechter sei. Das stimmt, zumindest wenn man nur die Fahrenergie betrachtet und die Emissionen bei der Akkuproduktion vernachlässigt. Wahr ist aber auch, dass bei einem traditionellen Verbrennungsmotor die Effizienz niemanden interessiert. Der potenzielle Käufer will lediglich wissen: Was kostet mich der Kraftstoff, egal in welcher Form? 
Und hier gibt es leider für den Wasserstoff noch keine saubere Zahl. Der aktuelle Preis von rund neun Euro pro Kilogramm wurde vor Jahren symbolisch festgelegt, spiegelt also nicht den Marktwert. Die Industriepreise jedenfalls liegen deutlich darunter. 
Auf die Frage, warum Toyota keine batterieelektrischen Autos baut, reagieren Unternehmensvertreter gelassen. Sie verweisen zuerst darauf, dass Toyota pro Jahr über 200 Millionen Zellen für die Hybridautos baue, also sehr viel Erfahrung im Thema habe und sich darum auskenne. Dann wird auf das Stadtmobil i-Road und den in Deutschland eingestellten Kleinwagen iQ gezeigt – ja, hier könne es Sinn ergeben, mit Akkus zu fahren. 
Man sei aber zu der Überzeugung gekommen, dass sich die von einem Massenhersteller geforderten Kostensenkungen leichter beim mit Wasserstoff betriebenen Brennstoffzellenauto als bei den Batterien verwirklichen ließen. Was Toyota mit diesem Satz sagen will: Die Skaleneffekte sind beim Mirai faktisch schon eingetreten. Er teilt sich viele Komponenten mit den fast acht Millionen Mal verkauften Toyota-Hybriden. Nur die Brennstoffzelle sowie der Tank sind wirklich neu. 
Der Eindruck, der vom Toyota Mirai bleibt, ist der einer großen Chance. Hier ist endlich ein Wettbewerber für die Direkteinspritzer-Benziner und Turbodiesels dieser Welt, der den druckvollen Komfort des Elektromotors mit der gewohnten Mobilität verbindet. Eine Erfolgsgarantie ist das nicht – das monetäre Risiko des Scheiterns allerdings ist für Japaner angesichts der Gleichteilestrategie gering."	technik
"Der Käfer stirbt am 30. Juli 2003. Oder schon am 1. Juli 1974? Am 19. Januar 1978? Oder doch 1985? Der Volkswagen-Klassiker hat viele Tode hinter sich. Nimmt man den Beetle als Nachfolger ernst, dann gibt es ihn sogar bis heute. 
Immerhin lässt sich der Tag bestimmen, an dem der Käfer gezeugt wird. Am 17. Januar 1934 legt der ehemalige Daimler-Chefkonstrukteur Ferdinand Porsche dem Reichsverkehrsministerium sein  ""Exposé betreffend den Bau eines Deutschen Volkswagens "" vor. Adolf Hitler lässt ihm bei Fallersleben ein Werk und eine Stadt für Zehntausende Arbeiter bauen. Doch dann kommt der Zweite Weltkrieg, und in Fallersleben werden Militärfahrzeuge statt Autos fürs Volk gebaut. 
 
Die britischen Besatzer nennen die aus dem Boden gestampfte Stadt nach Kriegsende Wolfsburg. Noch im Sommer 1945 lassen sie die Serienfertigung anlaufen. Der Klang des luftgekühlten Vierzylinder-Boxermotors im Heck wird zum allgegenwärtigen Soundtrack des Wirtschaftswunders. Im  ""Käfer "", wie er später genannt wird, rollen die Deutschen zu ihren ersten Urlauben. 
Doch die Firma Volkswagen verlässt sich zu lange auf ihren Bestseller, bis er zum Fluch wird. 1968 sind von mehr als 1,5 Millionen im Inland produzierten Volkswagen 68 Prozent Käfer. Zugleich aber schmälern die steigenden Lohnkosten den Gewinn, der sich mit dem schlichten Gefährt erzielen lässt. Und der luftgekühlte Heckmotor, der die Hinterachse antreibt, kommt langsam aus der Mode. Dumm, dass auch die anderen VW-Modelle – etwa der Passat-Vorläufer Typ 3, das Karmann-Ghia-Coupé und der Kleinbus – darauf aufbauen. 
Der VW Käfer überholt am 17. Februar 1972 mit damals mehr als 15 Millionen Fahrzeugen das Modell T von Ford (genannt  ""Tin Lizzy "") als meistgebautes Auto der Welt. 2002 schubst ihn der Golf vom Thron. Statistisch ist mit weltweit rund 40 Millionen Exemplaren der Toyota Corolla der Rekordhalter, gefolgt von den Pickups der F-Serie von Ford mit rund 34 Millionen. Der seit 1966 gebaute Corolla ebenso wie die schon 1948 begründete F-Serie haben aber noch gravierendere Neudesigns hinter sich als der Golf, so dass es sich nicht wirklich um dasselbe Auto handelt. 
Dass ein Design von 1934 sich in den sechziger Jahren überhaupt noch vermarkten lässt, liegt am niedrigen Preis und den geringen Unterhaltskosten. Getreu dem Werbespruch  ""Er läuft und läuft und läuft "" hält der VW lange und muss selten in die Werkstatt. Als in Europa Zuverlässigkeit als Verkaufsargument für ein sonst unmodisches Auto nicht mehr zieht, springen die Amis ein: Zeitweise wird fast jeder dritte Beetle in den USA verkauft. Die Hippie-Generation liebt den knuffigen  ""Love Bug "", als in der Alten Welt das Käfersterben schon begonnen hat. 
Kritiker spotten:  ""Er säuft und säuft und säuft "" 
Es hat sich aber auch alles Anfang der siebziger Jahre gegen ihn verschworen: Japaner produzieren billiger, der sinkende Dollar zwingt VW zu höheren Preisen in den USA, die US-Behörden erlassen immer schärfere Sicherheits- und Abgasvorschriften. Und in der Ölkrise fressen hohe Benzinpreise die Ersparnis bei den Wartungskosten auf. Offiziell schluckt der Käfer 10,5 Liter auf 100 Kilometer. Tatsächlich werden es schnell noch ein paar Liter mehr. Der Slogan erklingt nun als Hohn:  ""Er säuft und säuft und säuft. "" 
Vorstandsvize Kurt Lotz hatte schon 1967 gewarnt, neue Modelle müssten her, um die Abhängigkeit vom Käfer und vom US-Markt zu verringern. Im Mai 1968 wird Lotz Vorstandschef, nach dem plötzlichen Tod des seit 20 Jahren herrschenden Patriarchen Heinrich Nordhoff. Damit sind die Tage des Käfers gezählt: Lotz leitet ein Reformprogramm ein. Das Ziel: den Anteil des Käfers an der Konzernproduktion bis 1975 auf 15 Prozent zu reduzieren. 
 
Weil für neue Modelle Milliardeninvestitionen nötig sind, sinken die Gewinne zunächst weiter. Den Aktionären, darunter der Bund und das Land Niedersachsen, gefällt das gar nicht. Der Aufsichtsrat wirft Lotz hinaus, aber sein Nachfolger Rudolf Leiding setzt die Erweiterung der Produktpalette fort. Mit Erfolg: 1973 stößt der Autobauer mit dem Passat in die gehobene Mittelklasse vor, 1974 kommt der Golf. 2002 überholt er den Käfer als meistgebautes Auto der Welt. 
Den Käfer schrauben sie im Stammwerk in Wolfsburg nur noch bis zum 1. Juli 1974 zusammen, in Emden bis zum 19. Januar 1978. Mit einem melancholischen Festakt verabschieden die Emdener Arbeiter den letzten der rund 16 Millionen in Deutschland produzierten Käfer. Eine Blumengirlande auf die  ""dakotabeige "" Karosserie drapiert wie bei einer Beerdigung. Danach läuft der Bucklige nur noch in Südafrika, Brasilien und am längsten in Mexiko vom Band. 
Der Name  ""Käfer "" ist ein Re-Import aus den USA, wo das Auto  ""Beetle "" oder  ""Bug "" genannt wurde. In Deutschland setzt sich die Bezeichnung erst in den sechziger Jahren durch, als Volkswagen auch andere PKW einführt. In vielen Sprachen heißt der Wagen Käfer, in Frankreich Marienkäfer ( ""Coccinelle "") und in Italien Maikäfer ( ""Maggiolino ""). Schweden nennen ihn  ""Bubbla "" (Blase), Polen  ""Garbus "" (der Bucklige). In Mexiko ist er als  ""Vocho "" bekannt, vermutlich eine Ableitung der liebevollen Verkleinerung  ""Volkswagencito "". 
Von Volkswagen de Mexico aus Puebla importiert VW zunächst noch Käfer für den deutschen Markt. 1985 ist auch damit Schluss: Ein  ""Jubiläumskäfer "" und eine Lieferung für die Bundeswehr sind die letzten offiziellen Wagen für Deutschland. Privatimporteure, kleine Firmen, manchmal Supermarktketten holen ihn nun noch über den Ozean. 1995 gibt es Käfer für 18.000 Mark im Praktiker-Baumarkt, aber VW will für Garantien und Ersatzteile nicht mehr gerade stehen. 
Am 30. Juli 2003 kommt das Aus: Die Última Edición verlässt Puebla. Auf viele der rund 3.000 Fahrzeuge dieser letzten Käfer-Generation warten Liebhaber in Europa. Eines bekommt der Papst geschenkt; Johannes Paul II. fuhr schon als junger Priester in Polen Käfer. Den vorletzten Wagen sichert sich der Axel-Springer-Verlag als Werbemittel für seine Zeitschrift Auto Bild. Und der allerletzte steht in der VW-Sammlung in Wolfsburg. 
Und noch ein kleiner Schritt ins Auto-Nirvana: Seit diesem Jahr dürfen in Mexiko-Stadt keine Käfer mehr als Taxis fahren. Gut, der New Beetle, 1997 eingeführt, und seit 2011 der Beetle knüpfen mit ihrem kugeligen Retrodesign an die alten Zeiten an. Doch der Käfer-Klon mag ein Liebhaberstück sein – ein Volks-Wagen wird er wohl nie."	technik
"Sie nisten sich auf den Rechnern ihrer Opfer ein, verschlüsseln deren Dateien und geben sie erst wieder frei, wenn die Betroffenen in irgendeiner Form Lösegeld bezahlen: Ransomware werden bösartige Programme wie Shade oder der BKA-Trojaner (der natürlich nicht vom BKA kommt) genannt. Lange Zeit waren sie fast ausschließlich ein Problem für Windows-Nutzer. Nun gibt es sie auch für Apples OS X und für Linux, und eines befällt bereits die ersten Opfer. 
Linux.Encoder.1 haben die Entdecker vom russischen Antivirenspezialisten Dr.Web es getauft. Die Schadsoftware befällt vor allem linuxbetriebene Server, die Opfer sind also Websitebetreiber und nicht Nutzer von Desktop-Distributionen wie Ubuntu oder openSUSE. Bisher soll es  ""zig "" Opfer geben, teilt Dr.Web mit. 
Einmal unbemerkt installiert, verschlüsselt Linux.Encoder.1 verschiedene Verzeichnisse und Dateitypen mit dem als praktisch unknackbar geltenden Verfahren AES. Die Folge: Der Serverbetreiber kann nicht mehr darauf zugreifen und zum Beispiel Websites auf seinem Server nicht mehr verändern. Der AES-Schlüssel wird seinerseits verschlüsselt abgelegt. Erst wenn die Betroffenen eine Website im Tor-Netzwerk besuchen und dort eine Bitcoin als Lösegeld überweisen, entschlüsseln sich die gesperrten Dateien und Verzeichnisse automatisch. 
So weit, so gemein. Allerdings haben die Entwickler geschlampt, wie die Analysten der Firma Bitdefender bemerkten. Der AES-Schlüssel lässt sich relativ einfach erraten, Bitdefender stellt dafür ein kostenloses Tool bereit, das die Arbeit übernimmt. Ein potenzieller Linux.Encoder.2 jedoch dürfte nicht mehr so einfach abzuwehren sein, schließlich lernen auch die kriminellen Programmierer aus ihren Fehlern. 
Die erste echte Ransomware für OS X ist bisher nur ein Proof-of-concept. Genauer: zwei Proof-of-concepts. Das erste ist etwa zwei Monate alt, entwickelt hat es der Sicherheitsforscher Pedro Vilaça. Es ist aber nicht voll funktionstüchtig. Vilaça wollte lediglich demonstrieren, dass es prinzipiell möglich ist, Dateien in OS X gegen den Willen des Besitzers zu verschlüsseln. Frühere Ransomware für OS X und iOS hatte nur den Safari-Browser mit einem einfachen JavaScript-Programm blockiert oder das betroffene Gerät in den Sperrzustand versetzt. Beides ließ sich vergleichsweise einfach beheben. 
Das zweite, deutlich ausgereiftere Konzept hat der brasilianische Sicherheitsforscher Rafael Salema Marques gerade in einem Video demonstriert. Mabouia nennt er seine Erpressersoftware. Symantec hat mittlerweile bestätigt, dass sie funktioniert. 
Mabouia verschlüsselt Dateien und hinterlässt auf dem Rechner des Opfers eine Anleitung zur Lösegeldzahlung. Sie enthält einen einmaligen Identifizierungscode und den Link zur Website des Täters. Wer innerhalb von 72 Stunden 50 US-Dollar überweist, darf 20 Dateien entschlüsseln. Für 70 Dollar bekommt man 100 Dateien zurück. Für 100 Dollar gibt es ein  ""VIP-Paket "", das die Entschlüsselung aller Dateien beinhaltet. Wer innerhalb der drei Tage gar nicht zahlt, muss damit rechnen, dass der Schlüssel zu seinen Daten unwiederbringlich zerstört wird. 
Das alles klingt, als wolle sich Marques über Apple-Nutzer lustig machen, doch er meint es anders: Kriminelle wüssten, dass Mac-Nutzer in der Regel überdurchschnittlich viel Geld haben, sagte er im Gespräch mit Vice. Er glaubt,  ""Geschäftsmodelle "" wie sein VIP-Paket seien keineswegs unwahrscheinlich. 
Den Quellcode will Marques zwar nicht veröffentlichen. Dennoch ist nicht auszuschließen, dass bald auch andere, weniger ehrenwerte Menschen herausfinden werden, wie die feindliche Übernahme von Dateien in OS X funktioniert. 
Nutzer sollten die üblichen Vorsichtsmaßnahmen beachten: Daten regelmäßig sichern, keine Links oder Anhänge in E-Mails öffnen, wenn nicht klar ist, von wem sie kommen, die Antivirensoftware und – wenn sie denn wirklich installiert sein sollen – Plugins wie Flash und Adobe Reader aktuell halten, den Rechner möglichst nicht mit Admin-Rechten nutzen. Wer sich trotzdem eine Erpressersoftware einfängt, soll auf keinen Fall das Lösegeld zahlen, empfiehlt das Bundesamt für Sicherheit in der Informationstechnik (BSI) – denn es gibt keine Garantie, dass die Daten anschließend wirklich freigegeben werden."	technik
"Mit einem kurzen Tweet hat Kabir Alli aus Virginia am Dienstag eine Welle der Empörung ausgelöst. Er beinhaltet ein kurzes Video, in dem Alli erst  ""three black teenagers "" in die Google-Bildersuche eingibt, und danach  ""three white teenagers "". Das erste Suchergebnis besteht überwiegend aus sogenannten mugshots, also Polizeifotos von Festgenommenen. Das zweite zeigt fast ausschließlich Stockfotos von glücklich lächelnden Weißen. Mehr als 60.000 Menschen haben das Video seither per Retweet verbreitet. 
Die Frage, die sich viele von ihnen stellten, lautet: Ist Google rassistisch? 
Die kurze Antwort: Nein. 
Die lange Antwort beginnt mit dem britischen Vlogger Antoine Speaks, der sich regelmäßig mit dem Thema Rassismus beschäftigt. Am 31. März veröffentlichte Speaks ein Video, in dem er jenes Experiment durchführt, das Alli nun wiederholt hat. Die Idee stammt allerdings auch nicht von Speaks. Er war irgendwann im Netz darauf gestoßen und suchte eine Erklärung für die Diskrepanz in den Suchergebnissen. Das ist der Unterschied zwischen ihm und Alli. 
Speaks tippt  ""three white teenagers "" und  ""three black teenagers "" zunächst in die Google-Textsuche ein. In beiden Fällen bekommt er überwiegend Links auf Nachrichten über Straftaten und Gerichtsprozesse. Die entsprechende Bildersuche hingegen ergibt das Ergebnis, das Alli jetzt reproduziert hat: Von weißen Jugendlichen findet Speaks überwiegend Stockfotos, von schwarzen Jugendlichen vor allem mugshots. 
Seiner Meinung nach liegt das an Angebot und Nachfrage: In westlichen Ländern wie den USA und Großbritannien sei der Bevölkerungsanteil von Weißen sehr viel höher als der von Schwarzen. Dementsprechend gebe es eine höhere Nachfrage nach Stockfotos von Weißen, zum Beispiel für Werbung, und ein größeres Angebot von entsprechend verschlagworteten Motiven. Die mugshots wiederum seien von Nachrichten-Websites mit den Schlagworten three black teenagers versehen worden. Google habe damit nichts zu tun, die Suchmaschine zeige nur, was sie auf Nachrichten- und anderen Websites findet. (Weshalb das Ergebnis nun, zwei Tage später, schon etwas anders aussieht – die Medienberichte über Allis Tweet haben es beeinflusst.) 
Eine Gegenprobe macht Speaks auch. Die Suchergebnisse für  ""three white friends "" zeigen lächelnde Menschen, Schnappschüsse und Stockfotos – die für  ""three black friends "" auch. Hier fällt der Nachrichtenfaktor heraus, weil Nachrichtenbilder selten mit den Schlagworten  ""drei Freunde "" versehen werden. 
Mit den Schlagworten argumentiert auch Google selbst. Im April war jemandem aufgefallen, dass eine Google-Suche nach  ""unprofessional hairstyles for work "" vornehmlich auf Bilder von Schwarzen führte, eine Suche nach  ""professional hairstyles for work "" fast nur auf Fotos von blonden weißen Frauen. 
Das Unternehmen teilte daraufhin mit, Suchmaschinen würden nur reflektieren, was im Internet zu finden sei. Der vermeintliche Rassismus stecke in den Schlagworten zu den Bildern. Auf die hat ein Suchmaschinenbetreiber aber natürlich keinen Einfluss. 
Die jetzige Empörung über die Teenager-Motive ist also ein bekannter Reflex. Befeuert wurde er möglicherweise von einem Vorfall aus dem Juni 2015, an dem Google tatsächlich schuld war. Damals hatte die automatische Bilderkennung in Google Fotos das Schlagwort  ""Gorilla "" für mehrere Porträts einer schwarzen Frau vorgeschlagen. 
Google hatte seine selbstlernende Software einfach schlecht trainiert, wie einer der Ingenieure einräumte. Der Vorfall sei aber besonders peinlich, weil er den Algorithmus als rassistisch erscheinen lasse. Hätte der stattdessen ein Baby mit einem Seehund verwechselt oder einen Weißen mit einem Lemuren, hätte sich niemand großartig darüber aufgeregt. 
Das Fazit auch der langen Antwort: Nein, Google ist nicht rassistisch. Der Ansicht ist übrigens auch Kabir Alli, wie er auf Twitter klarzustellen versuchte. Retweets nach 14 Stunden: 1."	technik
"Der Computerwurm Stuxnet ist nach einem Bericht der New York Times gemeinsam von Israel und den USA entwickelt worden, um das iranische Atomprogramm zu sabotieren. Wie die US-Zeitung am Samstag unter Berufung auf Geheimdienst- und Militärexperten berichtete, war an der Entwicklung des Wurms vermutlich unwissentlich auch der deutsche Siemens-Konzern beteiligt, dessen Systeme zur Steuerung von Industrieanlagen Stuxnet angreift. Das Unternehmen hatte demnach mit einer Forschungseinrichtung des US-Energieministeriums an einem Programm zum Schutz vor Cyberattacken zusammengearbeitet. Die dabei gefundenen Sicherheitslücken seien dann bei der Entwicklung des Wurms ausgenutzt worden. 
Mithilfe des Wurms kann ein Angreifer die Kontrolle über zentrale Systeme etwa von Kraftwerken, Pipelines oder Fabriken übernehmen und die Anlagen im schlimmsten Fall zerstören. Der Computerwurm war erstmals im Juni aufgetaucht. Da die meisten  ""Infektionen "" in Iran festgestellt wurden, gab es Spekulationen, der Wurm sei zur Sabotage der Atomanlagen des Landes entwickelt worden. 
Laut New York Times wurde die Wirksamkeit des Stuxnet-Wurms in dem streng abgeriegelten Dimona-Komplex in der Negev-Wüste getestet, wo sich eine israelische Atomanlage befinden soll. Wegen des Tests sei er  ""effektiv "" gewesen, sagte ein US-Experte der Zeitung. Iran hatte stets bestritten, von Stuxnet getroffen worden zu sein. Nach Angaben der New York Times hingegen wurde durch Stuxnet ein Fünftel der iranischen Uranzentrifugen lahmgelegt und das gesamte Programm zurückgeworfen. Der israelische Minister für Strategische Angelegenheiten, Mosche Jaalon, hatte im vergangenen Monat gesagt, Teheran sei  ""wegen technologischer Herausforderungen und Schwierigkeiten "" noch Jahre vom Bau von Atomwaffen entfernt. 
Iran wird verdächtigt, unter dem Vorwand der zivilen Nutzung der Atomenergie heimlich an einer Atombombe zu bauen. Teheran weist den Verdacht von sich. Israel unterstützt zwar die Bemühungen der USA, über Sanktionen die Entwicklung von Atomwaffen zu verhindern, schließt aber auch ein militärisches Eingreifen nicht aus. Die fünf Vetomächte im UN-Sicherheitsrat und Deutschland führen ab Freitag in Istanbul erneut Gespräche mit der Regierung in Teheran über das iranische Atomprogramm."	technik
"US-Außenministerin Hillary Clinton und ihr russischer Kollege Sergej Lawrow haben am Wochenende bei der Münchner Sicherheitskonferenz das Start-Abkommen ratifiziert. Damit verpflichten sich beide Staaten, die im Wettrüsten des Kalten Krieges angehäuften Vorräte an Atomwaffen ein wenig abzubauen. 
Das ist durchaus ein Erfolg. Gleichzeitig aber beginnen viele Länder gerade ein neues Wettrüsten. Im Internet. Russland und China beispielsweise geben derzeit nach Informationen aus Geheimdienstkreisen viel Geld aus, um Offensivstrategien im Krieg der Daten zu entwickeln. Ein reiner Krieg zweier Staaten im Netz sei unwahrscheinlich, glaubt die OECD. Trotzdem hoffen viele Regierungen offensichtlich, dass Viren und Trojaner eine neue, mächtige Erstschlagswaffe sein können. 
Bundeskanzlerin Angela Merkel und ihr Innenminister Thomas de Maizière schlugen deshalb in München neue Strukturen zur Abwehr von Angriffen im Netz vor. Denn Saboteure und Kriminelle, Wirtschaftsspione, Geheimdienste und Armeen nähmen weder auf Staatsgrenzen noch auf die Ressortzuständigkeiten von Innen-, Wirtschafts- und Verteidigungsministern Rücksicht. 
Zusammenarbeit statt eigener Projekte in jedem Land forderten Merkel und de Maizère.  ""Es gibt Herausforderungen, auf die haben wir noch keine abschließende Antwort "", sagte Merkel in München.  ""Wir werden miteinander lernen müssen, wie wir auf diese Herausforderungen antworten. "" Nationale Abwehrkonzepte reichten nicht, nötig seien  ""internationale Antworten "". Merkel warnte, ohne Kooperation würde im Internet ein neues Wettrüsten befeuert:  ""Wir werden viel Geld damit vergeuden. "" 
Kaum hatte sie gesprochen, verkündete der britische Premier David Cameron, sein Land investiere allein in diesem Jahr mehr als eine Milliarde Euro in die Hacker-Abwehr. Projekte, bei denen zu viele Länder gemeinsame Sache machten, seien nicht effizient, sagte Cameron. Auch die USA setzen auf die eigenen Fähigkeiten und haben 2010 einen Cyber-Command gegründet; 3000 Mitarbeiter sollen mit einem Budget von knapp 15 Milliarden Dollar pro Jahr Strategien für die nationale Sicherheit entwerfen. 
Und die Bundesregierung will noch diesen Monat im Kabinett eine erste bundesdeutsche  ""Cybersicherheitsstrategie "" beschließen. Die Federführung hat der Innenminister. Er plant ein nationales Internet-Abwehrzentrum, angesiedelt beim Bundesamt für Sicherheit in der Informationstechnologie. 
Die Staaten sehen sich unter Handlungsdruck: Die Attackierer warten mühsame internationale Entscheidungsprozesse nicht ab. Schon bei der klassischen militärischen Zusammenarbeit sind die unterschiedlichen Verteidigungsstrukturen der Länder ein kaum überwindbares Problem. Bei der Netz-Nato kommt hinzu, dass nicht einmal auf nationaler Ebene klar ist, wer sich um was kümmern muss. Etwa 80 Prozent der Informations-, Kommunikations- und Energienetze in den Industriestaaten sind in privater Hand. 
 
 ""In der EU haben wir eine total zersplitterte Zuständigkeit "", kritisierte de Maizière. Außerdem werde bisher das Thema als Anhängsel der Wirtschaftspolitik behandelt. 
Das Durcheinander und die Alleingänge erhöhen jedoch das Risiko eher noch. Geht es um Bedrohungen wie Atom- oder Biowaffen, ist eine der größten Sorgen die  ""entwichene "" Technologie: Kriminelle und Terroristen, die sich solche Waffen beschaffen und sie einsetzen. 
Die Milzbranderreger (Anthrax), die Unbekannte im September 2001 an US-Regierungsstellen und Politiker verschickten und an denen damals fünf Menschen starben, wurden in den siebziger Jahren in amerikanischen Militärlabors gezüchtet. Was passiert, wenn für Computerangriffe entwickelte Software in die falschen Hände gerät oder sich unkontrolliert verbreitet, lässt sich kaum überblicken. 
Kriminelle sind längst aktiv im Netz. Zuletzt war die amerikanische Börse Nasdaq ihr Ziel. Immer wieder gibt es jedoch auch Meldungen über Angriffe, die offensichtlich von Staaten ausgehen. So legten massenhafte Spam-Mails im Frühjahr 2007 estnische Internet-Server lahm. Estland machte Russland dafür verantwortlich. Das Programm Stuxnet sollte im Sommer 2010 die Leittechnik iranischer Atomanlagen stören. Weil es gezielt bestimmte Siemensanlagen angriff und aufwändig konstruiert war, vermuten Fachleute US-Geheimdienste und den israelischen Mossad hinter der Attacke. 
Sandro Gaycken, Experte für Hochsicherheits-Infrastrukturen an der Freien Universität Berlin, sagte der WAZ, die USA hätten fünf Jahre lang Geld für die Forschung in Sachen Cyberwar aus dem Fenster geworfen.  ""Der Bundesinnenminister sollte diesen Fehler nicht wiederholen. "" 
Im eigenen Interesse: Das atomare Wettrüsten wurde am Ende so teuer, dass die Sowjetunion (unter anderem) daran zerbrach. De Maizìere ist sich dessen offensichtlich bewusst. Er sagte in München, das weltweite Netz, müsse weltweit geschützt werden. Wenn das nicht gelinge,  ""dann werden wir verschiedene Netze haben "", wie Thomas Wiegold in seinem Bundeswehrblog Augen geradeaus dokumentiert. Eins baue dann die Armee, eins die Versicherungen, eins die Banken. Das allerdings würde neben den rechtlichen Problemen vor allem eines gefährden: die Demokratie."	technik
"Im Bonner Gebäude des Bundesamts für Sicherheit in der Informationstechnik (BSI) geht es sonst eher ruhig zu. Selbst im Nationalen Cyber-Abwehrzentrum, in dem die Bedrohungslage der Behörden und der  ""kritischen Infrastrukturen "" wie Strom- und Kommunikationsnetze zusammengeführt wird, herrscht normalerweise konzentrierte Stille. Vom futuristischen Prunk eines Information Dominance Centers der US-Armee oder auch nur der Zentrale der Deutschen Telekom, die sich gleich neben dem BSI befindet, fehlt jede Spur. 
Doch mit der Bonner Ruhe ist es derzeit wohl vorbei. Die Behörde steht unter Hochdruck, die Pressestelle ist kaum erreichbar. Denn zu den Hauptaufgaben der Bonner IT-Spezialisten gehört die Sicherheit der Kommunikationsinfrastruktur der Bundesbehörden – und damit auch die Sicherheit von Angela Merkels Telefon. 
Durch die Enthüllungen des Nachrichtenmagazins Der Spiegel ist nun klar: Die Gespräche der Kanzlerin wurden vom US-Geheimdienst National Security Agency (NSA) erfasst. Unklar ist noch, ob es nur um das ungeschützte Partei-Handy der Kanzlerin geht, oder ob auch die von der Bonner Behörde zugelassenen Krypto-Telefone nicht mehr sicher sind. Am Freitag ging die Behörde noch davon aus, dass die von ihr in mühevoller Kleinarbeit zertifizierten Kommunikationslösungen nicht geknackt sind. 
Als das Cyber-Abwehrzentrum 2011 mit viel Pomp in Betrieb genommen wurde, erklärte Bundesinnenminister Hans-Peter Friedrich:  ""Die Infrastruktur wird zunehmend von international organisierten Angreifern attackiert. "" Dass sich die Bonner Experten derzeit weniger mit russischen Virenprogrammierern oder chinesischen Hackern, sondern mit den Angriffen der befreundeten Supermacht USA beschäftigen würden, hatte er nicht auf der Agenda. 
Das BSI steht für das gespaltene Verhältnis der Regierung zur Informationstechnik. Auf der einen Seite ist die Behörde mit ihren beiden Bonner Standorten weit von der Regierungszentrale in Berlin entfernt. Auf der anderen Seite ist das BSI in den letzten 22 Jahren immer weiter gewachsen: Inzwischen kümmern sich zirka 600 Mitarbeiter um immer vielfältigere Aufgaben: So klärt das Amt mit dem Angebot BSI für Bürger Normalanwender über Gefahren im Netz auf, entwickelt sichere Lösungen für den elektronischen Personalausweis und schreibt technische Richtlinien, die Mindestanforderungen an die elektronische Buchhaltung definieren. 
Dabei steht das BSI zwischen vielen Fronten: So will zwar die Privatwirtschaft von dem staatlichen Schutz profitieren, die großen Konzerne wollen dem Staat aber nicht zu viele Einblicke geben. Eine Meldepflicht für IT-Attacken ist zum Beispiel zum Streitfall geworden: Unternehmen fürchten einen Imageverlust und teure Nachrüstungen, wenn ihre Sicherheitsprobleme bekannt würden. 
Auch das Verhältnis zur NSA ist nicht ganz einfach. Auf der einen Seite arbeiten beide Behörden zusammen, wenn es zum Beispiel um die Sicherheit der Nato-Kommunikation in Afghanistan geht. Auf der anderen Seite muss das BSI die Bundesbehörden vor der Schnüffelei von ausländischen Mächten schützen. Dass dazu auch die NSA gehört, wissen die Bonner. 
 
Das BSI arbeitet laut Selbstauskunft rein defensiv. Während das Bundeskriminalamt sich bemüht, einen Staatstrojaner zu beschaffen oder selbst zu entwickeln, gibt das BSI praktische Hilfestellung dabei, wie man die Kommunikation verschlüsselt und Trojaner aussperrt. So hat die Behörde eine Adaption der anerkannten Verschlüsselungssoftware GPG für Windows finanziert. Und vor wenigen Tagen veröffentlichte sie neue Mindeststandards für verschlüsselte Internetverbindungen der Bundesbehörden – die sollen das Mitlauschen etwa des britischen Geheimdienstes GCHQ erschweren, der die transatlantischen Glasfaserleitungen abhört. 
Ohne den Segen des Amtes darf zudem keine Hardware in den Bundesbehörden angeschafft werden, jede Software muss geprüft werden. Dass die Beamten und Politiker aus Bequemlichkeit aber doch zum privaten Tablet oder Smartphone greifen und die Bemühungen des BSI damit zunichte machen können, steht auf einem anderen Blatt. 
Während die NSA mit ihren gewaltigen Ressourcen protzen kann, muss sich das BSI mit dem spröden Charme einer Bundesbehörde begnügen. So gehen die Amerikaner jedes Jahr bei den Hacker-Kongressen in Las Vegas auf Nachwuchssuche und haben mit In-Q-Tel einen eigenen Technologie-Investor, der sich bei den Top-Entwicklungsfirmen einkauft. Das BSI muss sich hingegen mit einem jährlichen Kongress in der Stadthalle Bad Godesberg begnügen. 
Dennoch ist das Urteil des BSI gefragt. So wirbt der Düsseldorfer Hersteller Secusmart mit seinem Kanzler-Handy. Auch Konkurrent Rohde und Schwarz berichtet, dass private und ausländische Kunden gerne zu den Sicherheitslösungen greifen, die auch von der Bundesregierung genutzt werden. Das Siegel der Bonner Behörde ist in Expertenkreisen durchaus angesehen. Diesen Ruf gilt es nun zu verteidigen."	technik
"Die Verhandlungen zwischen Gema und YouTube sind gescheitert. Seit einem Jahr streiten sich der Rechteverwerter und das Videoportal um einen neuen Lizenzvertrag für Musiknutzung in Deutschland. Am Montag brach die Gema die Verhandlungen unerwartet ab. Selbst YouTube-Besitzer Google wurde von der Nachricht überrascht. Nur wenige Minuten, bevor die Gema mit dem Abbruch der Verhandlungen an die Presse gegangen sei, habe man selbst davon erfahren, hieß es bei Google. 
Der zuletzt gültige Vertrag zwischen Gema und YouTube ist bereits zum 1. April 2009 ausgelaufen. Nach Auffassung der Gema stehen Werke der Künstler, die sie vertritt, seit mehr als einem Jahr also illegal auf YouTube. Tausende Musikvideos mussten deswegen schon damals gesperrt werden. Außerdem erhalten Gema-Künstler deswegen für ihre Musik auf YouTube derzeit keine Tantiemen. 
Harald Heker, Vorstandsvorsitzender der Gema, erklärte in einer Mitteilung und auf einer Pressekonferenz in München, man wolle ein Zeichen setzen: Musik habe ihren Wert.  ""Die Verhandlungen mit YouTube haben leider nicht zu einem akzeptablen Ergebnis geführt "", so Heker. Man wolle deutlich machen, dass YouTube für illegale Angebote zur Verantwortung gezogen werden könne. 
Nach Monaten des Schweigens eskaliert nun also offensichtlich die Situation aufgrund der Gema. Möglicherweise, weil man sich inzwischen in einer stärkeren Verhandlungsposition glaubt. Die Gema hat sich eigenen Angaben zufolge mit acht weiteren Musikautorengesellschaften zu einer Allianz zusammengeschlossen. Mitglied sind die US-amerikanische Autorengesellschaften ASCAP, BMI und SESAC, außerdem SACEM aus Frankreich und SIAE aus Italien. Damit repräsentiere man etwa 60 Prozent des Weltrepertoires. Gemeinsam fordern die Mitglieder von YouTube, rund 600 Werke zu löschen, beziehungsweise den Abruf in Deutschland zu sperren. Das  ""Problem der illegalen Nutzung "" sei ein internationales,  ""deshalb müssen wir weltweit agieren "", sagte Heker. 
Und die Drohung geht noch weiter: Wenn sie wolle, könnte die Allianz sämtliche Werkes ihres Repertoires von YouTube entfernen lassen, hieß es in der Erklärung der Gema. Mit Blick auf die Bedürfnisse der Musiknutzer und Musikurheber werde man darauf aber verzichten. 
 
Der Streit zwischen Gema und YouTube dreht sich in erster Linie um Geld. Während die Gema auf eine Abgabe pocht, die sich an Abrufzahlen und wirtschaftlichem Erfolg des Unternehmens orientiert, scheint Google eine Pauschale zu favorisieren. Die lehnt die Gema aber kategorisch ab. Man sei daran interessiert, eine Branchenlösung zu finden, hieß es als Begründung. Doch auch über die Höhe der Vergütung gibt es Streit. In der Vergangenheit war von einer Abgabe in Höhe von einem Cent pro Klick die Rede gewesen, Google gab sich entsetzt und lehnte ab. 
Patrick Walker, YouTube-Manager bei Google, ließ nun erklären, man sei von der Gema-Entscheidung enttäuscht. Je populärer Musikvideos seien, desto mehr Geld könne YouTube erwirtschaften, um es mit Verwertungsgesellschaften zu teilen. Man könne jedoch nicht erwarten, dass sich YouTube in ein Geschäft begibt, bei dem es jedes Mal, wenn ein Musikvideo abgerufen wird, Geld verliert.  ""Das ist einfach nicht nachhaltig "", so Walker. 
Wie man sich den Fortgang der Verhandlung denkt, sagte die Gema nicht. Offensichtlich wartet man nun auf eine Reaktion von Google."	technik
"In Lüneburg ist es üblich zu klingeln, wenn man einen vorausfahrenden Radfahrer überholen möchte. Meistens weicht der zu Überholende nach rechts aus, wenn der Radweg zu schmal ist, nutzt er dazu den Gehweg. Reagiert der Vorausfahrende nicht, wird auch rechts überholt. Neulich hat ein Bekannter genau das gemacht und wurde von dem Vorausfahrenden, einem Polizisten in Zivil, darauf hingewiesen, dass es ein Überholverbot gibt, wenn der Radweg zu schmal ist. Aber das bezweifle ich. Rouven Meier fragt: Wann dürfen Radfahrer auf Radwegen andere Radfahrer überholen und wann nicht? 
Grundsätzlich variiert die Geschwindigkeit von Radfahrern viel stärker als die von Autofahrern: Rennradfahrer nutzen genauso den Radweg wie Freizeitradler. Wer schneller unterwegs ist, kann daraus aber kein Recht zum Überholen ableiten, das unter allen Umständen gilt. Die Straßenverkehrsordnung (StVO) lässt auch für Radfahrer keine kreativen Überholmanöver zu.  ""Es ist links zu überholen "", heißt es in Paragraf 5, Absatz 1. 
 ""Das gilt auch für Radfahrer "", sagt Roland Huhn, Rechtsreferent beim Bundesverband des Allgemeinen Deutschen Fahrrad-Clubs (ADFC).  ""Und weil der Gehweg für sie tabu ist, dürfen sie auch nicht zum Überholen dorthin ausweichen. "" 
In dem geschilderten Beispiel muss der angeklingelte Radfahrer nicht einmal besonders stur gewesen sein, sagt Huhn.  ""Vielleicht war der Radweg zum sicheren Überholen tatsächlich zu schmal. Dann braucht der Vorausfahrende nicht etwa seinerseits auf den Gehweg auszuweichen, um dem schnelleren Hintermann Platz zu machen "", stellt er klar. 
Der Bundesgerichtshof hielt einen 1,40 Meter breiten Radweg in einem Urteil über einen Überholunfall für  ""verhältnismäßig schmal "". (Az.: VI ZR 131/84). Das Oberlandesgericht Frankfurt erklärte einen 1,70 Meter breiten Radweg zum Überholen für ausreichend bemessen, sofern der Vorausfahrende ein Klingelzeichen wahrgenommen hat (Az.: 17 U 129/88).  ""Einen so großen Seitenabstand wie Autofahrer müssen Radfahrer untereinander nicht einhalten, aber wegen der nie völlig schwankungsfreien Fahrlinie und der Gefahr, dass die Lenker aneinanderstoßen, darf er auch nicht ganz fehlen "", sagt Huhn. 
Ist das Fahrrad gut gewartet und macht somit bei der Annäherung keine Geräusche, kann Klingeln zum Ankündigen des Überholvorgangs hilfreich sein. Erlaubt ist es innerorts streng genommen nicht: Nur  ""außerhalb geschlossener Ortschaften darf das Überholen durch kurze Schall- oder Leuchtzeichen angekündigt werden "", vermerkt Paragraf 5, Absatz 5 der StVO. 
Ob überfahrene rote Ampeln, Unfälle oder Streit beim Gebrauchtwagenkauf: Rund um den Straßenverkehr gibt es viele knifflige Rechtsfragen. Eine davon beantworten Fachanwälte für Verkehrsrecht jede Woche donnerstags hier in unserer Serie  ""Gesetz der Straße "". 
Schreiben Sie uns Ihre Frage (und geben Sie dabei bitte Ihren Namen und Ihren Wohnort an). Wir wählen jede Woche eine Frage aus und beantworten sie hier. 
Bitte beachten Sie: ZEIT ONLINE, die Autorin und die beteiligten Fachanwälte übernehmen keinerlei Gewähr für die Aktualität, Korrektheit, Vollständigkeit oder Qualität der bereitgestellten Antworten und Informationen sowie der Rechtsprechung. Haftungsansprüche gegen ZEIT ONLINE und den Autor, welche sich auf Schäden materieller oder ideeller Art beziehen, die durch die Nutzung oder Nichtnutzung der dargebotenen Informationen bzw. durch die Nutzung fehlerhafter und unvollständiger Informationen verursacht wurden, sind grundsätzlich ausgeschlossen. 
Andererseits kann aus dem Gebot der Rücksichtnahme (Paragraf 1 StVO) sogar eine Pflicht erwachsen, auf schmalen Radwegen oder gegenüber unaufmerksamen Radlern vor dem Überholen zu warnen.  ""Die Lärmbelästigung, die durch das innerörtliche Hupverbot vermieden werden soll, ist beim kurzen Klingeln denkbar gering, sodass der Sicherheitsgewinn vorgeht "", sagt Huhn. 
Entscheidend ist, die Sicherheit der anderen Radfahrer nicht zu gefährden.  ""Wer schneller radelt als andere, muss unter Umständen warten, bis er problemlos überholen kann: an breiteren Stellen – etwa Radwegfurten im Verlauf des Radwegs – oder, wenn der Radweg nicht benutzungspflichtig ist, durch Wechsel auf die Fahrbahn "", sagt Huhn."	technik
"Das Urteil, das William H. Pauley III. gerade an einem New Yorker Gericht gefällt hat , dürfte Google und anderen Konzernen gefallen. Hatte der Richter doch befunden, dass der Cloud-Musikdienst MP3tunes nicht ohne Weiteres haftbar gemacht werden kann, wenn seine Nutzer illegal kopierte Inhalte hochladen. Er berief sich dabei auf das amerikanische Urheberrecht, genauer auf ein Gesetz namens Digital Millennium Copyright Act . 
Das Gesetz sieht Straffreiheit vor, wenn ein Betreiber im Zweifel dem Wunsch des Urhebers nachkommt, bestimmte Inhalte zu löschen. Das hatte auch schon YouTube in einem Rechtsstreit geholfen . Es genügt, dass das zu Google gehörende Unternehmen Filme von seiner Plattform löscht, wenn sich ein Urheber beschwert. Jeden Film auf seine Rechtmäßigkeit prüfen muss YouTube nicht. 
Ähnlich wurde nun auch bei MP3tunes geurteilt. Ein langjähriges Verfahren hat damit nun ein vorläufiges Ende gefunden: Die Plattenfirma EMI hatte vor fast vier Jahren und gemeinsam mit anderen Labels die Firma MP3tunes LLC und deren Gründer Michael Robertson verklagt. 
MP3tunes besteht eigentlich aus zwei Teilen: Es bietet seinen Nutzern erstens Online-Schließfächer für MP3-Dateien an, die dann von jedem internetfähigen Gerät aus abgespielt werden können. Mit der dazugehörigen Musiksuchmaschine sideload.com können Nutzer MP3s in Blogs und Tauschbörsen finden und von dort direkt in ihr Schließfach laden. Sideload erkennt, wenn ein Song hochgeladen wird, den es innerhalb des MP3tunes-Angebotes noch nicht gibt und generiert zweitens für seinen Suchindex einen Link auf das ursprüngliche Angebot – egal, ob die Datei dort eine illegale Kopie ist oder nicht. 
EMI sah darin eine Verletzung seiner Urheberrechte. MP3tunes leiste dem illegalen Download Vorschub, weil es die ungenehmigte Verbreitung der Inhalte fördere, argumentierte das Unternehmen. Richter Pauley vom District Court in Manhattan sah das anders. Die wichtigsten Punkte aus seinem 29 Seiten umfassenden Urteil: 
Erstens ist MP3tunes nicht für illegal erworbene Inhalte verantwortlich, die seine Nutzer hochladen. Auch Links auf solche Dateien bei Sideload darf die Firma zunächst einmal anbieten. Denn die Firma stehe unter dem Schutz des Digital Millennium Copyright Act (DMCA) . 
Zweitens: Der Schwarze Peter bleibt den Plattenfirmen. Die müssen die exakten Links auf die gelisteten Kopien suchen, an MP3tunes melden und eine Löschung verlangen. Erst dann ist MP3tunes verpflichtet, sowohl die Links als auch die betreffenden Dateien selbst aus den Ordnern der Nutzer zu entfernen.  ""Zwar kann ein vernunftbegabter Mensch nach einigen Untersuchungen durchaus darauf schließen, dass die Seiten von MP3tunes nicht autorisiert waren, um EMIs urheberrechtlich geschützte Inhalte zu verbreiten, der DMCA schiebt die Bürde der Untersuchung aber trotzdem nicht den Internet-Dienstanbietern zu "", heißt es im Urteil. 
Dieser Punkt ist auch für Google wichtig, dessen Cloud-Dienst Music Beta startete, ohne dass Google Lizenzvereinbarungen mit den Plattenfirmen getroffen hatte. Ähnlich wie bei YouTube kann Google nun davon ausgehen, Dateien erst dann aus seinem Angebot und aus seinem Suchmaschinenindex entfernen zu müssen, wenn ein Rechteinhaber sie entdeckt und die Löschung verlangt. 
 
Drittens brauchen Anbieter wie MP3tunes, Google oder auch Amazon mit seinem Dienst Cloud Drive keine Lizenz, um Inhalte aus den Ordnern der Nutzer streamen zu dürfen. Immerhin sei der Stream keine öffentliche Aufführung. Auch in diesem Punkt ist die Musikindustrie anderer Meinung und der Verlierer des Rechtsstreits. 
Ein Sieg auf der ganzen Linie bedeutet das Urteil für Robertson dennoch nicht. Denn MP3tunes versäumte es, nach Löschaufforderungen von EMI für mehrere Hundert Dateien diese auch aus den Ordnern der Nutzer zu entfernen. Stattdessen wurden nur die Sideload-Links gelöscht. Für diese Dateien kann EMI nun Schadenersatz verlangen. Bis zu 150.000 Dollar kann das kosten – pro Datei. Auch Robertson persönlich wird verantwortlich gemacht für das Hochladen illegaler Kopien in sein persönliches Schließfach bei MP3tunes. Dennoch äußerte er sich zufrieden :  ""Das ist definitiv ein Sieg für Cloud-Musikdienste und unser Geschäftsmodell. "" 
EMI dagegen zeigte sich  ""enttäuscht von der Entscheidung des Gerichts, MP3tunes den Schutz des DMCA zuzusprechen. Wir glauben, dass Unternehmen wie MP3tunes, die wissend ein Geschäftsmodell auf gestohlener Musik aufbauen, nicht von diesem Gesetz geschützt werden sollten. Wir prüfen, ob wir gegen diesen Teil der Entscheidung Rechtsmittel einlegen. ""
""Es hätte ein großer Tag für die Netzpolitik werden können. Europäische Richtlinien hatten sich geändert, weshalb der deutsche Gesetzgeber aufgefordert war, das Telekommunikationsgesetz (TKG) zu ändern. Das hat er am heutigen Donnerstag getan. Es enthält nun einige Verbesserungen. So dürfen Telefon-Warteschleifen (nach einer Übergangsfrist von einem Jahr) nichts mehr kosten, bei Call-by-Call-Angeboten muss der Preis angesagt werden, Werbeanrufer dürfen ihre Nummer nicht mehr unterdrücken. 
All das ist gut und schön. Doch die wahren Probleme hat die Bundesregierung so nicht bekämpft. Sie hat die Novelle nicht eingesetzt, um Netzneutralität zu verankern und sie hat auch den Breitbandausbau nicht zur Pflicht gemacht. So hat die Regierung verpasst, die beiden dringendsten Fragen in der Regulierung der Telekommunikation endlich zu klären. 
Mit verschiedenen Anträgen hatten die Fraktionen der Linken, der Grünen und der SPD versucht, das Gebot der Netzneutralität in dem Gesetzentwurf zu verankern. Sie wollten erreichen, dass alle Teilnehmer das Netz hierzulande gleichberechtigt und diskriminierungsfrei nutzen können und Dienstanbieter nicht durch technische Überwachung der Inhalte entscheiden dürfen, welches Angebot schneller durchgeleitet wird und welches langsamer. Alle Anträge wurden jedoch von der Regierungsmehrheit abgelehnt. 
Zum zweiten Mal – nach der vergeblichen Debatte in der Internet-Enquete-Kommission – ist der politische Versuch gescheitert, das Internet als lebenswichtige Infrastruktur und Basis unseres Zusammenlebens so vielen Einflüssen wie möglich zu öffnen. Das spielt all jenen in die Hände, denen es allein darum geht, mit dem Netz und durch das Netz Geld zu verdienen und Macht auszuüben. 
Das ist schade. Denn es ist durchaus vorstellbar, dass wir uns in ein paar Jahren wünschen, wir hätten dieses Konstrukt des freien Internets irgendwo verankert, als das noch problemlos möglich war. 
Wie ein Mantra wiederholen die Gegner des Konzeptes, es brauche keine Gesetze und keine Regulierung, der Markt werde alles zur Zufriedenheit aller regeln. Das ist eine schöne Hoffnung, doch sie trügt. Noch nie hat ein unregulierter Markt von sich aus dafür gesorgt, dass alle sich beteiligen können. Immer ergibt sich ein Ungleichgewicht. Wenige besitzen alles, viele nichts. 
Na und, lautet die Antwort, dann regulieren wir eben später. Das kann man tun. Aber warum warten, bis erst Millionen ausgebeutet, abgehängt und benachteiligt sind? Wenn es doch jetzt schon verhindert werden könnte? Wäre es angesichts einer so wichtigen Infrastruktur nicht geraten, sie von vornherein für alle zugänglich zu halten? 
 
Netzneutralität ist eigentlich ein simples Konzept. Es ist nur ein anderer Name für längst verinnerlichte Begriffe wie Meinungsfreiheit, Gleichberechtigung, Chancengleichheit. Sie bleiben nun im Netz weiterhin nicht sichergestellt. Die Abstimmung zum TKG ist in dieser Hinsicht, wie es bei der Digitalen Gesellschaft heißt , eine  ""verpasste Chance "". 
Noch eine Gelegenheit, so viele Menschen wie möglich am Netz zu beteiligen, wurde nicht genutzt. Ursprünglich wollte die Union erreichen, dass Telefonfirmen verpflichtet werden, Breitbandnetze auf dem Land auszubauen. Nun aber hat sich offenbar die FDP mit ihrer Haltung durchgesetzt: Auch das regele der Markt schon irgendwie. 
Was ebenso bezweifelt werden darf. Nicht umsonst hatte sich die Bundesnetz-Agentur entschieden, Mobilfunkfrequenzen für Breitbanddienste nur dann zu verkaufen, wenn der Käufer sich verpflichtet, eben jene neuen und teuren Netze erst auf dem Land zu installieren und dann erst in den lukrativen Ballungsräumen. Dass es für das Festnetz eine solche Verpflichtung nun nicht gibt, bedeutet, dass Menschen in dünn besiedelten Regionen jahrelang warten müssen, um vom Internet profitieren zu können. 
Die Union feiert die Entscheidung trotzdem als Erfolg. Man sei, sagte der Vorsitzende der Arbeitsgruppe Wirtschaft und Technologie der Unionsfraktion, Joachim Pfeiffer, auf dem besten Weg, das gesteckte Ziel zu erreichen. Spätestens bis 2018 könne man flächendeckend Zugänge mit 50 Megabit pro Sekunde anbieten. Bis dahin sind es allerdings noch sieben Jahre. Im Netz eine lange Zeit. Was bedeutet, dass diese Anschlüsse, wenn sie endlich in der Erde liegen, wahrscheinlich längst nicht mehr den aktuellen Anforderungen genügen. Mit einem Gesetz hätte dieses Ziel schneller erreicht werden können. 
Trivial ist das nicht, denn inzwischen ist die schlechte Internetversorgung einer der Gründe, warum Menschen ohnehin schon dünn besiedelte Regionen verlassen. Der Wirtschaft wird damit eine Zumutung erspart – den Kunden wird sie dafür aufgebürdet. 
Oder, wie es der grüne Netzpolitiker Konstantin von Notz in der parlamentarischen Aussprache mit Bezug auf seinen Unionskollegen ironisch sagte:  ""Ein zwitschernder Peter Altmaier macht eben noch keinen netzpolitischen Frühling. ""
""Das Telefon klingelt. Ein junger Mann sitzt neben dem Apparat, ein Bügel seiner schwarzen Brille ist notdürftig mit Klebeband repariert. Er hat Dienst und ist allein dort, im dritten Stock eines unscheinbaren Zweckbaus an der Godesberger Allee in der Bonner Innenstadt. Am Kopfende des Raumes winden sich Graphen, türmen sich Cluster und klettern Balkendiagramme an Skalen über sechs große Flachbildschirme. Jetzt, an einem Donnerstagmittag, rauscht der Cyberspace friedlich vor sich hin, und der Mann mit der kaputten Brille ist gerade dabei, eine Beschwerde wegen eines vermeintlich fehlerhaften Sicherheitszertifikats zu bearbeiten. Nun unterbricht er das Tippen und hebt den Hörer ab. Es gebe da ein Problem, sagt der Anrufer. 
Seit einem Jahr hat Deutschland ein eigenes Cyberabwehrzentrum. Es wird geleitet vom Bundesamt für Sicherheit in der Informationstechnik, kurz BSI. Als Innenminister Hans-Peter Friedrich das Nationale Cyberabwehrzentrum eröffnete, versprach er sich viel davon:  ""Die Infrastruktur wird zunehmend von international organisierten Angreifern attackiert "", sagte er. Mit Infrastruktur meinte er Industrieanlagen, die Strom- und Wasserversorgung, den öffentlichen Nahverkehr, und mit Attacken Angriffe aus dem Cyberspace. Was diese anrichten könnten, nannte er  ""nachhaltig wirkende Versorgungsengpässe "". Nun sollte Deutschlands Sicherheit auch in Bonn verteidigt werden. 
Der junge Mann mit der notdürftig reparierten Brille hat seinen Dienst um acht Uhr 30 angetreten. Seinen Namen möchte er lieber für sich behalten, denn seine Welt soll unzugänglich bleiben. Am Morgen, als er das Pförtnerhäuschen passierte, da ging er auch an jenem sorgsam laminierten Schild vorüber, das das  ""Mitführen von Informationstechnik "" untersagte. Auf dem Flur überwachten Kameras jeden Schritt. Die Metalltür zum  ""Lagezentrum "" öffnete sich mit einem Sirren, als er seinen elektronischen Hausausweis vor eine Metallplatte hielt. Seinen Rucksack stopft er unter den Schreibtisch. Wachdienste sind für jeden Mitarbeiter der Abteilung Operative Netzabwehr Pflicht, eine eher lästige, gibt der junge Brillenmann zu:  ""Lieber analysieren wir einzelne Gefahren in die Tiefe "", sagte er. Er hoffte, dass es ein ruhiger Tag werden würde. 
Und zunächst war er das auch. Was hier an Daten aufbereitet wird, sammelt ein Netz von Sensoren, das das BSI und die mit ihm verbündeten Institute und Firmen im Cyberspace ausgeworfen haben. Es ist ein maschinelles Immunsystem, das Gefahren aufspürt und von IT-Experten ständig an neue Virentypen angepasst wird. 
Gemessen wird in Viren pro Minute 
Längst ist die Zahl der Schadprogramme, die je nach Verbreitungsart und Zerstörungskraft Wurm, Virus oder Trojaner heißen, so groß, dass nur noch ihresgleichen mit ihnen fertig werden: Analyseprogramme, die genau wie die digitalen Erreger aus Codes bestehen. Erst, wenn ein Analyseprogramm ein besonders interessantes Exemplar findet, eine neue oder besonders schädliche Art, landet das Biest auf dem Seziertisch eines Analysten wie dem Informatiker mit der Brille. Das hatte er gemeint mit: Gefahren  ""in die Tiefe "" analysieren. 
Aber gerade jetzt war noch nichts dergleichen zu finden. Der junge Mann richtete seine Aufmerksamkeit auf eine rote Kurve. Sie zeigt den Puls des Netzes an. Er wird in  ""Messages per minute "" (Mpm) gemessen und beziffert die Zahl der virenverseuchten Nachrichten pro Minute, die an einer Art E-Mail-Falle im Netz haften bleiben. In dem Moment mäanderte der Wert bei 0,2 Mpm, deutlich unterhalb des Mittelwerts für diese Tageszeit.  ""No alerts found "", beruhigte ein grüner Balken. Dann flackerte eine Tabelle gelb auf. Vorwarnstufe. 
Der Brillenmann hob den Blick. Die Webseite des Bundesjustizministeriums war seit sechs Minuten und sechs Sekunden nicht mehr zu erreichen, meldete einer seiner digitalen Helfer, die nichts anderes tun, als eine Liste von Webseiten zu kontaktieren, um zu überprüfen, ob sie noch da sind. Wurde das Bundesjustizministerium mit einer  ""DDoS-Kanone "" beschossen, der üblichen Angriffswaffe der Internetaktivisten Anonymous? Sie pflegen ihre Gegner mit so vielen automatisierten Anfragen zu bombardieren, dass deren Internetpräsenz zusammenbricht. Aber der junge Mann winkte ab. Beim Bundesjustizministerium ruckelte es schon seit Tagen. Doch dann, endlich, klingelt das Telefon, und er bekommt etwas auf den Tisch. 
 
Woher der Anrufer mit dem Problem sich an ihn wendet, darf der junge Mann nicht verraten, aber es ist eine Bundesbehörde. Auf deren Webseite hat sich ein Erreger eingenistet. Und er hat sich gut getarnt. Auf die Schliche gekommen ist ihm am Vormittag ein aufmerksames kleines Wachprogramm. Es hatte bemerkt, dass mehr als nur der Inhalt der Seite verändert worden war. 
Inhalte verändert die Behörde, der die Webseite gehört, täglich. Das  ""Template "" aber, also der Rahmen, der Farben, Schriftarten und Logos definiert, wird selten verändert. Der Aufpasser-Automat schickte deshalb eine Warnmeldung. Und so kamen die Menschen ins Spiel. Nummern wurden gewählt, Telefonhörer abgenommen. Ob es Veränderungen am Layout gegeben habe? Nein, sagte der Administrator, da müsse ein Fehler vorliegen. Er überprüfte die Identifikationsadressen der Computer, die das  ""Backend "" der Webseite geöffnet hatten, also den Lieferanten- und Serviceeingang, den für gewöhnlich nur Techniker betreten. Nein, irgendetwas stimmte hier ganz und gar nicht. Hier hatten sich Unbefugte Zutritt verschafft. 
 ""Cyberkrieg "" ist dafür das Schlagwort. Sabotageviren wie Stuxnet oder High-Tech-Wanzen wie Flame prägen das Bild eines virtuellen Schlachtfeldes. Aber für Dirk Häger ist es das Wort, das ihn richtig nerven kann. Er ist der Chef des Brillenmannes und einer von Deutschlands obersten Cyberabwehrern. Der Mittvierziger mit der schlaksigen Statur eines Basketballspielers hat in Physik promoviert. Jetzt, da er an einem Konferenztisch sitzen geblieben ist, an dem eben die  ""Mittagslage "" zuende ging, ordnet er die langen Beine unter dem Tisch neu und fällt bald in einen für seinen Berufsstand üblichen maschinenschnellen Redefluss. Viren, Würmer, das ist seine Welt. 
 ""Es geht vor allem um Austausch von Informationen "" 
Doch das Wort  ""Cyberkrieg "" lässt Häger stocken. Er atmet tief ein und lehnt sich weit zurück.  ""Mit dem Namen Cyberabwehrzentrum ist eine Erwartungshaltung verbunden, die mit der Realität nicht übereinstimmt "", sagt er.  ""Es geht dort vor allem um den Austausch von Informationen. "" 
Das Cyberabwehrzentrum ist vor allem eine Videokonferenz. Jeden Morgen um neun spricht Häger über eine sichere Verbindung mit den Kollegen aus anderen Behörden, der Bundeswehr, den Geheimdiensten und der Polizei. Selten treffen sich die Mitglieder dieses Zentrums persönlich. Nur wenn das Problem als  ""VS "", Verschlusssache, eingestuft wird, fährt Häger zu einem abhörsicheren Raum in Bonn-Mehlem, um über das Vorgehen mit den Anderen direkt zu beraten. Das sei eigentlich schon alles, sagt Häger. 
Er ist kein Krieger, und er will auch keiner sein. Viren und Würmer interessieren ihn nicht als Waffen. Für den Beamten in ihm sind sie Aufgaben, die erledigt werden müssen, damit Deutschland sicher ist. Für den Wissenschaftler, der er aber auch ist, stellen sie Gefahren dar, für die er sich um ein Gegenmittel bemüht und die ihm manchmal sogar Anerkennung abringen. Wobei er oft nicht wissen kann, ob sich hinter den raffinierten Code-Konstruktionen ausländische Dienste verbergen oder Kriminelle. 
Im Kontrollraum nebenan hat der junge Mann mit der geflickten Brille inzwischen eine Diagnose fertig: Die Seite ist von einem  ""Drive-by-Exploit "" befallen.  ""Drive-by-Exploits "" waren im vergangenen Jahr eines der am schnellsten zunehmenden Probleme der IT-Sicherheit. Das Computernotfallteam der Bundesverwaltung registrierte pro Woche etwa 20 neue Fälle, bei denen deutsche Webseiten befallen wurden. 
Die Angreifer verstecken dabei einen Schadcode direkt im Code einer fremden Webseite. Das Perfide daran: Der vorbeisurfende PC-Benutzer muss nichts herunterladen, ja, nicht einmal etwas anklicken. Er infiziert sich quasi durch die Berührung mit der Webseite – ohne es zu merken. 
Die Epidemie hatte in den Schmuddelecken des Internets begonnen, auf Sexseiten. Nach und nach breitete sich die Gefahr auch in den bürgerlichen Vororten des Web aus: Unter den befallenen Seiten war 2011 das  ""Handelsblatt "" und jetzt, im Juni 2012, jene Behördenseite, deren Name geheim bleiben soll. 
Hinter solchen Wegelagerer-Programmen stecken natürlich Menschen. Aber selten führt zu denen eine Spur. Der junge Mann mit der kaputten Brille und ein Kollege suchen nach dieser Spur, sie zerlegen den Code, um eine Fährte zu finden. Aber allzu viel Hoffnung machen sie sich nicht. Immerhin, eine URL können sie isolieren, eine Adresse im Internet. Dort könnte der Virus liegen, dorthin wird der arglose Nutzer umgeleitet. 
Als die beiden dieser Spur folgen, stoßen sie auf eine bereits abgeschaltete Seite. Sie stehen vor einer Baulücke. Sie werden in diesem Fall nicht einmal herausfinden, was die Täter eigentlich vorhatten. Es gebe da viele Möglichkeiten, sagt der Brillenmann, das sei abhängig von ihrem Geschäftsmodell.  ""Vielleicht wollten die Hacker Bankdaten ausspähen oder den Computer als Abschussrampe für DDoS-Angriffe übernehmen. Oder sie wollten einen billigen, massenhaften Spamversand starten. "" 
Der Mann mit der kaputten Brille ruft den Administrator der Behörde zurück und rät ihm, die Seite kurz vom Netz zu nehmen und vorübergehend durch eine ältere Version zu ersetzen. 
Die Recherche ist mühsam 
In einigen Tagen, wenn die Seite repariert ist, wird das BSI einen  ""Penetrationstest "" machen. Das heißt, sie werden selbst versuchen, in die Seite einzubrechen, um zu sehen, ob die Tür wieder nachgibt. 
Eine Fährte werden sie weiterverfolgen. Um den Virus in Aktion zu setzen, kaufen die Cyber-Gangster von anderen Kriminellen die Zugangsdaten zu den Hintereingängen von Webseiten. Anders können auch sie nicht zu den sensiblen Bereichen vordringen, um einen Link auf ihr Schadprogramm einzuschleusen. Und die Frage ist: An welcher Stelle sind die Zugangsdaten für die Behörden-Webseite verloren gegangen? Ist es ein Trojaner auf einem Computer in der Webdesign-Firma, die die Behördenseite betreut? Oder, schlimmer, ist der Schlüssel direkt in der Behörde abgefischt worden? 
Bei wichtigen Zielen greifen Cyberkriminelle auch auf etwas zurück, was die IT-Experten freundlich  ""social engineering "" nennen. Es ist die älteste Spionagetechnik der Welt. Das Umfeld eines Behördenmitarbeiters ausforschen, ihn anrufen, sich als Kollegen oder Techniker ausgeben und versuchen, ihm seine Zugangsdaten zu entlocken. 
Die Recherche wird noch einige Zeit in Anspruch nehmen. Der Mann, der seine Brille auch an diesem Tag nicht zur Reparatur gebracht hat, seufzt:  ""Wenn jemand sich die Zugriffsdaten klauen lässt, können wir auch nichts machen. "" 
Erschienen im Tagesspiegel"	technik
"Smartphone-Hersteller wie Samsung haben es heutzutage nicht leicht, wenn es darum geht, den richtigen Termin für Produktvorstellungen zu finden. In den vergangenen Jahren präsentierte Samsung seine Phablet-Reihe Galaxy Note stets zur Internationalen Funkausstellung Ifa im September. Dieses Jahr hat der koreanische Konzern den Termin bereits in den August vorgezogen – offenbar, um sich etwas von der Präsentation des neuen iPhone abzusetzen, das vorausichtlich Mitte September vorgestellt wird. 
Die Abgrenzung zum großen Rivalen Apple bleibt aber das kleinere Problem von Samsung. Die jüngste Produktvorstellung machte nämlich ein ganz anderes deutlich: Samsung droht zum Opfer der eigenen Philosophie zu werden und sein ehemals so erfolgreiches Portfolio mit zwei Oberklasse-Modellen zu verwässern, die fast gleich sind und doch unterschiedlich sein sollen. 
Die Rede ist vom Galaxy Note 5 und dem Galaxy S6 Edge+. Beide Geräte präsentierte Samsung am Donnerstagabend bei einem erstaunlich kurzen Event in New York. 
Mit dem Galaxy S6 Edge+ erweitert Samsung die im März vorgestellte Galaxy-S6-Reihe um ein drittes und größeres Gerät. Das S6 Edge+ verfügt über einen 5,7 Zoll großen Bildschirm mit einer Auflösung von 2.560 mal 1.440 Pixeln, der an beiden Seiten über den Geräterand gebogen ist. Wie beim kleineren S6 Edge können diese seitlichen Displays unabhängig vom Hauptbildschirm funktionieren, etwa für Benachrichtigungen, haben gemeinhin aber wenig Mehrwert. Ansonsten unterscheidet sich das Gerät äußerlich nicht von seinem kleineren Bruder. 
Das Galaxy Note 5 dagegen ist das neuste Modell in Samsungs Phablet-Reihe, die schon immer auf besonders große Displays setzte und damit in den vergangenen Jahren vor allem unter Geschäftsleuten beliebt war. Das Note 5 führt diese Tradition fort. Es besitzt wie seine Vorgänger ebenfalls ein 5,7 Zoll großes Display, allerdings ist das Gerät noch einmal leichter und dünner. Auf die Bedienung mit dem für die Note-Reihe typischen Stift, dem S-Pen, müssen die Nutzer deshalb nicht verzichten. Er steckt weiterhin unten im Gehäuse. 
Die Rückseite des Geräts ist an den Rändern leicht gebogen und soll somit besser in der Hand liegen. Ein gebogenes Display aber gibt es nicht mehr; vergangenes Jahr noch hatte das Note 4 Edge das Zeitalter der gebogenen Displays bei Samsung eingeläutet. Jetzt ist diese Technik offenbar der S-Reihe vorbehalten, was einige Fans des Note enttäuschen dürfte. 
Im Gegenzug hat sich aber auch das Note 5 einiges vom Galaxy S6 abgeschaut. Die Rückseite ist nun ebenfalls aus Glas, Kamera und LED-Licht sind an der gleichen Stelle angesiedelt und beide Modelle verwenden einen ähnlichen Unibody-Metallrahmen. Das bedeutet, dass sich im Note 5 anders als im Vorgänger weder Akku noch Speicherkarte wechseln und erweitern lassen, denn die Rückseite ist nicht mehr abnehmbar. Liegen beide Modelle nebeneinander, sind sie auf den ersten Blick mit Ausnahme des gebogenen Displays kaum voneinander zu unterscheiden. 
Auch im Inneren sind sich das Note 5 und das S6 Edge+ ähnlich bis identisch. Beide verwenden den Achtkernprozessor des S6, haben aber mit vier Gigabyte jeweils einen Gigabyte mehr Speicher. Die Kamera ist in beiden Modellen die bekannte 16-Pixel-Kamera des S6, die aber über neue Softwarefunktionen einen zusätzlichen Stabilisator und eine Live-Funktion besitzt, über die Nutzer direkt auf Dienste wie YouTube streamen können. 
Ansonsten enthalten beide Modelle, was Kunden von einem Oberklasse-Smartphone erwarten dürfen. Die Akkus unterstützen Samsungs Schnellladetechnik sowie kabelloses Aufladen, NFC-Chips sollen mit dem ebenfalls vorgestellten neuen Bezahlservice Samsung Pay funktionieren und im Fall des Note 5 gibt es wieder einige besondere Softwarefunktionen, die gezielt auf die Bedienung per Stift ausgelegt sind. Zum Verkaufsstart werden beide Modelle mit Android 5.1 ausgeliefert, das natürlich von Samsung angepasst wurde. 
Zusammengefasst: Mit dem Galaxy Note 5 und dem Galaxy S6 Edge+ hat Samsung nun zwei technisch hochwertige und optisch ansprechende Phablets im Angebot. Doch genau das könnte dem Konzern zum Verhängnis werden. 
 
Bislang galt die Note-Reihe als heimlicher Star unter Samsungs Smartphones. Wie Samsungs CEO JK Shin zu Beginn der Präsentation sagte, habe das Unternehmen einst den Trend zum größeren Smartphonebildschirm losgetreten. Und tatsächlich war es die Note-Reihe, die noch vor einigen Jahren für verwirrte Blicke sorgte. Als  ""Frühstücksbrettchen "" wurde das für damalige Verhältnisse riesige Gerät verspottet und die Abgrenzung zu Tablets infrage gestellt. 
Die Verkaufszahlen und positiven Testberichte aber gaben Samsung Recht und schnell zogen andere Hersteller mit Displaygrößen von bis zu sechs Zoll nach. Spätestens als Apple vergangenes Jahr mit dem iPhone 6 Plus erstmals ein zweites, größeres iPhone präsentierte, war klar, dass Samsung der Konkurrenz in dieser Hinsicht einen Schritt voraus war. Und als das Note 4 erstmals ein gebogenes Display bekam, schien es sich sogar zur ersten Adresse für Innovationen zu mausern. 
Das scheint nun nicht mehr der Fall zu sein. Mehr noch, die Philosophie der großen Displays hat Samsung in ein Dilemma gebracht. Denn während Apples iPhone 6 auf dem Weg zum erfolgreichsten Modell des Unternehmens ist, blieb das Galaxy S6 hinter den Erwartungen zurück. Das schwächelnde Geschäft war auch ein Grund dafür, wieso Samsung die jüngsten Gewinnerwartungen erneut verfehlte. In China sorgen indes Hersteller wie Huawei und Xiaomi für zusätzliche Konkurrenz. 
Mit dem größeren S6 Edge+ möchte Samsung deshalb offenbar dem iPhone 6 Plus einen zusätzlichen direkten Konkurrenten vorsetzen und sieht das Note 5 dafür alleine nicht gewappnet. Stattdessen soll es das bekannte Zugpferd Galaxy S6 richten. Es ist insofern ein riskanter Plan, als dass sich der Konzern damit auch neue Konkurrenz in den eigenen Phablet-Reihen ins Haus holt. Das größte Alleinstellungsmerkmal der Note-Serie bleibt zwar weiterhin der Stylus-Pen, aber ob sich tatsächlich genug Käufer für zwei Oberklasse-Phablets finden, ist fraglich. 
Möglicherweise hat Samsung dieses Dilemma selbst erkannt und nimmt den Käufern deshalb die Entscheidung ab, jedenfalls in Europa. Denn während das Galaxy S6 Edge+ Anfang September für etwa 800 Euro nach Deutschland kommt, soll es das Galaxy Note 5 wohl vorerst nicht in Westeuropa geben. Das berichten unter anderem die Fachportale golem.de und Mobilegeeks unter Berufung auf Samsung-Vertreter."	technik
"Eine Hackerattacke von angeblichen Mitgliedern der Dschihadistenmiliz  ""Islamischer Staat "" (IS) hat den Sendebetrieb der französischen Fernsehsendergruppe TV5Monde zeitweise zum Erliegen gebracht. Elf Kanäle sind betroffen. Nach einem vollständigen Ausfall konnte der Sender am Morgen zunächst nur aufgezeichnete Sendungen ausstrahlen, sagte Generaldirektor Yves Begot dem Radiosender RTL. 
 ""Wir sind noch nicht in der Lage, die Ausstrahlung und Produktion unserer Nachrichten wieder aufzunehmen, aber wir haben die Kontrolle über unsere Facebook- und Twitterkonten zurückerlangt "", sagte Bigot. Nach der Attacke in der Nacht waren auf den Websites und Social-Media-Konten des Senders Forderungen der Terrormiliz  ""Islamischer Staat "" (IS) zu lesen gewesen. Der Angriff dauere an, sagte Bigot. Die Cyberattacke habe am Mittwochabend gegen 22.00 Uhr begonnen. TV5Monde ist eine französischsprachige Sendergruppe, die weltweit ausgestrahlt wird. 
Die Website der Sendergruppe war auch am Donnerstagvormittag nicht erreichbar. Es erschien lediglich eine Wartungsmeldung. Bigot sagte, er sei erschüttert gewesen, als er gesehen habe, dass alle elf Kanäle nur ein schwarzes Bild gezeigt hätten. Als die Bedeutung der Botschaft dann auf den eigenen Websited und Social-Media-Konten entdeckt worden sei, habe man verstanden, was geschehen sei. Die Botschaft auf der Website von TV5 Monde lautete:  ""Ich bin der IS "". Zu sehen war ein Banner der Gruppe, die sich selbst als Cyberkalifat bezeichnete. 
Bei Facebook waren zuvor Drohungen gegen französische Soldaten zu lesen.  ""Soldaten Frankreichs, bleibt dem Islamischen Staat fern! Ihr habt die Chance, eure Familien zu retten, nutzt sie! "", hieß es. Das  ""Cyber-Kalifat "" werde seinen  ""Cyber-Dschihad "" gegen die Feinde des IS fortsetzen. Frankreich nimmt als Teil einer internationalen Koalition am Militäreinsatz gegen die sunnitischen IS-Extremisten im Irak und in Syrien teil. 
Medien wie die New York Post, der Boston Globe und die britische Zeitung The Independent waren in den vergangenen Monaten häufiger Ziel von Hackerangriffen. Zu einigen der Attacken hatte sich allerdings die Gruppe Syrian Electronic Army bekannt, die der syrischen Regierung nahesteht. 
Bei Angriffen auf die Redaktion des Satiremagazins Charlie Hebdo, einen jüdischen Supermarkt und eine Polizistin waren im Januar in Frankreich 20 Menschen getötet worden, darunter die Terroristen. Einer der Attentäter hatte angegeben, er habe Instruktionen vom  ""Islamischen Staat "" erhalten."	technik
"Es ist eine Sache, ein paar Twitter-Konten zu hacken oder eine Website zu verunstalten. Es ist eine andere, aus der Ferne die Ausstrahlung von elf Kanälen eines Fernsehsenders für mehrere Stunden zu stören. Der jüngste Angriff des selbst ernannten Cyber-Kalifats auf den französischen Sender TV5Monde hat deshalb eine neue Qualität. Falls die Gruppe wirklich selbst dahintersteckt. 
In den Botschaften, die die Angreifer hinterlassen haben, ist zumindest von diesem  ""Cyber-Kalifat "" die Rede. Die Gruppe hat in der Vergangenheit mehrere Medienhäuser, aber auch das US-Militär angegriffen, dabei aber nur deren Websites und Social-Media-Präsenzen lahmgelegt und einige interne, aber nicht geheime Dokumente veröffentlicht. 
Um einen Fernsehsender zu sabotieren und seine Technik ernsthaft zu beschädigen, ist mehr nötig als ein paar geknackte Passwörter. Die Täter hatten sich auch Zugang zur Antennentechnik verschafft. Dieser Bereich sollte eigentlich vom Intranet getrennt sein, sagt ein deutscher Fernsehmacher im Gespräch mit ZEIT ONLINE. Von einem beliebigen Redakteursrechner aus sollte man nicht auf die Sendetechnik zugreifen können. Und selbst wenn das ginge: Für einen Sabotageakt wie diesen bräuchte es möglicherweise Schadsoftware, die speziell auf die Infrastruktur zugeschnitten ist. Der Generaldirektor von TV5Monde sagte, es müsse Wochen gedauert haben, um den Angriff vorzubereiten. Französische Experten versuchen derzeit, den Angriffsverlauf zu rekonstruieren. 
Ob das sogenannte Cyber-Kalifat solche Fähigkeiten hat, ist unklar – auch weil wenig über die Personen hinter der Gruppe bekannt ist. Sie hat wiederholt behauptet, im Auftrag der Terrormiliz IS zu handeln. Aber wahrscheinlicher ist, dass die Mitglieder der Gruppe nur Sympathisanten sind und nicht Teil der Miliz. Viel mehr ist über sie nicht bekannt. Nur ein einziger namentlich bekannter Hacker wird bisher mit dem IS in Verbindung gebracht: Der Brite Junaid Hussein wurde 2012 in Großbritannien verhaftet, weil er sich in Tony Blairs Adressbuch gehackt hatte. Er soll sich mittlerweile den Terroristen angeschlossen haben und nach Syrien gereist sein. 
Vielleicht war der Angriff aber auch eine Auftragsarbeit. In spezialisierten Foren bieten kriminelle Hacker solche Dienste schon seit Jahren an. Die Antiviren-Spezialisten von McAfee wiesen schon 2013 darauf hin, dass es  ""Cybercrime as a Service "" mittlerweile selbst für Laien gibt. Bestimmte Angriffe zu bestellen, ist demnach kaum schwieriger, als etwas bei Amazon zu kaufen. Europol bezeichnet solche Dienstleistungen als  ""Schlüsselelement der digitalen Untergrundwirtschaft "". 
Die politische Motivation für den Angriff ist offensichtlich das militärische Engagement der Franzosen im Kampf gegen die IS-Extremisten im Irak und in Syrien. Warum aber hat es ausgerechnet den Sender TV5Monde getroffen? Die einfachste Erklärung dürfte sein: Weil es ging. 
Vielleicht hat es doch nicht  ""Wochen "" gedauert, um den Angriff vorzubereiten, sondern nur ein paar Tage. Es wäre ohnehin verwunderlich, wenn eine Organisation so viel Energie (oder Geld) für einen reinen Propagandaerfolg verwenden würde. Und Medienhäuser sind ebenso gut oder schlecht auf derartige Attacken vorbereitet wie andere Unternehmen auch. Ein sorgloser Umgang von Mitarbeitern mit Passwörtern und mangelndes Training gegen Phishing-Versuche oder Social Engineering dürften eher die Regel als die Ausnahme sein. Einen ersten Weg in interne Netze einer Firma finden entschlossene Angreifer so relativ schnell. Wenn von dort auch die kritischen Infrastrukturen erreichbar sind, braucht es unter Umständen keine Schadsoftware der Stuxnet-Liga, um große Schäden anzurichten. 
Wenn das erst einmal passiert ist, wissen viele Unternehmen nicht, wie sie darauf reagieren sollen. In Deutschland hat laut dem Branchenverband Bitkom nur jedes zweite Unternehmen einen Notfallplan bei digitaler Wirtschaftsspionage, Sabotage oder Datendiebstahl. In Frankreich wird die Situation nicht viel besser sein. Was bedeutet: Es könnte auch noch andere treffen."	technik
"Langsam pirscht sich Takkar vom Stamm der Wenja an seinen Gegner heran. Sein Ziel ist ein Krieger der konkurrierenden Udam, einem gnadenlosen Kannibalen-Clan. Er hat sein Opfer fast erreicht, da ertönt hinter ihm das Kreischen eines Mammuts. Das schreckt den Udam auf, er dreht sich um und entdeckt Takkar. Nun muss alles ganz schnell gehen: Takkar rammt seinen Speer in den Leib des Gegenübers, Blut spritzt, der Feind geht zu Boden. Der Wenja schnappt sich die Beute des Getöteten und tritt die Flucht an, denn das Mammut hat bereits seine Verfolgung aufgenommen. 
Diese Szene stammt nicht aus Roland Emmerichs viel gescholtenem Abenteuerfilm 10.000 B.C. , sie steht exemplarisch für die martialische Action in Far Cry Primal, das ab sofort für PS4 und Xbox One erscheint und am 1. März für den PC veröffentlicht wird. Zuletzt konfrontierte Entwickler Ubisoft den Spieler in seiner Shooterserie Far Cry mit Bürgerkriegsszenarien. Im fünften Teil jedoch ist man in der Mittelsteinzeit unterwegs, mit Speer, Steinschleuder und Bogen. 
In der Rolle des Kriegers Takkargilt es, sich gemeinsam mit seinen Stammeskollegen in einem Tal namens Oros gegen zwei verfeindete Stämme zu behaupten – oder mit ihnen zu kooperieren. Zum einen gibt es die schon erwähnten Menschenfresser der Udam, zum anderen die Izila, die bereits Bewässerung und Landwirtschaft kennen und andere Menschen als Sklaven halten. 
Die Handlung dreht sich vor allem um die Kämpfe zwischen den Clans und den Aufbau des Wenja-Dorfes, hat aber auch einige mystische Komponenten und ist in zahlreiche aufwendige Zwischensequenzen eingebettet. Far Cry Primal beschäftigt Spieler 35 Stunden und mehr – je nachdem, ob man nur die Hauptmissionen absolviert oder sich einer der zahlreichen Nebentätigkeiten widmet. 
Schon nach der ersten Ankündigung zu Far Cry Primal stellten sich viele die Frage: Sind den Entwicklern die Ideen ausgegangen? Haben sie sich deshalb für die Steinzeit als Schauplatz entschieden? Schließlich feiern Spiele mit prähistorischem Hintergrund derzeit große Erfolge, allen voran die Dino-Überlebenssimulation Ark: Survival Evolved. Und auch das kommende PS4-Exklusivspiel Horizon: Zero Dawn setzt auf einen von der Steinzeit inspirierten Schauplatz, in dem allerdings Roboter-Saurier eine wichtige Rolle spielen werden. 
 ""Nein, diese Epoche hat sich schon vor der Fertigstellung von Far Cry 4 herauskristallisiert "", sagt Story-Designer Kevin Shortt von Ubisoft Montreal im Gespräch mit ZEIT ONLINE.  ""Die Far-Cry-Reihe drehte sich schon immer um ungezähmte, gesetzlose Szenarien. Die Steinzeit passt einfach perfekt in dieses Schema. "" 
 
Das möchte man dem Entwickler gerne glauben. Allerdings erinnert vieles in Far Cry Primal an die vorherigen Teile der Reihe. Es ist erneut ein Actionabenteuer aus der Ich-Perspektive mit einer frei erkundbaren Spielwelt. Der Spieler muss Flora und Fauna dabei stets zu seinem Vorteil nutzen: Auf der Jagd erbeutet man Fleisch und Tierhäute, zudem können allerlei Pflanzen, Hölzer und Steine gesammelt werden. Aus diesen Ressourcen fertigt man mithilfe eines einfachen Crafting-Systems Ledertaschen, Winterkleidung und neue Waffen. Obendrein baut man die Siedlung seines Stammes nach und nach immer weiter aus. 
 ""Das Crafting macht diesmal viel mehr Sinn "", sagt Shortt.  ""Schließlich gab es in der Steinzeit keine Shops, wo man sich einfach mal neue Waffen oder Ausrüstung kaufen konnte. In Far Cry Primal kommt man nur als versierter Jäger und Sammler zum Erfolg. "" 
Und dennoch, im Kern vertraut Ubisoft auf die bewährte Far-Cry-Formel: Auftraggeber schicken den Spieler kreuz und quer über die automatisch den Fortschritt mitzeichnende Spielkarte, er sammelt jede Menge Gegenstände und stellt neue her, tötet im Akkord und erobert feindliche Außenposten sowie Lagerfeuer. 
Allzu viel Realismus darf man von Far Cry Primal nicht erwarten. Takkar ist nämlich in der Lage, bis zu 17 Tiere zu zähmen und fortan als Begleiter zu nutzen. Dazu reicht es aus, der gewünschten Kreatur einen Fleischköder vorzusetzen, schon darf man sie auf Knopfdruck besänftigen. Während Wölfe oder Bären dem Spieler im Kampf zur Seite stehen, kann er eine Eule entsenden, um mit ihr ein Gebiet auszukundschaften, Feinde zu markieren und Luftangriffe durchzuführen. Selbst junge Mammuts oder Säbelzahntiger lassen sich als Reittier nutzen. 
Weiterhin hat Ubisoft ein Rollenspielsystem implementiert, das ebenfalls an die Vorgänger angelehnt ist. Für getötete Gegner und absolvierte Missionen erhält der Spieler Erfahrungspunkte, mit denen er die eigenen Fähigkeiten in Bereichen wie  ""Überleben "",  ""Bestienmeister "",  ""Jagd "",  ""Handwerk "" oder  ""Kampf "" kontinuierlich verbessert. So bekommt man etwa mehr Gesundheitspunkte oder darf sich auch in Bewegung heilen, erweitert sein Zähmungswissen oder kann seinen Standardbogen zum Exemplar mit Doppelpfeil ausbauen. 
Mitunter kommt man sich so vor, als würde man einen Open-World-Erlebnispark erkunden: Überall blinkt es auf der Übersichtskarte, ständig gibt es neue Nebenmissionen und Zusatzziele. Die vielen vertrauten Elemente sind durchaus gewollt, sagt Shortt.  ""Fans von Far Cry schätzen bestimmte Dinge an der Serie. Die wollten wir ihnen in Far Cry Primal auch wieder bieten. "" 
Das Gefühl, als schutzloser Steinzeitmensch zu agieren, vermittelt lediglich die Anfangsphase. Sobald man die ersten Tiere gezähmt hat und diese gegen die gegnerischen Kämpfer einsetzen kann, wird Far Cry Primal recht einfach. Es macht für erfahrene Spieler womöglich Sinn, eine der höheren von vier Schwierigkeitsstufen zu wählen. 
Was man seinen Machern in jedem Fall zugutehalten muss, ist die faszinierende Kulisse. Insbesondere die eigens für das Spiel erfundene Sprache Wenja beeindruckt: Das Linguisten-Ehepaar Brenna und Andrew Byrd von der Universität von Kentucky erarbeitete eine auf Proto-Indogermanisch basierende fiktive Steinzeitsprache mit je einem eigenen Dialekt für die drei in Far Cry Primal vorkommenden Stämme. 
 ""Darauf bin ich besonders stolz "", sagt Shortt.  ""Das war eine sehr riskante Entscheidung, die Spieler sind so etwas nicht gewöhnt. Doch diese eigene Sprache mit ihren kurzen Lauten hat massiv die Art und Weise beeinflusst, wie wir die Geschichte von Far Cry Primal erzählen. "" 
Es klingt anfangs sehr ungewohnt, wenn die virtuellen Steinzeitmenschen plaudern, aber der raue Slang passt zur Atmosphäre des Spiels. Einen positiven Nebeneffekt hat Wenja ebenfalls: Der Spieler wird förmlich gezwungen, die deutschen Untertitel zu lesen – und bekommt dadurch mehr von der Story mit. 
Darüber hinaus trägt das Tal von Oros, das in den heutigen Karpaten in Osteuropa angesiedelt ist, mit seiner abwechslungsreichen und technisch einwandfreien Gestaltung dazu bei, dass man trotz repetitiver Jagd- und Sammelaufgaben gerne weiterspielt. Wiesen, Wälder, Gebirge, Steppen, Flüsse und Seen laden zu ausgiebigen Entdeckungstouren ein, und auch grafische Effekte wie einfallende Sonnenstrahlen oder die Darstellung während der Nacht überzeugen. Die Liebe zum Detail ist auch zu spüren, wenn man etwa kunstvolle Wandzeichnungen in Höhlen entdeckt. 
Damit die offene Spielwelt von Far Cry Primal wie im Jahr 10.000 vor Christus aussieht, habe man intensive Recherchen durchgeführt und mit zahlreichen Historikern, Anthropologen und Spezialisten zusammengearbeitet, sagt Shortt.  ""Dr. Luc Doyon von der Universität von Montreal etwa hat uns unglaublich viele Informationen zum Steinzeitmenschen geliefert. Und mit Terry Notary hatten wir einen erfahrenen Stunt-Koordinator und Choreografen zur Seite, der unseren Grafikdesignern zeigte, wie sie die Bewegungsabläufe der damals lebenden Menschen umsetzen können. "" 
Man sieht Far Cry Primal in der Tat an, dass die Entwickler um historische Akkuratesse bemüht waren. Dennoch hätte man sich auch einige künstlerische Freiheiten herausgenommen, sagt Shortt.  ""Es ist eben immer noch ein Actionspiel. "" Das resultiert dann im Spiel beispielsweise in Brandpfeilen – die in Wirklichkeit erst einige Jahrtausende später erfunden wurden – oder in Fantasiewaffen wie  ""Tobsuchtbomben "", die Feinde gegeneinander aufhetzen. 
Was bleibt, ist ein unterhaltsamer, aber doch konservativer Ausflug in die Steinzeit.  ""Darsa! "", will man den Entwicklern da zurufen. Das bedeutet in der Steinzeitsprache von Far Cry Primal so viel wie  ""etwas wagen, mutig sein ""."	technik
"Seit 1989 hat Mazda fast eine Million Exemplare des MX-5 verkauft – damit darf sich das Modell der erfolgreichste Roadster der Welt nennen. Auch deshalb, weil er anders als einige Wettbewerber immer noch da ist. Mehr noch: Der Mazda MX-5 kommt im September in einer völlig neuen Auflage nach Deutschland, es ist die inzwischen vierte Generation des Zweisitzers. 
Bei jeder Neuauflage stellt sich die Frage: Bleiben die Tugenden des MX-5 erhalten? Durchzugstarke Motoren, ein knackiges Fahrwerk, eine direkte Lenkung und kurze Schaltwege – so hat sich der Roadster schließlich eine große Fangemeinde erschlossen. Mazda war schlau genug, diese Grundpfeiler nicht anzurühren. Der MX-5 wurde da verändert, wo es notwendig war. 
Etwa beim Gewicht: In der dritten Generation hatte der Wagen leicht Fett angesetzt, nun steht er wieder schlank wie zu Beginn seiner Karriere da. Je nach Ausstattung wurden bis zu 100 Kilogramm eingespart, das Basismodell wiegt ohne Fahrer gerade mal 975 Kilogramm. Mit dem größeren Motor kommt der MX-5 exakt auf eine Tonne. 
Mazda bietet für den Roadster jetzt außerdem jene Dinge, die für ein modernes Auto heute unerlässlich sind. Je nach Ausstattungsniveau enthält der MX-5 Assistenten für den Spurwechsel, die Spurhaltung, den toten Winkel oder zum Rückwärtsausparken. Es gibt eine Smartphone-Anbindung und erstmals ein Display, mit Navi ab Werk. Sogar LED-Scheinwerfer sind serienmäßig verbaut; LED-Tagfahrlicht ist aber erst ab der zweiten Stufe dabei. 
Das Wichtigste ist aber: Trotz all dieser Neuerungen ist der Zweisitzer im Kern der Alte geblieben. Dank der wiedergewonnenen Leichtigkeit wieselt er sogar noch etwas souveräner durch die Kurven als zuvor. Die sind nach wie vor seine Domäne, hier krallt sich der Mazda geradezu in den Asphalt. Wobei es nicht unbedingt darum geht, im MX-5 rasend schnell um die Ecke zu kommen. Eher animiert er dazu, sich auf kommende Kurven vorzufreuen und sie dann in einer Ideallinie nehmen zu wollen. Und sobald die Biegung absolviert ist, freut man sich schon auf die nächste. 
Bei den Motoren hält Mazda die Auswahl weiter klein. Der Kunde muss sich nur entscheiden zwischen einem 1,5-Liter-Ottomotor mit 96 kW (131 PS) und einem zwei Liter großen Benziner, der 118 kW (160 PS) leistet. Beide wirken mit ihren Drehmomenten von maximal 150 beziehungsweise 200 Nm auf dem Papier nicht gerade atemberaubend. Doch schon die Fahrleistungen sprechen eine andere Sprache. 204 und 214 km/h Höchsttempo sind durchaus in Ordnung, wichtiger ist die flotte Anfahrt: Nach 8,3 Sekunden ist mit dem kleineren Motor Tempo 100 erreicht, der größere Motor schafft es in 7,3 Sekunden. Der Kraftstoffverbrauch haut einen mit keinem der beiden vom Hocker. 
 
Der 1,5-Liter-Benziner ist eigentlich schon ausreichend, er benötigt allerdings höhere Drehzahlen, um Fahrspaß zu erzeugen. Da gibt sich der größere souveräner. Mit ihm macht das schaltfaule Cruisen durch die Stadt mehr Spaß. Die Preisdifferenz zwischen den beiden Motorisierungen liegt bei 1.500 Euro – allerdings gibt es den stärkeren Antrieb erst ab der dritten Ausstattungsstufe zum Preis von mindestens 26.890 Euro. 
Wer auf die zusätzliche Ausstattung verzichten kann, erhält für 22.990 Euro den kleineren Motor mit einer gar nicht mal so kargen Basisausrüstung: Neben den LED-Scheinwerfern sind eine Klimaanlage, elektrische Fensterheber, Zentralverriegelung, Leichtmetallfelgen und ein Audio-System schon dabei. 
Das Design des MX-5 wurde über die ersten drei Generationen eher behutsam weiterentwickelt. Jetzt hat Mazda den Roadster wie alle seine neuen Modelle nach den Vorgaben des Kodo-Designs entwickelt. Im Vergleich mit dem Vorgänger und dessen sehr klassischer Linienführung wirkt der neue MX-5 mit seinen 3,92 Metern Länge geradezu expressiv und deutlich maskuliner. Auffällig sind die kurzen Überhänge. Die Motorhaube fällt vorne stark nach unten ab, was den zur Verfügung stehenden Raum weiter einschränkt. Das ist vielleicht der Grund, warum der MX-5 schon in der Basisausstattung aus schmalen Scheinwerfern LED-Licht wirft. 
Zum großen Erfolg der ersten drei Generationen MX-5 hat sicher auch das einfach zu bedienende Stoffverdeck beigetragen. Das bleibt auch in der vierten Generation so. Geübte Fahrer schaffen es, das Verdeck über die mittige Verriegelung in einer Bewegung zu öffnen und nach hinten zu werfen. Dort rastet es nach sanftem Druck ein. Das Ganze muss nicht mehr als vier Sekunden dauern. 
Den MX-5 wird es wohl auch wieder mit versenkbarem festem Dach geben, wie schon in der letzten Generation. Diese Variante wurde in Deutschland immerhin von der Hälfte der Kunden gewählt. Zunächst ist der MX-5 aber nur mit hochwertigem Stoffdach zu haben. Das soll – so Mazdas Ziel – im nächsten Jahr hierzulande 3.000 Käufer überzeugen, immerhin zehn Prozent der globalen MX-5-Verkäufe. 
Technische Daten 
Motorbauart: Ottomotor, 1,5 Liter Hubraum Leistung: 96 kW (131 PS) Beschleunigung (0-100 km/h): 8,3 s Höchstgeschwindigkeit: 204 km/h Normverbrauch: 6,0 Liter je 100 km CO2-Emission: 139 g/km Abgasnorm: Euro 6 Preis: ab 22.990 Euro 
Motorbauart: Ottomotor, 2,0 Liter Hubraum Leistung: 118 kW (160 PS) Beschleunigung (0-100 km/h): 7,3 s Höchstgeschwindigkeit: 214 km/h Normverbrauch: 6,9 Liter je 100 km CO2-Emission: 161 g/km Abgasnorm: Euro 6 Preis: ab 26.890 Euro"	technik
"Wer Streamingdienste wie Netflix oder Amazon Video abonniert, hat vielleicht mit dem traditionellen Fernsehen nicht abgeschlossen, sucht aber zumindest nach Alternativen. Im Zeitalter der sozialen Medien möchten immer mehr Menschen preisgekrönte Serien wie House of Cards, Transparent oder Fargo sofort sehen und nicht warten, bis sie viel später vielleicht doch von einem Free-TV-Sender lizenziert werden. Der Erfolg der Dienste liegt darin begründet, dass sie eben Inhalte anbieten, die es im traditionellen Fernsehen nicht oder nur selten gibt. 
Netflix ist mittlerweile in 190 Ländern verfügbar und erfreut sich auch in Deutschland größerer Beliebtheit. Doch der Erfolg ruft offenbar die Regulierer der Europäischen Union auf den Plan. Wie die Financial Times berichtet, soll in der kommenden Woche von der EU-Kommission ein Vorschlag vorgestellt werden, der Diensten wie Netflix oder Amazon Auflagen bezüglich der Inhalte macht. Demnach müssten künftig 20 Prozent der gestreamten Filme und Serien auf den US-Plattformen europäischen Ursprungs sein. 
Das Portal EurActiv hat den Entwurf vorliegen. Es ist ein Update der 2010 vorgestellten Richtlinie über audiovisuelle Mediendienste (AVMSD). Diese macht europäischen Fernsehanstalten und  ""fernsehähnlichen Diensten "" gewisse Vorschriften, etwa was die Platzierung von Werbung, den Jugendschutz oder eben die Wahrung und Förderung europäischer Werke angeht. Video-on-Demand-Dienste (VoD) tauchten in der ursprünglichen Fassung nicht explizit auf. Nicht überraschend, schließlich war dieser Markt vor sechs Jahren noch überschaubar. Nun aber seien diese Techniken auf dem Vormarsch, heißt es in dem Vorschlag. Deshalb müsse die AVMSD angepasst werden. 
Der Vorschlag möchte auf allen Ebenen die gleichen Voraussetzungen für die Förderung europäischer Werke festlegen. Das bedeutet, On-Demand-Anbieter müssten mindestens 20 Prozent ihrer Bibliothek in EU-Ländern für  ""heimische "" Inhalte reservieren. Diese sollen zudem prominent im Angebot platziert werden. Das könnte folgenreich für Netflix sein, das seine Startseite individuell an die Gewohnheiten der Nutzer anpasst: Wer etwa keine deutschen Filme guckt, bekommt diese auch nicht prominent angezeigt. 
Netflix setzt in großen Märkten wie Deutschland, Frankreich oder Großbritannien bereits jetzt auf lokale Inhalte. In Deutschland zeigt der Dienst etwa Dokumentationen und Serien der öffentlich-rechtlichen Sender sowie bekannte deutschsprachige Filme. Wie hoch der Anteil im Vergleich zu internationalen Inhalten ist und ob eine Quote nicht vielleicht sogar schon erfüllt wird, ist zum jetzigen Zeitpunkt unklar. 
Neben einer Quote könnte die überarbeitete Richtlinie die Anbieter dazu verpflichten, in die Produktion europäischer Werke zu investieren – wenn es die jeweiligen Mitgliedsstaaten denn verlangen. Das könnte bedeuten, dass Netflix entweder einen Teil seines Gewinns in den jeweiligen Ländern in die lokale Film- und Fernsehproduktion steckt oder sich an Filmfonds und Förderungen beteiligt. Wie die Financial Times schreibt, investieren traditionelle Fernsehanstalten wie die BBC rund 20 Prozent ihres Umsatzes wieder in die Produktion neuer Inhalte, bei VoD-Diensten sei es lediglich ein Prozent. 
Zuletzt veröffentlichte Netflix mit Marseille die erste europäische Eigenproduktion. Auch in anderen Regionen, etwa in Südamerika, produziert Netflix für die lokalen Märkte. Es ist also nicht so, dass das Unternehmen bloß mit US-Inhalten den weltweiten Markt überschwemmt. Dennoch dürfte kein US-Anbieter über die Quotenregelung erfreut sein.  ""Starre Quoten ersticken den Markt "", zitiert die Financial Times einen Netflix-Sprecher. Die Europäische Union solle lieber neue Anreize für die Produktion heimischer Serien schaffen, anstatt Anbieter mit der Erfüllung bestimmter Quoten zu belasten. 
James Waterworth, Vertreter der Computer & Communications Industry Association (CCIA), deren Mitglied auch Netflix ist, weist im Gespräch mit EurActiv auf die Situation der Verbraucher hin:  ""Die Idee einer kulturellen Quote ist veraltet "", sagt Waterworth. Sie begünstige weder die Nutzer noch die Produzenten neuer Inhalte. 
Das Argument ist nachvollziehbar. Schon jetzt beklagen viele deutsche Netflix-Nutzer, dass Inhalte aus dem US-Angebot nicht in Deutschland verfügbar sind; die komplizierte Lizenzierung für einzelne Regionen scheint überholt für einen globalen Internetdienst. Dass Nutzer eben in vielen Fällen keine heimischen Filme, sondern exklusive US-Serien sehen möchten, scheint die EU-Kommission nicht zu stören. Ähnlich wie Google, Microsoft und Facebook könnten sich demnächst also auch Amazon und Netflix mitten in einer Regulierungsdebatte befinden."	technik
"Es war eine Reise in ein unbekanntes Land. In monatelanger Detailarbeit haben sich Experten des ADAC-Technikzentrums in die Elektronik zweier BMW-Modelle vorgearbeitet. Haben digitale Codes entschlüsselt und die Sprache übersetzt, mit der die Steuergeräte der Autos untereinander kommunizieren. Das Ziel dieser Odyssee durch das Datenlabyrinth: herauszufinden, welche Informationen über das Fahrverhalten und die Vorlieben der Autobesitzer die Mikrocomputer unterwegs speichern und was mit diesen Daten geschieht. 
Das Projekt stand unter der Regie des europäischen Automobilclub-Dachverbands FIA, der Ende vorigen Jahres eine große Kampagne über den Datenschutz in vernetzten Autos gestartet hat. Das Motto der Aktion:  ""Mein Auto – meine Daten "". 
Für die ADAC-Experten ist das Thema nicht neu. Schon Anfang 2015 hatten sie gravierende Sicherheitslücken im Bordnetz moderner BMW-Modelle entdeckt und herausgefunden, dass der drahtlose Datentransfer mit einfachen Mitteln abgehört werden kann. Autodiebe hätten deshalb leichtes Spiel gehabt und Hacker hätten persönliche Informationen über die BMW-Fahrer ausspähen können, warnte der ADAC seinerzeit und kündigte weitere Untersuchungen über die Datensicherheit und den Datenschutz in modernen Autos an. 
Diese Tests sind nun abgeschlossen. Ihre Ergebnisse stehen in einem 13-seitigen Bericht, der ZEIT ONLINE vorliegt. 
Viele der Daten, die von den Steuergeräten während der Fahrt aufgezeichnet werden, sind technischer Natur. Sie enthalten Angaben über die Funktionen und den Zustand der Autos, die möglicherweise für die BMW-Werkstätten von Interesse sind. Doch daneben gibt es offenbar auch andere Informationen, die der ADAC in seinem Bericht als  ""auffällige Daten "" bezeichnet. Sie sind nach Ansicht der Datenexperten geeignet, Erkenntnisse über die BMW-Kunden und ihre persönlichen Gewohnheiten auszukundschaften. Die Verwendung solcher Daten  ""sollte mit dem Hersteller diskutierten werden "", schreibt der ADAC.  ""Im Interesse des Verbraucherschutzes. "" 
Als  ""auffällig "" bezeichnet der Automobilclub, dass zum Beispiel an Bord des überprüften BMW 320d die Anzahl von Fahrtstrecken bestimmter Länge, die maximale Motordrehzahl je nach Kilometerstand, das vom Fahrer gewählte Programm des Automatikgetriebes sowie Auslösungen des Gurtstraffers  ""aufgrund starken Bremsens "" gespeichert werden. Warum der Autohersteller solche Angaben aufzeichnet, bleibt rätselhaft. Der ADAC ist der Meinung, dass Informationen über die Speicherung der Streckenlängen oder die manuelle Bedienung des Automatikgetriebes geeignet sind, um Rückschlüsse auf den Fahrstil und das Nutzungsprofil zu ziehen. 
Außerdem fanden die Techniker des Automobilclubs heraus, dass  ""einige Steuergeräte Verschleißdaten, wie die Betriebsstunden der Fahrzeugbeleuchtung oder Nutzungsdaten wie die Anzahl der Sitzverstellungen "" erfassen. Solche Informationen würden automatisch an den Autohersteller geschickt, sobald das Auto in der Werkstatt an das BMW-Diagnosesystem angeschlossen werde. 
Noch schneller und noch direkter erfolgt laut ADAC die Übertragung der Dateninhalte aus den Fehlerspeichern der Autos. Sie würden, schreibt der Autoclub, über den BMW-Dienst Connected Drive online direkt an die Autofirma übertragen, um eine Ferndiagnose zu stellen. Nur: Dahinter könnte womöglich ebenfalls eine Art Überwachungsfunktion stecken, mit der Garantieansprüche der Autobesitzer abgewimmelt werden.  ""Je nach gespeichertem Fehler wird damit auch ein nicht ordnungsgemäßer Umgang des Fahrers mit dem Fahrzeug dokumentiert, zum Beispiel bei zu hoher Motortemperatur oder -drehzahl "", stellt der ADAC fest. 
Als die europaweite FIA-Kampagne über Datenschutz im Auto startete und der ADAC-Bericht erschien, tat man in der BMW-Zentrale zunächst ratlos. Man kenne die Studie nicht und könne deshalb auch nichts dazu sagen, teilte die Pressestelle des Autoherstellers mit und war danach erst auf mehrmaliges Nachfragen zu einigen Auskünften bereit. 
 
Immerhin: Die unerklärliche Datenspeicherung, die der ADAC in einem Modell des Typs 320d nachwies, hat offenbar Methode.  ""Das gilt für alle BMW-Modelle "", räumt BMW-Sprecher Cypselus von Frankenberg ein. Die Daten seien  ""ausschließlich technischer Natur "" und dienten  ""der Erkennung und Behebung von Fehlern und der Optimierung von Fahrzeugfunktionen "". 
Einerseits erklärt man in München, die Daten aus den Autos würden nicht an Dritte weitergegeben und  ""nach vollständiger Abwicklung einer Dienstanfrage wieder gelöscht "". Doch andererseits räumt der Autohersteller auch ein, dass beim Dienst Connected Drive Daten an  ""mit der Durchführung der Dienste beauftragte Serviceprovider "" übermittelt werden. Wer das ist und was bei diesen Auftragnehmern von BMW mit den Daten geschieht, war nicht zu erfahren. Eines betont die Pressestelle aber mit Nachdruck:  ""Bewegungsprofile über gefahrene Strecken können aus diesen Daten nicht erstellt werden. "" 
Wirklich? Die ADAC-Untersuchung spricht eine andere Sprache – zumindest wenn es um den BMW i3 geht, den die Fachleute des Automobilclubs ebenfalls analysiert haben. Das Elektroauto speichert demzufolge nicht nur die Positionsdaten der zuletzt benutzten Ladestationen, sondern auch  ""die rund 100 letzten Abstellpositionen des Fahrzeugs "". Damit weiß der Autohersteller ziemlich genau, wo sich seine Elektromodelle befinden und welche Strecken ihre Besitzer zurücklegen. 
Denn anders als im Typ 320d bleiben viele Daten beim i3 nicht im Fahrzeug gespeichert, sondern werden regelmäßig per Mobilfunk an den Autohersteller übertragen. BMW nennt diesen Vorgang Last State Call: Sobald das Auto abgeschaltet und verschlossen wird, sendet es automatisch ein umfangreiches Datenpaket an den BMW-Server, das neben Informationen über den technischen Zustand des Fahrzeugs, den gewählten Fahrmodus und die letzten Ladepositionen auch Rückschlüsse auf die Mobilitätsgewohnheiten des i3-Besitzers ermöglicht. 
So erfährt BMW per Last State Call beispielsweise, wann das Auto morgens gestartet wird, wie schnell es unterwegs war und wie viel Strom es verbraucht hat. Und weil die Bordelektronik laut ADAC auch die sogenannten  ""intermodalen Verbindungspunkte "" speichert, verrät der i3 offenbar auch, wo und wann der i3-Fahrer auf Bus oder Bahn umgestiegen ist. 
Warum BMW so viele Daten und Informationen über seine Kunden sammelt und wofür sie genutzt werden, sagt der Autobauer nicht. Man beachte die gesetzlichen Bestimmungen und habe mit jedem Kunden einen Vertrag über die Datennutzung geschlossen, betont die Pressestelle. Den Wortlaut dieser Vereinbarung wollte man aber trotz mehrmaliger Nachfrage von ZEIT ONLINE nicht preisgeben. 
Kein Wunder also, dass der ADAC vor allem ein Wort an die oberste Stelle seines Forderungskatalogs stellt:  ""Transparenz "". Der Autofahrerclub kritisiert die Geheimniskrämerei der Autohersteller beim Datenschutz und verlangt  ""für jedes Modell eine Auflistung aller im Fahrzeug erhobenen, verarbeiteten, gespeicherten und extern übermittelten Daten "". Dieser Forderung schließen sich inzwischen europaweit über 110 Automobilclubs an. 
Zwar hatte der Verband der Automobilindustrie (VDA) schon Ende 2014 seine  ""Datenschutzprinzipien für vernetzte Fahrzeuge "" aufgestellt und festgelegt, dass Autofahrer  ""darüber informiert werden, welche Daten zu welchem Zweck erhoben und genutzt werden "". Doch dabei handelt es sich offenbar eher um gute Vorsätze als um konkrete Richtlinien. Die Automobilclubs kritisieren, dass Autofahrer nach wie vor nicht über Art und Umfang der in den Fahrzeugen erhobenen Daten informiert werden und deshalb auch nicht  ""Herr ihrer Daten "" sind. 
Daran änderten auch individuelle Vereinbarungen mit den Kunden nichts, so der ADAC. Wer heute beim Autohändler einen Kauf- oder Leasingvertrag für einen neuen Wagen abschließt, habe meist keine Vorstellung darüber, was er mit seiner Unterschrift unter der zusätzlichen Datenklausel alles preisgibt – vor allem, wenn Autokäufer noch nicht einmal eine Kopie dieser Vereinbarung erhalten und auch nicht über ihr Widerspruchsrecht informiert werden.  ""Autofahrer müssen die Datenhoheit über ihre Pkws haben und selbst bestimmen können, ob und welche Daten sie den Herstellern zur Verfügung stellen "", fordert Johann Grill, oberster Verbraucherschützer beim ADAC. 
Noch deutlicher formuliert es der österreichische Automobilclub:  ""Daten sind die Währung unserer Zeit "", stellt der ÖAMTC fest und warnt vor  ""Geschäftsmethoden vieler Fahrzeughersteller "", die darauf ausgerichtet seien, die Kontrolle über die in den Autos gesammelten Daten zu haben. So wolle man  ""Wartung und Reparatur "" in die eigenen Vertragswerkstätten lenken, meint der ÖAMTC. Überdies könnten Daten aus vernetzten Autos an Dritte verkauft werden, die damit  ""gutes Geld verdienen "". Auch der AvD warnt, dass sich durch die Online-Vernetzung der Autos  ""ein neuer lukrativer Markt für Fahrzeugborddaten abzeichnet "". 
BMW ist mit Connected Drive zweifellos der Vorreiter auf dem Gebiet der Online-Vernetzung, doch ein Einzelfall ist die Firma nach Ansicht des ADACs keineswegs. Die Clubs wollten deshalb auch nicht nur BMW kritisieren, betont die ADAC-Sprecherin Katja Legner. Ähnliche rätselhafte Datenspeicherungen und -übertragungen vermute man auch bei online vernetzten Modellen anderer Hersteller und werde die Tests deshalb weiter fortsetzen. 
Man sollte misstrauisch bleiben, meint auch Bundesjustizminister Heiko Maas:  ""Vielleicht kann das Auto der Zukunft irgendwann selbständig fahren, aber bei der Datenübermittlung muss auch künftig allein der Fahrer am Steuer sitzen. "" Eingedenk dieser Erkenntnis muss sich die Politik aber auch der Frage stellen, wie sie verhindern will, dass vernetzte Autos zu  ""Datenkraken "" werden. 95 Prozent der in Europa befragten Autofahrer fordern gesetzliche Vorschriften, die den Datentransfer im vernetzten Auto regeln."	technik
"Wenn Klaus Bröcheler mit seinem Taxi durch München fährt, sieht er oft Zeigefinger auf sich gerichtet. Wartet er auf Fahrgäste in der Innenstadt, bleiben Passanten stehen. Sie wollen nicht von ihm zum Flughafen gefahren werden. Sie wollen Fotos machen und Fahrberichte hören, denn Klaus Bröcheler fährt einen Tesla S. Seit zwei Jahren ist er mit dem Luxuswagen der kalifornischen Firma Tesla Motors in München unterwegs. Bei einer Tankstelle war er noch nie, der Strom kommt aus der Steckdose. 
Kein brummender Motor verrät, wenn der Tesla anfährt. Kein Auspuff bläst schädliche Emissionen in die Stadtluft. Das Design ist edel, der Innenraum wurde wohl von einem technikverliebten Minimalisten gestaltet. Da ein Elektroauto keine Gangschaltung braucht, ist ungewohnt viel Raum zwischen den beiden Vordersitzen. So ist die Sicht auf das übergroße Display in der Mittelkonsole frei. Neben der Batterieanzeige und der Navigation kann man Videos sehen oder im Internet surfen, das Datenvolumen dazu liefert Tesla mit. Im Tesla könnte man glatt vergessen, loszufahren – würde das Fahren mit ihm nicht einen solchen Spaß bereiten. 
Klaus Bröcheler schwärmt für seinen Wagen ohne Emissionen. Die überraschend kraftvolle Beschleunigung, das Dahingleiten im Straßenverkehr, die schnelle Reaktion auf jede Berührung des Steuers. Für manche Fahrer ist selbst die schon überflüssig geworden: Tesla wirbt mit seinem Autopiloten, der selbstständig Spuren wechselt, Abstand hält und einparken kann. Für einen Aufpreis kann das Auto auch nach Auslieferung per Software-Update übers WLAN noch lernen, autonom zu fahren. 
Wer kein Tesla-Taxi in der Nähe hat, kann sich in einem Tesla-Laden einen ersten Eindruck verschaffen. Das tun an einem Freitag Nachmittag am Kurfürstendamm in Berlin recht viele: Unwissende Neugierige, Touristen und Tesla-Begeisterte schlendern um ein strahlend rotes Fahrzeug. Auf den ersten Blick sieht es aus wie ein klassischer Oberklassewagen. Nur die daneben platzierte Zapfsäule – der Supercharger – weist darauf hin, dass es sich hier um ein Elektroauto handelt. Die erste Frage an die Mitarbeiter ist stets die nach der Reichweite. Ist das glänzende Spielzeug auch praxistauglich? Bis zu 500 Kilometer soll das Model S unter guten Bedingungen schaffen. 
Ein Herr schüttelt den Kopf und sagt:  ""Damit wäre die Fahrt in die Sommerferien nach Italien viel zu kompliziert und Ladesäulen gibt es auch noch zu wenige. "" Die Einschränkung seiner individuellen Mobilität schreckt ihn ab. Ganz anders geht es einem jungen Studenten, der sich über Tesla informiert hat und einer Freundin zeigen will, was sich alles in Sachen Elektromobilität tut. Er ist beeindruckt von der Reichweite, denn er weiß, dass Elektroautos anderer Hersteller nur einen Bruchteil der Strecke zurücklegen können. 
Für ihn spielt eine viel wichtigere Rolle, dass der Tesla S für die breite Masse mit einem Einstiegspreis von 80.000 Euro unbezahlbar ist. Seine Begeisterung für Tesla mindert das aber nicht. Ihn fasziniert die Kompromisslosigkeit, mit der Tesla in eine Zukunft mit Elektromobilität investiert. 
Geht es nach Tesla-Geschäftsführer Elon Musk, wird sein Elektroauto bald auch ein Fahrzeug für die Massen sein. Mit dem Pathos eines Weltretters stellte er am 31. März Teslas Model 3 vor, das mit seinem geplanten Einstiegspreis von 31.000 Euro eine viel größere Kundschaft ansprechen kann als die deutlich teureren Geschwistermodelle. Er begann seine Show mit Grafiken zum Anstieg der CO2-Emissionen und den katastrophalen Auswirkungen der Klimaerwärmung, um dann die vermeintliche Lösung zu präsentieren: das neue Model 3. Mit jedem gefahrenen Kilometer die Welt zu einem besseren Ort machen – so funktioniert Teslas Verkaufsstrategie. Aber reicht das für den Durchbruch? 
 
 ""Autos sind Persönlichkeitsmarkierer. Sie sind wie Kleider aus Blech, mit denen man seine Individualität zum Ausdruck bringen kann "", sagt Stephan Grünewald, Gesellschaftsanalytiker und Mitbegründer des rheingold Instituts.  ""Tesla-Fahrer markieren ihre Exklusivität, erhalten eine Gewissensabsolution, da sie sich für die Umwelt engagieren, und gleichzeitig färbt der Pioniergeist Musks ein Stück auf sie ab. Tesla ist erfolgreich, weil das Unternehmen auf der Klaviatur unserer Motivation spielt. "" 
Auf den ersten Blick scheint das gut zu funktionieren: Für das Model 3 gingen in den ersten Tagen weltweit mehr als 325.000 unverbindliche Vorbestellungen bei Tesla ein. Obwohl noch niemand in einem Tesla 3 gefahren ist. Ausgeliefert wird das Auto frühestens Ende 2017. Die große Frage ist, ob Tesla bis dahin in der Lage sein wird, so viele Autos herzustellen. Bis 2020 will die Firma ihre Produktionskapazitäten sogar auf 500.000 Autos im Jahr erhöhen. Im ersten Quartal 2016 schaffte sie aber noch nicht einmal die geplanten 16.000 Stück. Das schnelle Wachstum könnte Tesla Probleme bereiten, sagt Grünewald. 
Auch Holger Rust, Professor für Wirtschaftssoziologie an der Universität Hannover, ist skeptisch. Er sieht in Tesla eine unterhaltsame Utopie:  ""Im Augenblick ist es eine tolle Geschichte "", sagt er.  ""Aber es war schon immer ein Fehler im Marketing, von der kommunikativen Attraktivität eines Produktes auf seine Absatzchancen zu schließen. "" 
Noch sind Elektroautos in Deutschland ein Nischenphänomen. Im ersten Quartal dieses Jahres wurden nur 2.332 Elektrofahrzeuge zugelassen, das ist noch nicht einmal ein Prozent aller Neuwagen. Für Holger Rust liegt das daran, dass Elektroautos zu teuer und zu wenig bekannt seien.  ""Außerdem weiß man nicht, wie lange das Auto hält und wie funktional es ist. "" Grünewald sieht eher ein Motivationsproblem:  ""Pioniere wollen immer die Ersten sein. Mit einem Elektroauto sind sie oft die Letzten, die ankommen, weil ihre Reichweite nicht ausreicht. "" 
Dennoch setzt Tesla die traditionellen Autohersteller unter Druck. Um nicht zurückzufallen, müssten sie jetzt ihre eigenen E-Auto-Projekte schneller voranbringen, sagt Stephan Grünewald. Auch Holger Rust hofft auf Impulse für die gesamte Branche.  ""Tesla hat das Elektroauto wieder zum Gegenstand des öffentlichen Diskurses gemacht "", sagt er. 
Dem Unternehmen ist es gelungen, ein begehrenswertes E-Mobil zu bauen – der Chef des Taxifahrers Bröcheler hat gerade zwei neue Tesla 3 bestellt, weil das Geschäft damit gut läuft. Dass auch Tesla selbst mit dem Auto Geld verdienen kann, muss Elon Musk erst noch beweisen."	technik
"Das Stinker-Image abgelegt, das nervige Nageln abgedämmt und wegentwickelt: Noch vor wenigen Jahren hätte wohl jeder Experte dem Diesel eine große Zukunft prophezeit. Die Entwickler bei Audi bauten – einfach weil sie es konnten – einen Zwölfzylinder-Selbstzünder in den Q7 ein. Bei Peugeot und Citroën sollte er im Hybridverbund mit einem Elektromotor neue Effizienzgrenzen ausloten. Und die großen deutschen Hersteller wollten mit ihm das Benziner- und Hybridland USA erobern. 
Letzteres scheint nach den VW-Manipulationen vorerst gescheitert, der Zwölfzylinder-Q7 ist längst eingestellt und die Dieselhybride von PSA sind immer noch keine Bestseller. Damit nicht genug: Der Diesel leidet zunehmend unter dem Imageverlust im VW-Skandal. Und kommt nicht mehr aus der Kritik. 
Die langsame Abwärtsentwicklung setzte 2007 ein, als die EU-Verordnung zur Abgasnorm Euro 6 in Kraft trat. Sie machte das Stickoxidproblem des Diesels virulent. Bis Herbst 2015 mussten demnach alle neuen Selbstzünder den Ausstoß des gesundheitsschädlichen Gases NOx massiv reduziert haben: von 180 auf maximal noch 80 Milligramm pro Kilometer. Durchaus machbar, aber nur mit großem technischen und finanziellen Aufwand. 
Zu dieser Zeit ungefähr muss bei VW die folgenschwere Entscheidung gefallen sein, bei den Dieselemissionen zu manipulieren. Im September 2015 flog der Schwindel dann in den USA auf. Die Folgen für den Konzern sind noch immer nicht abzusehen: Volkswagen hat sich mit den US-Behörden bislang noch nicht auf eine Lösung für die betroffenen Autos in den USA geeinigt, und es liegen zahlreiche Klagen gegen den Hersteller vor. 
Doch auch die anderen Hersteller müssen sich zunehmend Sorgen machen, denn der Diesel kommt nicht zur Ruhe. In Deutschland sorgen vor allem die Vorstöße der Deutschen Umwelthilfe (DUH) für Aufregung. Die Organisation hatte in mehreren Tests deutlich erhöhte NOx-Emissionen außerhalb des genormten Laborzyklus bei verschiedenen Pkw festgestellt – und die Ergebnisse jeweils öffentlichkeitswirksam präsentiert. 
Fünf Modelle stehen seitdem am Pranger: der Renault Espace, der Fiat 500, der Smart Fortwo, der Opel Zafira und die C-Klasse von Mercedes-Benz. In den beiden letztgenannten Fällen hat die DUH beim Kraftfahrt-Bundesamt (KBA) den Entzug der Typerlaubnis für die betroffenen Motorvarianten beantragt und will dies notfalls vor Gericht durchsetzen. Gleichzeitig klagen die Umweltschützer gegen Daimler wegen irreführender Werbung für das betreffende Mercedes-Modell – die nämlich betont den  ""geringen Schadstoffausstoß "" und bezeichnet das Fahren als  ""reine "" Freude. Die DUH sieht darin eine Verbrauchertäuschung. Die Verhandlung soll frühestens im Sommer 2016 starten. 
Auch im Ausland sorgt der Diesel für Ärger. Frankreich, das lange Zeit wie Deutschland den Dieselboom vorantrieb, und wo der Dieselmotor einen noch höheren Anteil am Pkw-Bestand hat als in Deutschland, kündigte eine Kehrtwende an. Die Fokussierung auf den Diesel sei ein Fehler, sagte im Dezember 2015 Premierminister Manuel Valls. Die Pariser Bürgermeisterin Anne Hidalgo stellte ein Fahrverbot von Dieselautos in der Innenstadt in Aussicht. 
Auch vor Renault machte die Politik nicht halt. Nachdem das französische Umweltministerium Unregelmäßigkeiten bei den Dieselmotoren der Marke festgestellt hatte, gab es Anfang 2016 Durchsuchungen und weitläufige Ermittlungen bei Renault. Mittlerweile hat der Autohersteller einen Aktionsplan in Aussicht gestellt, mit dessen Hilfe die Emissionen auf ein legales Niveau gesenkt werden sollen. 
 
Daimler steht nicht nur in Deutschland unter Beschuss. In den USA reichten Besitzer von Bluetec-Dieselmodellen Anfang April eine Sammelklage wegen zu hohen Stickoxidausstoßes ein. Der Konzern beharrt dort jedoch wie in Deutschland darauf, keine unzulässige Technik eingebaut zu haben. 
Das ist einer der Kernpunkte des Streits. Grundsätzlich erlaubt die einschlägige Regelung ein automatisches Abschalten der Abgasreinigung unter bestimmten Bedingungen – etwa um den Motor in bestimmten Fahrsituationen vor Schäden zu schützen. Daimler erläuterte, die Abgasnachbehandlung werde  ""in Abhängigkeit vom jeweiligen Betriebszustand innerhalb des zulässigen Rahmens flexibel geregelt, um den Motorschutz und den sicheren Betrieb des Fahrzeugs zu gewährleisten "". 
Einige Hersteller schalten allerdings schon bei Temperaturen knapp unterhalb der 20-Grad-Grenze ab. Das ist die Temperatur, die bei den offiziellen Labormessungen als untere Norm gilt. Auch oberhalb von 35 Grad ist die Abgasreinigung offenbar häufig nicht mehr aktiv. Sauberes Abgas gäbe es bei solchen Autos dann höchstens in den wenigen Frühjahrsmonaten. 
Wie schwer es für die Hersteller sein dürfte, auffällig gewordene Fahrzeuge wieder sauber zu kriegen, zeigt das Beispiel Volkswagen. In Deutschland stockt die Rückrufaktion seit Wochen – offenbar gelingt es beim Mittelklassemodell Passat nicht, den Stickoxidausstoß zu reduzieren, ohne dass es zu einem Mehrverbrauch kommt. Das KBA zumindest verweigert nach wie vor die Freigabe für die Umrüstung. 
Zu allem Überfluss sorgt nun auch noch die Diskussion um eine blaue Umweltplakette für Verdruss unter Dieselkäufern und -fahrern. Schlimmstes denkbares Szenario für die Halter: Selbst vor kaum einem Jahr gekaufte Modelle könnten in einigen Innenstädten bald Fahrverbote erhalten. 
Ganz so schnell wird das Ende des Dieselmotors aber nicht kommen. Für die Autohersteller spielt der sparsame Antrieb eine große Rolle bei der Erreichung künftiger CO2-Obergrenzen. Sie werden deshalb um die Zukunft des Selbstzünders kämpfen. Zumindest so lange, bis es ausreichend Alternativen an Elektro-, Wasserstoff- oder Hybridmodelle gibt. 
Außerdem ist Diesel ein zwingendes Nebenprodukt bei der Benzinherstellung. Solange es an der Tankstelle Super gibt, wird also auch der Diesel irgendwo verbrannt werden müssen – auch in Dieselfahrzeugen."	technik
"Für seine selbst produzierte Transgender-Serie Transparent wird Amazon von Kritikern gefeiert und mit Preisen geehrt. Das Unternehmen selbst hat sich bisher alles andere als durchsichtig gezeigt. Nun ändert sich das, wenigstens ein bisschen: Amazon hat seinen ersten Transparenzbericht veröffentlicht. Er enthält Angaben darüber, wie oft Behörden aus den USA und anderen Ländern die Herausgabe von Kundendaten gefordert haben und wie oft das Unternehmen dem nachgekommen ist. Die Zahlen umfassen den Zeitraum zwischen dem ersten Januar und dem 31. Mai 2015. Künftig soll ein solcher Bericht zwei Mal jährlich erscheinen. 
Das Unternehmen aus Seattle unterscheidet zwischen subpoenas (von Behörden angeordnete Zwangsmaßnahmen ohne richterlichen Beschluss), Durchsuchungsbefehlen und anderen Gerichtsbeschlüssen, National Security Letters (NSL) inklusive den oft damit verbundenen Maulkorberlassen sowie Anfragen aus dem Ausland. 
Behörden können entweder non-content abfragen, wie Namen, Anschriften, E-Mail-Adressen, Kontodaten und Nutzungsdaten einschließlich  ""gewisser "" (aber nicht näher definierter) Informationen über Bestellungen bei Amazon. Oder sie können die Herausgaben von content-Daten verlangen. So nennt Amazon  ""Inhalte aus Dateien, die in einem Amazon-Konto gespeichert sind "". Gemeint sind offenbar die Inhalte persönlicher Cloudspeicher – einem zunehmend wichtigen Geschäftszweig des Unternehmens. Auf subpoenas reagiert Amazon nach eigenen Angaben wenn überhaupt, dann nur mit der Herausgabe von non-content-Daten. 
Die wichtigsten Zahlen im Überblick: 
813 subpoenas hat Amazon innerhalb der fünf Monate erhalten, in 542 Fällen hat das Unternehmen alle geforderten Daten herausgegeben, in 126 Fällen zumindest einen Teil. 25 Durchsuchungsbefehle gab es. 13 davon kam Amazon komplett nach, acht weiteren nur teilweise. 
Die genaue Anzahl der erhaltenen National Security Letters darf Amazon nicht nennen, gesetzlich erlaubt sind bisher nur 250er-Schritte. Im Transparenzbericht steht deshalb nur  ""0 – 249 "" als Angabe. 
132 Anfragen bekam Amazon von außerhalb der USA, davon kam Amazon 108 vollständig nach, sieben weiteren in Teilen. Aus welchen Ländern sie stammen, geht aus dem Bericht nicht hervor. Das ist eine der Schwächen des Berichts. Eine Anfrage von ZEIT ONLINE bei der deutschen Pressestelle, wie oft deutsche Behörden versucht haben, an Kundendaten zu gelangen, wurde bisher nicht beantwortet. 
Eine zweite Schwäche ist die fehlende Aufschlüsselung, wie oft Amazon Bestellinformationen und wie oft es Inhalte aus Cloudspeicherkonten herausgegeben hat. Letzteres wäre wichtig zu wissen für all jene, die Amazon als Alternative zu Dropbox und anderen Speicherdiensten oder auch in größerem Maßstab zum Beispiel für Firmendaten verwenden. 
Wie für die Nutzung aller Cloudspeicherdienste gilt auch hier: Wer seine Daten vor dem legalen oder auch illegalen Zugriff durch Dritte schützen will, sollte sie vor dem Hochladen verschlüsseln. 
Amazons Chief Information Security Officer Stephen Schmidt betonte in einem Blogpost, Amazon wehre sich – zum Teil erfolgreich – gegen zu allgemein gehaltene Zwangsmaßnahmen, unterrichte betroffene Kunden von einer Datenherausgabe, sofern es das darf, und habe nie am Prism-Programm der NSA teilgenommen. Prism verpflichtet Google, Apple, Microsoft, Yahoo und andere Unternehmen zur Herausgabe von Nutzerdaten an die NSA, sofern sie einen Beschluss des geheim tagenden Foreign Intelligence Surveillance Courts (Fisc) vorlegen kann. Das Programm soll dem Geheimdienst vor allem helfen, an Kommunikationsinhalte zu gelangen. 
Google war 2010 das erste Internetunternehmen, das einen solchen Bericht veröffentlicht hat, Twitter, Apple, Facebook, und viele andere folgten. Die deutschen E-Mail-Anbieter Posteo und mailbox.org sowie die Deutsche Telekom zogen 2014 nach, wobei die beiden letztgenannten auch schon Berichte für 2014 veröffentlicht haben. Ein neuer Bericht von Posteo soll in Kürze erscheinen."	technik
